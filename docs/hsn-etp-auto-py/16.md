# 十六、用 Boto3 自动化 AWS

在前几章中，我们探讨了如何使用 Python 自动化 OpenStack 和 VMware 私有云。我们将继续我们的云自动化之旅，自动化最流行的公共云之一：**亚马逊网络服务**（**AWS**）。在本章中，我们将探索如何使用 Python 脚本创建 Amazon**弹性计算云**（**EC2**）和 Amazon**简单存储系统**（**S3**）。

本章将介绍以下主题：

*   AWS Python 模块
*   管理 AWS 实例
*   自动化 AWS S3 服务

# AWS Python 模块

AmazonEC2 是一个可扩展的计算系统，用于为托管不同的虚拟机（如 OpenStack 生态系统中的 nova compute 项目）提供虚拟化层。它可以与其他服务（如 S3、Route 53 和 AMI）通信，以便实例化实例。基本上，您可以将 EC2 看作是在 virtual infrastructure manager（如 KVM 和 VMware）上设置的其他虚拟机监控程序之上的抽象层。EC2 将接收传入的 API 调用，然后将它们转换为每个虚拟机监控程序的合适调用。

**亚马逊机器镜像**（**AMI**是一个打包的镜像系统，包含启动虚拟机所需的操作系统和包（如 OpenStack 中的 Glance）。您可以从现有虚拟机创建自己的 AMI，并在需要在其他基础架构上复制这些机器时使用它，或者您可以从 internet 或 Amazon Marketplace 上的公开可用 AMI 中进行选择。我们需要从 AmazonWeb 控制台获取 AMI ID，并将其添加到 Python 脚本中。

AWS 设计了一个名为**Boto3**（[的 SDKhttps://github.com/boto/boto3](https://github.com/boto/boto3) ），允许 Python 开发人员使用脚本和软件来交互和使用不同服务的 API，如 Amazon EC2 和 Amazon S3。编写该库是为了提供对 Python 2.6.5、2.7+和 3.3 的本机支持。

Boto3 的主要功能在官方文件[中有描述 https://boto3.readthedocs.io/en/latest/guide/new.html](https://boto3.readthedocs.io/en/latest/guide/new.html) 和以下是一些重要特征：

*   **资源**：一个高级的、面向对象的接口。
*   **集合**：迭代和操作资源组的工具。
*   **客户端**：一个低级服务连接。
*   **分页器**：自动分页响应。
*   **等待者**：在达到某一状态或出现故障之前暂停执行的一种方式。每个 AWS 资源都有一个服务员名称，可以使用`<resource_name>.waiter_names`访问该名称。

# Boto3 安装

在连接到 AWS 之前，需要做以下几件事：

1.  首先，您需要一个 Amazon 管理员帐户，该帐户具有从基础架构中创建、修改和删除的权限。
2.  其次，安装用于与 AWS 交互的`boto3`Python 模块。您可以通过进入 AWS**身份和访问管理**（**IAM**控制台并添加新用户来创建一个专用于发送 API 请求的用户。您应该看到“访问类型”部分下提供的编程访问选项。

3.  现在，您需要分配一个允许跨 Amazon 服务（如 EC2 和 S3）进行完全访问的策略。通过单击“将现有策略附加到用户”并将 AmazonEC2FullAccess 和 AmazonS3FullAccess 策略附加到用户名来执行此操作。
4.  最后，单击 CreateUser 以添加配置了选项和策略的用户。

You can sign up for a free tier account on AWS, which will give you access to many services offered by Amazon for up to 12 months. Free access can be acquired at [https://aws.amazon.com/free/](https://aws.amazon.com/free/).

使用 Python 脚本管理 AWS 时，访问密钥 ID 用于发送 API 请求并从 API 服务器获取响应。我们不会使用用户名或密码发送请求，因为它们很容易被其他人捕获。此信息通过下载创建用户名后显示的文本文件获得。将此文件保存在安全的地方，并为其提供适当的 Linux 权限，以便打开和读取文件内容，这一点很重要。

另一种方法是在您的主用户目录下创建一个`.aws`目录，并在其下放置两个文件：`credentials`和`config`。第一个文件将同时具有访问密钥 ID 和机密访问 ID。

`~/.aws/credentials`显示如下：

```py
[default]
aws_access_key_id=AKIAIOSFODNN7EXAMPLE
aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
```

第二个文件将保存特定于用户的配置，例如托管创建的虚拟机的首选数据中心（区域）。（这类似于 OpenStack 中的 availability zone 选项。）在下面的示例中，我们指定要在`us-west-2`数据中心托管我们的机器。

配置文件`~/.aws/config`如下所示：

```py
[default]
region=us-west-2
```

现在，安装`boto3`需要使用常用的`pip`命令来获取最新的`boto3`版本：

```py
pip install boto3
```

![](img/00203.jpeg)

要验证模块是否已成功安装，请在 Python 控制台中导入`boto3`，您不应该看到报告的任何导入错误：

![](img/00204.jpeg)

# 管理 AWS 实例

现在，我们准备使用`boto3`创建我们的第一个虚拟机。正如我们所讨论的，我们需要 AMI，我们将从中实例化一个实例。将 AMI 看作一个 Python 类；创建实例将从中创建对象。我们将使用 AmazonLinuxAMI，这是一种由 Amazon 维护的特殊 Linux 操作系统，用于部署 Linux 机器，无需任何额外费用。您可以在[找到每个地区的完整 AMI IDhttps://aws.amazon.com/amazon-linux-ami/](https://aws.amazon.com/amazon-linux-ami/) ：

![](img/00205.jpeg)

```py
import boto3
ec2 = boto3.resource('ec2')
instance = ec2.create_instances(ImageId='ami-824c4ee2', MinCount=1, MaxCount=1, InstanceType='m5.xlarge',
                                Placement={'AvailabilityZone': 'us-west-2'},
                                )
print(instance[0])

```

在前面的示例中，以下内容适用：

1.  我们导入了之前安装的`boto3`模块。
2.  然后，我们指定了一个要与之交互的资源类型，即 EC2，并将其分配给`ec2`对象。

3.  现在，我们有资格使用`create_instance()`方法，并为其提供实例参数，如`ImageID`和`InstanceType`（类似 OpenStack 中的 flavor，它决定了实例在计算和内存方面的规格），我们应该在`AvailabilityZone`中的何处创建此实例。
4.  `MinCount`和`MaxCount`确定扩展实例时 EC2 可以走多远。例如，当其中一个实例出现高 CPU 时，EC2 将自动部署另一个实例，以共享负载并保持服务处于正常状态。
5.  最后，我们打印了将在下一个脚本中使用的实例 ID。

结果如下：

![](img/00206.jpeg)

You can check all valid Amazon EC2 instance types at the following link; please read them carefully, in order to not be overcharged from choosing the wrong type: [https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/)

# 实例终止

打印的 ID 在 CRUD 操作中用于稍后管理或终止实例。例如，我们可以使用前面创建的`ec2`资源也提供的`terminate()`方法终止实例：

```py
import boto3
ec2 = boto3.resource('ec2')
instance_id = "i-0a81k3ndl29175220"
instance = ec2.Instance(instance_id)
instance.terminate()

```

请注意，我们在前面的代码中硬编码了`instance_id`（当您需要创建可在不同环境中使用的动态 Python 脚本时，情况并不总是如此）。我们可以使用 Python 中可用的其他输入方法，例如`raw_input()`，从用户处获取输入或查询我们帐户中可用的实例，并让 Python 提示我们哪些实例需要终止。另一个用例是创建一个 Python 脚本，用于检查实例中的上次登录时间或资源消耗；如果它们超过特定值，我们将终止实例。这在实验室环境中非常有用，因为在实验室环境中，您不希望因使用恶意软件或设计不良的软件消耗额外的资源而受到指控。

# 自动化 AWS S3 服务

AWS**简单存储系统**（**S3**提供了安全且高度可扩展的对象存储服务。您可以使用此服务存储任意数量的数据，并从任何位置恢复数据。系统为您提供了版本控制选项，因此您可以回滚到文件的任何以前版本。此外，它还提供 RESTWeb 服务 API，因此您可以从外部应用程序访问它。

当数据到达 S3 时，S3 将为其创建一个`object`，这些对象将存储在`Buckets`中（把它们想象成文件夹）。您可以为每个创建的 bucket 提供复杂的用户权限，还可以控制其可见性（公共、共享或私有）。bucket 访问可以是策略，也可以是**访问控制列表**（**ACL**）。

bucket 还存储了元数据，元数据以键值对的形式描述对象，您可以通过 HTTP`POST`方法创建和设置。元数据可以包括对象的名称、大小和日期，或者您想要的任何其他自定义键值。用户帐户有 100 个 bucket 的限制，但每个 bucket 中承载的对象的大小没有限制。

# 创建桶

当与 AWS S3 服务交互时，第一件合乎逻辑的事情是创建一个可用于存储文件的 bucket。在这种情况下，我们将向`boto3.resource()`提供`S3`。这将通知`boto3`启动初始化过程，并加载与 S3 API 系统交互所需的命令：

```py
import boto3
s3_resource = boto3.resource("s3")

bucket = s3_resource.create_bucket(Bucket="my_first_bucket", CreateBucketConfiguration={
    'LocationConstraint': 'us-west-2'})
print(bucket)
```

在前面的示例中，以下内容适用：

1.  我们导入了之前安装的`boto3`模块。
2.  然后，我们指定了要与之交互的资源类型，即`s3`，并将其分配给`s3_resource`对象。
3.  现在，我们可以在资源内部使用`create_bucket()`方法，并为其提供创建 bucket 所需的参数，例如`Bucket`，在这里我们可以指定其名称。请记住，bucket 名称必须是唯一的，以前不能使用过。第二个参数是`CreateBucketConfiguration`字典，我们在其中为创建的 bucket 设置数据中心位置。

# 将文件上载到 bucket

现在，我们需要使用创建的 bucket 并向其上载一个文件。记住，bucket 中的文件表示是一个对象。因此，`boto3`提供了一些包含对象作为其一部分的方法。我们将从使用`put_object()`开始。此方法将文件上载到创建的 bucket 并将其存储为对象：

```py
import boto3
s3_resource = boto3.resource("s3")
bucket = s3_resource.Bucket("my_first_bucket")

with open('~/test_file.txt', 'rb') as uploaded_data:
    bucket.put_object(Body=uploaded_data)

```

在前面的示例中，以下内容适用：

1.  我们导入了之前安装的`boto3`模块。
2.  然后，我们指定了要与之交互的资源类型，即`s3`，并将其分配给`s3_resource`对象。
3.  我们通过`Bucket()`方法访问`my_first_bucket`，并将返回值赋给 bucket 变量。
4.  然后，我们使用`with`子句打开了一个文件，并将其命名为`uploaded_data`。请注意，我们使用`rb`标志将文件作为二进制数据打开。
5.  最后，我们使用桶空间中提供的`put_object()`方法将二进制数据上传到我们的桶中。

# 删除存储桶

要完成铲斗的积垢操作，我们需要做的最后一件事是卸下铲斗。这是通过调用 bucket 变量上的`delete()`方法实现的，因为它已经存在，并且我们正在按名称引用它，与我们创建它并将数据上传到它的方式相同。但是，当铲斗不是空的时，`delete()`可能会失败。因此，我们将使用`bucket_objects.all().delete()`方法获取 bucket 中的所有对象，然后对其应用`delete()`操作，最后删除 bucket：

```py
import boto3
s3_resource = boto3.resource("s3")
bucket = s3_resource.Bucket("my_first_bucket")
bucket.objects.all().delete()
bucket.delete()
```

# 总结

在本章中，我们学习了如何安装 Amazon**弹性计算云**（**EC2**），并学习了 Boto3 及其安装。我们还学习了如何自动化 AWSS3 服务。

在下一章中，我们将学习 SCAPY 框架，它是一个强大的 Python 工具，用于构建和制作数据包，并通过网络发送数据包。