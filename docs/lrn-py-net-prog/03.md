# 三、起作用的 API

当我们谈论与 Python 相关的 API 时，我们通常指的是模块呈现给我们以供交互的类和函数。在本章中，我们将讨论一些不同的东西，即 web API。

web API 是一种通过 HTTP 协议进行交互的类型的 API。如今，许多 web 服务提供了一组 HTTP 调用，这些调用被设计为以编程方式由客户端使用，也就是说，它们是由机器而不是由人使用的。通过这些接口，可以自动化与服务的交互，并执行诸如提取数据、以某种方式配置服务以及将您自己的内容上载到服务中等任务。

在本章中，我们将了解：

*   web API 使用的两种流行数据交换格式：XML 和 JSON
*   如何与两个主要的 web API 交互：AmazonS3 和 Twitter
*   当 API 不可用时，如何从 HTML 页面提取数据
*   如何让提供这些 API 和网站的网站管理员的生活更轻松

有数百种服务提供 web API。在[上可以找到这些服务的一个相当全面且不断增长的列表 http://www.programmableweb.com](http://www.programmableweb.com) 。

我们将首先介绍如何在 Python 中使用 XML，然后我们将解释一个称为 AmazonS3API 的基于 XML 的 API。

# XML 入门

**可扩展标记语言**（**XML**是一种以标准文本格式表示分层数据的方式。在使用基于 XML 的 web API 时，我们将创建 XML 文档，并将其作为 HTTP 请求体发送，接收 XML 文档作为响应体。

这是 XML 文档的文本表示形式，可能表示奶酪店的库存：

```py
<?xml version='1.0'?>
<inventory>
    <cheese id="c01">
        <name>Caerphilly</name>
        <stock>0</stock>
    </cheese>
    <cheese id="c02">
        <name>Illchester</name>
        <stock>0</stock>
    </cheese>
</inventory>
```

如果您以前使用过 HTML 编码，那么这可能看起来很熟悉。XML 是一种基于标记的格式。它与 HTML 来自同一个语系。数据以元素构成的层次结构进行结构化。每个元素由两个标记表示，一个开始标记（例如，`<name>`）和一个匹配的结束标记（例如，`</name>`）。在这两个标记之间，我们可以放置数据，例如`Caerphilly`，或者添加更多表示子元素的标记。

与 HTML 不同，XML 的设计使我们能够定义自己的标记并创建自己的数据格式。此外，与 HTML 不同，XML 语法始终严格执行。在 HTML 中，允许出现一些小错误，例如标记以错误的顺序关闭、关闭标记完全丢失或属性值缺少引号，但在 XML 中，这些错误将导致完全不可读的 XML 文档。格式正确的 XML 文档称为格式良好的文档。

## XML API

使用 XML 数据有两种主要方法：

*   读入整个文档并创建其基于对象的表示，然后使用面向对象的 API 对其进行操作
*   从头到尾处理文档，并在遇到特定标记时执行操作

现在，我们将通过使用一个名为**ElementTree**的 Python XML API 来关注基于对象的方法。第二种所谓的拉式或基于事件的方法（通常也称为**SAX**，因为 SAX 是这一类中最流行的 API 之一）的设置更为复杂，仅用于处理大型 XML 文件。我们不需要它来与 AmazonS3 一起工作。

## 元素树的基础知识

我们将使用`xml.etree.ElementTree`模块中的`ElementTree`API 的 Python 标准库实现。

让我们看看如何使用`ElementTree`创建上述示例 XML 文档。打开 Python 解释器并运行以下命令：

```py
>>> import xml.etree.ElementTree as ET
>>> root = ET.Element('inventory')
>>> ET.dump(root)
<inventory />

```

我们首先创建根元素，即文档的最外层元素。我们在这里创建一个根元素`<inventory>`，然后将其字符串表示形式打印到屏幕上。`<inventory />`表示是`<inventory></inventory>`的 XML 快捷方式。它用于显示空元素，即没有数据和子标记的元素。

我们通过创建一个新的`ElementTree.Element`对象来创建`<inventory>`元素。您会注意到，我们给`Element()`的参数是所创建标记的名称。

我们的`<inventory>`元素目前是空的，所以让我们在其中放入一些东西。这样做：

```py
>>> cheese = ET.Element('cheese')
>>> root.append(cheese)
>>> ET.dump(root)
<inventory><cheese /></inventory>

```

现在，我们在`<inventory>`元素中有一个名为`<cheese>`的元素。当一个元素直接嵌套在另一个元素中时，嵌套的元素称为外部元素的**子元素**，外部元素称为**父元素**。类似地，处于同一级别的元素被称为**兄弟**。

让我们添加另一个元素，这次让我们给它一些内容。添加以下命令：

```py
>>> name = ET.SubElement(cheese, 'name')
>>> name.text = 'Caerphilly'
>>> ET.dump(root)
<inventory><cheese><name>Caerphilly</name></cheese></inventory>

```

现在，我们的文件开始成形。我们在这里做了两件新事情：首先，我们使用快捷类方法`ElementTree.SubElement()`创建新的`<name>`元素，并在单个操作中将其作为`<cheese>`的子元素插入到树中。其次，我们通过给元素的`text`属性分配一些文本来为它提供一些内容。

我们可以在父元素上使用`remove()`方法删除元素，如下命令所示：

```py
>>> temp = ET.SubElement(root, 'temp')
>>> ET.dump(root)
<inventory><cheese><name>Caerphilly</name></cheese><temp /></inventory>
>>> root.remove(temp)
>>> ET.dump(root)
<inventory><cheese><name>Caerphilly</name></cheese></inventory>

```

### 漂亮的印刷品

我们可以以更清晰的格式生成输出，如本节开头所示的示例，这将非常有用。ElementTree API 没有实现这一点的函数，但标准库提供的另一个 XML API`minidom`有，而且使用简单。首先，导入`minidom`：

```py
>>> import xml.dom.minidom as minidom

```

其次，使用以下命令打印一些格式良好的 XML：

```py
>>> print(minidom.parseString(ET.tostring(root)).toprettyxml())
<?xml version="1.0" ?>
<inventory>
 <cheese>
 <name>Caerphilly</name>
 </cheese>
</inventory>

```

乍一看，这些不是最简单的代码行，所以让我们把它们分解一下。`minidom`库不能直接处理 ElementTree 元素，因此我们使用 ElementTree 的`tostring()`函数来创建 XML 的字符串表示。我们使用`minidom.parseString()`将字符串加载到`minidom`API 中，然后使用`toprettyxml()`方法输出格式化的 XML。

这可以包装成一个函数，使它变得更加方便。在 Python shell 中输入如下所示的命令块：

```py
>>> def xml_pprint(element):
...     s = ET.tostring(element)
...     print(minidom.parseString(s).toprettyxml())

```

现在，只需执行以下操作即可打印：

```py
>>> xml_pprint(root)
<?xml version="1.0" ?>
<inventory>
 <cheese>
...

```

### 元素属性

在本节开头的示例中，您可能在`<cheese>`元素的开始标记中发现了一些东西，即`id="c01"`文本。这被称为一个**属性**。我们可以使用属性向元素附加额外的信息，并且元素可以拥有的属性数量没有限制。属性总是由一个属性名（在本例中为`id`）和一个值（在本例中为`c01`）组成。值可以是任何文本，但必须用引号括起来。

现在，将`id`属性添加到`<cheese>`元素中，如下所示：

```py
>>> cheese.attrib['id'] = 'c01'
>>> xml_pprint(cheese)
<?xml version="1.0" ?>
<cheese id="c01">
 <name>Caerphilly</name>
</cheese>

```

元素的`attrib`属性是一个类似 dict 的对象，它保存元素的属性名称和值。我们可以像处理常规的`dict`一样处理 XML 属性。

现在，您应该能够完全重新创建本节开头所示的示例文档。去试试吧。

### 转换为文本

一旦我们有了一个我们满意的 XML 树，我们通常希望将它转换成字符串，通过网络发送。我们一直使用的`ET.dump()`函数不适用于此。`dump()`功能所做的就是将标签打印到屏幕上。它不会返回我们可以使用的字符串。我们需要使用`ET.tostring()`函数，如下命令所示：

```py
>>> text = ET.tostring(name)
>>> print(text)
b'<name>Caerphilly</name>'

```

请注意，它返回一个 bytes 对象。它为我们编码了字符串。默认字符集为`us-ascii`，但最好使用 UTF-8 通过 HTTP 进行传输，因为它可以对所有 Unicode 字符进行编码，并且它受到 web 应用的广泛支持。

```py
>>> text = ET.tostring(name, encoding='utf-8')

```

现在，关于创建 XML 文档，我们只需要知道这些，所以让我们看看如何将其应用到 web API。

# 亚马逊 S3API

AmazonS3 是一种数据存储服务。它支撑着当今许多备受瞩目的 web 服务。尽管它提供了企业级的恢复能力、性能和功能，但一开始就非常容易。它价格合理，并且为自动访问提供了一个简单的 API。它是成长中的**亚马逊网络服务**（**AWS**组合中的众多云服务之一。

API 时不时地变化，通常会给它们一个版本号，以便我们可以跟踪它们。我们将使用 S3RESTAPI 的当前版本“2006-03-01”。

您会注意到，在 S3 文档和其他地方，S3WebAPI 被称为**RESTAPI**。**REST**代表**代表性状态转移**，这是一个关于 HTTP 如何应用于 API 的相当学术的概念，最初由 Roy Fielding 在他的博士论文中提出。虽然 API 应该拥有的属性被认为是 RESTful 的，但实际上几乎所有基于 HTTP 的 API 现在都贴上了 RESTful 标签。S3API 实际上是最具 RESTful 特性的 API 之一，因为它恰当地使用了一系列 HTTP 方法。

### 注

如果你想了解更多关于这个主题的内容，可以在这里找到罗伊·菲尔丁的论文[http://ics.uci.edu/~fielding/pubs/deposition](http://ics.uci.edu/~fielding/pubs/dissertation)是推广这一概念的原著之一，也是*Leonard Richardson*和*Sam Ruby*的*RESTful Web 服务*的一本好书，现可在此网页[免费下载 http://restfulwebapis.org/rws.html](http://restfulwebapis.org/rws.html) 。

## 在 AWS 注册

在我们能够访问 S3 之前，我们需要向 AWS 注册。API 在允许访问其功能之前通常需要注册。您可以使用现有的亚马逊账户，也可以在[创建新账户 http://www.amazonaws.com](http://www.amazonaws.com) 。虽然 S3 最终是一项付费服务，但如果您是第一次使用 AWS，那么您将获得一年的免费试用期，供低容量使用。一年的时间足够完成这一章了！该试用版提供 5GB 的免费 S3 存储空间。

## 认证

接下来，我们需要讨论身份验证，这是使用许多 web API 时讨论的一个重要话题。我们使用的大多数 web API 都会指定提供身份验证凭据的方式，以允许向其发出请求，通常我们发出的每个 HTTP 请求都必须包含身份验证信息。

API 需要此信息的原因如下：

*   以确保其他人不会滥用您的应用的访问权限
*   适用每种应用的速率限制
*   管理访问权限的委派，以便应用可以代表服务或其他服务的其他用户进行操作
*   收集使用统计数字

所有 AWS 服务都使用 HTTP 请求签名机制进行身份验证。为了对请求进行签名，我们使用加密密钥对 HTTP 请求中的唯一数据进行散列和签名，然后将签名作为头添加到请求中。通过在服务器上重新创建签名，AWS 可以确保请求已由我们发送，并且在传输过程中不会被更改。

AWS 签名生成流程目前处于第四版，需要进行详细讨论，因此我们将采用第三方库，即`requests-aws4auth`。这是`Requests`模块的配套库，可为我们自动处理签名生成。在 PyPi 有售。因此，在`pip`的帮助下，将其安装在命令行上：

```py
$ pip install requests-aws4auth
Downloading/unpacking requests-aws4auth
...

```

### 设置 AWS 用户

要使用身份验证，我们需要获取一些凭证。

我们将通过 AWS 控制台设置此。在 AWS 注册后，登录控制台[https://console.aws.amazon.com](https://console.aws.amazon.com) 。

登录后，您需要执行以下步骤：

1.  点击右上角的姓名，然后选择**安全凭证**。
2.  点击屏幕左侧列表中的**用户**，然后点击顶部的**新建用户**按钮。
3.  输入**用户名**，并确认**为每个用户**生成一个访问密钥已选中，然后点击右下角的**创建**按钮。

您将看到一个新页面，说明用户已成功创建。点击右下角的**下载凭证**按钮，下载一个 CSV 文件，其中包含该用户的**访问 ID**和**访问秘密**。这些都很重要，因为它们将帮助我们向 S3API 进行身份验证。确保安全地存储它们，因为它们将允许对 S3 文件的完全访问。

然后，点击屏幕底部的**关闭**，点击将出现的列表中的新用户，然后点击**附加策略**按钮。将显示策略模板的列表。向下滚动此列表并选择**AmazonS3FullAccess**策略，如以下屏幕截图所示：

![Setting up an AWS user](img/6008OS_03_01.jpg)

最后，当出现时，点击右下方的**附加策略**按钮。现在，我们的用户可以完全访问 S3 服务。

## 区域

AWS 在世界各地都有数据中心，所以当我们在 AWS 中激活一项服务时，我们会选择我们想要它居住的地区。[上有 S3 的区域列表 http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region](http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) 。

最好选择一个距离将使用该服务的用户最近的区域。现在，您将是唯一的用户，因此只需为我们的第一个 S3 测试确定离您最近的区域。

## S3 桶和物体

S3 使用两个概念组织我们存储在其中的数据：bucket 和 objects。一个对象相当于一个文件，即一个有名称的数据块，一个 bucket 相当于一个目录。bucket 和目录之间的唯一区别是 bucket 不能包含其他 bucket。

每个 bucket 都有其自己的 URL 格式：

`http://<bucketname>.s3-<region>.amazonaws.com`。

在 URL 中，`<bucketname>`是 bucket 的名称，`<region>`是 bucket 所在的 AWS 区域，例如`eu-west-1`。bucket 名称和区域是在创建 bucket 时设置的。

Bucket 名称在所有 S3 用户之间是全局共享的，因此它们必须是唯一的。如果您拥有一个域，那么该域的子域将生成一个合适的 bucket 名称。您也可以使用您的电子邮件地址，将`@`符号替换为连字符或下划线。

对象是在我们首次上载时命名的。我们通过将对象名称作为路径添加到 bucket URL 的末尾来访问对象。例如，如果我们在包含对象`cheeseshop.txt`的`eu-west-1`区域中有一个名为`mybucket.example.com`的 bucket，那么我们可以使用 URL[访问它 http://mybucket.example.com.s3-eu-west-1.amazonaws.com/cheeseshop.txt](http://mybucket.example.com.s3-eu-west-1.amazonaws.com/cheeseshop.txt) 。

让我们通过 AWS 控制台创建第一个 bucket。我们可以通过此 web 界面手动执行 API 公开的大多数操作，这是检查 API 客户端是否正在执行所需任务的一种好方法：

1.  在[登录控制台 https://console.aws.amazon.com](https://console.aws.amazon.com) 。
2.  转到 S3 服务。您将看到一个页面，提示您创建一个 bucket。
3.  点击**创建桶**按钮。
4.  输入一个 bucket 名称，选择一个区域，然后点击**创建**。
5.  你将被带到桶列表，你将能够看到你的桶。

## S3 命令行客户端

好了，准备够了，让我们开始编码吧。关于 S3 的部分的其余部分，我们将编写一个小型命令行客户机，使我们能够与服务交互。我们将创建 bucket，然后上传和下载文件。

首先，我们将设置命令行解释器并初始化身份验证。创建一个名为`s3_client.py`的文件，并在其中保存以下代码块：

```py
import sys
import requests
import requests_aws4auth as aws4auth
import xml.etree.ElementTree as ET
import xml.dom.minidom as minidom

access_id = '<ACCESS ID>'
access_key = '<ACCESS KEY>'
region = '<REGION>'
endpoint = 's3-{}.amazonaws.com'.format(region)
auth = aws4auth.AWS4Auth(access_id, access_key, region, 's3')
ns = 'http://s3.amazonaws.com/doc/2006-03-01/'

def xml_pprint(xml_string):
    print(minidom.parseString(xml_string).toprettyxml())

def create_bucket(bucket):
    print('Bucket name: {}'.format(bucket))

if __name__ == '__main__':
    cmd, *args = sys.argv[1:]
    globals()[cmd](*args)
```

### 提示

**下载示例代码**

您可以下载您在[账户购买的所有 Packt 书籍的示例代码文件 http://www.packtpub.com](http://www.packtpub.com) 。如果您在其他地方购买了本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support) 并注册，将文件直接通过电子邮件发送给您。

您需要将`<ACCESS ID>`和`<ACCESS KEY>`替换为我们先前下载的凭证 CSV 中的值，并将`<REGION>`替换为您选择的 AWS 区域。

那么，我们在这里干什么？首先，我们设置端点。端点是用于访问 API 的 URL 的通用术语。有些 web API 只有一个端点，有些有多个端点，这取决于 API 的设计方式。我们在这里生成的端点实际上只是完整端点的一部分，我们将在使用 bucket 时使用它。我们的实际端点是以 bucket 名称为前缀的端点。

接下来，我们创建`auth`对象。我们将结合`Requests`将 AWS 身份验证添加到 API 请求中。

`ns`变量是一个字符串，我们需要它来处理 S3API 中的 XML。我们将在使用它时讨论这个问题。

我们加入了一个修改版的`xml_pprint()`函数，以帮助调试。现在，`create_bucket()`函数只是一个占位符。我们将在下一节中了解更多信息。

最后，我们有了命令解释器本身——它只需要在命令行上获取给定给脚本的第一个参数，并尝试运行具有相同名称的函数，将任何剩余的命令行参数传递给该函数。让我们对此进行一次测试。在命令提示中输入以下内容：

```py
$ python3.4 s3_client.py create_bucket mybucket
Bucket name: mybucket

```

您可以看到脚本从命令行参数中提取`create_bucket`，并调用函数`create_bucket()`，将`myBucket`作为参数传递。

此框架使添加功能以扩展客户能力成为一个简单的过程。让我们从让`create_bucket()`做一些有用的事情开始。

### 使用 API 创建一个 bucket

每当我们为 API 编写客户机时，我们的主要参考点就是 API 文档。文档告诉我们如何构造用于执行操作的 HTTP 请求。S3 文档可在[中找到 http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html](http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html) 。[http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html](http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html) URL 将提供 bucket 创建的详细信息。

该文档告诉我们，要创建一个 bucket，我们需要使用 HTTP`PUT`方法向新 bucket 的端点发出 HTTP 请求。它还告诉我们，请求主体必须包含一些 XML，这些 XML 指定了我们希望在其中创建 bucket 的 AWS 区域。

现在我们知道了我们的目标，让我们来讨论一下我们的函数。首先，让我们创建 XML。将`create_bucket()`的内容替换为以下代码块：

```py
def create_bucket(bucket):
    XML = ET.Element('CreateBucketConfiguration')
    XML.attrib['xmlns'] = ns
    location = ET.SubElement(XML, 'LocationConstraint')
    location.text = auth.region
    data = ET.tostring(XML, encoding='utf-8')
    xml_pprint(data)
```

这里我们按照 S3 文档中给出的格式创建一个 XML 树。如果我们现在运行客户端，那么我们将看到如下所示的 XML：

```py
$ python3.4 s3_client.py create_bucket mybucket.example.com
<?xml version="1.0" ?>
<CreateBucketConfiguration >
 <LocationConstraint>eu-west-1</LocationConstraint>
</CreateBucketConfiguration>

```

这与文档中指定的格式相匹配。您可以看到，我们使用了`ns`变量来填充`xmlns`属性。这个属性会在 S3XML 中弹出，预先定义了`ns`变量可以更快地使用它。

现在，让我们添加代码来发出请求。将`create_bucket()`末尾的`xml_pprint(data)`替换为以下内容：

```py
    url = 'http://{}.{}'.format(bucket, endpoint)
    r = requests.put(url, data=data, auth=auth)
    if r.ok:
        print('Created bucket {} OK'.format(bucket))
    else:
        xml_pprint(r.text)
```

这里显示的第一行将根据我们的 bucket 名称和端点生成完整的 URL。第二行将向 S3API 发出请求。请注意，我们使用了`requests.put()`函数来使用 HTTP`PUT`方法发出此请求，而不是使用`requests.get()`方法或`requests.post()`方法。另外，请注意，我们已经为调用提供了`auth`对象。这将允许`Requests`为我们处理所有 S3 身份验证！

如果一切顺利，我们就打印一条消息。如果一切都没有按预期进行，我们将打印出响应正文。S3 在响应体中以 XML 形式返回错误消息。所以我们使用`xml_pprint()`函数来显示它。稍后，我们将在*处理错误*部分中介绍如何处理这些错误。

现在运行客户端，如果一切正常，那么我们将收到一条确认消息。确保您已经选择了一个尚未创建的桶：

```py
$ python3.4 s3_client.py create_bucket mybucket.example.com
Created bucket mybucket.example.com OK

```

当我们在浏览器中刷新 S3 控制台时，我们将看到已经创建了 bucket。

### 上传文件

现在我们已经创建了一个 bucket，我们可以上传一些文件了。编写上传文件的函数类似于创建 bucket。我们检查文档以了解如何构造 HTTP 请求，确定应该在命令行中收集哪些信息，然后编写函数。

我们需要再次使用 HTTP`PUT`。我们需要存储文件的 bucket 的名称，以及在 S3 中存储文件的名称。请求主体将包含文件数据。在命令行中，我们将收集 bucket 名称、我们希望文件在 S3 服务中具有的名称以及要上载的本地文件的名称。

在`create_bucket()`函数之后，将以下函数添加到您的`s3_client.py`文件中：

```py
def upload_file(bucket, s3_name, local_path):
    data = open(local_path, 'rb').read()
    url = 'http://{}.{}/{}'.format(bucket, endpoint, s3_name)
    r = requests.put(url, data=data, auth=auth)
if r.ok:
        print('Uploaded {} OK'.format(local_path))
    else:
        xml_pprint(r.text)
```

在创建此函数时，我们遵循与创建 bucket 类似的模式：

1.  准备将进入请求正文的数据。
2.  构建我们的 URL。
3.  提出请求。
4.  检查结果。

请注意，我们以二进制模式打开本地文件。该文件可以包含任何类型的数据，因此我们不希望应用文本转换。我们可以从任何地方提取这些数据，例如数据库或其他 web API。这里，为了简单起见，我们只使用一个本地文件。

URL 与我们在`create_bucket()`中构造的端点相同，并将 S3 对象名附加到 URL 路径。稍后，我们可以使用此 URL 检索对象。

现在，运行此处显示的命令上传文件：

```py
$ python3.4 s3_client.py mybucket.example.com test.jpg ~/test.jpg
Uploaded ~/test.jpg OK

```

您需要用自己的 bucket 名称替换`mybucket.example.com`。文件上传后，您将在 S3 控制台中看到它。

我使用了存储在我的主目录中的 JPEG 图像作为源文件。您可以使用任何文件，只需将最后一个参数更改为适当的路径。但是，使用 JPEG 图像将使以下部分更易于复制。

### 通过网络浏览器检索上传的文件

默认情况下，S3 对 bucket 和对象应用限制性权限。创建它们的帐户具有完全的读写权限，但其他任何人的访问都被完全拒绝。这意味着，我们刚刚上传的文件只有在下载请求包含我们帐户的身份验证时才能下载。如果我们在浏览器中尝试生成的 URL，则会出现访问被拒绝错误。如果我们试图使用 S3 与其他人共享文件，那么这不是很有用。

解决方案是使用 S3 的一种机制来更改权限。让我们看一下公开上传文件的简单任务。将`upload_file()`更改为以下内容：

```py
def upload_file(bucket, s3_name, local_path, acl='private'):
    data = open(local_path, 'rb').read()
    url = 'http://{}.{}/{}'.format(bucket, endpoint, s3_name)
    headers = {'x-amz-acl': acl}
    r = requests.put(url, data=data, headers=headers, auth=auth)
if r.ok:
        print('Uploaded {} OK'.format(local_path))
    else:
        xml_pprint(r.text)
```

我们现在在 HTTP 请求中包含了一个头文件`x-amz-acl`，它指定了要应用于对象的权限集。我们还向函数签名添加了一个新参数，以便在命令行上指定权限集。我们使用了 S3 提供的所谓的**罐装****ACL****罐装****访问控制列表**，记录在[中 http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl](http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl)。

我们感兴趣的 ACL 称为`public-read`。这将允许任何人下载文件，而无需任何身份验证。我们现在可以重新运行我们的上传，但这次它将对其应用此 ACL：

```py
$ python3.4 s3_client.py mybucket.example.com test.jpg ~/test.jpg public-read
Uploaded test.jpg OK

```

现在，在浏览器中访问文件的 S3URL 将为我们提供下载文件的选项。

### 在网络浏览器中显示上传的文件

如果你上传了一张图片，那么你可能想知道为什么浏览器要求我们保存它而不是仅仅显示它。原因是我们没有设置文件的`Content-Type`。

如果您还记得上一章的内容，HTTP 响应中的`Content-Type`头会告诉客户机，在本例中，它是我们的浏览器，即主体中的文件类型。默认情况下，S3 应用的内容类型为`binary/octet-stream`。由于这个`Content-Type`，浏览器无法判断它正在下载图像，所以它只是将其显示为可以保存的文件。我们可以通过在上传请求中提供一个`Content-Type`头来解决这个问题。S3 将存储我们指定的类型，并在后续下载响应中将其用作`Content-Type`。

将此处显示的代码块添加到`s3_client.py`开头的导入中：

```py
import mimetypes
```

然后将`upload_file()`更改为：

```py
def upload_file(bucket, s3_name, local_path, acl='private'):
    data = open(local_path, 'rb').read()
    url = 'http://{}.{}/{}'.format(bucket, endpoint, s3_name)
    headers = {'x-amz-acl': acl}
    mimetype = mimetypes.guess_type(local_path)[0]
    if mimetype:
        headers['Content-Type'] = mimetype
    r = requests.put(url, data=data, headers=headers, auth=auth)
if r.ok:
        print('Uploaded {} OK'.format(local_path))
    else:
        xml_pprint(r.text)
```

在这里，我们使用`mimetypes`模块通过查看`local_path`的文件扩展名来猜测合适的`Content-Type`。如果`mimetypes`无法从`local_path`中确定`Content-Type`，那么我们不包括`Content-Type`头，让 S3 应用默认的`binary/octet-stream`类型。

不幸的是，在 S3 中，我们无法通过使用简单的`PUT`请求覆盖现有对象的元数据。可以使用`PUT`复制请求来完成，但这超出了本章的范围。现在，最好在再次上传之前使用 AWS 控制台从 S3 中删除该文件。我们只需要做一次。现在，我们的代码将自动为我们上传的任何新文件添加`Content-Type`。

删除文件后，如上一节所示重新运行客户端，即使用新的`Content-Type`上传文件，然后再次尝试在浏览器中下载文件。如果一切顺利，将显示图像。

### 使用 API 下载文件

通过 S3API 下载文件与上载文件类似。我们只需再次获取 bucket 名称、S3 对象名称和本地文件名，但发出`GET`请求而不是`PUT request`，然后将接收到的数据写入磁盘。

在`upload_file()`函数下面，将以下函数添加到程序中：

```py
def download_file(bucket, s3_name, local_path):
    url = 'http://{}.{}/{}'.format(bucket, endpoint, s3_name)
    r = requests.get(url, auth=auth)
    if r.ok:
        open(local_path, 'wb').write(r.content)
        print('Downloaded {} OK'.format(s3_name))
    else:
        xml_pprint(r.text)
```

现在，运行客户端，使用以下命令下载您之前上传的文件：

```py
$ python3.4 s3_client.py download_file mybucket.example.com test.jpg ~/test_downloaded.jpg
Downloaded test.jpg OK

```

## 解析 XML 和处理错误

如果您在运行上述代码时遇到任何错误，那么您将注意到不会显示清晰的错误消息。S3 将错误消息嵌入到响应体中返回的 XML 中，到目前为止，我们刚刚将原始 XML 转储到屏幕上。我们可以对此进行改进，并从 XML 中提取文本。首先，让我们生成一条错误消息，以便查看 XML 的外观。在`s3_client.py`中，将您的访问密码替换为空字符串，如下所示：

```py
access_secret = ''
```

现在，尝试在服务上执行以下操作：

```py
$ python3.4 s3_client.py create_bucket failbucket.example.com
<?xml version="1.0" ?>
<Error>
 <Code>SignatureDoesNotMatch</Code>
 <Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message>
 <AWSAccessKeyId>AKIAJY5II3SZNHZ25SUA</AWSAccessKeyId>
 <StringToSign>AWS4-HMAC-SHA256...</StringToSign>
 <SignatureProvided>e43e2130...</SignatureProvided>
 <StringToSignBytes>41 57 53 34...</StringToSignBytes>
 <CanonicalRequest>PUT...</CanonicalRequest>
 <CanonicalRequestBytes>50 55 54...</CanonicalRequestBytes>
 <RequestId>86F25A39912FC628</RequestId>
 <HostId>kYIZnLclzIW6CmsGA....</HostId>
</Error>

```

前面的 XML 是 S3 错误信息。我截断了几个字段，以便在这里显示。您的代码块将略长于此。在这种情况下，它告诉我们它无法验证我们的请求，这是因为我们设置了一个空白的访问秘密。

### 解析 XML

打印所有 XML 对于错误消息来说太多了。有很多无关的信息对我们没有用处。如果我们能够拉出错误消息的有用部分并显示它们，那就更好了。

`ElementTree`为我们提供了一些从 XML 中提取此类信息的强大工具。我们将回到 XML，稍微探讨一下这些工具。

首先，我们需要打开一个交互式 Python shell，然后使用以下命令再次生成上述错误消息：

```py
>>> import requests
>>> import requests_aws4auth
>>> auth = requests_aws4auth.AWS4Auth('<ID>', '', 'eu-west-1', '')
>>> r = requests.get('http://s3.eu-west-1.amazonaws.com', auth=auth)

```

您需要将`<ID>`替换为您的 AWS 访问 ID。打印出`r.text`以确保收到一条错误消息，这与我们之前生成的错误消息类似。

现在，我们可以探索我们的 XML。将 XML 文本转换为`ElementTree`树。一个方便的功能是：

```py
>>> import xml.etree.ElementTree as ET
>>> root = ET.fromstring(r.text)

```

我们现在有了一个 ElementTree 实例，`root`作为根元素。

### 寻找元素

导航树的最简单方法是使用元素作为迭代器。尝试执行以下操作：

```py
>>> for element in root:
...     print('Tag: ' + element.tag)
Tag: Code
Tag: Message
Tag: AWSAccessKeyId
Tag: StringToSign
Tag: SignatureProvided
...

```

迭代`root`返回其每个子元素，然后使用`tag`属性打印出元素的标记。

我们可以使用以下命令对迭代的标记应用过滤器：

```py
>>> for element in root.findall('Message'):
...     print(element.tag + ': ' + element.text)
Message: The request signature we calculated does not match the signature you provided. Check your key and signing method.

```

这里，我们使用了`root`元素的`findall()`方法。此方法将为我们提供与指定标记匹配的`root`元素的所有直接子元素的列表，在本例中为`<Message>`。

这将解决我们提取错误消息文本的问题。现在，让我们更新错误处理。

### 处理错误

我们可以返回并将添加到我们的`s3_client.py`文件中，但是让我们在输出中包含更多的信息，并构造代码以允许重用。将以下函数添加到 `download_file()`函数下面的文件中：

```py
def handle_error(response):
    output = 'Status code: {}\n'.format(response.status_code)
    root = ET.fromstring(response.text)
    code =  root.find('Code').text
    output += 'Error code: {}\n'.format(code)
    message = root.find('Message').text
    output += 'Message: {}\n'.format(message)
    print(output)
```

您会注意到我们在这里使用了一个新函数，即`root.find()`。它的工作方式与`findall()`相同，只是它只返回第一个匹配元素，而不是所有匹配元素的列表。

然后，用`handle_error(r)`替换文件中`xml_pprint(r.text)`的每个实例，然后用不正确的访问密码再次运行客户端。现在，您将看到一条信息更丰富的错误消息：

```py
$ python3.4 s3_client.py create_bucket failbucket.example.com
Status code: 403
Error code: SignatureDoesNotMatch
Message: The request signature we calculated does not match the signature you provided. Check your key and signing method.

```

## 进一步增强

这就是我们要带客户去的地方。我们已经编写了一个命令行程序，可以执行基本的操作，例如在 AmazonS3 服务上创建 bucket、上传和下载对象。仍然有很多操作可以实现，这些可以在 S3 文档中找到；列出存储桶的内容、删除对象和复制对象等操作。

我们还可以改进一些其他方面，特别是如果我们要将其应用到生产应用中。命令行解析机制虽然紧凑，但从安全角度来看并不令人满意，因为任何有权访问命令行的人都可以运行任何内置 python 命令。最好有一个函数白名单，并通过使用一个标准库模块（如`argparse`）来实现适当的命令行解析器。

在源代码中存储访问 ID 和访问机密也是一个安全问题。发生了几起严重的安全事件，因为密码存储在源代码中，然后上传到云代码存储库。最好在运行时从外部源（如文件或数据库）加载密钥。

## Boto 包

我们已经讨论了直接使用 S3 REST API，这为我们提供了一些有用的技术，使我们能够在将来针对类似 API 编程。在许多情况下，这将是我们与 web API 交互的唯一方式。

但是，包括 AWS 在内的一些 API 具有现成的包，这些包公开了服务的功能，而不必处理 HTTP API 的复杂性。这些软件包通常使代码更干净、更简单，如果可以的话，它们应该是进行生产工作的首选。

AWS 包称为**Boto**。我们将快速查看`Boto`包，看看它如何提供我们之前编写的一些功能。

`boto`软件包在 PyPi 中提供，因此我们可以使用`pip`进行安装：

```py
$ pip install boto
Downloading/unpacking boto
...

```

现在，启动一个 Python shell，让我们尝试一下。我们需要先连接到服务：

```py
>>> import boto
>>> conn = boto.connect_s3('<ACCESS ID>', '<ACCESS SECRET>')

```

您需要将`<ACCESS ID>`和`<ACCESS SECRET>`替换为您的访问 ID 和访问机密。现在，让我们创建一个 bucket：

```py
>>> conn.create_bucket('mybucket.example.com')

```

这将在默认的标准美国地区创建 bucket。我们可以提供不同的地区，如下所示：

```py
>>> from boto.s3.connection import Location
>>> conn.create_bucket('mybucket.example.com', location=Location.EU)

```

此函数需要使用的区域名称与前面创建 bucket 时使用的区域名称不同。要查看可接受区域名称的列表，请执行以下操作：

```py
>>> [x for x in dir(Location) if x.isalnum()]
['APNortheast', 'APSoutheast', 'APSoutheast2', 'CNNorth1', 'DEFAULT', 'EU', 'SAEast', 'USWest', 'USWest2']

```

执行以下操作以显示我们拥有的存储桶列表：

```py
>>> buckets = conn.get_all_buckets()
>>> [b.name for b in buckets]
['mybucket.example.com', 'mybucket2.example.com']

```

我们还可以列出桶中的内容。为此，首先，我们需要获得对它的引用：

```py
>>> bucket = conn.get_bucket('mybucket.example.com')

```

然后列出内容：

```py
>>> [k.name for k in bucket.list()]
['cheesehop.txt', 'parrot.txt']

```

上传文件是一个简单的过程。首先，我们需要获取一个对我们想要放入的 bucket 的引用，然后我们需要创建一个`Key`对象，它将表示 bucket 中的对象：

```py
>>> bucket = conn.get_bucket('mybucket.example.com')
>>> from boto.s3.key import Key
>>> key = Key(bucket)

```

接下来，我们需要设置`Key`名称，然后上传我们的文件数据：

```py
>>> key.key = 'lumberjack_song.txt'
>>> key.set_contents_from_filename('~/lumberjack_song.txt')

```

`boto`包在上传这样的文件时会自动设置`Content-Type`，它使用的`mimetypes`模块与我们之前在确定类型时使用的模块相同。

下载也遵循类似的模式。请尝试以下命令：

```py
>>> bucket = conn.get_bucket('mybucket.example.com')
>>> key = bucket.get_key('parrot.txt')
>>> key.get_contents_to_filename('~/parrot.txt')

```

这将下载`mybucket.example.com`bucket 中的`parrot.txt`S3 对象，然后将其存储在`~/parrot.txt`本地文件中。

一旦我们有了对密钥的引用，只需使用以下设置 ACL：

```py
>>> key.set_acl('public-read')

```

我将让您在教程的帮助下进一步探索`boto`包的功能，可以在[上找到 https://boto.readthedocs.org/en/latest/s3_tut.html](https://boto.readthedocs.org/en/latest/s3_tut.html) 。

显然，对于 Python 中的日常 S3 工作，`boto`应该是您的 go-to 包。

## 以 S3 收尾

因此，我们讨论了 AmazonS3API 的一些用法，并学习了一些关于在 Python 中使用 XML 的知识。这些技能应该能让您在使用任何基于 XML 的 RESTAPI 时有一个良好的开端，无论它是否有一个预构建的库，如`boto`。

然而，XML 并不是 web API 使用的唯一数据格式，S3 处理 HTTP 的方式也不是 web API 使用的唯一模型。因此，我们将继续看一看今天使用的另一种主要数据格式，JSON 和另一种 API:Twitter。

# JSON

**JavaScript 对象表示法（JSON）**是一种标准方式，以文本字符串的形式表示简单对象，如`lists`和`dicts`。虽然 JSON 最初是为 JavaScript 开发的，但它是独立于语言的，大多数语言都可以使用它。它轻量级，但足够灵活，可以处理广泛的数据。这使得它非常适合通过 HTTP 交换数据，许多 web API 都使用它作为其主要数据格式。

## 编解码

我们使用`json`模块在 Python 中处理 JSON。让我们使用以下命令创建 Python 列表的 JSON 表示：

```py
>>> import json
>>> l = ['a', 'b', 'c']
>>> json.dumps(l)
'["a", "b", "c"]'

```

我们使用`json.dumps()`函数将对象转换为 JSON 字符串。在本例中，我们可以看到 JSON 字符串似乎与 Python 自己的列表表示形式相同，但请注意，这是一个字符串。通过执行以下操作确认这一点：

```py
>>> s = json.dumps(['a', 'b', 'c'])
>>> type(s)
<class 'str'>
>>> s[0]
'['

```

将 JSON 转换为 Python 对象也很简单，如下所示：

```py
>>> s = '["a", "b", "c"]'
>>> l = json.loads(s)
>>> l
['a', 'b', 'c']
>>> l[0]
'a'

```

我们使用`json.loads()`函数，只需向其传递一个 JSON 字符串。正如我们将看到的，这在与 web API 交互时非常强大。通常，我们会收到一个 JSON 字符串作为 HTTP 响应的主体，可以使用`json.loads()`对其进行解码，以提供立即可用的 Python 对象。

## 将 dicts 与 JSON 结合使用

JSON 本身支持映射类型的对象，相当于 Python`dict`。这意味着我们可以通过 JSON 直接使用`dicts`。

```py
>>> json.dumps({'A':'Arthur', 'B':'Brian', 'C':'Colonel'})
'{"A": "Arthur", "C": "Colonel", "B": "Brian"}'

```

此外，了解 JSON 如何处理嵌套对象也很有用。

```py
>>> d = {
...     'Chapman': ['King Arthur', 'Brian'],
...     'Cleese': ['Sir Lancelot', 'The Black Knight'],
...     'Idle': ['Sir Robin', 'Loretta'],
... }
>>> json.dumps(d)
'{"Chapman": ["King Arthur", "Brian"], "Idle": ["Sir Robin", "Loretta"], "Cleese": ["Sir Lancelot", "The Black Knight"]}'

```

不过只有一个问题：JSON 字典键只能是字符串形式。

```py
>>> json.dumps({1:10, 2:20, 3:30})
'{"1": 10, "2": 20, "3": 30}'

```

请注意，JSON 字典中的键如何成为整数的字符串表示形式？要解码使用数字键的 JSON 字典，如果要将它们作为数字使用，需要手动键入 convert。为此，请执行以下操作：

```py
>>> j = json.dumps({1:10, 2:20, 3:30})
>>> d_raw = json.loads(j)
>>> d_raw
{'1': 10, '2': 20, '3': 30}
>>> {int(key):val for key,val in d_raw.items()}
{1: 10, 2: 20, 3: 30}

```

我们只是使用字典理解来应用`int()`字典的键。

## 其他对象类型

JSON 只干净地处理 Python`lists`和`dicts`，对于其他对象类型`json`可能尝试将对象类型转换为其中一种，或者完全失败。尝试一个元组，如下所示：

```py
>>> json.dumps(('a', 'b', 'c'))
'["a", "b", "c"]'

```

JSON 没有元组数据类型，因此`json`模块将其转换为`list`。如果我们将其转换回：

```py
>>> j = json.dumps(('a', 'b', 'c'))
>>> json.loads(j)
['a', 'b', 'c']

```

它仍然是一个`list`。`json`模块不支持`sets`，因此需要将其重铸为`lists`。请尝试以下命令：

```py
>>> s = set(['a', 'b', 'c'])
>>> json.dumps(s)
...
TypeError: {'a', 'c', 'b'} is not JSON serializable
>>> json.dumps(list(s))
'["a", "b", "c"]'

```

这将导致与元组引起的问题类似的问题。如果我们将 JSON 转换回 Python 对象，那么它将是一个`list`而不是`set`。

我们几乎从未遇到过需要这些专业 Python 对象的 web API，如果我们需要，那么 API 应该提供某种处理它的约定。但是，如果我们以`lists`或`dicts`以外的任何格式本地存储数据，我们确实需要跟踪我们需要应用于传出或传入对象的任何转换。

现在我们已经了解了 JSON，让我们看看它在 web API 中是如何工作的。

# 推特 API

Twitter API 提供了对我们可能希望 Twitter 客户端执行的所有功能的访问。通过 Twitter API，我们可以创建客户端，搜索最近的推文，了解趋势，查找用户详细信息，跟踪用户的时间表，甚至通过发布推文和直接消息来代表用户。

我们将关注 Twitter API 版本 1.1，在撰写本章时的最新版本。

### 注

Twitter 为其 API 保存了全面的文档，可在[上找到 https://dev.twitter.com/overview/documentation](https://dev.twitter.com/overview/documentation) 。

## 推特世界时钟

为了说明推特 API 的一些功能，我们将为一个简单的推特世界时钟编写代码。我们的应用将定期轮询其 Twitter 帐户，以查找包含可识别城市名称的提及，如果找到，则将使用该城市的当前本地时间回复该推文。在 Twitter 中，提及是指任何一条 Tweet，其中包括我们的账户名，前缀为`@`，例如`@myaccount`。

## 推特认证

与 S3 类似，我们需要在开始之前确定如何管理身份验证。我们需要注册，然后我们需要了解 Twitter 希望我们如何验证我们的请求。

### 为 Twitter API 注册您的应用

我们需要创建一个 Twitter 帐户，根据该帐户注册我们的应用，然后我们将收到我们应用的身份验证凭据。设置第二个帐户也是一个好主意，我们可以使用它向应用帐户发送测试 tweet。这提供了一种更干净的方法来检查应用是否正常工作，而不是让应用帐户自己发送推文。您可以创建的 Twitter 帐户数量没有限制。

要创建帐户，请转到[http://www.twitter.com](http://www.twitter.com) 并完成注册流程。一旦您拥有 Twitter 帐户，请执行以下操作以注册您的应用：

1.  登录[http://apps.twitter.com](http://apps.twitter.com) 使用您的主 Twitter 帐户，然后创建一个新的应用。
2.  填写新的应用表单，注意 Twitter 应用名称需要在全球范围内唯一。
3.  转到应用的设置，然后将应用权限更改为具有读写权限。您可能需要注册手机号码才能启用此功能。即使你不满意提供这个，我们可以创建完整的应用；但是，发送 tweet 作为回复的最后一个功能将不会激活。

现在我们需要获取访问凭据，如下所示：

1.  转到**密钥和访问令牌**部分，然后记下**消费者密钥**和**访问密钥**。
2.  生成一个**访问令牌**。
3.  记下**接入令牌**和**接入密钥**。

### 验证请求

我们现在有足够的信息来验证请求。Twitter 使用了一个名为**oAuth 的身份验证标准**版本 1.0a。详见[中的部分 http://oauth.net/core/1.0a/](http://oauth.net/core/1.0a/) 。

oAuth 身份验证标准有点棘手，但幸运的是，`Requests`模块有一个名为`requests-oauthlib`的配套库，它可以为我们处理大部分复杂性。这在 PyPi 上可用，因此我们可以下载并使用`pip`安装它。

```py
$ pip install requests-oauthlib
Downloading/unpacking requests-oauthlib
...

```

现在，我们可以在请求中添加身份验证，然后编写应用。

### 推特客户端

将此处提到的代码保存到一个文件中，并将其另存为`twitter_worldclock.py`。您需要将`<CONSUMER_KEY>`、`<CONSUMER_SECRET>`、`<ACCESS_TOKEN>`和`<ACCESS_SECRET>`替换为您从上述 Twitter 应用配置中获取的值：

```py
import requests, requests_oauthlib, sys

consumer_key = '<CONSUMER_KEY>'
consumer_secret = '<CONSUMER_SECRET>'
access_token = '<ACCESS_TOKEN>'
access_secret = '<ACCESS_KEY>'

def init_auth():
    auth_obj = requests_oauthlib.OAuth1(
                    consumer_key, consumer_secret,
                    access_token, access_secret)

    if verify_credentials(auth_obj):
        print('Validated credentials OK')
        return auth_obj
    else:
        print('Credentials validation failed')
        sys.exit(1)	

def verify_credentials(auth_obj):
    url = 'https://api.twitter.com/1.1/' \
          'account/verify_credentials.json'
    response = requests.get(url, auth=auth_obj)
    return response.status_code == 200

if __name__ == '__main__':
    auth_obj = init_auth()
```

请记住，`consumer_secret`和`access_secret`充当您的 Twitter 帐户的密码，因此在生产应用中，它们应该从安全的外部位置加载，而不是硬编码到源代码中。

在上述代码中，我们使用访问凭证在`init_auth()`函数中创建`OAuth1`身份验证实例`auth_obj`。每当我们需要发出 HTTP 请求时，我们都会将其传递给`Requests`，并通过`Requests`处理身份验证。你可以在`verify_credentials()`函数中看到一个例子。

在`verify_credentials()`函数中，我们测试 Twitter 是否识别我们的凭证。我们在这里使用的 URL 是 Twitter 提供的一个端点，纯粹用于测试我们的凭据是否有效。如果 HTTP 200 状态码有效，则返回 HTTP 200 状态码；如果无效，则返回 401 状态码。

现在，让我们运行`twitter_worldclock.py`，如果我们已经注册了应用并正确填写了令牌和机密，那么我们应该看到`Validated credentials OK`。现在，身份验证正在工作，我们程序的基本流程如下图所示：

![A Twitter client](img/6008OS_03_02.jpg)

我们的程序将作为一个守护进程运行，定期轮询 Twitter，查看是否有任何新的 tweet 需要我们处理和回复。当我们对提及时间线进行投票时，我们将下载自上次投票以来在单个批次中收到的任何新推文，这样我们就可以处理所有推文，而无需再次投票。

## 推特投票

让我们添加一个函数，用于检查和检索我们的提及时间线中的新推文。在添加循环之前，我们将使其工作。在`verify_credentials()`下方添加新函数，然后在 main 部分添加对该函数的调用，如图所示；另外，在文件开头的导入列表中添加`json`：

```py
def get_mentions(since_id, auth_obj):
    params = {'count': 200, 'since_id': since_id,
              'include_rts':  0, 'include_entities': 'false'}
    url = 'https://api.twitter.com/1.1/' \
          'statuses/mentions_timeline.json'
    response = requests.get(url, params=params, auth=auth_obj)
    response.raise_for_status()
    return json.loads(response.text)

if __name__ == '__main__':
    auth_obj = init_auth()
    since_id = 1
    for tweet in get_mentions(since_id, auth_obj):
        print(tweet['text'])
```

使用`get_mentions()`，我们通过连接`statuses/mentions_timeline.json`端点来检查并下载任何提及我们应用帐户的推文。我们提供了许多参数，`Requests`将其作为查询字符串传递。这些参数由 Twitter 指定，它们控制 tweet 将如何返回给我们。详情如下:

*   `'count'`：此指定将返回的最大推文数。Twitter 将允许通过向该端点发出的单个请求接收 200 条推文。
*   `'include_entities'`：这是用于从检索到的推文中删减一些无关信息。
*   `'include_rts'`：这告诉推特不要包含任何转发。如果有人转发我们的回复，我们不希望用户再次收到时间更新。
*   `'since_id'`：这告诉 Twitter 只返回 ID 高于此值的推文。每一条 tweet 都有一个唯一的 64 位整数 ID，以后的 tweet 比以前的 tweet 具有更高的值 ID。通过记住我们处理的最后一条 tweet 的 ID，然后将其作为这个参数传递，Twitter 将过滤掉我们已经看到的 tweet。

在运行前面提到的程序之前，我们希望为我们的帐户生成一些引用，这样我们就可以下载一些东西了。登录您的 Twitter 测试帐户，然后创建两条包含`@username`的推文，在其中您将`username`替换为应用帐户的用户名。之后，当您进入应用帐户的**通知**选项卡的**提及**部分时，您将看到这些推文。

现在，如果我们运行前面提到的代码，那么我们将把提到的内容打印到屏幕上。

## 处理推文

下一步是解析我们提到的内容，然后生成我们希望包含在回复中的时间。解析是一个简单的过程。在本例中，我们只需检查推文的“文本”值，但生成时间需要更多的工作。事实上，为此，我们需要一个城市及其时区的数据库。这在`pytz`包中提供，可在 PyPi 找到。为此，请安装以下软件包：

```py
$ pip install pytz
Downloading/unpacking pytz
...

```

然后，我们可以编写我们的 tweet 处理函数。在`get_mentions()`下方添加此功能，然后在文件开头的导入列表中添加`datetime`和`pytz`：

```py
def process_tweet(tweet):
    username = tweet['user']['screen_name']
    text = tweet['text']
    words = [x for x in text.split() if
                        x[0] not in ['@', '#']]
    place = ' '.join(words)
    check = place.replace(' ', '_').lower()
    found = False
    for tz in pytz.common_timezones:
        tz_low = tz.lower()
        if check in tz_low.split('/'):
            found = True
            break
    if found:
        timezone = pytz.timezone(tz)
        time = datetime.datetime.now(timezone).strftime('%H:%M')
        reply = '@{} The time in {} is currently {}'.format(username, place, time)
    else:
        reply = "@{} Sorry, I didn't recognize " \
                        "'{}' as a city".format(username, place)
    print(reply)

if __name__ == '__main__':
    auth_obj = init_auth()
    since_id = 1
    for tweet in get_mentions(since_id, auth_obj):
        process_tweet(tweet)
```

`process_tweet()`的大部分用于格式化推文文本和处理时区数据。首先，我们将删除推文中提到的任何`@username`和`#hashtags`。然后，我们准备剩余的 tweet 文本与时区名称数据库进行比较。时区名称数据库保存在`pytz.common_timezones`中，但名称也包含区域，区域与名称之间用斜杠（`/`分隔。此外，在这些名称中，下划线用于代替空格。

我们扫描数据库，检查格式化的 tweet 文本。如果找到匹配项，则我们构造一个回复，其中包含匹配时区的本地时间。为此，如果在时区数据库中找不到匹配项，我们将使用`datetime`模块和`pytz.`生成的时区对象，然后编写一个回复，让用户知道这一点。然后，我们将回复打印到屏幕上，以检查它是否按预期工作。

同样，在运行此功能之前，我们可能希望创建一些 tweet，其中只包含一个城市名称，并提及我们的世界时钟应用帐户，以便该功能有一些需要处理的内容。出现在时区数据库中的一些城市是都柏林、纽约和东京。

试试看！当你运行它时，你会在屏幕上看到一些 tweet 回复文本，其中包含城市和这些城市的当前本地时间。

## 费率限制

如果我们多次运行上面提到的，那么我们会发现它会在一段时间后停止工作。凭证将暂时无法验证，或者`get_mentions()`中的 HTTP 请求将失败。

这是因为 Twitter 对其 API 应用了速率限制，这意味着我们的应用只允许在给定的时间内向端点发出一定数量的请求。Twitter 文档中列出了这些限制，它们根据身份验证路由（稍后讨论）和端点而有所不同。我们正在使用`statuses/mentions_timeline.json`，所以我们的限制是每 15 分钟 15 次请求。如果我们超过这个值，Twitter 将以`429``Too many requests`状态码回应。这将迫使我们等到下一个 15 分钟的窗口启动后，我们才能获得任何有用的数据。

速率限制是 web API 的一个常见特性，因此在使用它们时，有一些有效的测试方法是很有用的。使用速率受限 API 中的数据进行测试的一种方法是下载一些数据一次，然后将其存储在本地。之后，从文件中加载它，而不是从 API 中提取它。使用 Python 解释器下载一些测试数据，如下所示：

```py
>>> from twitter_worldclock import *
>>> auth_obj = init_auth()
Credentials validated OK
>>> mentions = get_mentions(1, auth_obj)
>>> json.dump(mentions, open('test_mentions.json', 'w'))

```

当你运行这个`.` 时，你需要和`twitter_worldclock.py` 在同一个文件夹中。这个`.` 创建了一个名为`test_mentions.json,`的文件，其中包含我们的 JSONized 提及。这里，`json.dump()`函数将提供的数据写入文件，而不是以字符串形式返回。

不需要调用 API，我们可以通过修改程序的主要部分来使用这些数据，如下所示：

```py
if __name__ == '__main__':
    mentions = json.load(open('test_mentions.json'))
    for tweet in mentions:
        process_tweet(tweet)
```

## 发送回复

我们需要执行的最后一个功能是发送一条 tweet 以响应提及。为此，我们使用`statuses/update.json`端点。如果您尚未向应用帐户注册您的手机号码，则此操作将不起作用。所以，让你的程序保持原样。如果您已注册手机号码，则在`process_tweets()`下添加此功能：

```py
def post_reply(reply_to_id, text, auth_obj):
    params = {
        'status': text,
        'in_reply_to_status_id': reply_to_id}
    url = 'https://api.twitter.com/1.1./statuses/update.json'
    response = requests.post(url, params=params, auth=auth_obj)
    response.raise_for_status()
```

并将其添加到`process_tweet()`末尾的`print()`调用下方，处于相同的缩进级别：

```py
post_reply(tweet['id'], reply, auth_obj)
```

现在，如果您运行这个，然后检查您的测试帐户的 Twitter 通知，您将看到一些回复。

`post_reply()`函数只是通过使用以下参数调用端点来通知 Twitter 发布内容：

*   `status`：这是我们回复推文的文本。
*   `in_reply_to_status_id`：这是我们回复的推特的 ID。我们提供此功能，以便 Twitter 可以将推文链接为对话。

测试时，我们可能会得到一些`403`状态代码响应。这没关系，只是 Twitter 拒绝让我们连续发布两条文本相同的推文，我们可能会发现这种设置会发生，这取决于我们发送的测试推文。

## 最后的润色

构建块已经就位，我们可以添加主循环，使程序成为守护进程。将模块添加到顶部的导入中，然后将主要部分更改为如下所示：

```py
if __name__ == '__main__':
    auth_obj = init_auth()
    since_id = 1
    error_count = 0
    while error_count < 15:
        try:
            for tweet in get_mentions(since_id, auth_obj):
                process_tweet(tweet)
                since_id = max(since_id, tweet['id'])
            error_count =  0
        except requests.exceptions.HTTPError as e:
            print('Error: {}'.format(str(e)))
            error_count += 1
        time.sleep(60)
```

这将每隔 60 秒调用`get_mentions()`，然后处理任何已下载的新推文。如果我们遇到任何 HTTP 错误，它将在退出程序之前重试该过程 15 次。

现在，如果我们运行我们的程序，那么它将持续运行，回复提到世界时钟应用帐户的推文。尝试一下，运行程序，然后从测试帐户发送一些 tweet。一分钟后，您将看到一些对通知的回复。

## 更进一步

现在我们已经编写了一个基本的功能性 TwitterAPI 客户端，当然还有一些地方需要改进。虽然本章没有足够的篇幅详细探讨增强功能，但值得一提的是，有一些内容可以告诉您将来可能要进行的项目。

### 轮询和推特流媒体 API

您可能已经发现了一个问题，即我们的客户每次投票最多只能收到 200 条推文。在每次投票中，Twitter 首先提供最新的推文。这意味着，如果我们在 60 秒内收到 200 条以上的推文，那么我们将永久失去最先收到的推文。事实上，使用`statuses/mentions_timeline.json`端点并没有完整的解决方案。

Twitter 解决这个问题的方法是提供一种替代类型的 API，称为**流式 API**。当连接到这些 API 时，HTTP 响应连接实际上保持打开状态，传入的推文通过它连续传输。`Requests`包为处理此问题提供了简洁的功能。`Requests`响应对象有一个`iter_lines()`方法，该方法无限期运行。它能够在服务器发送数据时输出一行数据，然后由我们处理。如果您确实发现您需要这个，那么在请求文档中有一个示例可以帮助您入门，可以在[中找到 http://docs.python-requests.org/en/latest/user/advanced/#streaming-请求](http://docs.python-requests.org/en/latest/user/advanced/#streaming-requests)。

## 替代 oAuth 流程

我们的设置让我们的应用针对我们的主帐户运行，并让第二个帐户发送测试推文，这有点笨拙，尤其是如果你使用你的应用帐户进行常规推文。有一个专门处理世界时钟推文的独立帐户不是更好吗？

嗯，是的。理想的设置是拥有一个主帐户，您可以在其中注册应用，也可以将其作为常规 Twitter 帐户使用，并拥有第二个专用世界时钟帐户的应用进程 tweets。

oAuth 使这成为可能，但要让它工作还需要一些额外的步骤。我们需要世界时钟帐户来授权我们的应用代表它行事。您会注意到前面提到的 oAuth 凭证由两个主要元素组成，**消费者**和**访问**。消费者元素识别我们的应用，访问元素证明访问凭证来自的帐户授权我们的应用代表其行事。在我们的应用中，我们通过让应用代表注册它的帐户（即我们的应用帐户）来简化完整帐户授权过程。当我们这样做时，Twitter 允许我们直接从[dev.Twitter.com](http://dev.twitter.com)界面获取访问凭证。要使用不同的用户帐户，我们需要插入一个步骤，将用户带到 Twitter，该步骤将在 web 浏览器中打开，用户必须登录，然后明确授权我们的应用。

### 注

此过程在`requests-oauthlib` 文档中进行了演示，可在[中找到 https://requests-oauthlib.readthedocs.org/en/latest/oauth1_workflow.html](https://requests-oauthlib.readthedocs.org/en/latest/oauth1_workflow.html) 。

# HTML 与屏幕抓取

尽管越来越多的服务通过 API 提供数据，但当服务不这样做时，以编程方式获取数据的唯一方法是下载其网页，然后解析 HTML 源代码。这种技术被称为**刮屏**。

虽然从原则上讲这听起来很简单，但屏幕刮擦应该作为最后的手段。与 XML 不同，XML 严格执行语法，数据结构通常相当稳定，有时甚至有文档记录，web 页面源代码的世界是一个混乱的世界。这是一个流动的地方，在那里代码可能会意外地发生更改，并且会完全破坏脚本，迫使您从头开始重新编写解析逻辑。

尽管如此，有时这是获取基本数据的唯一方法，因此我们将简要介绍一下如何开发一种刮取方法。我们将讨论在 HTML 代码发生更改时减少影响的方法。

在删除之前，您应该始终检查站点的条款和条件。一些网站明确禁止自动解析和检索。违反条款可能导致您的 IP 地址被禁止。但是，在大多数情况下，只要您不重新发布数据，也不发出过多的请求，您就可以了。

## HTML 解析器

我们将解析 HTML，就像解析 XML 一样。我们再次可以在拉式 API 和面向对象 API 之间进行选择。我们将使用`ElementTree`的原因与前面提到的相同。

有几个可用的 HTML 解析库。它们的区别在于它们的速度、它们为在 HTML 文档中导航提供的接口，以及它们处理构造糟糕的 HTML 的能力。Python 标准库不包括面向对象的 HTML 解析器。普遍推荐的第三方软件包是`lxml`，它主要是一个 XML 解析器。但是，它确实包含一个非常好的 HTML 解析器。它很快，提供了多种浏览文档的方法，并且可以容忍破坏的 HTML。

`lxml`库可以通过`python-lxml`包安装在 Debian 和 Ubuntu 上。如果您需要最新版本或无法安装系统软件包，则可以通过`pip`安装`lxml`。请注意，您将需要一个构建环境。Debian 通常附带一个已经设置好的环境，但是如果缺少，那么下面将为 Debian 和 Ubuntu 安装一个环境：

```py
$ sudo apt-get install build-essential

```

那么您应该可以安装`lxml`，如下所示：

```py
$ sudo STATIC_DEPS=true pip install lxml

```

如果在 64 位系统上遇到编译问题，还可以尝试：

```py
$ CFLAGS="$CFLAGS -fPIC" STATIC_DEPS=true pip install lxml

```

在 Windows 上，可从`lxml`网站[获取安装程序包 http://lxml.de/installation.html](http://lxml.de/installation.html) 。如果您的 Python 版本的安装程序不可用，请查看页面中指向第三方安装程序的链接。

下一个最好的图书馆，以防`lxml`不适合你，是 BeautifulSoup。BeautifulSoup 是纯 Python 的，所以它可以与`pip`一起安装，并且应该在任何地方运行。尽管它有自己的 API，但它是一个受人尊敬且功能强大的库，事实上，它可以将`lxml`用作后端库。

## 给我看看数据

在我们开始解析 HTML 之前，我们需要一些东西来解析！让我们从 Debian 网站上获取最新的稳定 Debian 版本和代码名。有关当前稳定版本的信息，请参见[https://www.debian.org/releases/stable/](https://www.debian.org/releases/stable/) 。

我们想要的信息显示在页面标题和第一句话中：

![Show me the data](img/6008OS_03_03.jpg)

因此，我们应该提取*“jessie”*代码名和 8.0 版本号。

## 用 lxml 解析 HTML

让我们打开一个 Python shell 并开始解析。首先，我们将下载带有`Requests`的页面。

```py
>>> import requests
>>> response = requests.get('https://www.debian.org/releases/stable')

```

接下来，我们将源解析为`ElementTree`树。这与使用标准库的`ElementTree`解析 XML 相同，只是这里我们将使用`lxml`专家`HTMLParser`。

```py
>>> from lxml.etree import HTML
>>> root = HTML(response.content)

```

`HTML()`函数是一个快捷方式，它读取传递给它的 HTML，然后生成一个 XML 树。注意我们通过的是`response.content`而不是`response.text`。当`lxml`库使用原始响应而不是解码的 Unicode 文本时，它会产生更好的结果。

`lxml`库的`ElementTree`实现被设计为与标准库的实现 100%兼容，因此我们可以用与 XML 相同的方式开始探索文档：

```py
>>> [e.tag for e in root]
['head', 'body']
>>> root.find('head').find('title').text
'Debian –- Debian \u201cjessie\u201d Release Information'

```

在前面的代码中，我们已经打印出文档的`<title>`元素的文本内容，即前面屏幕截图中选项卡中显示的文本。我们已经可以看到它包含我们想要的代码名。

## 归零

屏幕抓取是一种艺术，它可以找到一种方法来明确地处理 HTML 中包含我们想要的信息的元素，并仅从这些元素中提取信息。

然而，我们也希望选择标准尽可能简单。我们对文档内容的依赖越少，如果页面的 HTML 发生变化，文档被破坏的可能性就越小。

让我们检查一下页面的 HTML 源代码，看看我们在处理什么。为此，可以在 web 浏览器中使用`View Source`，也可以将 HTML 保存到文件并在文本编辑器中打开。该页面的源代码也包含在本书的源代码下载中。搜索文本`Debian 8.0`，这样我们就可以直接找到我们想要的信息。对我来说，它看起来像下面的代码块：

```py
<body>
...
<div id="content">
<h1>Debian &ldquo;jessie&rdquo; Release Information</h1>
<p>Debian 8.0 was
released October 18th, 2014.
The release included many major
changes, described in
...
```

我跳过了`<body>`和`<div>`之间的 HTML，以表明`<div>` 是`<body>`元素的直接子元素。从上面可以看出，我们想要的是`<div>`元素的`<p>`标记子元素的内容。

如果我们使用之前使用过的`ElementTree`函数导航到此元素，那么我们将得到如下结果：

```py
>>> root.find('body').findall('div')[1].find('p').text
Debian 8.0 was.
...

```

但这并不是最好的方法，因为它很大程度上依赖于 HTML 结构。一个变化，比如在我们需要的标签之前插入一个`<div>`标签，就会破坏它。此外，在更复杂的文档中，这可能导致可怕的方法调用链，很难维护。我们在上一节中使用`<title>`标记来获取代码名是一个很好的技术示例，因为文档中总是只有一个`<head>`和一个`<title>`标记。找到 `<div>` 的更好方法是利用它包含的`id="content"`属性。一种常见的网页设计模式是将一个页面分成几个顶层`<divs>`，用于主要页面部分，如页眉、页脚和内容，并给出`<divs> id`属性来识别它们。

因此，如果我们可以搜索具有`id`属性`"content"`的`<div>`s，那么我们将有一个干净的方法来选择正确的`<div>.`，文档中只有一个`<div>`是匹配的，并且像这样的`<div>`不太可能被添加到文档中。这种方法不依赖于文档结构，因此不会受到对结构所做任何更改的影响。我们仍然需要依赖`<div>`中的`<p>`标记是第一个出现的`<p>`标记，但鉴于没有其他方法来识别它，这是我们能做的最好的事情。

那么，我们如何对我们的内容`<div>`进行这样的搜索呢？

## 使用 XPath 进行搜索

为了避免穷举迭代和对每个元素的检查，我们需要使用**XPath**，这比我们目前使用的功能更强大。它是一种专门为 XML 开发的查询语言，`lxml`支持它。另外，标准库实现为 it 提供了有限的支持。

我们将快速查看 XPath，在此过程中，我们将找到前面提出的问题的答案。

要开始，请使用上一节中的 Python shell，并执行以下操作：

```py
>>> root.xpath('body')
[<Element body at 0x39e0908>]

```

这是 XPath 表达式的最简单形式：它搜索具有与指定标记名匹配的标记名的当前元素的子元素。当前元素是我们称之为`xpath()`的元素，在本例中为`root`。`root`元素是 HTML 文档中的顶级`<html>`元素，因此返回的元素是`<body>`元素`.`

XPath 表达式可以包含多个级别的元素。搜索从进行`xpath()`调用的节点开始，在匹配表达式中的连续元素时沿树向下搜索。我们可以使用它来查找`<body>`的`<div>`子元素：

```py
>>> root.xpath('body/div')
[<Element div at 0x39e06c8>, <Element div at 0x39e05c8>, <Element div at 0x39e0608>]

```

`body/div`表达式表示匹配当前元素`<body>`子元素的`<div>`子元素。具有相同标记的元素可以在 XML 文档的同一级别多次出现，因此 XPath 表达式可以匹配多个元素，因此，`xpath()`函数始终返回列表。

前面的查询是相对于我们调用`xpath()`的元素的，但是我们可以通过在表达式的开头添加斜杠来强制从树的根进行搜索。我们还可以在双斜杠的帮助下对元素的所有子代执行搜索。要执行此操作，请尝试以下操作：

```py
>>> root.xpath('//h1')
[<Element h1 at 0x2ac3b08>]

```

在这里，我们通过只指定一个标记直接找到了`<h1>`元素，尽管它比`root`低几个级别。表达式开头的这个双斜杠将始终从根开始搜索，但是如果希望它从上下文元素开始搜索，我们可以在它前面加一个点。

```py
>>> root.find('head').xpath('.//h1')
[]

```

这将找不到任何东西，因为没有`<head>`的`<h1>`后代。

### XPath 条件

因此，我们可以通过提供路径非常具体，但 XPath 的真正威力在于对路径中的元素应用附加条件。特别是，我们前面提到的问题，即测试元素属性。

```py
>>> root.xpath('//div[@id="content"]')
[<Element div at 0x39e05c8>]

```

`div`、`[@id="content"]`后面的方括号构成了我们在匹配的`<div>`元素上放置的条件。`id`前面的`@`符号表示`id`引用了一个属性，因此条件表示：只有`id`属性等于`"content"`的元素。这就是我们如何找到我们的内容`<div>`。

在我们使用它来提取信息之前，让我们先讨论一下我们可以对条件做的一些有用的事情。我们可以只指定一个标记名，如下所示：

```py
>>> root.xpath('//div[h1]')
[<Element div at 0x39e05c8>]

```

这将返回所有具有子元素`<h1>`的`<div>`元素。还可以尝试：

```py
>>> root.xpath('body/div[2]'):
[<Element div at 0x39e05c8>]

```

将数字作为条件将返回匹配列表中该位置的元素。在本例中，这是`<body>`的第二个`<div>`子元素。请注意，这些索引从`1`开始，而 Python 索引从`0`开始。

XPath 还可以做很多事情，完整的规范是一个**万维网联盟**（**W3C**标准。最新版本见[http://www.w3.org/TR/xpath-3/](http://www.w3.org/TR/xpath-3/) 。

## 把它拉在一起

现在我们已经将 XPath 添加到我们的超级功能中，让我们通过编写一个脚本来获取 Debian 版本信息来结束。创建一个新文件`get_debian_version.py`，并将以下内容保存到其中：

```py
import re
import requests
from lxml.etree import HTML

response = requests.get('http://www.debian.org/releases/stable/')
root = HTML(response.content)
title_text = root.find('head').find('title').text
release = re.search('\u201c(.*)\u201d', title_text).group(1)
p_text = root.xpath('//div[@id="content"]/p[1]')[0].text
version = p_text.split()[1]

print('Codename: {}\nVersion: {}'.format(release, version))
```

在这里，我们已经下载并解析了 web 页面，方法是借助 XPath 提取所需的文本。我们使用正则表达式来提取*jessie*，使用`split`来提取 8.0 版本。最后我们把它打印出来。

因此，按如下所示运行：

```py
$ python3.4 get_debian_version.py
Codename: jessie
Version: 8.0

```

壮丽的好吧，至少他妈的漂亮。有一些第三方软件包可以加速抓取和表单提交，两个流行的是 Mechanize 和 Scrapy。在[查看 http://wwwsearch.sourceforge.net/mechanize/](http://wwwsearch.sourceforge.net/mechanize/) 和[http://scrapy.org](http://scrapy.org) 。

# 强大的力量。。。

作为 HTTP 客户端开发人员，您可能与运行网站的网站管理员有不同的优先级。网站管理员通常会为人类用户提供一个网站；可能会提供一项旨在创造收入的服务，而这一切很可能都需要借助非常有限的资源来完成。他们将对分析人类如何使用他们的站点感兴趣，并且可能有他们更喜欢的站点区域，而自动化客户不会去探索。

自动解析和下载网站页面的 HTTP 客户端被称为各种各样的东西，如*机器人*、*网络爬虫*和*蜘蛛*。机器人有许多合法的用途。所有的搜索引擎提供商都广泛使用机器人来抓取网页并建立巨大的页面索引。机器人可以用来检查死链接，并为存储库（如 Wayback 机器）归档站点。但是，也有许多用途可能被认为是非法的。自动遍历信息服务以提取其页面上的数据，然后在未经网站所有者许可的情况下将该数据重新打包以在其他地方展示，当该服务的精神是在线观看时，一次性下载大量媒体文件等都可能被视为非法。有些网站有明确禁止自动下载的服务条款。尽管一些行为，如复制和重新发布受版权保护的材料，显然是非法的，但其他一些行为则需要解释。这一灰色地带是一个正在进行辩论的主题，它不可能得到所有人满意的解决。

然而，即使它们确实有合法的用途，总的来说，机器人确实使网站管理员的生活更加困难。它们污染了网站服务器日志，网站管理员使用这些日志计算人类用户如何使用网站的统计数据。机器人还消耗带宽和其他服务器资源。

使用我们在本章中介绍的方法，编写一个执行上述许多功能的 bot 非常简单。网站管理员为我们提供我们将要使用的服务，因此作为回报，我们应该尊重上述领域，并以尽可能少地影响他们的方式设计我们的机器人。

## 选择用户代理

我们可以做一些事情来帮助我们的网站管理员。我们应该为我们的客户选择合适的用户代理。网站管理员从日志文件中过滤出 bot 流量的主要方法是执行用户代理分析。

存在已知机器人的用户代理列表，例如，可以在[找到一个这样的列表 http://www.useragentstring.com/pages/Crawlerlist/](http://www.useragentstring.com/pages/Crawlerlist/) 。

网站管理员可以在他们的过滤器中使用这些。许多网站管理员还简单地过滤掉任何包含单词*bot*、*spider*或*crawler*的用户代理。因此，如果我们编写的是一个自动机器人而不是浏览器，那么如果我们使用一个包含其中一个单词的用户代理，那么网站管理员的生活就会变得简单一些。搜索引擎提供商使用的许多机器人程序都遵循此约定，下面列出了一些示例：

*   `Mozilla/5.0` `compatible; bingbot/2.0; http://www.bing.com/bingbot.htm`
*   `Baiduspider: http://www.baidu.com/search/spider.htm`
*   `Mozilla/5.0 compatible; Googlebot/2.1; http://www.google.com/bot.html`

HTTP RFC 7231 第 5.5.3 节中也有一些指南。

## Robots.txt 文件

有一个非官方的但标准的机制告诉机器人是否有网站的任何部分不应该爬行。这种机制被称为`robots.txt`，它的形式是一个名为`robots.txt`的文本文件。此文件始终位于网站的根目录中，以便机器人程序始终可以找到它。它有描述网站可访问部分的规则。文件格式在[中描述 http://www.robotstxt.org](http://www.robotstxt.org) 。

Python 标准库提供`urllib.robotparser`模块，用于解析和处理`robots.txt`文件。您可以创建一个解析器对象，向它提供一个`robots.txt`文件，然后您可以简单地查询它，查看给定的 URL 是否允许用于给定的用户代理。在标准库的文档中可以找到一个很好的例子。如果你在访问之前检查你的客户可能想要访问的每个 URL，并尊重网站管理员的意愿，那么你将帮助他们。

最后，由于在测试初出茅庐的客户机时，我们可能会发出很多请求，因此最好制作您希望客户机对其进行解析和测试的网页或文件的本地副本。这样，我们将为自己和网站节省带宽。

# 总结

在本章中我们已经介绍了很多内容，但是现在您应该能够开始真正使用您遇到的 web API 了。

我们研究了 XML，以及如何使用`ElementTree`API 构造文档、解析文档和从文档中提取数据。我们研究了 Python`ElementTree`实现和`lxml`实现。我们还研究了如何有效地使用 XPath 查询语言从文档中提取信息。

我们查看了 AmazonS3 服务，并编写了一个客户端，它允许我们执行基本操作，例如创建 bucket，以及通过 S3RESTAPI 上传和下载文件。我们学习了如何设置访问权限和内容类型，以便文件在 web 浏览器中正常工作。

我们研究了 JSON 数据格式，如何将 Python 对象转换为 JSON 数据格式，以及如何将它们转换回 Python 对象。

然后，我们探索了 Twitter API 并编写了一个按需世界时钟服务，通过该服务，我们学习了如何读取和处理帐户的推文，以及如何将推文作为回复发送。

我们了解了如何从网页的 HTML 源中提取或刮取信息。我们了解了如何在使用`ElementTree`和`lxml`HTML 解析器时使用 HTML。我们还学习了如何使用 XPath 帮助提高此过程的效率。

最后，我们研究了如何向向我们提供所有数据的网站管理员提供反馈。我们讨论了几种方法，我们可以为我们的客户编写代码，使网站管理员的生活更轻松，并尊重他们希望我们如何使用他们的网站。

现在，HTTP 就是这样。我们将在[第 9 章](09.html "Chapter 9. Applications for the Web")、*Web 应用*中再次访问 HTTP，其中我们将研究如何使用 Python 构建 Web 应用的服务器端。在下一章中，我们将讨论互联网的另一个重要工具：电子邮件。