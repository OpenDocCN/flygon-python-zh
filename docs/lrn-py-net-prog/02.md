# 二、HTTP 与网络

**超文本传输协议**（**HTTP**）可能是最广泛使用的应用层协议。它最初是为了让学者共享 HTML 文档而开发的。如今，它已成为互联网上无数应用的核心协议，也是万维网的主要协议。

在本章中，我们将介绍以下主题：

*   HTTP 协议结构
*   使用 Python 通过 HTTP 与服务对话
*   下载文件
*   HTTP 功能，如压缩和 cookie
*   处理错误
*   网址
*   Python 标准库`urllib`包
*   Kenneth Reitz 的第三方`Requests`包

`urllib`包是推荐用于 HTTP 任务的 Python 标准库包。标准库还有一个名为`http`的低级模块。尽管这提供了对协议几乎所有方面的访问，但它并不是为日常使用而设计的。`urllib`包有一个更简单的接口，它处理我们将在本章中介绍的所有内容。

第三方`Requests`包是`urllib`的一个非常流行的替代方案。它有一个优雅的界面和一个强大的功能集，是一个优化 HTTP 工作流的好工具。我们将在本章末尾讨论如何使用它来代替`urllib`。

# 请求与响应

HTTP 是一种应用层协议，几乎总是在 TCP 之上使用。HTTP 协议被故意定义为使用人类可读的消息格式，但它仍然可以用于传输任意字节的数据。

HTTP 交换由两个元素组成。客户端发出的**请求**，向服务器请求 URL 指定的特定资源；服务器发送的**响应**，提供客户端请求的资源。如果服务器无法提供客户端请求的资源，则响应将包含有关失败的信息。

此事件顺序在 HTTP 中是固定的。所有交互都是由客户机发起的。如果客户端没有明确请求，服务器永远不会向客户端发送任何内容。

本章将教您如何使用 Python 作为 HTTP 客户机。我们将学习如何向服务器发出请求，然后解释它们的响应。我们将在[第 9 章](09.html "Chapter 9. Applications for the Web")、*Web 应用*中介绍如何编写服务器端应用。

到目前为止，使用最广泛的 HTTP 版本是 1.1，定义在 RFCs 7230 到 7235 中。HTTP2 是最新版本，在本书即将出版之际正式批准。在版本 1.1 和版本 2 之间，大多数语义和语法保持不变，主要变化在于 TCP 连接的使用方式。到目前为止，HTTP 2 还没有得到广泛的支持，因此我们将在本书中重点介绍 1.1 版。如果您确实想了解更多，HTTP 2 在 RFCs 7540 和 7541 中有文档记录。

RFC 1945 中记录的 HTTP 版本 1.0 仍然被一些旧软件使用。尽管版本 1.1 与 1.0 向后兼容，`urllib`包和`Requests`都支持 HTTP 1.1，所以当我们用 Python 编写客户机时，我们不需要担心是否连接到 HTTP 1.0 服务器。只是有些更高级的功能不可用。现在几乎所有的服务都使用 1.1 版，所以我们这里不讨论它们之间的区别。堆栈溢出问题是一个很好的起点，如果您需要进一步的信息：[http://stackoverflow.com/questions/246859/http-1-0-vs-1-1](http://stackoverflow.com/questions/246859/http-1-0-vs-1-1) 。

# 使用 urllib 的请求

在[第 1 章](01.html "Chapter 1. Network Programming and Python")、*网络编程和 Python*中讨论 RFC 下载程序时，我们已经看到了一些 HTTP 交换的示例。`urllib`包被分为几个子模块，用于处理使用 HTTP 时可能需要执行的不同任务。为了发出请求和接收响应，我们使用了`urllib.request`模块。

使用`urllib`检索 URL 的内容是一个简单的过程。加载 Python 解释器并执行以下操作：

```py
>>> from urllib.request import urlopen
>>> response = urlopen('http://www.debian.org')
>>> response
<http.client.HTTPResponse object at 0x7fa3c53059b0>
>>> response.readline()
b'<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">\n'

```

我们使用`urllib.request.urlopen()`功能为[处的资源发送请求并接收响应 http://www.debian.org](http://www.debian.org) ，在本例中为 HTML 页面。然后，我们将打印出收到的 HTML 的第一行。

# 响应对象

让我们仔细看看我们的响应对象。我们可以从前面的示例中看到`urlopen()`返回一个`http.client.HTTPResponse`实例。response 对象允许我们访问请求资源的数据以及响应的属性和元数据。要查看我们在上一节中收到的响应的 URL，请执行以下操作：

```py
>>> response.url
'http://www.debian.org'

```

我们使用`readline()`和`read()`方法通过类似文件的接口获取请求资源的数据。我们在上一节中看到了`readline()`方法。这就是我们使用`read()`方法的方式：

```py
>>> response = urlopen('http://www.debian.org')
>>> response.read(50)
b'g="en">\n<head>\n  <meta http-equiv="Content-Type" c'

```

`read()`方法从数据返回指定的字节数。这是前 50 个字节。不带参数的`read()`方法调用将一次性返回所有数据。

类似文件的接口是有限的。一旦读取了数据，就不可能使用上述任何一个函数返回并重新读取数据。要演示这一点，请尝试执行以下操作：

```py
>>> response = urlopen('http://www.debian.org')
>>> response.read()
b'<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">\n<html lang="en">\n<head>\n  <meta http-equiv
...
>>> response.read()
b''

```

我们可以看到，当我们第二次调用`read()`函数时，它返回一个空字符串。没有`seek()`或`rewind()`方法，因此我们无法重置位置。因此，最好在变量中捕获`read()`输出。

`readline()`和`read()`函数都返回字节对象，`http`和`urllib`都不会努力将它们接收到的数据解码为 Unicode。在本章的后面，我们将研究一种方法，在`Requests`库的帮助下处理这个问题。

# 状态码

如果我们想知道我们的请求是否发生了意外，该怎么办？或者，如果我们想在读取数据之前知道我们的响应是否包含任何数据，该怎么办？也许我们期待着一个巨大的响应，我们希望在不阅读整个响应的情况下快速查看我们的请求是否成功。

HTTP 响应通过**状态码**为我们提供了一种方式。我们可以使用响应的`status`属性读取响应的状态代码。

```py
>>> response.status
200

```

状态代码是告诉我们请求如何进行的整数。`200`代码告诉我们一切都很顺利。

代码有很多种，每一种都传达着不同的含义。根据其第一位数字，状态代码分为以下几组：

*   100：信息性
*   200：成功
*   300：重定向
*   400：客户端错误
*   500:服务器错误

一些更常见的代码及其消息如下：

*   `200`：`OK`
*   `404`：`Not Found`
*   `500`：`Internal Server Error`

状态代码的官方列表由 IANA 维护，可在[找到 https://www.iana.org/assignments/http-status-codes](https://www.iana.org/assignments/http-status-codes) 。我们将在本章中查看各种代码。

# 处理问题

状态代码帮助我们查看我们的响应是否成功。200 范围内的任何代码都表示成功，而 400 范围或 500 范围内的任何代码都表示失败。

应始终检查状态代码，以便我们的程序在出现问题时能够做出适当的响应。`urllib`包通过在遇到问题时引发异常来帮助我们检查状态代码。

让我们看看如何捕捉这些并有效地处理它们。为此，请尝试以下命令块：

```py
>>> import urllib.error
>>> from urllib.request import urlopen
>>> try:
...   urlopen('http://www.ietf.org/rfc/rfc0.txt')
... except urllib.error.HTTPError as e:
...   print('status', e.code)
...   print('reason', e.reason)
...   print('url', e.url)
...
status: 404
reason: Not Found
url: http://www.ietf.org/rfc/rfc0.txt

```

这里我们请求了 RFC 0，它不存在。所以服务器返回了 404 状态码，`urllib`发现了这一点并发出了`HTTPError`。

您可以看到`HTTPError`提供了关于请求的有用属性。在前面的示例中，我们使用了`status`、`reason`和`url`属性来获取有关响应的一些信息。

如果网络堆栈下部出现问题，则相应的模块将引发异常。`urllib`包捕获这些异常，然后将它们包装为`URLErrors`。例如，我们可能指定了一个不存在的主机或 IP 地址，如下所示：

```py
>>> urlopen('http://192.0.2.1/index.html')
...
urllib.error.URLError: <urlopen error [Errno 110] Connection timed out>

```

在本例中，我们从`192.0.2.1`请求了`index.html`。主办`192.0.2.0/24`IP 地址范围仅保留供文档使用，因此您永远不会遇到使用前面 IP 地址的主机。因此 TCP 连接超时，`socket`引发超时异常，`urllib`捕捉、重新包装并为我们重新引发。我们可以用与前面示例中相同的方法捕获这些异常。

# HTTP 头

请求和响应由两个主要的部分组成，**头**和**体**。当我们在[第 1 章](01.html "Chapter 1. Network Programming and Python")、*网络编程和 Python*中使用我们的 TCP RFC 下载程序时，我们简要地看到了一些 HTTP 头。标头是特定于协议的信息行，显示在通过 TCP 连接发送的原始消息的开头。正文是消息的其余部分。它与标题之间用一个空行隔开。正文是可选的，它的存在取决于请求或响应的类型。下面是 HTTP 请求的一个示例：

```py
GET / HTTP/1.1
Accept-Encoding: identity
Host: www.debian.com
Connection: close
User-Agent: Python-urllib/3.4
```

第一行称为**请求行**。它由请求**方法**组成，在本例中为`GET`，资源路径为`/`，HTTP 版本为`1.1`。其余的行是请求头。每行由标题名、冒号和标题值组成。前面输出中的请求只包含头，没有正文。

标题有多种用途。在请求中，它们可用于传递额外的数据，如 cookie 和授权凭据，以及请求服务器提供首选的资源格式。

例如，一个重要的头是`Host`头。许多 web 服务器应用能够使用相同的 IP 地址在同一服务器上托管多个网站。DNS 别名是为各种网站域名设置的，因此它们都指向相同的 IP 地址。实际上，web 服务器有多个主机名，每个主机名对应一个网站。IP 和 TCP（HTTP 在其上运行）不能用来告诉服务器客户端要连接到哪个主机名，因为它们都只在 IP 地址上运行。HTTP 协议允许客户端通过包含`Host`头在 HTTP 请求中提供主机名。

我们将在下一节中查看更多的请求头。

下面是一个响应示例：

```py
HTTP/1.1 200 OK
Date: Sun, 07 Sep 2014 19:58:48 GMT
Content-Type: text/html
Content-Length: 4729
Server: Apache
Content-Language: en

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">\n...
```

第一行包含协议版本、状态代码和状态消息。后续行包含标题、空行，然后是正文。在响应中，服务器可以使用头通知客户机有关正文的长度、响应正文包含的内容类型以及客户机应存储的 cookie 数据等信息。

执行以下操作以查看响应对象的标题：

```py
>>> response = urlopen('http://www.debian.org)
>>> response.getheaders()
[('Date', 'Sun, 07 Sep 2014 19:58:48 GMT'), ('Server', 'Apache'), ('Content-Location', 'index.en.html'), ('Vary', 'negotiate,accept- language,Accept-Encoding')...

```

`getheaders()`方法以元组列表的形式返回标题（`header name`、`header value`）。可以在 RFC 7231 中找到 HTTP 1.1 头及其含义的完整列表。让我们看看如何在请求和响应中使用一些头。

# 定制请求

为了利用头提供的功能，我们在发送请求之前向请求添加头。要做到这一点，我们不能只使用`urlopen()`。我们需要遵循以下步骤：

*   创建一个`Request`对象
*   向请求对象添加头
*   使用`urlopen()`发送请求对象

我们将学习如何定制检索 Debian 主页瑞典版本的请求。我们将使用`Accept-Language`头，它告诉服务器它返回的资源的首选语言。请注意，并非所有服务器都拥有多种语言版本的资源，因此并非所有服务器都会响应`Accept-Language`Linux 主页。

首先，我们创建一个`Request`对象：

```py
>>> from urllib.request import Request
>>> req = Request('http://www.debian.org')

```

接下来，我们添加标题：

```py
>>> req.add_header('Accept-Language', 'sv')

```

`add_header()`方法将头的名称和头的内容作为参数。`Accept-Language`标题采用两个字母的 ISO 639-1 语言代码。瑞典语代码为`sv`。

最后，我们使用`urlopen()`提交定制请求：

```py
>>> response = urlopen(req)

```

我们可以通过打印前几行来检查响应是否为瑞典语：

```py
>>> response.readlines()[:5]
[b'<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">\n',
 b'<html lang="sv">\n',
 b'<head>\n',
 b'  <meta http-equiv="Content-Type" content="text/html; charset=utf-  8">\n',
 b'  <title>Debian -- Det universella operativsystemet </title>\n']

```

捷达文胸！`Accept-Language`报头已通知服务器响应内容的首选语言。

要查看请求中的标头，请执行以下操作：

```py
>>> req = Request('http://www.debian.org')
>>> req.add_header('Accept-Language', 'sv')
>>> req.header_items()
[('Accept-language', 'sv')]

```

`urlopen()`方法在我们对请求运行时会添加一些自己的头：

```py
>>> response = urlopen(req)
>>> req.header_items()
[('Accept-language', 'sv'), ('User-agent': 'Python-urllib/3.4'), ('Host': 'www.debian.org')]

```

添加头的快捷方式是在创建请求对象的同时添加头，如下所示：

```py
>>> headers = {'Accept-Language': 'sv'}
>>> req = Request('http://www.debian.org', headers=headers)
>>> req.header_items()
[('Accept-language', 'sv')]

```

我们将头作为`dict`提供给`Request`对象构造函数作为`headers`关键字参数。这样，我们可以通过向`dict`添加更多条目一次添加多个标题。

让我们来看看更多的东西，我们可以用标题。

## 内容压缩

`Accept-Encoding`请求报头和`Content-Encoding`响应报头可以一起工作，允许我们临时编码响应主体，以便通过网络传输。这通常用于压缩响应和减少需要传输的数据量。

此过程遵循以下步骤：

*   客户机发送一个请求，并在`Accept-Encoding`头中列出可接受的编码
*   服务器选择它支持的编码方法
*   服务器使用此编码方法对主体进行编码
*   服务器发送响应，指定在`Content-Encoding`头中使用的编码
*   客户端使用指定的编码方法对响应体进行解码

让我们讨论如何请求文档并让服务器对响应体使用`gzip`压缩。首先，让我们构造请求：

```py
>>> req = Request('http://www.debian.org')

```

接下来，添加`Accept-Encoding`标题：

```py
>>> req.add_header('Accept-Encoding', 'gzip')

```

然后在`urlopen()`的帮助下提交：

```py
>>> response = urlopen(req)

```

我们可以通过查看响应的`Content-Encoding`头来检查服务器是否使用`gzip`压缩：

```py
>>> response.getheader('Content-Encoding')
'gzip'

```

然后我们可以使用`gzip`模块解压身体数据：

```py
>>> import gzip
>>> content = gzip.decompress(response.read())
>>> content.splitlines()[:5]
[b'<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">',
 b'<html lang="en">',
 b'<head>',
 b'  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">',
 b'  <title>Debian -- The Universal Operating System </title>']

```

编码在 IANA 注册。当前列表包括：`gzip`、`compress`、`deflate`和`identity`。前三种是指特定的压缩方法。最后一个允许客户端指定它不希望对内容应用任何编码。

让我们看看如果我们使用`identity`编码要求不压缩会发生什么：

```py
>>> req = Request('http://www.debian.org')
>>> req.add_header('Accept-Encoding', 'identity')
>>> response = urlopen(req)
>>> print(response.getheader('Content-Encoding'))
None

```

当服务器使用`identity`编码类型时，响应中不包含`Content-Encoding`头。

## 多值

要告诉服务器我们可以接受多个编码，请在`Accept-Encoding`头中添加更多值并用逗号分隔。让我们试试看。我们创建我们的`Request`对象：

```py
>>> req = Request('http://www.debian.org')

```

然后，我们添加了标题，这次我们添加了更多编码：

```py
>>> encodings = 'deflate, gzip, identity'
>>> req.add_header('Accept-Encoding', encodings)

```

现在，我们提交请求，然后检查响应编码：

```py
>>> response = urlopen(req)
>>> response.getheader('Content-Encoding')
'gzip'

```

如果需要，可以通过添加一个`q`值为特定编码提供相对权重：

```py
>>> encodings = 'gzip, deflate;q=0.8, identity;q=0.0'

```

`q`值跟在编码名称后面，用分号分隔。最大`q`值为`1.0`，如果没有给出`q`值，这也是默认值。因此，前一行应该被解释为我的第一个编码偏好是`gzip`，第二个偏好是`deflate`，第三个偏好是`identity`，如果没有其他可用的话。

# 内容协商

带有头的内容压缩和带有`Accept-Language`头的语言选择是**内容协商**的示例，其中客户机指定其对所请求资源的格式和内容的偏好。以下标题也可用于此目的：

*   `Accept`：用于请求首选文件格式
*   `Accept-Charset`：用于请求首选字符集中的资源

内容协商机制还有其他方面，但由于它的支持不一致，并且可能会涉及很多问题，因此本章将不介绍内容。RFC 7231 包含您需要的所有详细信息。如果您发现您的应用需要此功能，请查看第 3.4、5.3、6.4.1 和 6.5.6 节。

## 内容类型

HTTP 可以用作任何类型文件或数据的传输。服务器可以在响应中使用`Content-Type`头通知客户端它在主体中发送的数据类型。这是 HTTP 客户端决定如何处理服务器返回给它的主体数据的主要方法。

要查看内容类型，我们检查响应头的值，如下所示：

```py
>>> response = urlopen('http://www.debian.org')
>>> response.getheader('Content-Type')
'text/html'

```

此标题中的值取自 IANA 维护的列表。这些值被不同地称为**内容类型**、**互联网媒体类型**或**MIME 类型**（**MIME**代表**多用途互联网邮件扩展**，该协议最初建立于中的规范）。完整列表可在[中找到 http://www.iana.org/assignments/media-types](http://www.iana.org/assignments/media-types) 。

通过互联网传输的许多类型的数据都有注册媒体类型，一些常见的媒体类型有：

<colgroup><col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

媒体类型

 | 

描述

 |
| --- | --- |
| 文本/html | HTML 文档 |
| 文本/纯文本 | 纯文本文档 |
| 图像/jpeg | JPG 图像 |
| 申请表格/pdf | PDF 文件 |
| 应用/json | JSON 数据 |
| application/xhtml+xml | XHTML 文档 |

另一种感兴趣的媒体类型是`application/octet-stream`，它实际上用于没有适用媒体类型的文件。这方面的一个例子是 pickled Python 对象。它还用于服务器不知道其格式的文件。为了正确处理此媒体类型的响应，我们需要以其他方式发现格式。可能的办法如下：

*   检查已下载资源的文件扩展名（如果有）。然后可以使用`mimetypes`模块确定媒体类型（参见[第 3 章](03.html "Chapter 3. APIs in Action")、*API 动作*中的示例）。
*   下载数据，然后使用文件类型分析工具。使用 Python 标准库`imghdr`模块可用于图像，第三方`python-magic`包或`GNU`文件命令可用于其他类型。
*   检查我们下载的网站，看看文件类型是否在任何地方都有记录。

内容类型值可以包含可选的附加参数，这些参数提供有关该类型的更多信息。这通常用于提供数据使用的字符集。例如：

```py
Content-Type: text/html; charset=UTF-8.
```

在本例中，我们被告知文档的字符集是 UTF-8。参数包含在分号之后，并且始终采用键/值对的形式。

让我们讨论一个示例，下载 Python 主页并使用它返回的`Content-Type`值。首先，我们提出以下要求：

```py
>>> response = urlopen('http://www.python.org')

```

然后，我们检查响应的`Content-Type`值，并提取字符集：

```py
>>> format, params = response.getheader('Content-Type').split(';')
>>> params
' charset=utf-8'
>>> charset = params.split('=')[1]
>>> charset
'utf-8'

```

最后，我们使用提供的字符集解码响应内容：

```py
>>> content = response.read().decode(charset)

```

注意，通常情况下，服务器要么在`Content-Type`头中不提供`charset`，要么提供错误的`charset`。所以，这个值应该作为一个建议。这也是我们在本章后面将要讨论`Requests`库的原因之一。它将自动收集所有提示，它可以找到什么字符集应该用于解码响应体，并为我们做出最佳猜测。

# 用户代理

另一个值得了解的请求头是`User-Agent`头。任何使用 HTTP 进行通信的客户端都可以称为**用户代理**。RFC 7231 建议用户代理应该使用`User-Agent`头在每个请求中标识自己。其中的内容取决于发出请求的软件，尽管它通常包含一个字符串，用于标识程序和版本，可能还包括操作系统和运行它的硬件。例如，我当前版本的 Firefox 的用户代理如下所示：

```py
Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20140722 Firefox/24.0 Iceweasel/24.7.0
```

虽然它在这里被打断了两行，但它是一根很长的绳子。正如您可能理解的，我正在 64 位 Linux 系统上运行 Iceweasel（Debian 版本的 Firefox）版本 24。用户代理字符串不用于标识单个用户。他们只识别用于提出请求的产品。

我们可以查看`urllib`使用的用户代理。执行以下步骤：

```py
>>> req = Request('http://www.python.org')
>>> urlopen(req)
>>> req.get_header('User-agent')
'Python-urllib/3.4'

```

在这里，我们已经创建了一个请求，并使用`urlopen`提交它，`urlopen`将用户代理头添加到请求中。我们可以使用`get_header()`方法检查此标题。此标头及其值包含在`urllib`发出的每个请求中，因此我们向其发出请求的每个服务器都可以看到我们正在使用 Python 3.4 和`urllib`库。

网站管理员可以检查请求的用户代理，然后将信息用于各种用途，包括：

*   根据网站统计数据对访问进行分类
*   使用某些用户代理字符串阻止客户端
*   为存在已知问题的用户代理发送替代版本的资源，例如在解释某些语言（如 CSS）时出现错误，或者根本不支持某些语言（如 JavaScript）

最后两个可能会给我们带来问题，因为它们会阻止或干扰我们访问我们所关注的内容。为了解决这个问题，我们可以尝试设置我们的用户代理，使其模仿知名的浏览器。这就是所谓的**欺骗**，如下所示：

```py
>>> req = Request('http://www.debian.org')
>>> req.add_header('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20140722 Firefox/24.0 Iceweasel/24.7.0')
>>> response = urlopen(req)

```

服务器将做出响应，就像我们的应用是一个普通的 Firefox 客户端一样。web 上提供了不同浏览器的用户代理字符串。我还没有为他们找到一个全面的资源，但谷歌搜索浏览器和版本号通常会发现一些东西。或者，您可以使用 Wireshark 捕获由要模拟的浏览器发出的 HTTP 请求，并查看捕获的请求的用户代理标头。

# 饼干

cookie 是服务器在`Set-Cookie`头中发送的小块数据，作为响应的一部分。客户端在本地存储 cookie，并将其包含在将来发送到服务器的任何请求中。

服务器以各种方式使用 cookie。他们可以向其添加唯一的 ID，这使他们能够在客户端访问站点的不同区域时跟踪客户端。他们可以存储一个登录令牌，该令牌将自动让客户机登录，即使客户机离开站点然后稍后访问它。它们还可用于存储客户的用户偏好或个性化信息片段等。

Cookie 是必需的，因为服务器没有其他方法在请求之间跟踪客户端。HTTP 被称为**无状态**协议。它不包含一个明确的机制，服务器可以确定两个请求来自同一个客户机。如果没有 Cookie 允许服务器向请求添加一些唯一的标识信息，那么购物车（这是 Cookie 开发用来解决的原始问题）之类的东西将变得不可能构建，因为服务器将无法确定哪个购物篮与哪个请求一起使用。

我们可能需要在 Python 中处理 cookie，因为没有 cookie，一些站点的行为就无法达到预期。在使用 Python 时，我们可能还希望访问站点中需要登录的部分，并且登录会话通常通过 cookie 进行维护。

## 饼干处理

我们将与`urllib`讨论如何处理 cookies。首先，我们需要创建一个存储服务器将发送给我们的 cookie 的位置：

```py
>>> from http.cookiejar import CookieJar
>>> cookie_jar = CookieJar()

```

接下来，我们构建一个名为`urllib``opener`**的东西。**这将自动从我们收到的响应中提取 cookie，然后将它们存储在我们的 cookie 罐中：

```py
>>> from urllib.request import build_opener, HTTPCookieProcessor
>>> opener = build_opener(HTTPCookieProcessor(cookie_jar))

```

然后，我们可以使用 opener 发出 HTTP 请求：

```py
>>> opener.open('http://www.github.com')

```

最后，我们可以检查服务器是否向我们发送了一些 cookie：

```py
>>> len(cookie_jar)
2

```

无论何时我们使用`opener`发出进一步的请求，`HTTPCookieProcessor`功能将检查我们的`cookie_jar`是否包含该站点的任何 cookie，然后它将自动将它们添加到我们的请求中。它还会将收到的任何其他 cookie 添加到 cookie jar 中。

`http.cookiejar`模块还包含一个`FileCookieJar`类，其工作方式与`CookieJar`相同，但它提供了一个附加功能，可以轻松地将 cookie 保存到文件中。这允许跨 Python 会话持久化 cookie。

## 知道你的饼干吗

值得更详细地了解 cookies 的属性。让我们检查 GitHub 在上一节中发送给我们的 cookies。

要做到这一点，我们需要从饼干罐中取出饼干。`CookieJar`模块不允许我们直接访问它们，但它支持迭代器协议。因此，快速获取它们的方法是从中创建一个`list`：

```py
>>> cookies = list(cookie_jar)
>>> cookies
[Cookie(version=0, name='logged_in', value='no', ...),
 Cookie(version=0, name='_gh_sess', value='eyJzZxNzaW9uX...', ...)
]

```

你可以看到我们有两个`Cookie`物体。现在，让我们从第一部分中提取一些信息：

```py
>>> cookies[0].name
'logged_in'
>>> cookies[0].value
'no'

```

cookie 的名称允许服务器快速引用它。这个 cookie 显然是 GitHub 用于查明我们是否已登录的机制的一部分。接下来，让我们执行以下操作：

```py
>>> cookies[0].domain
'.github.com'
>>> cookies[0].path
'/'

```

域和路径是此 cookie 有效的区域，因此我们的`urllib`开启器将在发送到[www.github.com](http://www.github.com)及其子域的任何请求中包含此 cookie，其中路径位于根目录下的任何位置。

现在，让我们看看 cookie 的生命周期：

```py
>>> cookies[0].expires
2060882017

```

这是一个 Unix 时间戳；我们可以将其转换为`datetime`：

```py
>>> import datetime
>>> datetime.datetime.fromtimestamp(cookies[0].expires)
datetime.datetime(2035, 4, 22, 20, 13, 37)

```

因此，我们的饼干将于 2035 年 4 月 22 日到期。到期日是服务器希望客户端保留 cookie 的时间量。一旦到期日过去，客户端可以扔掉 cookie，服务器将在下一个请求中发送一个新的 cookie。当然，没有什么可以阻止客户端立即扔掉 cookie，尽管在某些网站上，这可能会破坏依赖 cookie 的功能。

让我们讨论两个常见的 cookie 标志：

```py
>>> print(cookies[0].get_nonstandard_attr('HttpOnly'))
None

```

可以通过多种方式访问存储在客户端上的 Cookie：

*   由客户端作为 HTTP 请求和响应序列的一部分执行
*   通过客户端中运行的脚本，如 JavaScript
*   通过客户端中运行的其他进程，如 Flash

`HttpOnly`标志表示，当访问是 HTTP 请求或响应的一部分时，客户端应仅允许访问 cookie。其他方法应被拒绝访问。这将保护客户端免受跨站点脚本攻击（有关这些攻击的更多信息，请参见[第 9 章](09.html "Chapter 9. Applications for the Web")、*Web 应用*）。这是一个重要的安全特性，当服务器设置它时，我们的应用应该相应地运行。

还有一个`secure`标志：

```py
>>> cookies[0].secure
True

```

如果该值为 true，`Secure`标志指示 cookie 只能通过安全连接（如 HTTPS）发送。同样，如果标志设置为当我们的应用发送包含此 cookie 的请求时，它只将它们发送到 HTTPS URL，那么我们应该尊重这一点。

你可能已经在这里发现了一个不一致的地方。我们的 URL 已通过 HTTP 请求响应，但服务器已向我们发送 cookie，它请求仅通过安全连接发送 cookie。网站设计者肯定没有忽视这样的安全漏洞吗？放心；他们没有。响应实际上是通过 HTTPS 发送的。但是，这是怎么发生的？答案在于重定向。

# 重定向

有时服务器会移动其内容。他们还将一些内容过时，并在不同的位置发布新内容。有时他们希望我们使用更安全的 HTTPS 协议，而不是 HTTP。在所有这些情况下，他们可能会获得请求旧 URL 的流量，在所有这些情况下，他们可能更希望能够自动将访问者发送到新 URL。

300 种 HTTP 状态代码就是为此而设计的。这些代码向客户表明，他们需要采取进一步行动来完成请求。最常见的操作是在不同的 URL 重试请求。这称为**重定向**。

我们将在使用`urllib`时了解其工作原理。让我们提出一个请求：

```py
>>> req = Request('http://www.gmail.com')
>>> response = urlopen(req)

```

很简单，但现在，请查看响应的 URL：

```py
>>> response.url
'https://accounts.google.com/ServiceLogin?service=mail&passive=true&r m=false...'

```

这不是我们请求的 URL！如果我们在浏览器中打开这个新 URL，那么我们将看到它实际上是 Google 登录页面（如果您已经有一个缓存的 Google 登录会话，您可能需要清除浏览器 cookies 才能看到这个页面）。谷歌将我们从[重新定向 http://www.gmail.com](http://www.gmail.com) 进入其登录页面，`urllib`自动跟随重定向。此外，我们可能不止一次被重定向。请看我们请求对象的`redirect_dict`属性：

```py
>>> req.redirect_dict
{'https://accounts.google.com/ServiceLogin?service=...': 1, 'https://mail.google.com/mail/': 1}

```

`urllib`包添加了我们重定向到此`dict`的每个 URL。我们可以看到，我们实际上已经被重定向了两次，第一次是[https://mail.google.com](https://mail.google.com) ，第二个进入登录页面。

当我们发送第一个请求时，服务器发送一个带有重定向状态代码的响应，该代码是 301、302、303 或 307 中的一个。所有这些都表示重定向。此响应包括一个`Location`头，其中包含新的 URL。`urllib`包将向该 URL 提交一个新请求，在上述情况下，它将收到另一个重定向，这将导致它进入 Google 登录页面。

由于`urllib`为我们执行重定向，它们通常不会影响我们，但值得一提的是`urllib`返回的响应可能与我们请求的 URL 不同。此外，如果我们针对单个请求点击了太多重定向（对于`urllib`超过 10 个），那么`urllib`将放弃并引发`urllib.error.HTTPError`异常。

# 网址

统一资源定位器，或**URL**是网络运行方式的基础，RFC 3986 中对其进行了正式描述。URL 表示给定主机上的资源。URL 如何映射到远程系统上的资源完全由系统管理员决定。URL 可以指向服务器上的文件，也可以在收到请求时动态生成资源。尽管 URL 映射到什么并不重要，只要 URL 在我们请求它们时起作用。

URL 由几个部分组成。Python 使用`urllib.parse`模块处理 URL。让我们使用 Python 将 URL 分解为其组件部分：

```py
>>> from urllib.parse import urlparse
>>> result = urlparse('http://www.python.org/dev/peps')
>>> result
ParseResult(scheme='http', netloc='www.python.org', path='/dev/peps', params='', query='', fragment='')

```

`urllib.parse.urlparse()`函数解释我们的 URL 并将`http`识别为**方案**、[https://www.python.org/](https://www.python.org/) 作为**网络位置**，`/dev/peps`作为**路径**。我们可以访问这些组件作为`ParseResult`的属性：

```py
>>> result.netloc
'www.python.org'
>>> result.path
'/dev/peps'

```

对于网络上的几乎所有资源，我们将使用`http`或`https`方案。在这些方案中，要定位一个特定的资源，我们需要知道它所在的主机和我们应该连接的 TCP 端口（这些是`netloc`组件），我们还需要知道主机上资源的路径（`path`组件）。

通过将端口号附加到主机，可以在 URL 中显式指定端口号。它们通过冒号与主机分开。让我们看看当我们尝试使用`urlparse`时会发生什么。

```py
>>> urlparse('http://www.python.org:8080/')
ParseResult(scheme='http', netloc='www.python.org:8080', path='/', params='', query='', fragment='')

```

`urlparse`方法只是将其解释为 netloc 的一部分。这很好，因为这是`urllib.request.urlopen()`等处理程序希望格式化的方式。

如果我们不提供端口（通常情况下），那么默认端口 80 用于`http`，默认端口 443 用于`https`。这通常是我们想要的，因为它们分别是 HTTP 和 HTTPS 协议的标准端口。

## 路径和相对 URL

URL 中的路径是主机和端口之后的任何内容。路径总是以正斜杠（`/`开头，当斜杠单独出现时，它被称为**根**。我们可以通过执行以下操作来了解这一点：

```py
>>> urlparse('http://www.python.org/')
ParseResult(scheme='http', netloc='www.python.org', path='/', params='', query='', fragment='')

```

如果请求中没有提供路径，则默认情况下 `urllib`将发送根目录的请求。

当方案和主机包含在 URL 中时（如上例所示），该 URL 称为**绝对 URL**。相反，也可以有**相对 URL**s，其中包含只是一个路径组件，如下所示：

```py
>>> urlparse('img/tux.png')
ParseResult(scheme='', netloc='', path='img/tux.png', params='', query='', fragment='')

```

我们可以看到，`ParseResult`只包含一个`path.`，如果我们想使用相对 URL 来请求资源，那么我们需要提供缺少的方案、主机和基本路径。

通常，我们会在已经从 URL 检索到的资源中遇到相对 URL。因此，我们可以使用此资源的 URL 来填充缺少的组件。让我们看一个例子。

假设我们检索到了[http://www.debian.org](http://www.debian.org) URL，在网页源代码中，我们找到了“关于”页面的相对 URL。我们发现它是`intro/about`的相对 URL。

我们可以使用原始页面的 URL 和`urllib.parse.urljoin()`函数创建一个绝对 URL。让我们看看如何做到这一点：

```py
>>> from urllib.parse import urljoin
>>> urljoin('http://www.debian.org', 'intro/about')
'http://www.debian.org/intro/about'

```

通过向`urljoin`提供一个基本 URL 和一个相对 URL，我们创建了一个新的绝对 URL。

这里，请注意是如何填充主机和路径之间的斜线的。`urljoin`为我们填充斜杠的唯一时间是基本 URL 没有路径时，如前一示例所示。让我们看看如果基本 URL 确实有路径会发生什么。

```py
>>> urljoin('http://www.debian.org/intro/', 'about')
'http://www.debian.org/intro/about'
>>> urljoin('http://www.debian.org/intro', 'about')
'http://www.debian.org/about'

```

这将给我们不同的结果。请注意，如果基本 URL 以斜杠结尾，`urljoin`将如何附加到路径，但如果基本 URL 不以斜杠结尾，它将替换基本 URL 中的最后一个 path 元素。

我们可以通过在基本 URL 前面加一个斜杠来强制路径替换它的所有元素。请执行以下操作：

```py
>>> urljoin('http://www.debian.org/intro/about', '/News')
'http://www.debian.org/News'

```

导航到父目录怎么样？让我们试试标准的点语法，如下所示：

```py
>>> urljoin('http://www.debian.org/intro/about/', '../News')
'http://www.debian.org/intro/News'
>>> urljoin('http://www.debian.org/intro/about/', '../../News')
'http://www.debian.org/News'
>>> urljoin('http://www.debian.org/intro/about', '../News')
'http://www.debian.org/News'

```

它按我们预期的那样工作。请注意具有和不具有尾部斜杠的基本 URL 之间的差异。

最后，如果“相对”URL 实际上是一个绝对 URL，该怎么办

```py
>>> urljoin('http://www.debian.org/about', 'http://www.python.org')
'http://www.python.org'

```

相对 URL 完全替换基本 URL。这很方便，因为这意味着在将 URL 与`urljoin`一起使用之前，我们不需要担心测试 URL 是否是相对的。

## 查询字符串

RFC 3986 定义了 URL 的另一个属性。它们可以包含路径后面出现的键/值对形式的附加参数。它们与路径之间用问号隔开，如下所示：

[http://docs.python.org/3/search.html?q=urlparse &面积=默认值](http://docs.python.org/3/search.html?q=urlparse&area=default)

此参数字符串称为查询字符串。多个参数由符号（`&`）分隔。让我们看看`urlparse`是如何处理的：

```py
>>> urlparse('http://docs.python.org/3/search.html? q=urlparse&area=default')
ParseResult(scheme='http', netloc='docs.python.org', path='/3/search.html', params='', query='q=urlparse&area=default', fragment='')

```

因此，`urlparse`将查询字符串识别为`query`组件。

查询字符串用于向我们希望检索的资源提供参数，这通常以某种方式自定义资源。在前面的示例中，我们的查询字符串告诉 Python 文档搜索页面，我们希望对术语`urlparse`进行搜索。

`urllib.parse`模块有一个功能，可以帮助我们将`urlparse`返回的`query`组件转换成更有用的东西：

```py
>>> from urllib.parse import parse_qs
>>> result = urlparse ('http://docs.python.org/3/search.html?q=urlparse&area=default')
>>> parse_qs(result.query)
{'area': ['default'], 'q': ['urlparse']}

```

`parse_qs()`方法读取查询字符串，然后将其转换为字典。看看字典值实际上是如何以列表的形式出现的？这是因为参数可以在查询字符串中出现多次。请使用重复的参数进行尝试：

```py
>>> result = urlparse ('http://docs.python.org/3/search.html?q=urlparse&q=urljoin')
>>> parse_qs(result.query)
{'q': ['urlparse', 'urljoin']}

```

查看如何将这两个值添加到列表中？由服务器决定如何解释这一点。如果我们发送这个查询字符串，那么它可能只选择一个值并使用它，而忽略重复。你只能试试，看看会发生什么。

通常，您可以通过使用 web 浏览器通过 web 界面提交查询，并检查结果页面的 URL，来确定需要在给定页面的查询字符串中输入什么内容。您应该能够发现搜索文本，从而推断出搜索文本的对应键。通常，查询字符串中的许多其他参数实际上并不需要获得基本结果。尝试仅使用 search text 参数请求页面，然后查看结果。然后，如果它不能按预期工作，则添加其他参数。

如果您向页面提交表单，并且结果页面的 URL 没有查询字符串，那么页面将使用不同的方法发送表单数据。在讨论 POST 方法的同时，我们将在下面的*HTTP 方法*一节中介绍这一点。

## URL 编码

URL 仅限于 ASCII 字符，在此集合中，许多字符是保留的，需要在 URL 的不同组件中转义。我们使用一种叫做 URL 编码的方法来逃避它们。它通常被称为**百分比编码**，因为它使用百分比符号作为转义字符。让我们对一个字符串进行 URL 编码：

```py
>>> from urllib.parse import quote
>>> quote('A duck?')
'A%20duck%3F'

```

特殊字符`' '`和`?`已被转义序列替换。转义序列中的数字是字符的十六进制 ASCII 码。

RFC 3986 中给出了保留字符需要转义的完整规则，但是`urllib`为我们提供了一些方法来帮助我们构建 URL。这意味着我们不需要记住所有这些！

我们只需要：

*   URL 编码路径
*   URL 编码查询字符串
*   使用`urllib.parse.urlunparse()`功能组合它们

让我们看看如何在代码中使用上述步骤。首先，我们对路径进行编码：

```py
>>> path = 'pypi'
>>> path_enc = quote(path)

```

然后，我们对查询字符串进行编码：

```py
>>> from urllib.parse import urlencode
>>> query_dict = {':action': 'search', 'term': 'Are you quite sure this is a cheese shop?'}
>>> query_enc = urlencode(query_dict)
>>> query_enc
'%3Aaction=search&term=Are+you+quite+sure+this+is+a+cheese+shop%3F'

```

最后，我们将所有内容组成一个 URL：

```py
>>> from urllib.parse import urlunparse
>>> netloc = 'pypi.python.org'
>>> urlunparse(('http', netloc, path_enc, '', query_enc, ''))
'http://pypi.python.org/pypi?%3Aaction=search&term=Are+you+quite+sure +this+is+a+cheese+shop%3F'

```

已针对特定的编码路径设置了`quote()`功能。默认情况下，它会忽略斜杠字符，并且不会对其进行编码。在前面的示例中，这一点并不明显，请尝试以下操作以了解其工作原理：

```py
>>> from urllib.parse import quote
>>> path =img/users/+Zoot+/'
>>> quote(path)
'/img/users/%2BZoot%2B/'

```

请注意，它忽略斜杠，但避开了`+`。这是完美的路径。

`urlencode()`函数同样用于直接从 DICT 对查询字符串进行编码。请注意它是如何正确地对我们的值进行百分比编码，然后将它们与`&`连接起来，从而构造查询字符串的。

最后，`urlunparse()`方法需要一个 6 元组，其中包含与`urlparse()`结果匹配的元素，因此需要两个空字符串。

路径编码有一个警告。如果路径元素本身包含斜杠，那么我们可能会遇到问题。该示例显示在以下命令中：

```py
>>> username = '+Zoot/Dingo+'
>>> path = 'img/users/{}'.format(username)
>>> quote(path)
'img/user/%2BZoot/Dingo%2B'

```

注意用户名中的斜杠怎么没有转义？这将被错误地解释为额外的目录结构级别，这不是我们想要的。为了避免这种情况，首先我们需要单独转义可能包含斜杠的任何 path 元素，然后手动将它们连接起来：

```py
>>> username = '+Zoot/Dingo+'
>>> user_encoded = quote(username, safe='')
>>> path = '/'.join(('', 'images', 'users', username))
'/img/users/%2BZoot%2FDingo%2B'

```

注意用户名斜杠现在是如何百分比编码的吗？我们分别对用户名进行编码，通过提供`safe=''`参数告知`quote`不要忽略斜杠，这将覆盖其默认忽略列表`/`。然后，我们使用一个简单的`join()`函数组合路径元素。

这里值得一提的是，通过网络发送的主机名必须严格使用 ASCII 码，但是，`socket`和`http`模块支持将 Unicode 主机名透明编码为 ASCII 兼容编码，因此在实践中我们不需要担心主机名的编码问题。在`codecs`模块文档的`encodings.idna`部分中有关于此过程的更多详细信息。

## URL 摘要

在前面的章节中，我们使用了很多函数。让我们回顾一下每个函数的用途。所有这些功能都可以在`urllib.parse`模块中找到。详情如下:

*   将 URL 拆分为其组件：`urlparse`
*   将绝对 URL 与相对 URL 组合：`urljoin`
*   将查询字符串解析为`dict`：`parse_qs`
*   URL 编码路径：`quote`
*   从`dict`：`urlencode`创建 URL 编码的查询字符串
*   从组件创建 URL（与`urlparse`相反）：`urlunparse`

# HTTP 方法

到目前为止，我们一直在使用请求请求服务器向我们发送 web 资源，但 HTTP 提供了更多我们可以执行的操作。我们请求行中的`GET`是一个 HTTP**方法**，有几种方法，比如`HEAD`、`POST`、`OPTION`、`PUT`、`DELETE`、`TRACE`、`CONNECT`、`PATCH`。

我们将在下一章中详细介绍其中的一些方法，但是有两种方法，我们现在将快速介绍。

## 头法

`HEAD`方法与`GET`方法相同。唯一的区别是服务器永远不会在响应中包含主体，即使请求的 URL 中存在有效的资源。`HEAD`方法用于检查资源是否存在或是否已更改。请注意，有些服务器没有实现这种方法，但当它们实现时，可以证明这是一个巨大的带宽节约。

我们在`urllib`中使用替代方法，在创建`Request`对象时为其提供方法名称：

```py
>>> req = Request('http://www.google.com', method='HEAD')
>>> response = urlopen(req)
>>> response.status
200
>>> response.read()
b''

```

在这里，服务器返回了一个`200 OK`响应，但正如预期的那样，主体是空的。

## POST 方法

`POST`方法在某种意义上与`GET`方法相反。我们使用方法向服务器发送数据。但是，作为回报，服务器仍然可以向我们发送完整响应。`POST`方法用于从 HTML 表单提交用户输入，并将文件上载到服务器。

使用`POST`时，我们希望发送的数据将进入请求正文。我们可以将任何字节的数据放入其中，并通过使用适当的 MIME 类型向请求添加`Content-Type`头来声明其类型。

让我们看一个使用 POST 请求将一些 HTML 表单数据发送到服务器的示例，就像浏览器在网站上提交表单时所做的那样。表单数据总是由键/值对组成；`urllib`让我们使用常规词典来提供这些数据（我们将在下一节中了解这些数据的来源）：

```py
>>> data_dict = {'P': 'Python'}

```

发布 HTML 表单数据时，表单值的格式必须与**查询字符串**在 URL 中的格式相同，并且必须是 URL 编码的。`Content-Type`头还必须设置为特殊 MIME 类型`application/x-www-form-urlencoded`。

由于此格式与 QueryString 相同，我们只需使用`dict`上的`urlencode()`函数来准备数据：

```py
>>> data = urlencode(data_dict).encode('utf-8')

```

在这里，我们还将结果额外编码为字节，因为它将作为请求主体发送。在本例中，我们使用 UTF-8 字符集。

接下来，我们将构建我们的请求：

```py
>>> req = Request('http://search.debian.org/cgi-bin/omega', data=data)

```

通过添加数据作为`data`关键字参数，我们告诉`urllib`我们希望数据作为请求主体发送。这将使请求使用`POST`方法而不是`GET method`方法。

接下来，我们添加`Content-Type`标题：

```py
>>> req.add_header('Content-Type', 'application/x-www-form-urlencode;  charset=UTF-8')

```

最后，我们提出以下请求：

```py
>>> response = urlopen(req)

```

如果我们将响应数据保存到一个文件中并在 web 浏览器中打开它，那么我们应该会看到一些与 Python 相关的 Debian 网站搜索结果。

# 正式检查

在上一节中，我们使用了 URL`http://search.debian.org/cgibin/omega`和字典`data_dict = {'P': 'Python'}`。但是这些是从哪里来的呢？

我们通过访问包含表单的网页来获取这些信息，我们将提交表单以手动获取结果。然后，我们检查网页的 HTML 源代码。如果我们在网络浏览器中执行上述搜索，那么我们很可能在[上 http://www.debian.org](http://www.debian.org) 页面，我们将通过在右上角的搜索框中键入搜索词，然后单击**搜索**来运行搜索。

大多数现代浏览器允许您直接检查页面上任何元素的源代码。要执行此操作，右键单击元素（本例中为搜索框），然后选择**检查元素**选项，如此处屏幕截图所示：

![Formal inspection](img/6008OS_02_01.jpg)

源代码将在窗口的某个部分弹出。在前面的屏幕截图中，它位于屏幕的左下角。在这里，您将看到一些类似以下示例的代码行：

```py
<form action="http://search.debian.org/cgi-bin/omega"
method="get" name="P">
  <p>
    <input type="hidden" value="en" name="DB"></input>
    <input size="27" value="" name="P"></input>
    <input type="submit" value="Search"></input>
  </p>
</form>
```

您应该看到第二个`<input>`突出显示。这是与搜索文本框对应的标记。突出显示的`<input>`标记上的`name`属性的值是我们在`data_dict`中使用的键，在本例中是`P`。我们的`data_dict`中的值是我们想要搜索的术语。

要获取 URL，我们需要在突出显示的`<input>`上方查找所附的`<form>`标记。在这里，我们的 URL 将具有`action`属性[的值 http://search.debian.org/cgi-bin/omega](http://search.debian.org/cgi-bin/omega) 。本网页的源代码包含在本书的源代码下载中，以防 Debian 在阅读本书之前更改其网站。

此过程可应用于大多数 HTML 页面。为此，请查找与输入文本框对应的`<input>`，然后从所附的`<form>`标记中找到 URL。如果您不熟悉 HTML，那么这可能是一个尝试和错误的过程。在下一章中，我们将介绍更多解析 HTML 的方法。

一旦有了输入名称和 URL，我们就可以构建并提交 POST 请求，如前一节所示。

# HTTPS

除非另有保护，否则所有 HTTP 请求和响应均以明文形式发送。任何人只要能够访问信息所经过的网络，就有可能拦截我们的流量，并毫无阻碍地阅读。

由于网络被用于传输大量敏感数据，因此已经创建了防止窃听者读取流量的解决方案，即使他们能够拦截流量。这些解决方案大多采用某种形式的加密。

加密 HTTP 流量的标准方法称为 HTTP 安全，或**HTTPS**。它使用了一种称为 TLS/SSL 的加密机制，并应用于 HTTP 流量所经过的 TCP 连接。HTTPS 通常使用 TCP 端口 443，而不是默认的 HTTP 端口 80。

对大多数用户来说，这个过程几乎是透明的。原则上，我们只需要将 URL 中的 http 更改为 https。由于`urllib`支持 HTTPS，因此我们的 Python 客户端也是如此。

请注意，并非所有服务器都支持 HTTPS，因此简单地将 URL 方案更改为`https:`并不保证适用于所有站点。如果是这种情况，那么连接尝试可能会以多种方式失败，包括套接字超时、连接重置错误，甚至可能是 HTTP 错误，例如 400 范围错误或 500 范围错误。然而，越来越多的站点正在启用 HTTPS。许多其他人正在切换到它并使用它作为他们的默认协议，因此值得研究它是否可用，以便为应用的用户提供额外的安全性。

# 请求库

这就是`urllib`包的。如您所见，对标准库的访问对于大多数 HTTP 任务来说已经足够了。我们还没有提到它的所有功能。有许多我们没有讨论过的处理程序类，加上 opener 接口是可扩展的。

然而，API 并不是最优雅的，已经有过几次尝试来改进它。其中一个是非常流行的第三方库**请求**。它在 PyPi 上作为`requests`包提供。可以通过 Pip 安装，也可以从[下载 http://docs.python-requests.org](http://docs.python-requests.org) ，它承载文档。

`Requests`库自动化并简化了我们一直在研究的许多任务。说明这一点的最快方法是尝试一些例子。

使用`Requests`检索 URL 的命令类似于使用`urllib`包检索 URL，如下所示：

```py
>>> import requests
>>> response = requests.get('http://www.debian.org')

```

我们可以看看响应对象的属性。尝试：

```py
>>> response.status_code
200
>>> response.reason
'OK'
>>> response.url
'http://www.debian.org/'
>>> response.headers['content-type']
'text/html'

```

请注意，前面命令中的头名称是小写的。`Requests`响应对象的`headers`属性中的键不区分大小写。

响应对象中添加了一些便利属性：

```py
>>> response.ok
True

```

`ok`属性表示请求是否成功。也就是说，请求包含 200 范围内的状态代码。也：

```py
>>> response.is_redirect
False

```

`is_redirect`属性表示请求是否被重定向。我们还可以通过响应对象访问请求属性：

```py
>>> response.request.headers
{'User-Agent': 'python-requests/2.3.0 CPython/3.4.1 Linux/3.2.0-4- amd64', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*'}

```

请注意，`Requests`为我们自动处理压缩。它包括`gzip`和`deflate`在`Accept-Encoding`标题中。如果我们查看`Content-Encoding`响应，那么我们将看到响应实际上是`gzip`压缩的，并且`Requests`为我们透明地解压缩了它：

```py
>>> response.headers['content-encoding']
'gzip'

```

我们可以用更多的方式查看响应内容。要获得与我们从`HTTPResponse`对象获得的相同字节对象，请执行以下操作：

```py
>>> response.content
b'<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">\n<html lang="en">...

```

但`Requests`也为我们执行自动解码。要获取解码内容，请执行以下操作：

```py
>>> response.text
'<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">\n<html lang="en">\n<head>\n
...

```

注意现在是`str`而不是`bytes`。`Requests`库使用标题中的值为我们选择字符集并将内容解码为 Unicode。如果无法从标题中获取字符集，则使用`chardet`库（[http://pypi.python.org/pypi/chardet](http://pypi.python.org/pypi/chardet) 从内容本身进行估算。我们可以在这里看到`Requests`选择了什么编码：

```py
>>> response.encoding
'ISO-8859-1'

```

我们甚至可以要求它更改使用的编码：

```py
>>> response.encoding = 'utf-8'

```

更改编码后，后续对此响应的`text`属性的引用将返回使用新编码设置解码的内容。

`Requests`库自动处理 cookies。尝试以下方法：

```py
>>> response = requests.get('http://www.github.com')
>>> print(response.cookies)
<<class 'requests.cookies.RequestsCookieJar'>
[<Cookie logged_in=no for .github.com/>,
 <Cookie _gh_sess=eyJzZxNz... for ..github.com/>]>

```

`Requests`库还有一个`Session`类，允许重用 cookie，这类似于使用`http`模块的`CookieJar`和`urllib`模块的`HTTPCookieHandler`对象。执行以下操作以在后续请求中重用 cookie：

```py
>>> s = requests.Session()
>>> s.get('http://www.google.com')
>>> response = s.get('http://google.com/preferences')

```

`Session`对象与`requests`模块具有相同的接口，因此我们使用其`get()`方法的方式与使用`requests.get()method`的方式相同。现在，遇到的任何 cookie 都存储在`Session`对象中，当我们将来使用`get()`方法时，它们将与相应的请求一起发送。

重定向也会自动跟随，与使用`urllib`时的方式相同，任何重定向请求都会在`history`属性中捕获。

不同的 HTTP 方法很容易访问，它们有自己的功能：

```py
>>> response = requests.head('http://www.google.com')
>>> response.status_code
200
>>> response.text
''

```

自定义头以与使用`urllib`时类似的方式添加到请求中：

```py
>>> headers = {'User-Agent': 'Mozilla/5.0 Firefox 24'}
>>> response = requests.get('http://www.debian.org', headers=headers)

```

使用查询字符串发出请求是一个简单的过程：

```py
>>> params = {':action': 'search', 'term': 'Are you quite sure this is a cheese shop?'}
>>> response = requests.get('http://pypi.python.org/pypi', params=params)
>>> response.url
'https://pypi.python.org/pypi?%3Aaction=search&term=Are+you+quite+sur e+this+is+a+cheese+shop%3F'

```

`Requests`库为我们处理所有的编码和格式化。

尽管我们在这里使用了`data`关键字参数，发布也同样简化了：

```py
>>> data = {'P', 'Python'}
>>> response = requests.post('http://search.debian.org/cgi- bin/omega', data=data)

```

## 处理请求错误

`Requests`中的错误处理方式与`urllib`中的错误处理方式略有不同。让我们研究一些错误条件，看看它是如何工作的。通过执行以下操作生成 404 错误：

```py
>>> response = requests.get('http://www.google.com/notawebpage')
>>> response.status_code
404

```

在这种情况下，`urllib`可能会引发异常，但请注意`Requests`不会。`Requests`库可以检查状态码并引发相应的异常，但我们必须要求它这样做：

```py
>>> response.raise_for_status()
...
requests.exceptions.HTTPError: 404 Client Error

```

现在，请在成功请求时尝试：

```py
>>> r = requests.get('http://www.google.com')
>>> r.status_code
200
>>> r.raise_for_status()
None

```

它不做任何事情，在大多数情况下，它会让我们的程序退出一个`try/except`块，然后按照我们希望的方式继续。

如果我们得到协议栈中较低的错误，会发生什么？请尝试以下操作：

```py
>>> r = requests.get('http://192.0.2.1')
...
requests.exceptions.ConnectionError: HTTPConnectionPool(...

```

我们请求了一个不存在的主机，一旦它超时，我们会得到一个`ConnectionError`异常。

与`urllib`相比，`Requests`库简单地减少了在 Python 中使用 HTTP 所涉及的工作量。除非您有使用`urllib`的要求，否则我始终建议您在项目中使用`Requests`。

# 总结

我们研究了 HTTP 协议的原理。我们了解了如何使用标准库`urllib`和第三方`Requests`包执行许多基本任务。

我们研究了 HTTP 消息的结构、HTTP 状态码、我们在请求和响应中可能遇到的不同头，以及如何解释它们并使用它们来定制我们的请求。我们研究了 URL 是如何形成的，以及如何操作和构造它们。

我们了解了如何处理 cookie 和重定向，如何处理可能发生的错误，以及如何使用安全的 HTTP 连接。

我们还介绍了如何以在网页上提交表单的方式向网站提交数据，以及如何从网页的源代码中提取所需的参数。

最后，我们看了第三方`Requests`包。我们看到，与`urllib`包相比，`Requests`自动化并简化了许多我们通常需要使用 HTTP 执行的任务。这使得它成为日常 HTTP 工作的最佳选择。

在下一章中，我们将利用我们在这里学到的知识与不同的 web 服务进行详细的交互，查询 API 以获取数据，并将我们自己的对象上载到 web。