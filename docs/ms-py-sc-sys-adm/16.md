# 十六、网页抓取——从网站中提取有用数据

在本章中，您将学习有关网页抓取的知识。您还将了解 Python 中的`beautifulsoup`库，该库用于从网站中提取信息。

在本章中，我们将介绍以下主题：

*   什么是网页抓取？
*   数据提取
*   从维基百科中提取信息

# 什么是网页抓取？

网页抓取是一种从网站中提取信息的技术。该技术用于将非结构化数据转换为结构化数据。

网页抓取的用途是从网站中提取数据。提取的信息保存为系统上的本地文件，您也可以将其以表格格式存储到数据库中。刮网软件通过 HTTP 或 web 浏览器直接访问**万维网**（**WWW**）。这是一个使用网络爬虫或机器人实现的自动化过程。

抓取网页包括抓取网页，然后提取数据。网络爬虫获取网页。网络爬虫是网络抓取中的必备组件。提取后，将进行提取。您可以搜索、解析、将数据保存到表中以及重新格式化页面。

# 数据提取

在本节中，我们将看到实际的数据提取过程。Python 有`beautifulsoup`库来执行数据提取任务。我们还将使用 Python 的请求库。

首先，我们必须安装这两个库。运行以下命令安装`requests`和`beautifulsoup`库：

```py
$ pip3 install requests $ pip3 install beautifulsoup4
```

# 请求图书馆

`requests`库的用途是在 Python 脚本中以人类可读的格式使用 HTTP。我们可以使用 Python 中的`requests`库下载页面。`requests`库有不同类型的请求。在这里，我们将了解`GET`请求。`GET`请求用于从 web 服务器检索信息。`GET`请求下载指定网页的 HTML 内容。每个请求都有一个状态代码。我们向服务器发出的每个请求都会返回状态代码。这些状态代码为我们提供了有关请求发生了什么的信息。此处列出了状态代码的类型：

*   `200`：表示一切正常并返回结果（如果有）
*   `301`：如果服务器已切换域名或必须更改端点名称，则表示服务器正在重定向到其他端点
*   `400`：表示您的请求不正确
*   `401`：表示我们未通过身份验证
*   `403`：表示您正在尝试访问被禁止的资源
*   `404`：表示您试图访问的资源在服务器上不可用

# 美丽的乌苏图书馆

`beautifulsoup`是 Python 中的一个库，用于 web 抓取。它有搜索、导航和修改的简单方法。它只是一个用于从网页中提取所需数据的工具包。

现在，要在脚本中使用`requests`和`beautifulsoup`功能，必须使用`import`语句导入这两个库。现在，我们将看到一个解析网页的示例。在这里，我们将解析一个网页，它是来自 IMDb 网站的顶级新闻页面。为此，创建一个`parse_web_page.py`脚本并在其中写入以下内容：

```py
import requests from bs4 import BeautifulSoup page_result = requests.get('https://www.imdb.com/news/top?ref_=nv_nw_tp') parse_obj = BeautifulSoup(page_result.content, 'html.parser') print(parse_obj)
```

运行脚本，您将获得如下输出：

```py
student@ubuntu:~/work$ python3 parse_web_page.py Output: <!DOCTYPE html> <html xmlns:fb="http://www.facebook.com/2008/fbml" xmlns:og="http://ogp.me/ns#"> <head> <meta charset="utf-8"/> <meta content="IE=edge" http-equiv="X-UA-Compatible"/> <meta content="app-id=342792525, app-argument=imdb:///?src=mdot" name="apple-itunes-app"/> <script type="text/javascript">var IMDbTimer={starttime: new Date().getTime(),pt:'java'};</script> <script>
 if (typeof uet == 'function') { uet("bb", "LoadTitle", {wb: 1}); } </script> <script>(function(t){ (t.events = t.events || {})["csm_head_pre_title"] = new Date().getTime(); })(IMDbTimer);</script> <title>Top News - IMDb</title> <script>(function(t){ (t.events = t.events || {})["csm_head_post_title"] = new Date().getTime(); })(IMDbTimer);</script> <script>
 if (typeof uet == 'function') { uet("be", "LoadTitle", {wb: 1}); } </script> <script>
 if (typeof uex == 'function') { uex("ld", "LoadTitle", {wb: 1}); } </script> <link href="https://www.imdb.com/news/top" rel="canonical"/> <meta content="http://www.imdb.com/news/top" property="og:url"> <script>
 if (typeof uet == 'function') { uet("bb", "LoadIcons", {wb: 1}); }
```

在前面的示例中，我们收集了一个页面，并使用`beautifulsoup`对其进行解析。首先，我们导入了`requests`和`beautifulsoup`模块。然后，我们使用`GET`请求收集 URL，并将该 URL 分配给`page_result`变量。接下来，我们创建了一个`beautifulsoup`对象`parse_obj`。此对象将使用`page_result`.content 作为来自请求的参数，然后使用`html.parser`解析页面。

现在，我们将从类和标记中提取内容。要执行此操作，请转到 web 浏览器，右键单击要提取的内容并向下滚动，以便查看**检查**选项。点击它，你会得到类名。在程序中提到它并运行脚本。为此，创建一个`extract_from_class.py`脚本，并在其中写入以下内容：

```py
import requests from bs4 import BeautifulSoup page_result = requests.get('https://www.imdb.com/news/top?ref_=nv_nw_tp') parse_obj = BeautifulSoup(page_result.content, 'html.parser') top_news = parse_obj.find(class_='news-article__content') print(top_news)
```

运行脚本，您将获得以下输出：

```py
student@ubuntu:~/work$ python3 extract_from_class.py Output : <div class="news-article__content"> <a href="/name/nm4793987/">Issa Rae</a> and <a href="/name/nm0000368/">Laura Dern</a> are teaming up to star in a limited series called “The Dolls” currently in development at <a href="/company/co0700043/">HBO</a>.<br/><br/>Inspired by true events, the series recounts the aftermath of Christmas Eve riots in two small Arkansas towns in 1983, riots which erupted over Cabbage Patch Dolls. The series explores class, race, privilege and what it takes to be a “good mother.”<br/><br/>Rae will serve as a writer and executive producer on the series in addition to starring, with Dern also executive producing. <a href="/name/nm3308450/">Laura Kittrell</a> and <a href="/name/nm4276354/">Amy Aniobi</a> will also serve as writers and co-executive producers. <a href="/name/nm0501536/">Jayme Lemons</a> of Dern’s <a href="/company/co0641481/">Jaywalker Pictures</a> and <a href="/name/nm3973260/">Deniese Davis</a> of <a href="/company/co0363033/">Issa Rae Productions</a> will also executive produce.<br/><br/>Both Rae and Dern currently star in HBO shows, with Dern appearing in the acclaimed drama “<a href="/title/tt3920596/">Big Little Lies</a>” and Rae starring in and having created the hit comedy “<a href="/title/tt5024912/">Insecure</a>.” Dern also recently starred in the film “<a href="/title/tt4015500/">The Tale</a>,
 </div>
```

在前面的示例中，我们首先导入了请求和`beautifulsoup`模块。然后，我们创建了一个请求对象并为其分配了一个 URL。接下来，我们创建了一个`beautifulsoup`对象`parse_obj`。此对象从请求中将`page_result.content`作为其参数，然后使用`html.parser`解析页面。接下来，我们使用 beautifulsoup 的`find()`方法从`'news-article__content'`类中获取内容。

现在，我们将看到一个从特定标记中提取内容的示例。在本例中，我们将从`<a>`标记中提取内容。创建一个`extract_from_tag.py`脚本，并在其中写入以下内容：

```py
import requests from bs4 import BeautifulSoup page_result = requests.get('https://www.imdb.com/news/top?ref_=nv_nw_tp') parse_obj = BeautifulSoup(page_result.content, 'html.parser') top_news = parse_obj.find(class_='news-article__content') top_news_a_content = top_news.find_all('a') print(top_news_a_content)
```

运行脚本，您将获得如下输出：

```py
student@ubuntu:~/work$ python3 extract_from_tag.py Output: [<a href="/name/nm4793987/">Issa Rae</a>, <a href="/name/nm0000368/">Laura Dern</a>, <a href="/company/co0700043/">HBO</a>, <a href="/name/nm3308450/">Laura Kittrell</a>, <a href="/name/nm4276354/">Amy Aniobi</a>, <a href="/name/nm0501536/">Jayme Lemons</a>, <a href="/company/co0641481/">Jaywalker Pictures</a>, <a href="/name/nm3973260/">Deniese Davis</a>, <a href="/company/co0363033/">Issa Rae Productions</a>, <a href="/title/tt3920596/">Big Little Lies</a>, <a href="/title/tt5024912/">Insecure</a>, <a href="/title/tt4015500/">The Tale</a>]
```

在前面的示例中，我们从`<a>`标记中提取内容。我们使用`find_all()`方法从`'news-article__content'`类中提取所有`<a>`标记内容。

# 从维基百科中提取信息

在本节中，我们将看到一个来自维基百科的舞蹈形式列表示例。我们将列出所有印度古典舞蹈。为此，创建一个`extract_from_wikipedia.py`脚本，并在其中写入以下内容：

```py
import requests from bs4 import BeautifulSoup page_result = requests.get('https://en.wikipedia.org/wiki/Portal:History') parse_obj = BeautifulSoup(page_result.content, 'html.parser') h_obj = parse_obj.find(class_='hlist noprint')
h_obj_a_content = h_obj.find_all('a') print(h_obj) print(h_obj_a_content)
```

运行脚本，您将获得以下输出：

```py
student@ubuntu:~/work$ python3 extract_from_wikipedia.py
Output:
<div class="hlist noprint" id="portals-browsebar" style="text-align: center;">
<dl><dt><a href="/wiki/Portal:Contents/Portals" title="Portal:Contents/Portals">Portal topics</a></dt>
<dd><a href="/wiki/Portal:Contents/Portals#Human_activities" title="Portal:Contents/Portals">Activities</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Culture_and_the_arts" title="Portal:Contents/Portals">Culture</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Geography_and_places" title="Portal:Contents/Portals">Geography</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Health_and_fitness" title="Portal:Contents/Portals">Health</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#History_and_events" title="Portal:Contents/Portals">History</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Mathematics_and_logic" title="Portal:Contents/Portals">Mathematics</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Natural_and_physical_sciences" title="Portal:Contents/Portals">Nature</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#People_and_self" title="Portal:Contents/Portals">People</a></dd>
In the preceding example, we extracted the content from Wikipedia. In this example also, we extracted the content from class as well as tag.
....
```

# 总结

在本章中，您了解了什么是网页抓取。我们了解了用于从网页提取数据的两个库。我们还从维基百科中提取了信息。

在下一章中，您将学习统计数据收集和报告。您将了解 NumPy 模块、数据可视化以及使用绘图、图形和图表显示数据。

# 问题

1.  什么是网络报废？
2.  什么是网络爬虫？
3.  你能在登录页面后面刮取数据吗？
4.  你能在推特上爬行吗？
5.  是否可以废弃 Java 脚本页面？如果是，如何进行？

# 进一步阅读

*   Urllib 文档：[https://docs.python.org/3/library/urllib.html](https://docs.python.org/3/library/urllib.html)
*   机械化：[https://mechanize.readthedocs.io/en/latest/](https://mechanize.readthedocs.io/en/latest/)
*   刮痕：[https://pypi.org/project/scrape/](https://pypi.org/project/scrape/)
*   刮痧：[https://doc.scrapy.org/en/latest/index.html](https://doc.scrapy.org/en/latest/index.html)