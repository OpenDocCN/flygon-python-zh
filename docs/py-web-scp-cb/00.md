# 零、前言

互联网包含大量的数据。这些数据既通过结构化 API 提供，也通过直接通过网站交付的内容提供。虽然 API 中的数据是高度结构化的，但网页中的信息通常是非结构化的，需要收集、提取和处理才能有价值。而收集数据只是旅程的开始，因为这些数据还必须被存储、挖掘，然后以增值的形式暴露给他人。

通过这本书，你将学习从网站收集各种形式信息所需的许多核心任务。我们将介绍如何收集数据，如何执行几种常见的数据操作（包括在本地和远程数据库中的存储），如何执行常见的基于媒体的任务，如将图像和视频转换为缩略图，如何使用 NTLK 清理非结构化数据，如何检查几种数据挖掘和可视化工具，最后是构建基于微服务的 scraper 和 API 的核心技能，它可以并且将在云上运行。

通过基于配方的方法，我们将学习独立的技术来解决特定的任务，这些任务不仅涉及到抓取，还涉及到数据操作和管理、数据挖掘、可视化、微服务、容器和云操作。这些食谱将以一种渐进和整体的方式培养技能，不仅教你如何执行爬取的基本原理，还将把你从爬取的结果带到通过云提供给他人的服务。我们将使用 Python、容器和云生态系统中的通用工具构建一个真正的 web-scraper-as-a-service。

# 这本书是给谁的

这本书是为那些想要学习使用抓取过程从网站提取数据，以及如何使用各种数据管理工具和云服务的人准备的。编码需要 Python 编程语言的基本技能。

本书还面向那些希望了解更大的检索、存储和搜索数据工具生态系统，以及使用现代工具和 Python 库创建数据 API 和云服务的人。您还可以使用 Docker 和 AmazonWeb 服务在云上打包和部署刮板。

# 这本书涵盖的内容

[第一章](01.html)*抓取入门*，介绍了几种抓取网页的概念和工具。我们将研究如何使用 requests、urllib、BeautifulSoup、Scrapy、PhantomJS 和 Selenium 等工具安装和执行基本任务。

[第 2 章](02.html)*数据采集与提取*基于对 HTML 结构的理解以及如何查找和提取嵌入式数据。我们将介绍 DOM 中的许多概念，以及如何使用 BeautifulSoup、XPath、LXML 和 CSS 选择器查找和提取数据。我们还简要介绍了如何使用 Unicode/UTF8。

[第三章](03.html)*处理数据*教您加载和操作多种格式的数据，然后如何将这些数据存储在各种数据存储中（S3、MySQL、PostgreSQL 和 ElasticSearch）。网页中的数据以各种格式表示，最常见的是 HTML、JSON、CSV、，和 XML，我们还将研究消息队列系统（主要是 AWS SQS）的使用，以帮助构建健壮的数据处理管道。

[第 4 章](04.html)*使用图像、音频和其他资产*检查检索多媒体项目、本地存储它们的方法，并执行多项任务，如 OCR、生成缩略图、制作网页截图、从视频中提取音频以及在 YouTube 播放列表中查找所有视频 URL。

[第 5 章](05.html)*刮伤–行为准则*涵盖了刮伤合法性涉及的几个概念，以及进行礼貌刮伤的实践。我们将研究处理 robots.txt 和站点地图的工具，以尊重 web 主机对可接受行为的期望。我们还将研究爬行的几个方面的控制，例如使用延迟、包含爬行的深度和长度、使用用户代理以及实现缓存以防止重复请求。

[第 6 章](06.html)*刮片挑战与解决方案*涵盖了编写一个健壮的刮片机所面临的许多挑战，以及如何处理许多场景。这些场景包括分页、重定向、登录表单、将爬虫程序保持在同一个域中、失败时重试请求以及处理 CAPTCHA。

[第 7 章](07.html)*文本整理与分析*探讨了各种工具，如使用 NLTK 进行自然语言处理，以及如何去除常见的干扰词和标点符号。我们通常需要处理网页的文本内容，以查找网页上的信息，这些信息是文本的一部分，既不是结构化/嵌入式数据，也不是多媒体。这需要使用各种概念和工具来清理和理解文本的知识

[第 8 章](08.html)*搜索、挖掘和可视化数据*涵盖了在 Web 上搜索数据、存储和组织数据以及从已识别的关系中得出结果的几种方法。我们将了解如何了解维基百科贡献者的地理位置，在 IMDB 上查找参与者之间的关系，以及在堆栈溢出上查找与特定技术匹配的作业。

[第 9 章](09.html)*创建一个简单的数据 API*，教我们如何创建一个作为服务的 scraper。我们将使用 Flask 为 scraper 创建一个 RESTAPI。我们将在这个 API 后面以服务的形式运行 scraper，并且能够提交请求来刮取特定页面，以便动态地查询来自刮取和本地 ElasticSearch 实例的数据。

[第 10 章](10.html)*通过 Docker*创建 Scraper 微服务，通过将服务和 API 打包到 Docker swarm 中，并通过消息队列系统（AWS SQS）跨 Scraper 分发请求，继续我们的 Scraper as a service 的发展。我们还将介绍如何使用 Docker swarm 工具上下扩展 scraper 实例。

[第 11 章](11.html)*使刮板成为真正的服务*，最后充实了前一章中包装的服务，添加了一个将前面介绍的各种概念结合在一起的刮板。此刮刀可以帮助分析 StackOverflow 上的工作岗位，以找到并比较使用特定技术的雇主。该服务将收集帖子，并允许查询来查找和比较这些公司。

# 充分利用这本书

本书中的食谱所需的主要工具是 Python3 解释器。这些配方是使用 Anaconda Python 发行版的免费版本编写的，特别是 3.6.1 版。其他 Python 版本 3 发行版应该可以很好地工作，但还没有经过测试。

配方中的代码通常需要使用各种 Python 库。这些都可以使用`pip`进行安装，也可以使用`pip install`进行访问。如果需要，这些装置将在配方中详细说明。

有几种配方需要亚马逊 AWS 帐户。AWS 帐户第一年可免费访问。食谱只需要免费的服务。可在[创建新账户 https://portal.aws.amazon.com/billing/signup](https://portal.aws.amazon.com/billing/signup) 。

一些食谱将利用 Elasticsearch。GitHub 上有一个免费的开源版本，网址为[https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch) ，该页有安装说明。Elastic.co 还提供了一个全功能版本（也有 Kibana 和 Logstash），在云上托管，在[提供 14 天免费试用 http://info.elastic.co](http://info.elastic.co) （我们将使用）。docker compose 有一个版本，在[上提供了所有 x-pack 功能 https://github.com/elastic/stack-docker](https://github.com/elastic/stack-docker) ，所有这些都可以通过简单的`docker-compose up`命令启动。

最后，一些配方使用 MySQL 和 PostgreSQL 作为数据库示例，并使用这些数据库的几个常见客户机。对于这些配方，需要在本地安装。MySQL 社区服务器在[提供 https://dev.mysql.com/downloads/mysql/](https://dev.mysql.com/downloads/mysql/) ，PostgreSQL 可在[找到 https://www.postgresql.org/](https://www.postgresql.org/) 。

我们还将研究如何为一些配方创建和使用 docker 容器。Docker CE 是免费的，可在[上获得 https://www.docker.com/community-edition](https://www.docker.com/community-edition) 。

# 下载示例代码文件

您可以从您的账户[www.packtpub.com](http://www.packtpub.com)下载本书的示例代码文件。如果您在其他地方购买了本书，您可以访问[www.packtpub.com/support](http://www.packtpub.com/support)并注册，将文件通过电子邮件直接发送给您。

您可以通过以下步骤下载代码文件：

1.  登录或注册[www.packtpub.com](http://www.packtpub.com/support)。
2.  选择“支持”选项卡。
3.  点击代码下载和勘误表。
4.  在搜索框中输入图书名称，然后按照屏幕上的说明进行操作。

下载文件后，请确保使用以下最新版本解压或解压缩文件夹：

*   WinRAR/7-Zip for Windows
*   适用于 Mac 的 Zipeg/iZip/UnRarX
*   适用于 Linux 的 7-Zip/PeaZip

该书的代码包也托管在 GitHub 上的[https://github.com/PacktPublishing/Python-Web-Scraping-Cookbook](https://github.com/PacktPublishing/Python-Web-Scraping-Cookbook) 。我们的丰富书籍和视频目录中还有其他代码包，请访问**[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)** 。看看他们！

# 使用的惯例

本书中使用了许多文本约定。

`CodeInText`：表示文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 句柄。下面是一个示例：“这将循环最多 20 个字符，并将它们放入文档类型为`people`的`sw`索引中”

代码块设置如下：

```py
from elasticsearch import Elasticsearch
import requests
import json

if __name__ == '__main__':
    es = Elasticsearch(
        [
```

任何命令行输入或输出的编写方式如下：

```py
$ curl https://elastic:tduhdExunhEWPjSuH73O6yLS@7dc72d3327076cc4daf5528103c46a27.us-west-2.aws.found.io:9243
```

**粗体**：表示一个新术语、一个重要单词或您在屏幕上看到的单词。例如，菜单或对话框中的单词出现在文本中，如下所示。下面是一个示例：“从管理面板中选择系统信息。”

Warnings or important notes appear like this. Tips and tricks appear like this.

# 联系

我们欢迎读者的反馈。

**一般反馈**：发送电子邮件`feedback@packtpub.com`并在邮件主题中提及书名。如果您对本书的任何方面有疑问，请发送电子邮件至`questions@packtpub.com`。

**勘误表**：尽管我们已尽一切努力确保内容的准确性，但还是会出现错误。如果您在本书中发现错误，如果您能向我们报告，我们将不胜感激。请访问[www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择您的书籍，点击 errata 提交表单链接，然后输入详细信息。

**盗版**：如果您在互联网上发现我们作品的任何形式的非法复制品，请您提供我们的位置地址或网站名称，我们将不胜感激。请通过`copyright@packtpub.com`与我们联系，并提供该材料的链接。

**如果您有兴趣成为一名作家**：如果您对某个主题有专业知识，并且您有兴趣撰写或贡献一本书，请访问[authors.packtpub.com](http://authors.packtpub.com/)。

# 评论

请留下评论。一旦你阅读并使用了这本书，为什么不在你购买它的网站上留下评论呢？然后，潜在读者可以看到并使用您的无偏见意见做出购买决定，我们 Packt 可以了解您对我们产品的看法，我们的作者可以看到您对他们书籍的反馈。非常感谢。

有关 Packt 的更多信息，请访问[packtpub.com](https://www.packtpub.com/)。