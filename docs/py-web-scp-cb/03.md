# 三、处理数据

在本章中，我们将介绍：

*   使用 CSV 和 JSON 数据
*   使用 AWS S3 存储数据
*   使用 MySQL 存储数据
*   使用 PostgreSQL 存储数据
*   使用 Elasticsearch 存储存储数据
*   如何使用 AWS SQS 构建健壮的 ETL 管道

# 介绍

在本章中，我们将介绍 JSON、CSV 和 XML 格式数据的使用。这将包括解析这些数据并将其转换为其他格式的方法，包括将这些数据存储在关系数据库、搜索引擎（如 Elasticsearch）和云存储（包括 AWS S3）中。我们还将讨论通过使用消息传递系统（包括 AWS Simple Queue Service（SQS））创建分布式和大规模刮取任务。目标是让您了解可能检索和需要解析的各种形式的数据，最后，我们将首先介绍 one 和 Amazon Web Service（AWS）产品。在本书结束时，我们将对 AWS 进行大量介绍，这是一个温和的介绍。

# 使用 CSV 和 JSON 数据

从 HTML 页面中提取数据是使用前一章中的技术完成的，主要是通过各种工具使用 XPath，也可以使用 BeautifulSoup。虽然我们将主要关注 HTML，但 HTML 是 XML（可扩展标记语言）的一个变体。XML 是最流行的在 web 上表达数据的语言之一，但其他语言已经变得流行，甚至超过了 XML

您将看到的两种常见格式是 JSON（JavaScript 对象表示法）和 CSV（逗号分隔值）。CSV 易于创建，并且是许多电子表格应用程序的常见格式，因此许多网站都以该格式提供数据，或者您需要将刮取的数据转换为该格式以进行进一步存储或协作。JSON 确实已经成为首选格式，因为它在 JavaScript（和 Python）等编程语言中易于使用，现在许多数据库都支持它作为本机数据格式。

在本食谱中，让我们研究如何将刮取的数据转换为 CSV 和 JSON，以及如何将数据写入文件，以及如何从远程服务器读取这些数据文件。我们将研究的工具是 Python CSV 和 JSON 库。我们还将对这些技术使用`pandas`进行检查。

Also implicit in these examples is the conversion of XML data to CSV and JSON, so we won't have a dedicated section for those examples.

# 准备

我们将使用 planets 数据页面，并将该数据转换为 CSV 和 JSON 文件。让我们首先将页面中的 planets 数据加载到 python 字典对象列表中。以下代码（可在（`03/get_planet_data.py`中找到）提供了执行此任务的功能，该功能将在本章中重复使用：

```py
import requests
from bs4 import BeautifulSoup

def get_planet_data():
   html = requests.get("http://localhost:8080/planets.html").text
   soup = BeautifulSoup(html, "lxml")

   planet_trs = soup.html.body.div.table.findAll("tr", {"class": "planet"})

   def to_dict(tr):
      tds = tr.findAll("td")
      planet_data = dict()
      planet_data['Name'] = tds[1].text.strip()
      planet_data['Mass'] = tds[2].text.strip()
      planet_data['Radius'] = tds[3].text.strip()
      planet_data['Description'] = tds[4].text.strip()
      planet_data['MoreInfo'] = tds[5].findAll("a")[0]["href"].strip()
      return planet_data

   planets = [to_dict(tr) for tr in planet_trs]

   return planets

if __name__ == "__main__":
   print(get_planet_data())
```

运行脚本会产生以下输出（短暂截断）：

```py
03 $python get_planet_data.py
[{'Name': 'Mercury', 'Mass': '0.330', 'Radius': '4879', 'Description': 'Named Mercurius by the Romans because it appears to move so swiftly.', 'MoreInfo': 'https://en.wikipedia.org/wiki/Mercury_(planet)'}, {'Name': 'Venus', 'Mass': '4.87', 'Radius': '12104', 'Description': 'Roman name for the goddess of love. This planet was considered to be the brightest and most beautiful planet or star in the\r\n heavens. Other civilizations have named it for their god or goddess of love/war.', 'MoreInfo': 'https://en.wikipedia.org/wiki/Venus'}, {'Name': 'Earth', 'Mass': '5.97', 'Radius': '12756', 'Description': "The name Earth comes from the Indo-European base 'er,'which produced the Germanic noun 'ertho,' and ultimately German 'erde,'\r\n Dutch 'aarde,' Scandinavian 'jord,' and English 'earth.' Related forms include Greek 'eraze,' meaning\r\n 'on the ground,' and Welsh 'erw,' meaning 'a piece of land.'", 'MoreInfo': 'https://en.wikipedia.org/wiki/Earth'}, {'Name': 'Mars', 'Mass': '0.642', 'Radius': '6792', 'Description': 'Named by the Romans for their god of war because of its red, bloodlike color. Other civilizations also named this planet\r\n from this attribute; for example, the Egyptians named it "Her Desher," meaning "the red one."', 'MoreInfo':
...
```

可能需要安装 csv、json 和 pandas。您可以使用以下三个命令：

```py
pip install csv
pip install json
pip install pandas
```

# 怎么做

我们首先将行星数据转换为 CSV 文件。

1.  这将使用`csv`执行。以下代码将行星数据写入 CSV 文件（代码在`03/create_csv.py`中）：

```py
import csv
from get_planet_data import get_planet_data

planets = get_planet_data()

with open('../../www/planets.csv', 'w+', newline='') as csvFile:
    writer = csv.writer(csvFile)
    writer.writerow(['Name', 'Mass', 'Radius', 'Description', 'MoreInfo'])
for planet in planets:
        writer.writerow([planet['Name'], planet['Mass'],planet['Radius'], planet['Description'], planet['MoreInfo']])

```

2.  输出文件被放入我们项目的 www 文件夹中。检查它可以看到以下内容：

```py
Name,Mass,Radius,Description,MoreInfo
Mercury,0.330,4879,Named Mercurius by the Romans because it appears to move so swiftly.,https://en.wikipedia.org/wiki/Mercury_(planet)
Venus,4.87,12104,Roman name for the goddess of love. This planet was considered to be the brightest and most beautiful planet or star in the heavens. Other civilizations have named it for their god or goddess of love/war.,https://en.wikipedia.org/wiki/Venus
Earth,5.97,12756,"The name Earth comes from the Indo-European base 'er,'which produced the Germanic noun 'ertho,' and ultimately German 'erde,' Dutch 'aarde,' Scandinavian 'jord,' and English 'earth.' Related forms include Greek 'eraze,' meaning 'on the ground,' and Welsh 'erw,' meaning 'a piece of land.'",https://en.wikipedia.org/wiki/Earth
Mars,0.642,6792,"Named by the Romans for their god of war because of its red, bloodlike color. Other civilizations also named this planet from this attribute; for example, the Egyptians named it ""Her Desher,"" meaning ""the red one.""",https://en.wikipedia.org/wiki/Mars
Jupiter,1898,142984,The largest and most massive of the planets was named Zeus by the Greeks and Jupiter by the Romans; he was the most important deity in both pantheons.,https://en.wikipedia.org/wiki/Jupiter
Saturn,568,120536,"Roman name for the Greek Cronos, father of Zeus/Jupiter. Other civilizations have given different names to Saturn, which is the farthest planet from Earth that can be observed by the naked human eye. Most of its satellites were named for Titans who, according to Greek mythology, were brothers and sisters of Saturn.",https://en.wikipedia.org/wiki/Saturn
Uranus,86.8,51118,"Several astronomers, including Flamsteed and Le Monnier, had observed Uranus earlier but had recorded it as a fixed star. Herschel tried unsuccessfully to name his discovery ""Georgian Sidus"" after George III; the planet was named by Johann Bode in 1781 after the ancient Greek deity of the sky Uranus, the father of Kronos (Saturn) and grandfather of Zeus (Jupiter).",https://en.wikipedia.org/wiki/Uranus
Neptune,102,49528,"Neptune was ""predicted"" by John Couch Adams and Urbain Le Verrier who, independently, were able to account for the irregularities in the motion of Uranus by correctly predicting the orbital elements of a trans- Uranian body. Using the predicted parameters of Le Verrier (Adams never published his predictions), Johann Galle observed the planet in 1846\. Galle wanted to name the planet for Le Verrier, but that was not acceptable to the international astronomical community. Instead, this planet is named for the Roman god of the sea.",https://en.wikipedia.org/wiki/Neptune
Pluto,0.0146,2370,"Pluto was discovered at Lowell Observatory in Flagstaff, AZ during a systematic search for a trans-Neptune planet predicted by Percival Lowell and William H. Pickering. Named after the Roman god of the underworld who was able to render himself invisible.",https://en.wikipedia.org/wiki/Pluto
```

我们将此文件写入 www 目录，以便使用 web 服务器下载。

3.  这些数据现在可以在支持 CSV 内容的应用程序中使用，例如 Excel：

![](img/a00f3815-56b8-4bfb-bcd7-e9dbd035caa9.png)

The File Opened in Excel

4.  也可以使用`csv`库从 web 服务器读取 CSV 数据，并首先使用`requests`检索内容。以下代码在`03/read_csv_from_web.py`中：

```py
import requests
import csv

planets_data = requests.get("http://localhost:8080/planets.csv").text
planets = planets_data.split('\n')
reader = csv.reader(planets, delimiter=',', quotechar='"')
lines = [line for line in reader][:-1]
for line in lines: print(line)
```

以下是输出的一部分

```py
['Name', 'Mass', 'Radius', 'Description', 'MoreInfo']
['Mercury', '0.330', '4879', 'Named Mercurius by the Romans because it appears to move so swiftly.', 'https://en.wikipedia.org/wiki/Mercury_(planet)']
['Venus', '4.87', '12104', 'Roman name for the goddess of love. This planet was considered to be the brightest and most beautiful planet or star in the heavens. Other civilizations have named it for their god or goddess of love/war.', 'https://en.wikipedia.org/wiki/Venus']
['Earth', '5.97', '12756', "The name Earth comes from the Indo-European base 'er,'which produced the Germanic noun 'ertho,' and ultimately German 'erde,' Dutch 'aarde,' Scandinavian 'jord,' and English 'earth.' Related forms include Greek 'eraze,' meaning 'on the ground,' and Welsh 'erw,' meaning 'a piece of land.'", 'https://en.wikipedia.org/wiki/Earth']
```

需要指出的一点是，CSV 编写器留下了一个尾随空格，如未处理，将添加一个空列表项。这是通过切片行来处理的：以下语句返回除最后一行之外的所有行：

`lines = [line for line in reader][:-1]`

5.  使用熊猫也可以很容易地做到这一点。以下内容将从已刮取的数据构造一个数据帧。代码在`03/create_df_planets.py`中：

```py
import pandas as pd
planets_df = pd.read_csv("http://localhost:8080/planets_pandas.csv", index_col='Name')
print(planets_df)
```

运行此命令将产生以下输出：

```py
                                               Description Mass Radius
Name 
Mercury Named Mercurius by the Romans because it appea...  0.330 4879
Venus   Roman name for the goddess of love. This plane...   4.87 12104
Earth   The name Earth comes from the Indo-European ba...   5.97 12756
Mars    Named by the Romans for their god of war becau...  0.642 6792
Jupiter The largest and most massive of the planets wa...   1898 142984
Saturn  Roman name for the Greek Cronos, father of Zeu...    568 120536
Uranus  Several astronomers, including Flamsteed and L...   86.8 51118
Neptune Neptune was "predicted" by John Couch Adams an...    102 49528
Pluto   Pluto was discovered at Lowell Observatory in ... 0.0146 2370
```

6.  只需调用`.to_csv()`（代码在`03/save_csv_pandas.py`中），即可将`DataFrame`保存到 CSV 文件中：

```py
import pandas as pd
from get_planet_data import get_planet_data

# construct a data from from the list
planets = get_planet_data()
planets_df = pd.DataFrame(planets).set_index('Name')
planets_df.to_csv("../../www/planets_pandas.csv")
```

7.  使用`pd.read_csv()`可以很容易地从`URL`读入 CSV 文件-不需要其他库。您可以使用`03/read_csv_via_pandas.py`中的代码：

```py
import pandas as pd
planets_df = pd.read_csv("http://localhost:8080/planets_pandas.csv", index_col='Name')
print(planets_df)
```

8.  将数据转换为 JSON 也很容易。使用 Python 对 JSON 的操作可以通过 Python`json`库来完成。该库可用于将 Python 对象与 JSON 进行转换。下面将行星列表转换为 JSON 并打印到控制台：将行星数据打印为 JSON（代码在`03/convert_to_json.py`中）：

```py
import json
from get_planet_data import get_planet_data
planets=get_planet_data()
print(json.dumps(planets, indent=4))
```

执行此脚本将生成以下输出（省略部分输出）：

```py
[
    {
        "Name": "Mercury",
        "Mass": "0.330",
        "Radius": "4879",
        "Description": "Named Mercurius by the Romans because it appears to move so swiftly.",
        "MoreInfo": "https://en.wikipedia.org/wiki/Mercury_(planet)"
    },
    {
        "Name": "Venus",
        "Mass": "4.87",
        "Radius": "12104",
        "Description": "Roman name for the goddess of love. This planet was considered to be the brightest and most beautiful planet or star in the heavens. Other civilizations have named it for their god or goddess of love/war.",
        "MoreInfo": "https://en.wikipedia.org/wiki/Venus"
    },
```

9.  这也可以用于轻松地将 JSON 保存到文件中（`03/save_as_json.py`：

```py
import json
from get_planet_data import get_planet_data
planets=get_planet_data()
with open('../../www/planets.json', 'w+') as jsonFile:
   json.dump(planets, jsonFile, indent=4)
```

10.  使用`!head -n 13 ../../www/planets.json`检查输出显示：

```py
[
    {
        "Name": "Mercury",
        "Mass": "0.330",
        "Radius": "4879",
        "Description": "Named Mercurius by the Romans because it appears to move so swiftly.",
        "MoreInfo": "https://en.wikipedia.org/wiki/Mercury_(planet)"
    },
    {
        "Name": "Venus",
        "Mass": "4.87",
        "Radius": "12104",
        "Description": "Roman name for the goddess of love. This planet was considered to be the brightest and most beautiful planet or star in the heavens. Other civilizations have named it for their god or goddess of love/war.",
```

11.  JSON 可以通过`requests`从 web 服务器读取并转换为 Python 对象（`03/read_http_json_requests.py`：

```py
import requests
import json

planets_request = requests.get("http://localhost:8080/planets.json")
print(json.loads(planets_request.text))
```

12.  pandas 还提供 JSON 功能以保存到 CSV（`03/save_json_pandas.py`）：

```py
import pandas as pd
from get_planet_data import get_planet_data

planets = get_planet_data()
planets_df = pd.DataFrame(planets).set_index('Name')
planets_df.reset_index().to_json("../../www/planets_pandas.json", orient='records')
```

不幸的是，目前还没有一种方法可以漂亮地打印从`.to_json()`输出的 JSON。还要注意`orient='records'`和`rest_index()`的用法。这对于将相同的 JSON 结构复制到使用 JSON 库示例编写的 JSON 是必要的。

13.  JSON 可以使用`.read_json()`读入数据帧，也可以从 HTTP 和文件（`03/read_json_http_pandas.py`读入）：

```py
import pandas as pd
planets_df = pd.read_json("http://localhost:8080/planets_pandas.json").set_index('Name')
print(planets_df)
```

# 工作原理

`csv`和`json`库是 Python 的标准部分，提供了以两种格式读取和写入数据的简单方法。

pandas 在某些 Python 发行版中不是标准版本，您可能需要安装它。CSV 和 JSON 的 pandas 函数也具有更高的操作级别，有许多强大的数据操作可用，还支持从远程服务器访问数据。

# 还有更多。。。

csv、json 或 pandas 库由您选择，但我倾向于喜欢 pandas，我们将在本书中更多地研究它的使用，尽管我们不会深入讨论它的使用。

要深入了解熊猫，请查看`pandas.pydata.org`，或者从 Packt 中选择我的另一本书，学习熊猫，2ed。

有关 csv 库的更多信息，请参见[https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html)

有关 json 库的更多信息，请参见[https://docs.python.org/3/library/json.html](https://docs.python.org/3/library/json.html)

# 使用 AWS S3 存储数据

在许多情况下，我们只想将内容保存到本地副本中，以用于存档、备份或以后的批量分析。我们还可能希望保存这些站点中的媒体以供以后使用。我已经为广告合规公司建立了刮板，我们将在网站上跟踪和下载基于广告的媒体，以确保正确使用，并存储以供日后分析、合规和转码。

这些类型的系统所需的存储空间可能非常大，但随着 AWS S3（简单存储服务）等云存储服务的出现，这比在您自己的 IT 部门中管理大型 SAN（存储区域网络）要容易得多，也更具成本效益。此外，S3 还可以自动将数据从热存储移动到冷存储，然后再移动到长期存储，如冰川，这可以为您节省更多的钱。

我们不会讨论所有这些细节，只需将`planets.html`文件存储到 S3 存储桶中即可。一旦你能做到这一点，你就可以保存任何你想要的内容。

# 准备

要执行以下示例，您将需要一个 AWS 帐户，并且可以访问在 Python 代码中使用的密钥。它们对您的帐户是唯一的。我们将使用`boto3`库访问 S3。您可以使用`pip install boto3`来安装。此外，您还需要将环境变量设置为进行身份验证。这些变量如下所示：

`AWS_ACCESS_KEY_ID=AKIAIDCQ5PH3UMWKZEWA`
`AWS_SECRET_ACCESS_KEY=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`

这些可在 AWS 门户网站的 IAM（身份访问管理）部分中找到。

It's a good practice to put these keys in environment variables.  Having them in code can lead to their theft.  During the writing of this book, I had this hard coded and accidentally checked them in to GitHub.  The next morning I woke up to critical messages from AWS that I had thousands of servers running!  There are GitHub scrapers looking for these keys and they will get found and use for nefarious purposes.  By the time I had them all turned off, my bill was up to $6000, all accrued overnight. Thankfully, AWS waived these fees!

# 怎么做

我们不会解析`planets.html`文件中的数据，只需使用以下请求从本地 web 服务器检索数据：

1.  以下代码（位于`03/S3.py`中）读取 planets 网页并将其存储在 S3 中：

```py
import requests
import boto3

data = requests.get("http://localhost:8080/planets.html").text

# create S3 client, use environment variables for keys
s3 = boto3.client('s3')

# the bucket
bucket_name = "planets-content"

# create bucket, set
s3.create_bucket(Bucket=bucket_name, ACL='public-read')
s3.put_object(Bucket=bucket_name, Key='planet.html',
              Body=data, ACL="public-read")
```

2.  此应用程序将为您提供类似于以下内容的输出，即 S3 信息，告诉您有关新项目的各种事实。

```py

{'ETag': '"3ada9dcd8933470221936534abbf7f3e"',
 'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',
   'date': 'Sun, 27 Aug 2017 19:25:54 GMT',
   'etag': '"3ada9dcd8933470221936534abbf7f3e"',
   'server': 'AmazonS3',
   'x-amz-id-2': '57BkfScql637op1dIXqJ7TeTmMyjVPk07cAMNVqE7C8jKsb7nRO+0GSbkkLWUBWh81k+q2nMQnE=',
   'x-amz-request-id': 'D8446EDC6CBA4416'},
  'HTTPStatusCode': 200,
  'HostId': '57BkfScql637op1dIXqJ7TeTmMyjVPk07cAMNVqE7C8jKsb7nRO+0GSbkkLWUBWh81k+q2nMQnE=',
  'RequestId': 'D8446EDC6CBA4416',
  'RetryAttempts': 0}}
```

3.  此输出向我们显示对象已在 bucket 中成功创建。此时，您可以导航到 S3 控制台并查看您的存储桶：

![](img/29fbd119-7ee5-43eb-8b2f-9bc34998ff53.png)

The Bucket in S3

4.  在桶内，您将看到`planet.html`文件：

![](img/49cc32c4-5ac3-4177-a397-35385afbcf4e.png)

The File in the Bucket

5.  通过单击文件，您可以看到 S3 中文件的属性和 URL：

![](img/6c5b035d-009f-4878-9806-034b4db8e500.png)

The Properties of the File in S3

# 工作原理

boto3 库以 Python 语法包装 AWS S3 API。这个`client()`调用使用 AWS 进行身份验证，并为我们提供一个用于与 S3 通信的对象。确保您的密钥位于环境变量中，否则这将不起作用。

bucket 名称必须是全局唯一的。在编写本文时，这个 bucket 是可用的，但是您可能需要更改名称。`.create_bucket()`调用创建 bucket 并设置其 ACL。`put_object()`使用`boto3`上传管理器将刮取的数据上传到 bucket 中的对象中。

# 还有更多。。。

使用 S3 有很多细节需要学习。您可以在以下网址找到 API 文档：[http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html](http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html) 。Boto3 文件可在以下网址找到：[https://boto3.readthedocs.io/en/latest/](https://boto3.readthedocs.io/en/latest/) 。

虽然我们只保存了一个网页，但这个模型可以用于在 S3 中存储任何类型的基于文件的数据。

# 使用 MySQL 存储数据

MySQL 是一个免费提供的开源关系数据库管理系统（RDBMS）。在本例中，我们将从网站读取行星数据并将其存储到 MySQL 数据库中。

# 准备

您需要访问 MySQL 数据库。您可以在云中的容器中安装一个本地安装的。我使用的是本地安装的 MySQL 服务器，`root`密码设置为`mypassword`。您还需要安装 MySQL python 库。您可以通过`pip install mysql-connector-python`完成此操作。

1.  首先要做的是在终端使用`mysql`命令连接到数据库：

```py
# mysql -uroot -pmypassword
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor. Commands end with ; or \g.
Your MySQL connection id is 4
Server version: 5.7.19 MySQL Community Server (GPL)

Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>
```

2.  现在，我们可以创建一个数据库，用于存储我们收集的信息：

```py
mysql> create database scraping;
Query OK, 1 row affected (0.00 sec)
```

3.  现在使用新数据库：

```py
mysql> use scraping;
Database changed
```

4.  并在数据库中创建一个 Planets 表来存储我们的数据：

```py

mysql> CREATE TABLE `scraping`.`planets` (
 `id` INT NOT NULL AUTO_INCREMENT,
 `name` VARCHAR(45) NOT NULL,
 `mass` FLOAT NOT NULL,
 `radius` FLOAT NOT NULL,
 `description` VARCHAR(5000) NULL,
 PRIMARY KEY (`id`));
Query OK, 0 rows affected (0.02 sec)

```

现在我们已经准备好抓取数据并将其放入 MySQL 数据库。

# 怎么做

1.  以下代码（在`03/store_in_mysql.py`中找到）将读取行星数据并将其写入 MySQL：

```py
import mysql.connector
import get_planet_data
from mysql.connector import errorcode
from get_planet_data import get_planet_data

try:
    # open the database connection
    cnx = mysql.connector.connect(user='root', password='mypassword',
                                  host="127.0.0.1", database="scraping")

    insert_sql = ("INSERT INTO Planets (Name, Mass, Radius, Description) " +
                  "VALUES (%(Name)s, %(Mass)s, %(Radius)s, %(Description)s)")

    # get the planet data
    planet_data = get_planet_data()

    # loop through all planets executing INSERT for each with the cursor
    cursor = cnx.cursor()
    for planet in planet_data:
        print("Storing data for %s" % (planet["Name"]))
        cursor.execute(insert_sql, planet)

    # commit the new records
    cnx.commit()

    # close the cursor and connection
    cursor.close()
    cnx.close()

except mysql.connector.Error as err:
    if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:
        print("Something is wrong with your user name or password")
    elif err.errno == errorcode.ER_BAD_DB_ERROR:
        print("Database does not exist")
    else:
        print(err)
else:
    cnx.close()
```

2.  这将产生以下输出：

```py
Storing data for Mercury
Storing data for Venus
Storing data for Earth
Storing data for Mars
Storing data for Jupiter
Storing data for Saturn
Storing data for Uranus
Storing data for Neptune
Storing data for Pluto
```

3.  使用 MySQL Workbench，我们可以看到记录被写入数据库的情况（您也可以使用 MySQL 命令行）：

![](img/c8a2c090-dce7-40f2-b0b3-0c6d72ff3885.png)

Records displayed using MySQL Workbench

4.  以下代码可用于检索数据（`03/read_from_mysql.py`：

```py
import mysql.connector
from mysql.connector import errorcode

try:
  cnx = mysql.connector.connect(user='root', password='mypassword',
                  host="127.0.0.1", database="scraping")
  cursor = cnx.cursor(dictionary=False)

  cursor.execute("SELECT * FROM scraping.Planets")
  for row in cursor:
    print(row)

  # close the cursor and connection
  cursor.close()
  cnx.close()

except mysql.connector.Error as err:
  if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:
    print("Something is wrong with your user name or password")
  elif err.errno == errorcode.ER_BAD_DB_ERROR:
    print("Database does not exist")
  else:
    print(err)
finally:
  cnx.close()

```

5.  这将产生以下输出：

```py
(1, 'Mercury', 0.33, 4879.0, 'Named Mercurius by the Romans because it appears to move so swiftly.', 'https://en.wikipedia.org/wiki/Mercury_(planet)')
(2, 'Venus', 4.87, 12104.0, 'Roman name for the goddess of love. This planet was considered to be the brightest and most beautiful planet or star in the heavens. Other civilizations have named it for their god or goddess of love/war.', 'https://en.wikipedia.org/wiki/Venus')
(3, 'Earth', 5.97, 12756.0, "The name Earth comes from the Indo-European base 'er,'which produced the Germanic noun 'ertho,' and ultimately German 'erde,' Dutch 'aarde,' Scandinavian 'jord,' and English 'earth.' Related forms include Greek 'eraze,' meaning 'on the ground,' and Welsh 'erw,' meaning 'a piece of land.'", 'https://en.wikipedia.org/wiki/Earth')
(4, 'Mars', 0.642, 6792.0, 'Named by the Romans for their god of war because of its red, bloodlike color. Other civilizations also named this planet from this attribute; for example, the Egyptians named it "Her Desher," meaning "the red one."', 'https://en.wikipedia.org/wiki/Mars')
(5, 'Jupiter', 1898.0, 142984.0, 'The largest and most massive of the planets was named Zeus by the Greeks and Jupiter by the Romans; he was the most important deity in both pantheons.', 'https://en.wikipedia.org/wiki/Jupiter')
(6, 'Saturn', 568.0, 120536.0, 'Roman name for the Greek Cronos, father of Zeus/Jupiter. Other civilizations have given different names to Saturn, which is the farthest planet from Earth that can be observed by the naked human eye. Most of its satellites were named for Titans who, according to Greek mythology, were brothers and sisters of Saturn.', 'https://en.wikipedia.org/wiki/Saturn')
(7, 'Uranus', 86.8, 51118.0, 'Several astronomers, including Flamsteed and Le Monnier, had observed Uranus earlier but had recorded it as a fixed star. Herschel tried unsuccessfully to name his discovery "Georgian Sidus" after George III; the planet was named by Johann Bode in 1781 after the ancient Greek deity of the sky Uranus, the father of Kronos (Saturn) and grandfather of Zeus (Jupiter).', 'https://en.wikipedia.org/wiki/Uranus')
(8, 'Neptune', 102.0, 49528.0, 'Neptune was "predicted" by John Couch Adams and Urbain Le Verrier who, independently, were able to account for the irregularities in the motion of Uranus by correctly predicting the orbital elements of a trans- Uranian body. Using the predicted parameters of Le Verrier (Adams never published his predictions), Johann Galle observed the planet in 1846\. Galle wanted to name the planet for Le Verrier, but that was not acceptable to the international astronomical community. Instead, this planet is named for the Roman god of the sea.', 'https://en.wikipedia.org/wiki/Neptune')
(9, 'Pluto', 0.0146, 2370.0, 'Pluto was discovered at Lowell Observatory in Flagstaff, AZ during a systematic search for a trans-Neptune planet predicted by Percival Lowell and William H. Pickering. Named after the Roman god of the underworld who was able to render himself invisible.', 'https://en.wikipedia.org/wiki/Pluto')
```

# 工作原理

使用`mysql.connector`访问 MySQL 数据库需要使用库中的两个类：`connect`和`cursor`。`connect`类打开并管理与数据库服务器的连接。从该连接对象，我们可以创建一个游标对象。此游标用于使用 SQL 语句读取和写入数据。

在第一个示例中，我们使用光标将九条记录插入数据库。在调用连接的`commit()`方法之前，这些记录不会写入数据库。这将执行所有行对数据库的写入。

读取数据使用类似的模型，不同的是我们使用游标执行 SQL 查询（`SELECT`，并在检索到的行之间进行迭代。因为我们是在读而不是在写，所以不需要在连接上调用`commit()`。

# 还有更多。。。

您可以了解有关 MySQL 的更多信息并从以下位置安装：`https://dev.mysql.com/doc/refman/5.7/en/installing.html`。有关 MySQL 工作台的信息，请访问：`https://dev.mysql.com/doc/workbench/en/`。

# 使用 PostgreSQL 存储数据

在此配方中，我们将行星数据存储在 PostgreSQL 中。PostgreSQL 是一个开源关系数据库管理系统（RDBMS）。它由全球志愿者团队开发，不受任何公司或其他私人实体控制，源代码免费提供。它具有许多独特的功能，如分层数据模型

# 准备

首先确保您有权访问 PostgreSQL 数据实例。同样，您可以在本地安装一个实例，在容器中运行一个实例，或者在云中获取一个实例。

与 MySQL 一样，我们需要首先创建一个数据库。该过程与 MySQL 几乎相同，但命令和参数略有不同。

1.  在终端执行 psql 命令。这将带您进入 psql 命令处理器：

```py
# psql -U postgres
psql (9.6.4)
Type "help" for help.
postgres=# 
```

2.  现在创建刮片数据库：

```py
postgres=# create database scraping;
CREATE DATABASE
postgres=#
```

3.  然后切换到新数据库：

```py
postgres=# \connect scraping
You are now connected to database "scraping" as user "postgres".
scraping=# 
```

4.  现在我们可以创建行星表了。我们首先需要创建一个序列表：

```py
scraping=# CREATE SEQUENCE public."Planets_id_seq"
scraping-#  INCREMENT 1
scraping-#  START 1
scraping-#  MINVALUE 1
scraping-#  MAXVALUE 9223372036854775807
scraping-#  CACHE 1;
CREATE SEQUENCE
scraping=# ALTER SEQUENCE public."Planets_id_seq"
scraping-#  OWNER TO postgres;
ALTER SEQUENCE
scraping=# 
```

5.  现在我们可以创建表：

```py
scraping=# CREATE TABLE public."Planets"
scraping-# (
scraping(# id integer NOT NULL DEFAULT nextval('"Planets_id_seq"'::regclass),
scraping(# name text COLLATE pg_catalog."default" NOT NULL,
scraping(# mass double precision NOT NULL,
scraping(# radius double precision NOT NULL,
scraping(# description text COLLATE pg_catalog."default" NOT NULL,
scraping(# moreinfo text COLLATE pg_catalog."default" NOT NULL,
scraping(# CONSTRAINT "Planets_pkey" PRIMARY KEY (name)
scraping(# )
scraping-# WITH (
scraping(# OIDS = FALSE
scraping(# )
</span>scraping-# TABLESPACE pg_default;
CREATE TABLE
scraping=# 
scraping=# ALTER TABLE public."Planets"
scraping-# OWNER to postgres;
ALTER TABLE
scraping=# \q
```

要从 Python 访问 PostgreSQL，我们将使用`psycopg2`库，因此请确保使用`pip install psycopg2`将其安装在您的 Python 环境中。

我们现在已经准备好编写 Python 来将行星数据存储在 PostgreSQL 中。

# 怎么做

我们按照以下方法进行配方：

1.  以下代码将读取行星数据并将其写入数据库（代码在`03/save_in_postgres.py`中）：

```py
import psycopg2
from get_planet_data import get_planet_data

try:
  # connect to PostgreSQL
  conn = psycopg2.connect("dbname='scraping' host='localhost' user='postgres' password='mypassword'")

  # the SQL INSERT statement we will use
  insert_sql = ('INSERT INTO public."Planets"(name, mass, radius, description, moreinfo) ' +
          'VALUES (%(Name)s, %(Mass)s, %(Radius)s, %(Description)s, %(MoreInfo)s);')

  # open a cursor to access data
  cur = conn.cursor()

  # get the planets data and loop through each
  planet_data = get_planet_data()
  for planet in planet_data:
    # write each record
    cur.execute(insert_sql, planet)

  # commit the new records to the database
  conn.commit()
  cur.close()
  conn.close()

  print("Successfully wrote data to the database")

except Exception as ex:
  print(ex)

```

2.  如果成功，您将看到以下内容：

```py
Successfully wrote data to the database
```

3.  使用 GUI 工具（如 pgAdmin），您可以检查数据库中的数据：

![](img/e1060188-c3d3-4a2d-aaf4-4f9124294d9e.png)

Records Displayed in pgAdmin

4.  可以使用以下 Python 代码查询数据（见`03/read_from_postgresql.py`：

```py
import psycopg2

try:
  conn = psycopg2.connect("dbname='scraping' host='localhost' user='postgres' password='mypassword'")

  cur = conn.cursor()
  cur.execute('SELECT * from public."Planets"')
  rows = cur.fetchall()
  print(rows)

  cur.close()
  conn.close()

except Exception as ex:
  print(ex)

```

5.  并产生以下输出（略为截断）：

```py
[(1, 'Mercury', 0.33, 4879.0, 'Named Mercurius by the Romans because it appears to move so swiftly.', 'https://en.wikipedia.org/wiki/Mercury_(planet)'), (2, 'Venus', 4.87, 12104.0, 'Roman name for the goddess of love. This planet was considered to be the brightest and most beautiful planet or star in the heavens. Other civilizations have named it for their god or goddess of love/war.', 'https://en.wikipedia.org/wiki/Venus'), (3, 'Earth', 5.97, 12756.0, "The name Earth comes from the Indo-European base 'er,'which produced the Germanic noun 'ertho,' and ultimately German 'erde,' Dutch 'aarde,' Scandinavian 'jord,' and English 'earth.' Related forms include Greek 'eraze,' meaning 'on the ground,' and Welsh 'erw,' meaning 'a piece of land.'", 'https://en.wikipedia.org/wiki/Earth'), (4, 'Mars', 0.642, 6792.0, 'Named by the Romans for their god of war because of its red, bloodlike color. Other civilizations also named this planet from this attribute; for example, the Egyptians named it 
```

# 工作原理

像我们一样使用`psycopg2`库访问 PostgreSQL 数据库需要使用库中的两个类：`connect`和`cursor`。`connect`类打开并管理与数据库服务器的连接。从这个连接对象，我们可以创建一个`cursor`对象。此游标用于使用 SQL 语句读取和写入数据。

在第一个示例中，我们使用光标将九条记录插入数据库。在调用连接的`commit()`方法之前，这些记录不会写入数据库。这将执行所有行对数据库的写入。

读取数据使用类似的模型，不同的是我们使用游标执行 SQL 查询（`SELECT`，并在检索到的行之间进行迭代。因为我们是在读而不是在写，所以不需要在连接上调用`commit()`。

# 还有更多。。。

有关 PostgreSQL 的信息，请访问`https://www.postgresql.org/`。pgAdmin 可在`https://www.pgadmin.org/`处获得`psycopg`的参考资料在`http://initd.org/psycopg/docs/usage.html`处

# 在 Elasticsearch 中存储数据

Elasticsearch 是一个基于 Lucene 的搜索引擎。它提供了一个分布式、多租户的全文搜索引擎，具有 HTTP web 界面和无模式的 JSON 文档。它是一个非关系数据库（通常称为 NoSQL），主要用于存储文档而不是记录。这些文档可以有多种格式，其中一种对我们很有用：JSON。这使得使用 Elasticsearch 变得非常简单，因为我们不需要将数据转换为 JSON 或从 JSON 转换为 JSON

现在，让我们将行星数据存储在 Elasticsearch 中。

# 准备

我们将访问本地安装的 Elasticsearch 服务器。要从 Python 实现这一点，我们将使用`Elasticsearch-py`库。您很可能需要使用 pip:`pip install elasticsearch`安装此软件。

与 PostgreSQL 和 MySQL 不同，我们不需要提前在 Elasticsearch 中创建表。Elasticsearch 不关心结构化数据模式（尽管它有索引），所以我们不必经历这个过程。

# 怎么做

将数据写入 Elasticsearch 非常简单。以下 Python 代码使用我们的 planets 数据（`03/write_to_elasticsearch.py`）执行此任务：

```py
from elasticsearch import Elasticsearch
from get_planet_data import get_planet_data

# create an elastic search object
es = Elasticsearch()

# get the data
planet_data = get_planet_data()

for planet in planet_data:
  # insert each planet into elasticsearch server
  res = es.index(index='planets', doc_type='planets_info', body=planet)
  print (res)
```

执行此操作将产生以下输出：

```py
{'_index': 'planets', '_type': 'planets_info', '_id': 'AV4qIF3_T0Z2t9T850q6', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'created': True}{'_index': 'planets', '_type': 'planets_info', '_id': 'AV4qIF5QT0Z2t9T850q7', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'created': True}
{'_index': 'planets', '_type': 'planets_info', '_id': 'AV4qIF5XT0Z2t9T850q8', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'created': True}
{'_index': 'planets', '_type': 'planets_info', '_id': 'AV4qIF5fT0Z2t9T850q9', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'created': True}
{'_index': 'planets', '_type': 'planets_info', '_id': 'AV4qIF5mT0Z2t9T850q-', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'created': True}
{'_index': 'planets', '_type': 'planets_info', '_id': 'AV4qIF5rT0Z2t9T850q_', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'created': True}
{'_index': 'planets', '_type': 'planets_info', '_id': 'AV4qIF50T0Z2t9T850rA', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'created': True}
{'_index': 'planets', '_type': 'planets_info', '_id': 'AV4qIF56T0Z2t9T850rB', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'created': True}
{'_index': 'planets', '_type': 'planets_info', '_id': 'AV4qIF6AT0Z2t9T850rC', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'created': True}
```

输出显示每次插入的结果，为我们提供信息，如 elasticsearch 分配给文档的`_id`。

如果您也安装了 logstash 和 kibana，您可以看到 kibana 内部的数据：

![](img/d9054e7d-acb5-4324-8bcd-33c00accd7c8.png)

Kibana Showing and Index

我们可以使用下面的 Python 代码查询数据。此代码检索“行星”索引中的所有文档，并打印每个行星的名称、质量和半径（`03/read_from_elasticsearch.py`）：

```py
from elasticsearch import Elasticsearch

# create an elastic search object
es = Elasticsearch()

res = es.search(index="planets", body={"query": {"match_all": {}}})

```

```py
print("Got %d Hits:" % res['hits']['total'])
for hit in res['hits']['hits']:
 print("%(Name)s %(Mass)s: %(Radius)s" % hit["_source"])Got 9 Hits:
```

这将产生以下输出：

```py
Mercury 0.330: 4879
Mars 0.642: 6792
Venus 4.87: 12104
Saturn 568: 120536
Pluto 0.0146: 2370
Earth 5.97: 12756
Uranus 86.8: 51118
Jupiter 1898: 142984
Neptune 102: 49528
```

# 工作原理

Elasticsearch 既是一个 NoSQL 数据库，也是一个搜索引擎。您将文档交给 Elasticsearch，它会解析文档中的数据，并自动为该数据创建搜索索引。

在插入过程中，我们使用了`elasticsearch`库的`.index()`方法，并指定了一个名为“planets”的索引、一个文档类型`planets_info`，最后是文档主体，即我们的 planet Python 对象。`elasticsearch`库将对象转换为 JSON 并发送给 Elasticsearch 进行存储和索引。

index 参数用于通知 Elasticsearch 如何创建索引，它将使用该索引编制索引，我们可以使用该索引指定查询时要搜索的一组文档。在执行查询时，我们指定了相同的索引“planets”，并执行了一个查询以匹配所有文档。

# 还有更多。。。

有关 elasticsearch 的更多信息，请访问：`https://www.elastic.co/products/elasticsearch`。有关 python API 的信息，请访问：`http://pyelasticsearch.readthedocs.io/en/latest/api/`

在本书后面的章节中，我们还会回到 Elasticsearch。

# 如何使用 AWS SQS 构建健壮的 ETL 管道

刮取大量站点和数据可能是一个复杂而缓慢的过程。但它可以充分利用并行处理的优势，可以使用多个处理器线程在本地进行，也可以使用消息队列系统将刮取请求分发给报告刮取器。在一个类似于提取、转换和加载管道（ETL）的过程中，可能还需要多个步骤。这些管道也可以使用消息队列体系结构和刮取来轻松构建。

使用消息队列体系结构为我们的管道提供了两个优势：

*   健壮性
*   可伸缩性

处理变得健壮，就好像单个消息的处理失败一样，那么消息可以重新排队等待再次处理。因此，如果刮板失败，我们可以重新启动它，并且不会丢失刮板页面的请求，或者消息队列系统将请求传递给另一个刮板。

它提供了可伸缩性，因为相同或不同系统上的多个刮取器可以侦听队列。然后，可以在不同的核心或更重要的是，在不同的系统上同时处理多条消息。在基于云的 scraper 中，您可以根据需要扩大 scraper 实例的数量，以处理更大的负载。

可以使用的常见消息队列系统包括：Kafka、RabbitMQ 和 AmazonSQS。我们的示例将使用 AmazonSQS，尽管 Kafka 和 RabbitMQ 都非常好用（我们将在本书后面看到 RabbitMQ 的使用）。我们使用 SQS 来保持使用 AWS 基于云的服务的模型，正如我们在前面的 S3 一章中所做的那样。

# 准备

例如，我们将构建一个简单的 ETL 过程，该过程将读取主 planets 页面并将 planets 数据存储在 MySQL 中。它还将为页面中的每个*更多信息*链接向队列传递一条消息，其中 0 个或多个进程可以接收这些请求并对这些链接执行进一步处理。

要从 Python 访问 SQS，我们将使用`boto3`库重新访问。

# 如何操作-将消息发布到 AWS 队列

`03/create_messages.py`文件包含读取 planets 数据和将 MoreInfo 属性中的 URL 发布到 SQS 队列的代码：

```py
from urllib.request import urlopen
from bs4 import BeautifulSoup

import boto3
import botocore

# declare our keys (normally, don't hard code this)
access_key="AKIAIXFTCYO7FEL55TCQ"
access_secret_key="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"

# create sqs client
sqs = boto3.client('sqs', "us-west-2",
                   aws_access_key_id = access_key, 
                   aws_secret_access_key = access_secret_key)

# create / open the SQS queue
queue = sqs.create_queue(QueueName="PlanetMoreInfo")
print (queue)

# read and parse the planets HTML
html = urlopen("http://127.0.0.1:8080/pages/planets.html")
bsobj = BeautifulSoup(html, "lxml")

planets = []
planet_rows = bsobj.html.body.div.table.findAll("tr", {"class": "planet"})

for i in planet_rows:
  tds = i.findAll("td")

  # get the URL
  more_info_url = tds[5].findAll("a")[0]["href"].strip()

  # send the URL to the queue
  sqs.send_message(QueueUrl=queue["QueueUrl"],
           MessageBody=more_info_url)
  print("Sent %s to %s" % (more_info_url, queue["QueueUrl"]))
```

在终端中运行代码，您将看到类似于以下内容的输出：

```py
{'QueueUrl': 'https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo', 'ResponseMetadata': {'RequestId': '2aad7964-292a-5bf6-b838-2b7a5007af22', 'HTTPStatusCode': 200, 'HTTPHeaders': {'server': 'Server', 'date': 'Mon, 28 Aug 2017 20:02:53 GMT', 'content-type': 'text/xml', 'content-length': '336', 'connection': 'keep-alive', 'x-amzn-requestid': '2aad7964-292a-5bf6-b838-2b7a5007af22'}, 'RetryAttempts': 0}}
Sent https://en.wikipedia.org/wiki/Mercury_(planet) to https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
Sent https://en.wikipedia.org/wiki/Venus to https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
Sent https://en.wikipedia.org/wiki/Earth to https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
Sent https://en.wikipedia.org/wiki/Mars to https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
Sent https://en.wikipedia.org/wiki/Jupiter to https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
Sent https://en.wikipedia.org/wiki/Saturn to https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
Sent https://en.wikipedia.org/wiki/Uranus to https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
Sent https://en.wikipedia.org/wiki/Neptune to https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
Sent https://en.wikipedia.org/wiki/Pluto to https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
```

现在进入 AWS SQS 控制台。您应该看到队列已经创建，它包含 9 条消息：

![](img/2ad3b7c1-9f39-4d02-ac61-d23619a9c409.png)

The Queue in SQS

# 工作原理

代码连接到给定帐户和 AWS 的 us-west-2 区域。如果队列不存在，则创建一个队列。然后，对于源内容中的每个行星，程序发送一条包含该行星的*更多信息*URL 的消息。

此时，没有人在侦听队列，因此消息将一直放在那里，直到最终读取或过期。每条消息的默认生存期为 4 天。

# 如何做-阅读和处理信息

要处理消息，请运行`03/process_messages.py`程序：

```py
import boto3
import botocore
import requests
from bs4 import BeautifulSoup

print("Starting")

# declare our keys (normally, don't hard code this)
access_key = "AKIAIXFTCYO7FEL55TCQ"
access_secret_key = "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"

# create sqs client
sqs = boto3.client('sqs', "us-west-2", 
          aws_access_key_id = access_key, 
          aws_secret_access_key = access_secret_key)

print("Created client")

# create / open the SQS queue
queue = sqs.create_queue(QueueName="PlanetMoreInfo")
queue_url = queue["QueueUrl"]
print ("Opened queue: %s" % queue_url)

while True:
  print ("Attempting to receive messages")
  response = sqs.receive_message(QueueUrl=queue_url,
                 MaxNumberOfMessages=1,
                 WaitTimeSeconds=1)
  if not 'Messages' in response:
    print ("No messages")
    continue

  message = response['Messages'][0]
  receipt_handle = message['ReceiptHandle']
  url = message['Body']

  # parse the page
  html = requests.get(url)
  bsobj = BeautifulSoup(html.text, "lxml")

  # now find the planet name and albedo info
  planet=bsobj.findAll("h1", {"id": "firstHeading"} )[0].text
  albedo_node = bsobj.findAll("a", {"href": "/wiki/Geometric_albedo"})[0]
  root_albedo = albedo_node.parent
  albedo = root_albedo.text.strip()

  # delete the message from the queue
  sqs.delete_message(
    QueueUrl=queue_url,
    ReceiptHandle=receipt_handle
  )

  # print the planets name and albedo info
  print("%s: %s" % (planet, albedo))
```

使用`python process_messages.py`运行脚本。您将看到类似于以下内容的输出：

```py
Starting
Created client
Opened queue: https://us-west-2.queue.amazonaws.com/414704166289/PlanetMoreInfo
Attempting to receive messages
Jupiter: 0.343 (Bond)
0.52 (geom.)[3]
Attempting to receive messages
Mercury (planet): 0.142 (geom.)[10]
Attempting to receive messages
Uranus: 0.300 (Bond)
0.51 (geom.)[5]
Attempting to receive messages
Neptune: 0.290 (bond)
0.41 (geom.)[4]
Attempting to receive messages
Pluto: 0.49 to 0.66 (geometric, varies by 35%)[1][7]
Attempting to receive messages
Venus: 0.689 (geometric)[2]
Attempting to receive messages
Earth: 0.367 geometric[3]
Attempting to receive messages
Mars: 0.170 (geometric)[8]
0.25 (Bond)[7]
Attempting to receive messages
Saturn: 0.499 (geometric)[4]
Attempting to receive messages
No messages
```

# 工作原理

程序连接到 SQS 并打开队列。打开队列进行读取也是使用`sqs.create_queue`完成的，如果队列已经存在，它将简单地返回队列。

然后，它进入一个调用`sqs.receive_message`的循环，指定队列的 URL、每次读取要接收的消息数，以及如果没有可用消息，以秒为单位的最大等待时间。

如果一条消息被读取，消息中的 URL 将被检索，并使用抓取技术读取 URL 处的页面，提取行星的名称和有关其反照率的信息。

注意，我们检索消息的接收句柄。这是从队列中删除消息所必需的。如果我们不删除该消息，它将在一段时间后在队列中可用。因此，如果我们的 scraper 崩溃并且没有执行此确认，则 SQS 将再次使该消息可供另一个 scraper 处理（或备份时相同的消息）。

# 还有更多。。。

有关 S3 的更多信息，请访问：`https://aws.amazon.com/s3/`。有关 API 的详细信息，请访问：`https://aws.amazon.com/documentation/s3/`。