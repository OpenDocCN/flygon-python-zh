# 五、爬取——行为准则

在本章中，我们将介绍：

*   刮合法性与刮礼貌
*   关于 robots.txt
*   使用站点地图进行爬网
*   拖拖拉拉
*   使用可识别的用户代理
*   设置每个域的并发请求数
*   使用自动节流
*   缓存响应

# 介绍

虽然你可以从技术上刮取任何网站，但重要的是要知道刮取是否合法。我们将讨论刮除法律问题，探索一般经验法则，并查看礼貌刮除的最佳实践，最大限度地减少对目标网站的潜在损害

# 刮合法性与刮礼貌

这个配方中没有真正的代码。它只是对与刮泥相关的法律问题相关的一些概念的阐述。我不是律师，所以不要把我在这里写的任何东西当作法律建议。我只会指出使用刮泥机时需要注意的几件事。

# 准备

爬取的合法性分为两个问题：

*   内容所有权
*   拒绝服务

从根本上说，网上发布的任何内容都是开放阅读的。每次你加载一个页面，任何一个页面，你的浏览器都会从 web 服务器下载该内容，并以视觉方式呈现给你。因此，从某种意义上说，你和你的浏览器已经在抓取你在网络上看到的任何东西。而且，由于网络的性质，因为有人在网络上公开发布内容，他们本质上要求你获取这些信息，但通常只用于特定目的。

最大的问题是创建自动工具，在互联网上直接查找和复制*事物*，其中*事物*要么是数据、图像、视频或音乐——本质上是由他人创建的，代表对创造者或所有者有价值的事物。这些项目在明确制作项目副本供您个人使用时可能会产生问题，而在制作副本并使用该副本为您或他人谋利时，这些项目更可能产生问题

视频、书籍、音乐和图像是对个人或商业用途复制品合法性的一些明显担忧。一般来说，如果你从开放式网站（比如那些不需要授权访问或需要付费访问内容的网站）中删除此类内容，那么你就没事了。还有*合理使用*规则允许在某些情况下重复使用内容，例如，课堂场景中的少量文档共享，为人们学习而发布的知识是共享的，没有实际的经济影响

从网站上截取*数据*通常是一个更加模糊的问题。我所说的数据是指作为服务提供的信息。根据我的经验，一个很好的例子是发布到供应商网站上的能源价格。这些通常是为了方便客户，但这并不是为了让你自由地刮取数据并将其用于你自己的商业分析服务。如果你只是为了一个非公共数据库而收集这些数据，或者你只是为了自己的目的使用这些数据，那么这些数据通常是可以不用担心的。但是如果你使用这些数据库来驱动自己的网站并以自己的名义共享这些内容，那么你可能需要小心。

关键是，请查看网站上的免责声明/服务条款，了解如何处理这些信息。它应该被记录下来，但如果不是，那并不意味着你就可以发疯了。始终要小心，运用常识，因为你是为了自己的目的而获取他人的内容。

另一个问题，我把它归为一个被称为拒绝服务的概念，与收集信息的实际过程以及收集信息的频率有关。在网站上手动读取内容的过程与编写自动机器人程序的过程有很大的不同，自动机器人程序会不断地在 web 服务器上寻找内容。从极端的角度来看，这种访问频率可能非常高，以至于它会拒绝其他合法用户访问内容，从而拒绝他们提供服务。它还可以通过增加内容宿主的带宽成本，甚至运行服务器的电气成本来增加内容宿主的成本。

一个管理良好的网站将识别这些类型的重复和频繁访问，并使用 web 应用程序防火墙等工具将其关闭，这些工具具有基于 IP 地址、标头和 Cookie 阻止您访问的规则。在其他情况下，可能会识别这些访问，并与您的 ISP 联系，让您停止执行这些任务。记住，你从来都不是真正的匿名者，聪明的主持人可以知道你是谁，你访问了什么，什么时候访问。

# 怎么做

那么，你如何才能成为一名优秀的刮刀呢？我们将在本章介绍几个因素：

*   您可以从尊重`robots.txt`文件开始
*   不要抓取你在网站上找到的每个链接，只抓取网站地图上给出的链接
*   限制你的要求，就像汉·索洛对丘巴卡说的那样：随意飞行；或者，不要让人觉得你是在重复抓取内容
*   确定自己的身份，以便网站了解您

# 关于 robots.txt

许多网站都希望被爬网。这是野兽的本性所固有的：网络主持人将内容放在他们的网站上，让人们看到。但其他计算机查看内容也很重要。搜索引擎优化（SEO）就是一个很好的例子。SEO 是一个过程，在这个过程中，你实际上是在设计你的站点，让它被谷歌这样的蜘蛛抓取，所以你实际上是在鼓励抓取。但同时，发布者可能只希望对其网站的特定部分进行爬网，并告诉爬网者不要让其爬网器进入网站的某些部分，这可能不是为了共享，也可能不够重要，不足以进行爬网并浪费 web 服务器资源。

关于你是什么和不允许爬网的规则通常包含在大多数网站上称为`robots.txt`的文件中。`robots.txt`是一个可读但可解析的文件，可用于识别允许和不允许刮取的位置。

不幸的是，`robots.txt`文件的格式不是标准的，任何人都可以自己修改，但在格式上有很强的共识。通常会在站点的根 URL 处找到一个`robots.txt`文件。为了演示一个`robots.txt`文件，下面的代码包含了亚马逊在[上提供的一个文件的摘录 http://amazon.com/robots.txt](http://amazon.com/robots.txt) 。我对其进行了编辑，以显示重要的概念：

```py
User-agent: *
Disallow: /exec/obidos/account-access-login
Disallow: /exec/obidos/change-style
Disallow: /exec/obidos/flex-sign-in
Disallow: /exec/obidos/handle-buy-box
Disallow: /exec/obidos/tg/cm/member/
Disallow: /gp/aw/help/id=sss
Disallow: /gp/cart
Disallow: /gp/flex

...

Allow: /wishlist/universal*
Allow: /wishlist/vendor-button*
Allow: /wishlist/get-button*

...

User-agent: Googlebot
Disallow: /rss/people/*/reviews
Disallow: /gp/pdp/rss/*/reviews
Disallow: /gp/cdp/member-reviews/
Disallow: /gp/aw/cr/

...
Allow: /wishlist/universal*
Allow: /wishlist/vendor-button*
Allow: /wishlist/get-button*

```

可以看出，文件中有三个主要元素：

*   一种用户代理声明，在文件结尾或下一个用户代理语句之前，将对其应用以下行
*   允许爬网的一组 URL
*   禁止对一组 URL 进行爬网

语法其实很简单，Python 库的存在是为了帮助我们实现`robots.txt`中包含的规则。我们将使用`reppy`库来实现`robots.txt`。

# 准备

让我们来看看如何在 reppy 库中演示如何使用`robots.txt`。有关 reppy 的更多信息，请参阅其 GitHub 页面上的[https://github.com/seomoz/reppy](https://github.com/seomoz/reppy)

`reppy`可以这样安装：

```py
pip install reppy
```

但是，我发现在我的 Mac 电脑上，我在安装过程中遇到了一个错误，它需要以下命令：

```py
CFLAGS=-stdlib=libc++ pip install reppy
```

一般信息/在 Google 上搜索`robots.txt`Python 解析库通常会指导您使用 robotparser 库。该库可用于 Python2.x。对于 Python3，它已被移动到`urllib`库中。但是，我发现该库在特定场景中报告了不正确的值。我将在我们的示例中指出这一点。

# 怎么做

要运行配方，请执行`05/01_sitemap.py`中的代码。脚本将检查是否允许在 amazon.com 上对多个 URL 进行爬网。运行时，您将看到以下输出：

```py
True: http://www.amazon.com/
False: http://www.amazon.com/gp/dmusic/
True: http://www.amazon.com/gp/dmusic/promotions/PrimeMusic/
False: http://www.amazon.com/gp/registry/wishlist/
```

# 工作原理

1.  脚本首先导入`reppy.robots`：

```py
from reppy.robots import Robots
```

2.  然后代码使用`Robots`为 amazon.com 获取`robots.txt`。

```py
url = "http://www.amazon.com"
robots = Robots.fetch(url + "/robots.txt")
```

3.  使用提取的内容，脚本检查多个 URL 的可访问性：

```py
paths = [
    '/',
    '/gp/dmusic/',
    '/gp/dmusic/promotions/PrimeMusic/',
    '/gp/registry/wishlist/'
]

for path in paths:
 print("{0}: {1}".format(robots.allowed(path, '*'), url + path))
```

此代码的结果如下所示：

```py
True: http://www.amazon.com/
False: http://www.amazon.com/gp/dmusic/
True: http://www.amazon.com/gp/dmusic/promotions/PrimeMusic/
False: http://www.amazon.com/gp/registry/wishlist/
```

对`robots.allowed`的调用提供了 URL 和用户代理。根据 URL 是否允许爬网返回`True`或`False`。在本例中，指定 URL 的结果为 True、False、True 和 False。让我们来看看如何。

/URL 在`robots.txt`中没有条目，默认允许，但在*user-agent 组下的文件中有以下两行：

```py
Disallow: /gp/dmusic/
Allow: /gp/dmusic/promotions/PrimeMusic
```

/不允许 gp/dmusic，因此返回 False/明确允许 gp/dmusic/promotions/PrimeMusic。如果未指定 Allowed:条目，则 Disallow:/gp/dmusic/行还将禁止从/gp/dmusic/向下的任何其他路径。这本质上意味着不允许任何以/gp/dmusic/开头的 URL，但允许爬网/gp/dmusic/promotions/PrimeMusic 除外。

Here is where there is a difference when using the `robotparser` library. `robotparser` reports that `/gp/dmusic/promotions/PrimeMusic` is disallowed. The library does not handle this type of scenario correctly, as it stops scanning `robots.txt` at the first match, and does not continue further into the file to look for any overrides of this kind.

# 还有更多。。。

首先，有关`robots.txt`的详细信息，请参见[https://developers.google.com/search/reference/robots_txt](https://developers.google.com/search/reference/robots_txt) 。

Note that not all sites have a `robots.txt`, and its absence does not imply you have free rights to crawl all the content.  

此外，`robots.txt`文件可能包含有关在何处找到网站的网站地图的信息。我们将在下一步中检查这些站点地图。

Scrapy can also read `robots.txt` and find sitemaps for you.  

# 使用站点地图进行爬网

网站地图是一种协议，允许网站管理员向搜索引擎通知网站上可用于爬网的 URL。网站管理员希望使用此协议，因为他们实际上希望搜索引擎对其信息进行爬网。网站管理员想让你可以找到这些内容，至少是通过搜索引擎。但你也可以利用这些信息为自己谋利。

站点地图列出站点上的 URL，并允许网站管理员指定关于每个 URL 的附加信息：

*   上次更新的时间
*   内容更改的频率
*   URL 相对于其他 URL 的重要性

网站地图在以下网站上很有用：

*   网站的某些区域无法通过可浏览界面访问；也就是说，您无法访问这些页面
*   使用 Ajax、Silverlight 或 Flash 内容，但搜索引擎通常不处理这些内容
*   该网站非常大，网络爬虫有可能忽略一些新的或最近更新的内容
*   当网站上有大量孤立或链接不好的页面时
*   当一个网站几乎没有外部链接时

站点地图文件具有以下结构：

```py
<?xml version="1.0" encoding="utf-8"?>
<urlset  
   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
   xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd">
    <url>
        <loc>http://example.com/</loc>
        <lastmod>2006-11-18</lastmod>
        <changefreq>daily</changefreq>
        <priority>0.8</priority>
    </url>
</urlset>
```

站点中的每个 URL 都将用一个`<url></url>`标记表示，所有这些标记都包装在一个外部`<urlset></urlset>`标记中。总会有一个`<loc></loc>`标记指定 URL。其他三个标记是可选的。

站点地图文件可能非常大，因此它们通常被分解为多个文件，然后由单个站点地图索引文件引用。此文件具有以下格式：

```py
<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex >
   <sitemap>
      <loc>http://www.example.com/sitemap1.xml.gz</loc>
      <lastmod>2014-10-01T18:23:17+00:00</lastmod>
   </sitemap>
</sitemapindex>
```

在大多数情况下，`sitemap.xml`文件位于域的根目录下。例如，对于 nasa.gov，它是[https://www.nasa.gov/sitemap.xml](https://www.nasa.gov/sitemap.xml) 。但请注意，这不是一个标准，不同的站点可能在不同的位置有一张或多张地图。

特定网站的网站地图也可能位于网站的`robots.txt`文件中。例如，microsoft.com 的`robots.txt`文件以以下内容结尾：

```py
Sitemap: https://www.microsoft.com/en-us/explore/msft_sitemap_index.xml
Sitemap: https://www.microsoft.com/learning/sitemap.xml
Sitemap: https://www.microsoft.com/en-us/licensing/sitemap.xml
Sitemap: https://www.microsoft.com/en-us/legal/sitemap.xml
Sitemap: https://www.microsoft.com/filedata/sitemaps/RW5xN8
Sitemap: https://www.microsoft.com/store/collections.xml
Sitemap: https://www.microsoft.com/store/productdetailpages.index.xml
```

因此，要获取 microsoft.com 的网站地图，我们首先需要读取`robots.txt`文件并提取该信息。

现在让我们看一下解析站点地图。

# 准备

您所需要的一切都在`05/02_sitemap.py`脚本中，以及当时同一文件夹中的`sitemap.py`文件中。`sitemap.py`文件实现了我们将在主脚本中使用的基本站点地图解析器。在本例中，我们将获得 nasa.gov 的站点地图数据。

# 怎么做

首先执行`05/02_sitemap.py`文件。确保关联的`sitemap.py`文件位于同一目录或您的路径中。运行时，几秒钟后，您将获得类似以下内容的输出：

```py
Found 35511 urls
{'lastmod': '2017-10-11T18:23Z', 'loc': 'http://www.nasa.gov/centers/marshall/history/this-week-in-nasa-history-apollo-7-launches-oct-11-1968.html', 'tag': 'url'}
{'lastmod': '2017-10-11T18:22Z', 'loc': 'http://www.nasa.gov/feature/researchers-develop-new-tool-to-evaluate-icephobic-materials', 'tag': 'url'}
{'lastmod': '2017-10-11T17:38Z', 'loc': 'http://www.nasa.gov/centers/ames/entry-systems-vehicle-development/roster.html', 'tag': 'url'}
{'lastmod': '2017-10-11T17:38Z', 'loc': 'http://www.nasa.gov/centers/ames/entry-systems-vehicle-development/about.html', 'tag': 'url'}
{'lastmod': '2017-10-11T17:22Z', 'loc': 'http://www.nasa.gov/centers/ames/earthscience/programs/MMS/instruments', 'tag': 'url'}
{'lastmod': '2017-10-11T18:15Z', 'loc': 'http://www.nasa.gov/centers/ames/earthscience/programs/MMS/onepager', 'tag': 'url'}
{'lastmod': '2017-10-11T17:10Z', 'loc': 'http://www.nasa.gov/centers/ames/earthscience/programs/MMS', 'tag': 'url'}
{'lastmod': '2017-10-11T17:53Z', 'loc': 'http://www.nasa.gov/feature/goddard/2017/nasa-s-james-webb-space-telescope-and-the-big-bang-a-short-qa-with-nobel-laureate-dr-john', 'tag': 'url'}
{'lastmod': '2017-10-11T17:38Z', 'loc': 'http://www.nasa.gov/centers/ames/entry-systems-vehicle-development/index.html', 'tag': 'url'}
{'lastmod': '2017-10-11T15:21Z', 'loc': 'http://www.nasa.gov/feature/mark-s-geyer-acting-deputy-associate-administrator-for-technical-human-explorations-and-operations', 'tag': 'url'}
```

该项目在所有的 nasa.gov 网站地图上找到了 35511 个 URL！代码只打印前 10 个，因为这将是相当多的输出。使用此信息初始化所有这些 URL 的爬网肯定需要相当长的时间！

但这也是网站地图的魅力所在。这些结果中的很多（如果不是全部的话）都有一个`lastmod`标记，告诉你相关 URL 末尾的内容最后一次被修改的时间。如果您正在实现 nasa.gov 的礼貌爬虫程序，您可能希望将这些 URL 及其时间戳保存在数据库中，然后在对该 URL 进行爬虫之前，检查内容是否确实发生了更改，如果没有，则不要进行爬虫。

现在让我们看看这到底是怎么回事。

# 工作原理

配方如下所示：

1.  脚本首先调用`get_sitemap()`：

```py
map = sitemap.get_sitemap("https://www.nasa.gov/sitemap.xml")
```

2.  这是一个指向 sitemap.xml 文件（或任何其他非 gzip 文件）的 URL。实现只需在 URL 处获取内容并返回：

```py
def get_sitemap(url):
    get_url = requests.get(url)

    if get_url.status_code == 200:
        return get_url.text
    else:
        print ('Unable to fetch sitemap: %s.' % url)

```

3.  大部分工作是通过将该内容传递给`parse_sitemap()`完成的。对于 nasa.gov，此网站地图包含以下内容，即网站地图索引文件：

```py
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="//www.nasa.gov/sitemap.xsl"?>
<sitemapindex >
<sitemap><loc>http://www.nasa.gov/sitemap-1.xml</loc><lastmod>2017-10-11T19:30Z</lastmod></sitemap>
<sitemap><loc>http://www.nasa.gov/sitemap-2.xml</loc><lastmod>2017-10-11T19:30Z</lastmod></sitemap>
<sitemap><loc>http://www.nasa.gov/sitemap-3.xml</loc><lastmod>2017-10-11T19:30Z</lastmod></sitemap>
<sitemap><loc>http://www.nasa.gov/sitemap-4.xml</loc><lastmod>2017-10-11T19:30Z</lastmod></sitemap>
</sitemapindex>
```

4.  `process_sitemap()`以调用`process_sitemap()`开始：

```py
def parse_sitemap(s):
    sitemap = process_sitemap(s)
```

5.  此函数首先调用`process_sitemap()`，返回包含`loc`、`lastmod`、`changeFreq`和优先级键值对的 Python 字典对象列表：

```py
def process_sitemap(s):
    soup = BeautifulSoup(s, "lxml")
    result = []

    for loc in soup.findAll('loc'):
        item = {}
        item['loc'] = loc.text
        item['tag'] = loc.parent.name
        if loc.parent.lastmod is not None:
            item['lastmod'] = loc.parent.lastmod.text
        if loc.parent.changeFreq is not None:
            item['changeFreq'] = loc.parent.changeFreq.text
        if loc.parent.priority is not None:
            item['priority'] = loc.parent.priority.text
        result.append(item)

    return result
```

6.  这是通过使用`BeautifulSoup`和`lxml`解析站点地图来实现的。`loc`属性始终为`set`，如果存在关联的 XML 标记，则设置`lastmod`、`changeFreq`和优先级。.tag 属性本身只是说明此内容是从`<sitemap>`标记还是`<url>`标记中检索的（`<loc>`标记可以在任何一个上）。
    `parse_sitemap()`然后继续逐一处理这些结果：

```py
while sitemap:
    candidate = sitemap.pop()

    if is_sub_sitemap(candidate):
        sub_sitemap = get_sitemap(candidate['loc'])
        for i in process_sitemap(sub_sitemap):
            sitemap.append(i)
    else:
        result.append(candidate)
```

7.  每一项都要检查。如果它来自一个站点地图索引文件（URL 以.xml 结尾，.tag 是站点地图），那么我们需要读取该.xml 文件并解析其内容，其结果将放入要处理的项目列表中。在本例中，标识了四个站点地图文件，每个文件都被读取、处理、解析，并将其 URL 添加到结果中。
    为了演示其中一些内容，下面是 sitemap-1.xml 的前几行：

```py
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="//www.nasa.gov/sitemap.xsl"?>
<urlset >
<url><loc>http://www.nasa.gov/</loc><changefreq>daily</changefreq><priority>1.0</priority></url>
<url><loc>http://www.nasa.gov/connect/apps.html</loc><lastmod>2017-08-14T22:15Z</lastmod><changefreq>yearly</changefreq></url>
<url><loc>http://www.nasa.gov/socialmedia</loc><lastmod>2017-09-29T21:47Z</lastmod><changefreq>monthly</changefreq></url>
<url><loc>http://www.nasa.gov/multimedia/imagegallery/iotd.html</loc><lastmod>2017-08-21T22:00Z</lastmod><changefreq>yearly</changefreq></url>
<url><loc>http://www.nasa.gov/archive/archive/about/career/index.html</loc><lastmod>2017-08-04T02:31Z</lastmod><changefreq>yearly</changefreq></url>
```

总的来说，这个站点地图有 11006 行，所以大约 11000 个 URL！据报道，三个网站地图总共有 35511 个 URL。

# 还有更多。。。

站点地图文件也可以压缩，并以.gz 扩展名结尾。这是因为它可能包含许多 URL，压缩将节省大量空间。虽然我们使用的代码不处理 gzip 站点地图文件，但使用 gzip 库中的函数很容易添加这些文件。

Scrapy 还提供了使用站点地图启动爬网的功能。其中之一是 Spider 类 SitemapSpider 的专门化。此类具有为您解析站点地图的智能，然后开始跟踪 URL。为了演示，脚本`05/03_sitemap_scrapy.py`将在 nasa.gov 顶级站点地图索引处开始爬网：

```py
import scrapy
from scrapy.crawler import CrawlerProcess

class Spider(scrapy.spiders.SitemapSpider):
    name = 'spider'
    sitemap_urls = ['https://www.nasa.gov/sitemap.xml']

    def parse(self, response):
        print("Parsing: ", response)

if __name__ == "__main__":
    process = CrawlerProcess({
        'DOWNLOAD_DELAY': 0,
        'LOG_LEVEL': 'DEBUG'
    })
    process.crawl(Spider)
    process.start()
```

运行此操作时，将有大量输出，因为爬行器将开始对所有 30000 多个 URL 进行爬网。在输出的早期，您将看到如下输出：

```py
2017-10-11 20:34:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nasa.gov/sitemap.xml> (referer: None)
2017-10-11 20:34:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nasa.gov/sitemap-4.xml> from <GET http://www.nasa.gov/sitemap-4.xml>
2017-10-11 20:34:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nasa.gov/sitemap-2.xml> from <GET http://www.nasa.gov/sitemap-2.xml>
2017-10-11 20:34:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nasa.gov/sitemap-3.xml> from <GET http://www.nasa.gov/sitemap-3.xml>
2017-10-11 20:34:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nasa.gov/sitemap-1.xml> from <GET http://www.nasa.gov/sitemap-1.xml>
2017-10-11 20:34:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nasa.gov/sitemap-4.xml> (referer: None)
```

Scrapy 已经找到了所有的网站地图并阅读了它们的内容。不久之后，您将开始看到一些重定向和通知，说明某些页面正在被解析：

```py
2017-10-11 20:34:30 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.nasa.gov/image-feature/jpl/pia21629/neptune-from-saturn/> from <GET https://www.nasa.gov/image-feature/jpl/pia21629/neptune-from-saturn>
2017-10-11 20:34:30 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.nasa.gov/centers/ames/earthscience/members/nasaearthexchange/Ramakrishna_Nemani/> from <GET https://www.nasa.gov/centers/ames/earthscience/members/nasaearthexchang
```

```py
e/Ramakrishna_Nemani>
Parsing: <200 https://www.nasa.gov/exploration/systems/sls/multimedia/sls-hardware-being-moved-on-kamag-transporter.html>
Parsing: <200 https://www.nasa.gov/exploration/systems/sls/M17-057.html>
```

# 拖拖拉拉

快速爬取被认为是一种不好的做法。不断地冲击一个网站的页面会消耗 CPU 和带宽，一个健壮的网站会发现你在这样做，并阻止你的 IP。如果你运气不好，你可能会因为违反服务条款而收到一封恶劣的信！

在爬虫程序中延迟请求的技术取决于爬虫程序的实现方式。如果您使用的是 Scrapy，那么您可以设置一个参数来通知爬虫在请求之间等待多长时间。在一个简单的爬虫程序中，只要按顺序处理列表中的 URL，就可以插入 thread.sleep 语句。

如果实现了分散页面请求负载的分布式爬虫集群，事情可能会变得更加复杂，例如使用与竞争消费者的消息队列。这可能有许多不同的解决方案，它们超出了本文提供的范围。

# 准备

我们将研究如何延迟使用 Scrapy。样本在`o5/04_scrape_with_delay.py`中。

# 怎么做

默认情况下，Scrapy 会在页面请求之间施加 0 秒的延迟。也就是说，默认情况下，它不会在请求之间等待

1.  这可以通过`DOWNLOAD_DELAY`设置来控制。为了演示，让我们从命令行运行脚本：

```py
05 $ scrapy runspider 04_scrape_with_delay.py -s LOG_LEVEL=WARNING
Parsing: <200 https://blog.scrapinghub.com>
Parsing: <200 https://blog.scrapinghub.com/page/2/>
Parsing: <200 https://blog.scrapinghub.com/page/3/>
Parsing: <200 https://blog.scrapinghub.com/page/4/>
Parsing: &lt;200 https://blog.scrapinghub.com/page/5/>
Parsing: <200 https://blog.scrapinghub.com/page/6/>
Parsing: <200 https://blog.scrapinghub.com/page/7/>
Parsing: <200 https://blog.scrapinghub.com/page/8/>
Parsing: <200 https://blog.scrapinghub.com/page/9/>
Parsing: <200 https://blog.scrapinghub.com/page/10/>
Parsing: <200 https://blog.scrapinghub.com/page/11/>
Total run time: 0:00:07.006148
Michaels-iMac-2:05 michaelheydt$ 
```

这将对 blog.scrapinghub.com 上的所有页面进行爬网，并报告执行爬网的总时间。`LOG_LEVEL=WARNING`删除大部分日志输出，并仅给出打印语句的输出。这使用了默认的 0 页之间的等待，并导致大约 7 秒长的爬网。

2.  可使用`DOWNLOAD_DELAY`设置设置页面之间的等待。以下页面请求之间的延迟为 5 秒：

```py
05 $ scrapy runspider 04_scrape_with_delay.py -s DOWNLOAD_DELAY=5 -s LOG_LEVEL=WARNING
Parsing: <200 https://blog.scrapinghub.com>
Parsing: <200 https://blog.scrapinghub.com/page/2/>
Parsing: <200 https://blog.scrapinghub.com/page/3/>
Parsing: <200 https://blog.scrapinghub.com/page/4/>
Parsing: <200 https://blog.scrapinghub.com/page/5/>
Parsing: <200 https://blog.scrapinghub.com/page/6/>
Parsing: <200 https://blog.scrapinghub.com/page/7/>
Parsing: <200 https://blog.scrapinghub.com/page/8/>
Parsing: <200 https://blog.scrapinghub.com/page/9/>
Parsing: <200 https://blog.scrapinghub.com/page/10/>
Parsing: <200 https://blog.scrapinghub.com/page/11/>
Total run time: 0:01:01.099267
```

默认情况下，这实际上不会等待 5 秒。它将等待`DOWNLOAD_DELAY`秒，但会以 0.5 到 1.5 倍`DOWNLOAD_DELAY`的随机系数进行等待。为什么要这样做？这会让你的爬虫看起来“不那么机器人化”。你可以使用`RANDOMIZED_DOWNLOAD_DELAY=False`设置来关闭它。

# 工作原理

这个爬虫程序是作为一个爬行器实现的。类定义以声明爬行器名称和开始 URL 开始：

```py
class Spider(scrapy.Spider):
    name = 'spider'
    start_urls = ['https://blog.scrapinghub.com']
```

解析方法查找 CSS“div.prev-post>a”，并遵循这些链接。

scraper 还定义了一个 close 方法，当爬网完成时，Scrapy 调用该方法：

```py
def close(spider, reason):
    start_time = spider.crawler.stats.get_value('start_time')
    finish_time = spider.crawler.stats.get_value('finish_time')
    print("Total run time: ", finish_time-start_time)
```

这将访问 spider crawler stats 对象，检索 spider 的开始和完成时间，并向用户报告差异。

# 还有更多。。。

该脚本还定义了直接使用 Python 执行脚本时的代码：

```py
if __name__ == "__main__":
    process = CrawlerProcess({
        'DOWNLOAD_DELAY': 5,
        'RANDOMIZED_DOWNLOAD_DELAY': False,
        'LOG_LEVEL': 'DEBUG'
    })
    process.crawl(Spider)
    process.start()
```

首先创建一个 CrawlerProcess 对象。可以向该对象传递一个字典，该字典表示用于配置爬网的设置和值。这默认为 5 秒延迟，没有随机化，输出级别为调试。

# 使用可识别的用户代理

如果您违反服务条款并被网站所有者标记，会发生什么情况？你如何帮助网站业主联系你，这样他们就可以很好地要求你退回到他们认为合理的爬取水平？

您可以做的是在请求的用户代理头中添加关于您自己的信息。我们在`robots.txt`文件中看到了一个例子，比如来自 amazon.com 的文件。在他们的`robots.txt`中，有一个谷歌用户代理的明确声明：谷歌机器人。

在抓取过程中，您可以将自己的信息嵌入 HTTP 请求的用户代理标头中。出于礼貌，您可以输入诸如“MyCompany MyCrawler”之类的内容(mybot@mycompany.com)'. 如果远程服务器违反了您的标记，它肯定会捕获此信息，如果这样提供，它将为他们提供一种方便的方式来联系您，而不仅仅是关闭您。

# 怎么做

根据使用的工具不同，设置用户代理也会有所不同。最终，它只是确保用户代理头被设置为您指定的字符串。使用浏览器时，这通常由浏览器设置，以标识浏览器和操作系统。但是你可以把你想要的任何东西放到这个标题中。使用请求时，非常简单：

```py
url = 'https://api.github.com/some/endpoint'
headers = {'user-agent': 'MyCompany-MyCrawler (mybot@mycompany.com)'}
r = requests.get(url, headers=headers)

```

使用 Scrapy 时，只需配置一个设置即可：

```py
process = CrawlerProcess({
    'USER_AGENT': 'MyCompany-MyCrawler (mybot@mycompany.com)'
})
process.crawl(Spider)
process.start()
```

# 工作原理

传出 HTTP 请求有许多不同的头。这些可确保针对目标 web 服务器发出的所有请求，将 User Agent 标头设置为此值。

# 还有更多。。。

虽然可以在用户代理标头中设置您想要的任何内容，但一些 web 服务器将检查用户代理标头，并根据内容决定如何响应。这方面的一个常见示例是使用标题识别移动设备以提供移动演示。

但有些网站也只允许访问特定用户代理值的内容。设置您自己的值可能会导致 web 服务器不响应或返回其他错误，例如未经授权的错误。所以当你使用这种技术时，一定要检查它是否有效。

# 设置每个域的并发请求数

一次抓取一个 URL 通常效率低下。因此，在任何给定时间，通常都会同时向目标站点发出多个页面请求。通常情况下，远程 web 服务器可以非常有效地处理多个同时请求，而在您的终端，您只需等待每个请求的数据返回，因此并发通常对您的应用程序很有效。

但这也是智能网站可以识别并标记为可疑活动的一种模式。在你的爬虫端和网站上都有实际的限制。并发请求越多，双方需要的内存、CPU、网络连接和网络带宽就越多。这些都涉及成本，而且这些价值也有实际限制。

因此，对同时向任何 web 服务器发出的请求数量设置限制通常是一种好的做法。

# 工作原理

有许多技术可用于控制并发级别，并且控制多个请求和执行线程的过程通常非常复杂。我们在这里不讨论如何在线程级别实现这一点，只提到 Scrapy 中内置的构造。

Scrapy 的请求本质上是并发的。默认情况下，Scrapy 最多会同时向任何给定域发送八个请求。您可以使用`CONCURRENT_REQUESTS_PER_DOMAIN`设置进行更改。以下内容将值设置为 1 并发请求：

```py
process = CrawlerProcess({
    'CONCURRENT_REQUESTS_PER_DOMAIN': 1
})
process.crawl(Spider)
process.start()
```

# 使用自动节流

节流的概念与控制最大并发级别有着相当密切的联系。网站处理请求的能力各不相同，无论是在多个网站上还是在不同的时间在单个网站上。在响应时间较慢的时期，在这段时间内减少请求的数量是有意义的。这是一个用手监控和调整的繁琐过程

幸运的是，scrapy 还通过名为`AutoThrottle`的扩展提供了这样做的能力

# 怎么做

可使用`AUTOTHROTTLE_TARGET_CONCURRENCY`设置轻松配置自动油门：

```py
process = CrawlerProcess({
    'AUTOTHROTTLE_TARGET_CONCURRENCY': 3
})
process.crawl(Spider)
process.start()
```

# 工作原理

scrapy 跟踪每个请求的延迟。使用该信息，它可以调整到特定域的请求之间的延迟，以便该域中同时活动的请求不超过`AUTOTHROTTLE_TARGET_CONCURRENCY`，并且请求在任何给定的时间跨度内均匀分布。

# 还有更多。。。

有很多控制节流的选项。您可以在[上了解它们的概况 https://doc.scrapy.org/en/latest/topics/autothrottle.html? &_ga=2.54316072.1404351387.1507758575-507079265.1505263737#设置。](https://doc.scrapy.org/en/latest/topics/autothrottle.html?&_ga=2.54316072.1404351387.1507758575-507079265.1505263737#settings)

# 使用 HTTP 缓存进行开发

网络爬虫的开发是一个探索的过程，它将迭代各种改进以检索所请求的信息。在开发过程中，您经常会反复访问远程服务器以及这些服务器上的相同 URL。这是不礼貌的。幸运的是，scrapy 还提供了专门设计用于帮助解决这种情况的缓存中间件。

# 怎么做

Scrapy 将使用名为 HttpCacheMiddleware 的中间件模块缓存请求。启用只需将`HTTPCACHE_ENABLED`设置配置为`True`即可：

```py
process = CrawlerProcess({
    'AUTOTHROTTLE_TARGET_CONCURRENCY': 3
})
process.crawl(Spider)
process.start()
```

# 工作原理

HTTP 缓存的实现很简单，但同时也很复杂。Scrapy 提供的`HttpCacheMiddleware`根据您的需要提供了大量的配置选项。最终，它归结为在存储中存储每个 URL 及其内容以及缓存过期的相关持续时间。如果在过期时间间隔内对 URL 发出第二个请求，则将检索本地副本，而不是发出远程请求。如果时间已过期，则从 web 服务器获取内容，存储在缓存中，并设置新的过期时间。

# 还有更多。。。

配置碎片缓存有许多选项，包括存储内容的方式（文件系统、DBM 或 LevelDB）、缓存策略以及如何处理来自服务器的 Http 缓存控制指令。要探索这些选项，请查看以下 URL:[https://doc.scrapy.org/en/latest/topics/downloader-middleware.html?_ga=2.50242598.1404351387.1507758575-507079265.1505263737#虚拟策略默认值。](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html?_ga=2.50242598.1404351387.1507758575-507079265.1505263737#dummy-policy-default.)