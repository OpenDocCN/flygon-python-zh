# 九、创建简单的数据 API

在本章中，我们将介绍：

*   使用 Flask RESTful 创建 REST API
*   将 RESTAPI 与刮片代码集成
*   添加 API 以查找工作列表的技能
*   将数据存储在 Elasticsearch 中作为刮取请求的结果
*   在抓取前检查 Elasticsearch 以查找列表

# 介绍

现在，我们在学习刮片方面已经达到了一个激动人心的转折点。从这一点开始，我们将学习如何使用多个 API、微服务和容器工具将刮片作为一种服务，所有这些工具都将允许刮片在本地或云中运行，并通过标准化的 REST API 访问刮板。60；

在本章中，我们将通过使用 Flask RESTful 创建一个简单的 REST API 开始这一新的旅程，我们最终将使用该 API 向服务发出请求，以便按需刮取页面。我们将此 API 连接到 Python 模块中实现的刮取函数，该模块重用刮取 StackOverflow 作业的概念，如中所述[第 7 章](07.html)*文本整理与分析*。

最后几条建议将重点放在使用 Elasticsearch 作为这些结果的缓存，存储我们从刮板检索到的文档，然后首先在缓存中查找它们。我们将研究 ElasticCache 更详细的用途，例如在[第 11 章](11.html)的后面部分，对具有给定技能的工作进行搜索，*将爬虫作为一种服务真正实现*。

# 使用 Flask RESTful 创建 REST API

我们首先使用 Flask RESTful 创建一个简单的 REST API。这个初始 API 将由一个方法组成，该方法允许调用者传递一个整数值，并返回一个 JSON blob。在此配方中，参数及其值以及返回值，在这个时候并不重要，因为我们想首先简单地使用 Flask RESTful 启动并运行一个 API。

# 准备

Flask 是一个 web 微框架，它使创建简单的 web 应用程序功能变得非常容易。Flask RESTful 是 Flask 的一个扩展，它可以使 REST API 变得同样简单。您可以在`flask.pocoo.org`获取 Flask 并阅读更多有关它的信息。Flask RESTful 可以在`https://flask-restful.readthedocs.io/en/latest/`上阅读。Flask 可以使用`pip install flask`安装到 Python 环境中。Flask RESTful 也可以使用`pip install flask-restful`安装。

The remainder of the recipes in the book will be in a subfolder of the chapter's directory.  This is because most of these recipes either require multiple files to operate, or use the same filename (ie: `apy.py`).

# 怎么做

初始 API 在`09/01/api.py`中实现。API 本身和 API 的逻辑在这个文件`api.py`中实现。API 可以以两种方式运行，第一种方式是简单地将文件作为 Python 脚本执行

然后可以通过以下方式启动 API：

```py
python api.py
```

运行时，最初将看到类似以下内容的输出：

```py
Starting the job listing API
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
Starting the job listing API
 * Debugger is active!
 * Debugger pin code: 362-310-034
```

此程序在`127.0.0.1:5000`上公开了一个 REST API，我们可以使用`GET`对路径`/joblisting/<joblistingid>`的请求来请求工作列表。我们可以使用 curl 来尝试：

```py
curl localhost:5000/joblisting/1
```

此命令的结果如下所示：

```py
{
 "YouRequestedJobWithId": "1"
}
```

就像那样，我们已经启动并运行了一个 RESTAPI。现在让我们看看它是如何实现的。

# 工作原理

代码真的不多，这就是 Flask RESTful 的美妙之处。代码从导入`flask`和`flask_restful`开始。

```py
from flask import Flask
from flask_restful import Resource, Api
```

下面是设置 Flask RESTful 初始配置的代码：

```py
app = Flask(__name__)
api = Api(app)
```

接下来是一个类的定义，它表示我们 API 的实现：

```py
class JobListing(Resource):
    def get(self, job_listing_id):
        print("Request for job listing with id: " + job_listing_id)
        return {'YouRequestedJobWithId': job_listing_id}
```

我们将让 Flask RESTful 做的是将 HTTP 请求映射到此类中的方法。具体来说，按照约定，`GET`请求将映射到名为`get`的成员函数。将作为 URL 的一部分传递的值映射到函数的`jobListingId`参数。该函数然后返回一个 Python 字典，RESTful 为我们将其转换为 JSON。

下一行代码告诉 Flask RESTful 如何将 URL 的部分映射到我们的类：

```py
api.add_resource(JobListing, '/', '/joblisting/<string:job_listing_id>')
```

这定义了路径以`/joblisting`开头的 URL 将被映射到我们的`JobListing`类，并且 URL 的下一部分表示要传递给`get`方法的`jobListingId`参数的字符串。GET HTTP 谓词被暗示为此映射中没有定义其他谓词。

最后，我们有代码指定当文件作为脚本运行时，我们只需执行`app.run()`（在本例中传递一个参数以提供调试输出）。

```py
if __name__ == '__main__':
    print("Starting the job listing API")
    app.run(debug=True)
```

Flask RESTful 然后找到我们的类和映射集，开始监听`127.0.0.1:5000`（默认设置），并将请求转发给我们的类和方法。

# 还有更多。。。

Flask RESTful 的默认值是在端口`5000`上运行。这可以使用`app.run()`的替代形式进行更改。我们可以将其保留在 5000，以备我们的食谱使用。最终，您将在容器中运行此服务，并使用反向代理（如 NGINX）将其前置，然后执行到内部服务端口的公共端口映射。

# 将 RESTAPI 与刮片代码集成

在这个配方中，我们将把我们为刮取和从 StackOverflow 获得干净作业列表而编写的代码与我们的 API 集成。这将产生一个可重用的 API，可用于执行按需刮取，而客户不需要任何刮取过程的知识。本质上，我们将创建一个*刮取器作为服务*，这是一个概念，我们将在本书剩余的食谱中花很多时间讨论。

# 准备

这个过程的第一部分是用我们先前存在的代码创建一个模块，该代码是在[第 7 章](07.html)、*文本整理和分析*中编写的因此，我们可以重用它。在本书的其余部分中，我们将在几个配方中重用此代码。在将其与 API 集成之前，让我们简要检查一下此模块的结构和内容。

模块代码位于项目模块文件夹中的`sojobs`（用于 StackOverflow 作业）模块中。

![](img/c192716d-ac40-476d-936a-36091d78b2fb.png)

The sojobs folder

大多数情况下，这些文件是从[第 7 章](07.html)、*文本整理和分析*中使用的文件复制而来的。重用的主要文件是`scraping.py`，其中包含几个便于刮取的功能。我们在本配方中使用的功能是`get_job_listing_info`：

```py
def get_job_listing(job_listing_id):
    print("Got a request for a job listing with id: " + job_listing_id)

    req = requests.get("https://stackoverflow.com/jobs/" + job_listing_id)
    content = req.text

    bs = BeautifulSoup(content, "lxml")
    script_tag = bs.find("script", {"type": "application/ld+json"})

    job_listing_contents = json.loads(script_tag.contents[0])
    desc_bs = BeautifulSoup(job_listing_contents["description"], "lxml")
    just_text = desc_bs.find_all(text=True)

    joined = ' '.join(just_text)
    tokens = word_tokenize(joined)

    stop_list = stopwords.words('english')
    with_no_stops = [word for word in tokens if word.lower() not in stop_list]
    two_grammed = tech_2grams(with_no_stops)
    cleaned = remove_punctuation(two_grammed)

    result = {
        "ID": job_listing_id,
        "JSON": job_listing_contents,
        "TextOnly": just_text,
        "CleanedWords": cleaned
    }

    return json.dumps(result)
```

回到[第 7 章](07.html)、*文本整理和分析*中的代码，您可以看到此代码是我们在这些配方中创建的重用代码。不同之处在于，该函数不是读取单个本地`.html`文件，而是传递作业列表的标识符，然后构造该作业列表的 URL，通过请求读取内容，执行若干分析，然后返回结果

请注意，该函数返回一个 Python 字典，该字典由请求的作业 ID、原始 HTML、列表文本和清理后的单词列表组成。这个 API 向调用者返回这些结果的集合，带有`ID`，因此很容易知道请求的作业，以及我们为执行各种清理所做的所有其他结果。因此，我们为作业列表创建了一个增值服务，而不仅仅是获取原始 HTML。

Make sure that you either have your PYTHONPATH environment variable pointing to the modules directory, or that you have set up your Python IDE to find modules in this directory.  Otherwise, you will get errors that this module cannot be found.

# 怎么做

我们按照以下方法进行配方：

1.  此配方的 API 代码在`09/02/api.py`中。这扩展了前一配方中的代码，在`sojobs`模块中调用此函数。服务的代码如下：

```py
from flask import Flask
from flask_restful import Resource, Api
from sojobs.scraping import get_job_listing_info

app = Flask(__name__)
api = Api(app)

class JobListing(Resource):
    def get(self, job_listing_id):
        print("Request for job listing with id: " + job_listing_id)
        listing = get_job_listing_info(job_listing_id)
        print("Got the following listing as a response: " + listing)
        return listing

api.add_resource(JobListing, '/', '/joblisting/<string:job_listing_id>')

if __name__ == '__main__':
    print("Starting the job listing API")
    app.run(debug=True)
```

Note that the main difference is the import of the function from the module, and the call to the function and return of the data from the result.

2.  该服务通过使用 Python`api.py`执行脚本来运行。然后，我们可以使用`curl`测试 API。以下请求我们之前检查过的 SpaceX 作业列表。

```py
curl localhost:5000/joblisting/122517
```

3.  这会产生相当多的输出。以下是响应的开始部分：

```py
"{\"ID\": \"122517\", \"JSON\": {\"@context\": \"http://schema.org\", \"@type\": \"JobPosting\", \"title\": \"SpaceX Enterprise Software Engineer, Full Stack\", \"skills\": [\"c#\", \"sql\", \"javascript\", \"asp.net\", \"angularjs\"], \"description\": \"<h2>About this job</h2>\\r\\n<p><span>Location options: <strong>Paid relocation</strong></span><br/><span>Job type: <strong>Permanent</strong></span><br/><span>Experience level: <strong>Mid-Level, Senior</strong></span><br/><span>Role: <strong>Full Stack Developer</strong></span><br/><span>Industry: <strong>Aerospace, Information Technology, Web Development</strong></span><br/><span>Company size: <strong>1k-5k people</strong></span><br/><span>Company type: <strong>Private</strong></span><br/></p><br/><br/><h2>Technologies</h2> <p>c#, sql, javascr
```

# 添加 API 以查找工作列表的技能

在此配方中，我们在 API 中添加了一个额外的操作，允许我们请求与工作列表相关的技能。这说明了一种方法，可以仅检索数据的一个子集，而不是列表的全部内容。虽然我们仅针对技能执行此操作，这个概念可以很容易地扩展到数据的任何其他子集，例如作业的位置、标题或对 API 用户有意义的几乎任何其他内容。

# 准备

我们要做的第一件事就是在`sojobs`模块中增加一个刮片功能，这个功能将被命名为`get_job_listing_skills`，下面是这个功能的代码：

```py
def get_job_listing_skills(job_listing_id):
    print("Got a request for a job listing skills with id: " + job_listing_id)

    req = requests.get("https://stackoverflow.com/jobs/" + job_listing_id)
    content = req.text

    bs = BeautifulSoup(content, "lxml")
    script_tag = bs.find("script", {"type": "application/ld+json"})

    job_listing_contents = json.loads(script_tag.contents[0])
    skills = job_listing_contents['skills']

    return json.dumps(skills)
```

此函数检索作业列表，提取 StackOverflow 提供的 JSON，然后只返回 JSON 的`skills`属性。

现在，让我们看看如何向 RESTAPI 添加一个方法来调用它。

# 怎么做

我们按照以下方法进行配方：

1.  此配方的 API 代码在`09/03/api.py`中。此脚本添加了一个附加类`JobListingSkills`，实现如下：

```py
class JobListingSkills(Resource):
    def get(self, job_listing_id):
        print("Request for job listing's skills with id: " + job_listing_id)
        skills = get_job_listing_skills(job_listing_id)
        print("Got the following skills as a response: " + skills)
        return skills
```

This implementation is similar to that of the previous recipe, except that it calls the new function for getting skills.

2.  我们仍然需要添加一条语句来通知 Flask RESTful 如何将 URL 映射到此类的`get`方法。因为我们实际上是在检索较大作业列表的子属性，所以我们将扩展 URL 方案，以包含表示整个作业列表资源的子属性的附加段。

```py
api.add_resource(JobListingSkills, '/', '/joblisting/<string:job_listing_id>/skills')
```

3.  现在，我们可以使用以下方法检索技能：

```py
curl localhost:5000/joblisting/122517/skills
```

这给了我们以下结果：

```py
"[\"c#\", \"sql\", \"javascript\", \"asp.net\", \"angularjs\"]"
```

# 将数据存储在 Elasticsearch 中作为刮取请求的结果

在此配方中，我们扩展了 API，将从 Slaper 接收到的数据保存到 Elasticsearch 中。我们稍后（在下一配方中）将使用此 API，通过将 Elasticsearch 中的内容用作缓存来优化请求，这样我们就不会对已刮取的作业列表重复刮取过程。因此，我们可以很好地使用 StackOverflows 服务器

# 准备

确保您在本地运行 Elasticsearch，因为代码将在`localhost:9200`处访问 Elasticsearch。在[处有一个很好的快速启动 https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html) ，或者您可以在[第 10 章](10.html)、*中查看 docker Elasticsearch 配方如果您想在 Docker 中运行，请使用 Docker*创建 Scraper 微服务。

安装后，您可以通过以下`curl`检查安装是否正确：

```py
curl 127.0.0.1:9200?pretty
```

如果安装正确，您将获得与以下类似的输出：

```py
{
 "name": "KHhxNlz",
 "cluster_name": "elasticsearch",
 "cluster_uuid": "fA1qyp78TB623C8IKXgT4g",
 "version": {
 "number": "6.1.1",
 "build_hash": "bd92e7f",
 "build_date": "2017-12-17T20:23:25.338Z",
 "build_snapshot": false,
 "lucene_version": "7.1.0",
 "minimum_wire_compatibility_version": "5.6.0",
 "minimum_index_compatibility_version": "5.0.0"
 },
 "tagline": "You Know, for Search"
}
```

您还需要安装 elasticsearch py。此功能可在[上获得 https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html) ，但可以使用`pip install elasticsearch`快速安装。

# 怎么做

我们将对我们的 API 代码进行一些小的修改。前面配方中的代码已经复制到`09/04/api.py`中，只做了一些修改

1.  首先，我们为 elasticsearch py 添加一个导入：

```py
from elasticsearch import Elasticsearch
```

2.  现在我们对`JobListing`类的`get`方法进行快速修改（我在 JobListingSkills 中也做了同样的修改，但为了简洁起见，这里省略了它）：

```py
class JobListing(Resource):
    def get(self, job_listing_id):
        print("Request for job listing with id: " + job_listing_id)
        listing = get_job_listing_info(job_listing_id)

        es = Elasticsearch()
        es.index(index='joblistings', doc_type='job-listing', id=job_listing_id, body=listing)

        print("Got the following listing as a response: " + listing)
        return listing
```

3.  这两行新行创建了一个`Elasticsearch`对象，然后将生成的文档插入 ElasticSearch。在第一次调用 API 之前，我们可以通过使用以下 curl 看到没有内容，也没有`'joblistings'`索引：

```py
curl localhost:9200/joblistings
```

4.  鉴于我们刚刚安装了 Elasticsearch，这将导致以下错误。

```py
{"error":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"joblistings","index_uuid":"_na_","index":"joblistings"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"joblistings","index_uuid":"_na_","index":"joblistings"},"status":404}
```

5.  现在使用`python api.py`启动 API。然后发出`curl`以获取作业列表（`curl localhost:5000/joblisting/122517`），这将导致类似于前面配方的输出。现在的区别是该文档将存储在 Elasticsearch 中
6.  现在为索引重新发出上一个卷曲：

```py
curl localhost:9200/joblistings
```

7.  现在您将得到以下结果（仅显示前几行）：

```py
{
 "joblistings": {
  "aliases": {},
  "mappings": {
   "job-listing": {
     "properties": {
       "CleanedWords" {
         "type": "text",
         "fields": {
           "keyword": {
           "type": "keyword",
           "ignore_above": 256
          }
        }
       },
     "ID": {
       "type": "text",
       "fields": {
         "keyword": {
         "type": "keyword",
         "ignore_above": 256
        }
      }
    },
```

已经创建了一个名为`joblistings`的索引，该结果显示了 Elasticsearch 通过检查文档识别的索引结构

While Elasticsearch is schema-less, it does examine the documents submitted and build indexes based upon what it finds.

8.  我们刚才存储的特定文档可以通过以下方式检索：

```py
curl localhost:9200/joblistings/job-listing/122517
```

9.  这将给我们以下结果（同样，只是显示内容的开头）：

```py
{
 "_index": "joblistings",
 "_type": "job-listing",
 "_id": "122517",
 "_version": 1,
 "found": true,
 "_source": {
  "ID": "122517",
  "JSON": {
   "@context": "http://schema.org",
   "@type": "JobPosting",
   "title": "SpaceX Enterprise Software Engineer, Full Stack",
   "skills": [
    "c#",
    "sql",
    "javascript",
    "asp.net",
    "angularjs"
  ],
  "description": "<h2>About this job</h2>\r\n<p><span>Location options: <strong>Paid relocation</strong></span><br/><span>Job type: <strong>Permanent</strong></span><br/><span>Experience level: <strong>Mid-Level,
```

就像这样，通过两行代码，我们将文档存储在我们的 Elasticsearch 数据库中。现在让我们简单地检查一下这是如何工作的。

# 工作原理

使用以下行执行文档的存储：

```py
es.index(index='joblistings', doc_type='job-listing', id=job_listing_id, body=listing)
```

让我们检查一下这些参数中的每一个相对于存储此文档的作用。

`index`参数指定要在其中存储文档的 Elasticsearch 索引。这也成为用于检索文档的 URL 的第一部分。

每个 Elasticsearch 索引也可以有多个文档“类型”，它们是文档的逻辑集合，可以表示索引中不同类型的文档。我们使用了`'job-listing'`，该值也构成了我们用于检索特定文档的 URL 的第二部分。

Elasticsearch 不要求为每个文档指定一个标识符，但是如果我们提供了一个标识符，我们可以查找特定的文档，而无需进行搜索。我们将使用作业列表 ID 作为文档 ID。

最后一个参数 body 指定文档的实际内容。此代码只传递从 scraper 接收到的结果。

# 还有更多。。。

让我们通过查看此文档检索的结果，简要了解 Elasticsearch 为我们所做的更多方面

首先，我们可以在结果的前几行中看到索引、文档类型和 ID：

```py
{
 "_index": "joblistings",
 "_type": "job-listing",
 "_id": "122517",
```

这使得在使用这三个值时检索文档非常高效，就像我们在这个查询中所做的那样。

每个文档还存储一个版本，在本例中为 1。

```py
    "_version": 1,
```

如果我们在代码保持不变的情况下执行相同的查询，那么该文档将使用相同的索引、文档类型和 ID 再次存储，因此版本将增加。相信我，再次在 API 上执行 curl，您将看到该增量为 2。

现在检查`"JSON"`属性的前几个属性的内容。我们将 API 结果的这个属性指定为嵌入 HTML 中的 StackOverflow 返回的作业描述的 JSON。

```py
 "JSON": {
  "@context": "http://schema.org",
  "@type": "JobPosting",
  "title": "SpaceX Enterprise Software Engineer, Full Stack",
  "skills": [
   "c#",
   "sql",
   "javascript",
   "asp.net",
   "angularjs"
  ],
```

这是 StackOverflow 这样的网站给我们提供结构化数据的一些好处，通过使用 Elasticsearch 这样的工具，我们可以得到结构良好的数据，我们可以使用 Elasticsearch 轻松地执行查询，根据特定技能（我们将在下一个菜谱中这样做）、行业、工作福利和其他属性识别工作列表。

我们的 API 的结果还返回了一个属性“【T0]”，这是我们的几个 NLP 过程提取高值单词和术语的结果。以下是 Elasticsearch 中的值摘录：

```py
 "CleanedWords": [
  "job",
  "Location",
  "options",
  "Paid relocation",
  "Job",
  "type",
  "Permanent",
  "Experience",
  "level",
```

同样，我们将能够使用这些来执行丰富的查询，这些查询可以帮助我们根据这些特定的单词找到特定的匹配项。

# 在抓取前检查 Elasticsearch 以查找列表

现在让我们利用 Elasticsearch 作为缓存，检查是否已经存储了作业列表，因此不需要再次点击 StackOverflow。我们扩展了 API，用于对工作列表进行刮取，以首先搜索 Elasticsearch，如果在那里找到结果，我们将返回该数据。因此，我们通过将 Elasticsearch 作为工作列表缓存来优化流程。

# 怎么做

我们按照以下方法进行配方：

此配方的代码在`09/05/api.py`中。`JobListing`类现在有以下实现：

```py
class JobListing(Resource):
    def get(self, job_listing_id):
        print("Request for job listing with id: " + job_listing_id)

        es = Elasticsearch()
        if (es.exists(index='joblistings', doc_type='job-listing', id=job_listing_id)):
            print('Found the document in ElasticSearch')
            doc =  es.get(index='joblistings', doc_type='job-listing', id=job_listing_id)
            return doc['_source']

        listing = get_job_listing_info(job_listing_id)
        es.index(index='joblistings', doc_type='job-listing', id=job_listing_id, body=listing)

        print("Got the following listing as a response: " + listing)
        return listing
```

在调用 scraper 代码之前，API 检查 Elasticsearch 中是否已经存在文档。这是通过适当命名的“`exists`”方法执行的，该方法传递我们试图获取的索引、文档类型和 ID

如果返回 true，则使用 Elasticsearch 对象的`get`方法检索文档，该方法也被赋予相同的参数。这将返回一个 Python 字典，表示 Elasticsearch 文档，而不是我们存储的实际数据。通过访问`'_source'`引用实际数据/文档字典的钥匙。

# 还有更多。。。

`JobListingSkills`API 实现遵循稍微不同的模式。其代码如下：

```py
class JobListingSkills(Resource):
    def get(self, job_listing_id):
        print("Request for job listing's skills with id: " + job_listing_id)

        es = Elasticsearch()
        if (es.exists(index='joblistings', doc_type='job-listing', id=job_listing_id)):
            print('Found the document in ElasticSearch')
            doc =  es.get(index='joblistings', doc_type='job-listing', id=job_listing_id)
            return doc['_source']['JSON']['skills']

        skills = get_job_listing_skills(job_listing_id)

        print("Got the following skills as a response: " + skills)
        return skills
```

此实现仅使用 ElasticSearch 检查文档是否已在 ElasticSearch 中。它不会尝试保存从刮板中新检索的文档。这是因为`get_job_listing`刮板的结果只是技能列表，而不是整个文档。因此，此实现可以使用缓存，但它不添加新数据。这是一个设计决策，即使用不同的刮取方法，只返回被刮取的实际文档的子集。

对此的一个潜在解决方案是使用此 API 方法调用`get_job_listing_info`，然后保存文档，最后只返回特定子集（在本例中为技能）。同样，这也是围绕 sojobs 模块用户需要的方法类型的最终设计考虑。对于这些初始配方，人们认为最好在该级别使用两个不同的函数来返回不同的数据集。