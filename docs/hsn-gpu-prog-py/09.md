# 九、一种深度神经网络的实现

现在，我们将利用我们积累的 GPU 编程知识，用 PyCUDA 实现我们自己的深层神经网络（DNN）。DNN 在过去的十年中吸引了很多人的兴趣，因为它们为机器学习（ML）提供了一个健壮而优雅的模型。DNNs 也是首批能够通过利用其巨大的并行吞吐量展示 GPU 真正威力的应用程序（图形渲染之外）之一，最终帮助 NVIDIA 成为人工智能领域的主要参与者。

在本书的学习过程中，我们主要是在*气泡*中一章一章地涵盖各个主题。在这里，我们将以我们迄今为止所学的许多主题为基础，来实现 DNN。尽管目前有几种基于 GPU 的 DNN 开源框架可供公众使用，例如谷歌的 TensorFlow 和 Keras、微软的 CNTK、Facebook 的 Caffe2 和 PyTorch，但从头开始实施一个框架非常有启发性，这将使我们对 DNN 所需的底层技术有更深入的了解和了解。我们这里有很多资料要介绍，所以在简要介绍一些基本概念之后，我们将直接切入主题。

在本章中，我们将了解以下内容：

*   理解什么是**人工神经元****和**
*   了解在一个**深度神经网络**（**DNN**中可以组合多少 ANs）
*   在 CUDA 和 Python 中从头实现 DNN
*   了解如何使用交叉熵损失来评估神经网络的输出
*   实现梯度下降训练神经网络
*   学习如何在小数据集上训练和测试神经网络

# 技术要求

本章要求 Linux 或 Windows 10 PC 配备现代 NVIDIA GPU（2016 年起），并安装所有必要的 GPU 驱动程序和 CUDA 工具包（9.0 年起）。还需要使用 PyCUDA 模块安装合适的 Python 2.7（如 Anaconda Python 2.7）。

本章的代码也可在 GitHub 的[上找到 https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA) 。

For more information about the prerequisites for this chapter, check out the preface of this book. For the software and hardware requirements, check out the README file in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).

# 人工神经元与神经网络

让我们简要回顾一下**机器学习（ML）**和**神经网络（NNs）**的一些基础知识。在机器学习中，我们的目标是收集具有一组特定标记类或特征的数据，并使用这些示例来训练我们的系统以预测未来数据的值。我们将基于先前训练数据预测未来数据的类或标签的程序或函数称为**分类器**。

有许多类型的分类器，但这里我们将重点介绍 NNs。NNs 背后的理念是，它们（据称）的工作方式类似于人脑，它们使用**人工神经元（ANs）**的集合来学习和分类数据，所有这些神经元连接在一起形成一个特定的结构。让我们退一步，看看什么是个体。在数学上，这只是一个从线性空间**R<sup class="calibre54">n</sup>**到**R**的*仿射*函数，如下所示：

![](img/25478608-c882-49f1-85a9-9e3f6b0d472d.png)

我们可以看到，这可以被描述为恒定权重向量***w***和输入向量***x***之间的点积，并在末端添加了额外的偏置常数*b*。（同样，这里唯一输入到这个函数的*输入*是*x*；其他值都是常量！）

现在，个体*y*单个 AN 是相当无用的（也是愚蠢的），因为他们的*智能*只有在与大量其他 AN 合作时才会显现。我们的第一步是将*m*个类似 AN 的集合堆叠在彼此之上，以形成我们称之为**的致密层（DL）**。这是密集的，因为每个神经元将处理来自*x 的每个输入值–*每个 AN 将接收来自**R<sup class="calibre54">n</sup>n**的数组或向量值，并在**R**中输出单个值，因为存在*m*神经元，这意味着我们可以说它们的输出集中在空间**R<sup class="calibre54">m</sup>**中。我们会注意到，如果我们将层中每个神经元的权重进行叠加，从而形成一个*m x n*权重矩阵，那么我们就可以通过矩阵乘法，然后添加适当的偏差来计算每个神经元的输出：

![](img/03bf9c48-e6dc-4f86-9b18-9a0eac05c7cb.png)

现在，假设我们想要构建一个 NN 分类器，它可以对*k*不同的类进行分类；我们可以创建一个新的附加密集层，从先前的密集层接收*m*值，并输出*k*值。假设我们对每一层都有适当的权重和偏差值（这肯定不是很容易找到的），并且我们在每一层之后也设置了适当的**激活函数**（我们将在后面定义），这将作为我们*k*不同类之间的分类器，根据最后一层的输出，给出*x*落入各个类别的概率。当然，我们在这方面已经走在了前面，但简而言之，这就是 NN 的工作原理。

现在，我们似乎可以继续将密集层彼此连接成长链，以实现分类。这就是所谓的 DNN。当我们有一个不直接连接到输入或输出的层时，它被称为隐藏层。DNN 的优势在于，附加层允许 NN 捕获浅层 NN 无法获取的抽象和微妙的数据。

# 实现一个密集的人工神经元层

现在，让我们实现 NN 最重要的构建块，**密集层**。让我们先声明一个 CUDA 内核，如下所示：

```py
__global__ void dense_eval(int num_outputs, int num_inputs, int relu, int sigmoid, float * w, float * b, float * x, float *y, int batch_size, int w_t, int b_t, float delta)
```

让我们逐一检查输入。`num_outputs`当然表示该层的输出总数；这正是层中神经元的数量。`num_inputs`告诉我们输入数据的大小。为`relu`和`sigmoid`设置一个正值表示我们应该在该层的输出上使用相应的激活函数，我们将在后面定义。`w`和`b`是包含该层权重和偏差的数组，而`x`和`y`将作为我们的输入和输出。通常，我们希望一次对多个数据进行分类。我们可以通过将`batch_size`设置为我们希望预测的点数来表示。最后，将在训练过程中使用`w_t`、`b_t`、`delta`通过**梯度下降**确定该层合适的权重和偏差。（我们将在后面的部分中看到有关渐变下降的更多信息。）

现在，让我们开始编写内核。我们将在每个输出上并行计算，因此我们将为此目的设置一个整数`i`作为全局线程 ID，并让运行此内核的任何不必要的额外线程不使用适当的`if`语句执行任何操作：

```py
{
 int i = blockDim.x*blockIdx.x + threadIdx.x;

 if (i < num_outputs)
 {
```

现在，让我们使用适当的`for`循环迭代批处理中的每个数据点：

```py
for(int k=0; k < batch_size; k++)
 { 
```

我们将把来自权重和输入的 32 位浮点相乘并累加到 64 位双精度`temp`中，然后添加适当的偏置点。然后，我们将其类型转换回 32 位浮点，并将值放入输出数组中，然后关闭`k`上的循环：

```py
double temp = 0.0f;
 for (int j = 0; j < num_inputs; j++)
 {
   temp += ((double) w[(num_inputs)*i + j ] ) * ( (double) x[k*num_inputs + j]);
 }
 temp += (double) b[i];
 y[k * num_outputs + i] = (float) temp;  
}
```

*Multiply and accumulate* types of operations are generally subject to a great loss of numerical precision. This can be mitigated by using a temporary variable of higher precision to store values in the course of the operation, and then typecasting this variable back to the original precision after the operation is completed.

为了训练神经网络，我们最终必须计算神经网络相对于每个单独层中每个权重和偏差的导数（来自微积分），这是相对于特定批次的输入。请记住，数学函数*f*在*x*处的导数可以估计为*f**（x+δ）-f（x）/δ*，其中δ（δ）是一些足够小的正值。我们将使用输入值`w_t`和`b_t`向内核指示我们是否要计算关于特定权重或偏差的导数；否则，我们将这些输入值设置为负值，以便仅对此层进行计算。我们还将 delta 设置为计算导数的适当小值，并使用该值增加适当偏差或权重的值：

```py
if( w_t >= 0 && i == (w_t / num_inputs))
 {
 int j = w_t % num_inputs;
 for(int k=0; k < batch_size; k++)
  y[k*num_outputs + i] += delta*x[k*num_inputs+j];
}
if( b_t >= 0 && i == b_t )
 {
  for(int k=0; k < batch_size; k++)
  y[k*num_outputs + i] += delta;
 }
```

现在，我们将为所谓的**校正线性单元**（或**ReLU**和**乙状激活函数**添加一些代码。这些用于处理密集神经层的即时输出。ReLU 只将所有负值设置为 0，同时作为正输入的标识，而 sigmoid 只计算每个值上的`sigmoid`函数值（*1/（1+e<sup class="calibre54">-x</sup>*。ReLU（或任何其他激活函数）用于 NN 中的隐藏层之间，作为使整个 NN 作为非线性函数的手段；否则，整个 NN 将构成一个简单（且计算效率低）的矩阵运算。（虽然层之间可以使用许多其他非线性激活函数，但已发现 ReLU 是一种特别有效的训练函数。）Sigmoid 用作 NN 中的最后一层，用于**标记**，也就是说，可以为给定输入分配多个标签，与将输入分配给单个类不同。

在我们开始定义这个 CUDA 内核之前，让我们在文件中再上一点，并将这些操作定义为 C 宏。我们还将记住，在编写 CUDA-C 代码时，我们刚刚编写了以下代码：

```py
DenseEvalCode = '''
#define _RELU(x) ( ((x) > 0.0f) ? (x) : 0.0f )
#define _SIGMOID(x) ( 1.0f / (1.0f + expf(-(x)) ))
```

现在，我们将使用内核输入`relu`和`sigmoid`来指示是否应该使用这些附加层；我们将从中获得积极的信息，以表明应分别使用它们。我们可以添加它，关闭内核，并将其编译成可用的 Python 函数：

```py
if(relu > 0 || sigmoid > 0)
for(int k=0; k < batch_size; k++)
 { 
   float temp = y[k * num_outputs + i];
   if (relu > 0)
    temp = _RELU(temp);
   if (sigmoid > 0)
    temp = _SIGMOID(temp);
   y[k * num_outputs + i] = temp; 
  }
 }
 return;
}
'''
eval_mod = SourceModule(DenseEvalCode)
eval_ker = eval_mod.get_function('dense_eval')
```

现在，让我们转到文件的开头并设置适当的导入语句。请注意，我们将包括`csv`模块，用于处理测试和培训的数据输入：

```py
from __future__ import division
import pycuda.autoinit
import pycuda.driver as drv
from pycuda import gpuarray
from pycuda.compiler import SourceModule
from pycuda.elementwise import ElementwiseKernel
import numpy as np
from Queue import Queue
import csv
import time
```

现在，让我们继续设置稠密层；为了便于使用，我们希望将其封装在 Python 类中，当我们开始将这些密集层连接到一个完整的 NN 中时，这将使我们的生活更加轻松。我们将调用`class DenseLayer`并从编写构造函数开始。这里的大多数输入和设置应该是不言自明的：我们肯定应该添加一个选项来加载来自预训练网络的权重和偏差，我们还将包括指定默认*delta*值以及默认流的选项。（如果没有给出权重或偏差，权重将初始化为随机值，而所有偏差都设置为 0。）我们还将在此处指定是否使用 ReLU 或 sigmoid 层。最后，请注意我们如何设置块和栅格大小：

```py
class DenseLayer:
    def __init__(self, num_inputs=None, num_outputs=None, weights=None, b=None, stream=None, relu=False, sigmoid=False, delta=None):
        self.stream = stream

        if delta is None:
            self.delta = np.float32(0.001)
        else:
            self.delta = np.float32(delta)

        if weights is None:
            weights = np.random.rand(num_outputs, num_inputs) - .5
            self.num_inputs = np.int32(num_inputs)
        self.num_outputs = np.int32(num_outputs) 

        if type(weights) != pycuda.gpuarray.GPUArray:
            self.weights = gpuarray.to_gpu_async(np.array(weights, 
            dtype=np.float32) , stream = self.stream)
        else:
            self.weights = weights

        if num_inputs is None or num_outputs is None:
            self.num_inputs = np.int32(self.weights.shape[1])
            self.num_outputs = np.int32(self.weights.shape[0])

        else:
            self.num_inputs = np.int32(num_inputs)
            self.num_outputs = np.int32(num_outputs)

        if b is None:
            b = gpuarray.zeros((self.num_outputs,),dtype=np.float32)

        if type(b) != pycuda.gpuarray.GPUArray:
            self.b = gpuarray.to_gpu_async(np.array(b, 
            dtype=np.float32) , stream = self.stream)
        else:
            self.b = b 

        self.relu = np.int32(relu)
        self.sigmoid = np.int32(sigmoid)

        self.block = (32,1,1)
        self.grid = (int(np.ceil(self.num_outputs / 32)), 1,1)
```

现在，我们将在这个类中设置一个函数来评估来自这个层的输入；我们将仔细检查输入（x）以确定它是否已经在 GPU 上（如果没有，则将其转移到一个`gpuarray`，我们将让用户为输出（y）指定一个预分配的`gpuarray`，如果没有指定，则手动分配一个输出数组。我们还将检查训练情况下的增量和`w_t`/`b_t`值，以及`batch_size`。然后在`x`输入上运行内核，输出进入`y`，最后返回`y`作为输出值：

```py
def eval_(self, x, y=None, batch_size=None, stream=None, delta=None, w_t = None, b_t = None):

if stream is None:
    stream = self.stream

if type(x) != pycuda.gpuarray.GPUArray:
    x = gpuarray.to_gpu_async(np.array(x,dtype=np.float32), stream=self.stream)

if batch_size is None:
    if len(x.shape) == 2:
        batch_size = np.int32(x.shape[0])
    else:
        batch_size = np.int32(1)

if delta is None:
    delta = self.delta

delta = np.float32(delta)

if w_t is None:
    w_t = np.int32(-1)

if b_t is None:
    b_t = np.int32(-1)

if y is None:
    if batch_size == 1:
        y = gpuarray.empty((self.num_outputs,), dtype=np.float32)
    else:
        y = gpuarray.empty((batch_size, self.num_outputs), dtype=np.float32)

    eval_ker(self.num_outputs, self.num_inputs, self.relu, self.sigmoid, self.weights, self.b, x, y, np.int32(batch_size), w_t, b_t, delta , block=self.block, grid=self.grid , stream=stream)

 return y
```

好了。我们已经完全实现了密集层！

# softmax 层的实现

现在我们来看看如何实现**softmax 层**。正如我们已经讨论过的，sigmoid 层用于将标签分配给类，也就是说，如果您想要从输入推断出多个非排他性特征，则应该使用 sigmoid 层。当您只想通过推理将单个类分配给样本时，使用**softmax 层**。这是通过计算每个可能类的概率来完成的（当然，所有类的概率总和为 100%）。然后，我们可以选择概率最高的类来进行最终分类。

现在，让我们看看 softmax 层在给定一组*N*实数（*c<sub class="calibre21">0</sub>…，c<sub class="calibre21">N-1</sub>*的集合时做了什么，我们首先计算每个数上的指数函数之和（![](img/90e15296-aedc-4f2b-a9bf-8099d5a28a07.png)），然后计算每个数字除以该和的指数，得出 softmax：

![](img/ce20fc54-36ee-46a9-86a9-40387fb73545.png)

让我们从实现开始。我们将首先编写两个非常短的 CUDA 内核：一个取每个输入的指数，另一个取所有点的平均值：

```py
SoftmaxExpCode='''
__global__ void softmax_exp( int num, float *x, float *y, int batch_size)
{
 int i = blockIdx.x * blockDim.x + threadIdx.x;

 if (i < num)
 {
  for (int k=0; k < batch_size; k++)
  {
   y[num*k + i] = expf(x[num*k+i]);
  }
 }
}
'''
exp_mod = SourceModule(SoftmaxExpCode)
exp_ker = exp_mod.get_function('softmax_exp')

SoftmaxMeanCode='''
__global__ void softmax_mean( int num, float *x, float *y, int batch_size)
{
 int i = blockDim.x*blockIdx.x + threadIdx.x;

 if (i < batch_size)
 {
  float temp = 0.0f;

  for(int k=0; k < num; k++)
   temp += x[i*num + k];

  for(int k=0; k < num; k++)
   y[i*num+k] = x[i*num+k] / temp;
 }

 return;
}'''

mean_mod = SourceModule(SoftmaxMeanCode)
mean_ker = mean_mod.get_function('softmax_mean')
```

现在，让我们像前面一样编写一个 Python 包装器类。首先，我们将从构造函数开始，并用`num`指示输入和输出的数量。如果愿意，我们还可以指定默认流：

```py
class SoftmaxLayer:
    def __init__(self, num=None, stream=None):
     self.num = np.int32(num)
     self.stream = stream
```

现在，让我们用一种类似于致密层的方式来写`eval_ function`：

```py
def eval_(self, x, y=None, batch_size=None, stream=None):
 if stream is None:
 stream = self.stream

 if type(x) != pycuda.gpuarray.GPUArray:
  temp = np.array(x,dtype=np.float32)
  x = gpuarray.to_gpu_async( temp , stream=stream)

 if batch_size==None:
  if len(x.shape) == 2:
   batch_size = np.int32(x.shape[0])
  else:
   batch_size = np.int32(1)
 else:
  batch_size = np.int32(batch_size)

 if y is None:
  if batch_size == 1:
   y = gpuarray.empty((self.num,), dtype=np.float32)
 else:
  y = gpuarray.empty((batch_size, self.num), dtype=np.float32)

 exp_ker(self.num, x, y, batch_size, block=(32,1,1), grid=(int( np.ceil( self.num / 32) ), 1, 1), stream=stream)

 mean_ker(self.num, y, y, batch_size, block=(32,1,1), grid=(int( np.ceil( batch_size / 32)), 1,1), stream=stream)

 return y

```

# 交叉熵损失的实现

现在，让我们实现所谓的**交叉熵损失**函数。这用于测量在训练过程中，神经网络对一小部分数据点的精确度；损失函数输出的值越大，我们的神经网络对给定数据的正确分类就越不准确。我们通过计算 NN 的预期输出和实际输出之间的标准平均对数熵差来实现这一点。为了数值稳定性，我们将输出值限制为`1`：

```py
MAX_ENTROPY = 1

def cross_entropy(predictions=None, ground_truth=None):

 if predictions is None or ground_truth is None:
  raise Exception("Error! Both predictions and ground truth must be float32 arrays")

 p = np.array(predictions).copy()
 y = np.array(ground_truth).copy()

 if p.shape != y.shape:
  raise Exception("Error! Both predictions and ground_truth must have same shape.")

 if len(p.shape) != 2:
  raise Exception("Error! Both predictions and ground_truth must be 2D arrays.")

 total_entropy = 0

 for i in range(p.shape[0]):
  for j in range(p.shape[1]):
   if y[i,j] == 1: 
    total_entropy += min( np.abs( np.nan_to_num( np.log( p[i,j] ) ) ) , MAX_ENTROPY) 
   else: 
    total_entropy += min( np.abs( np.nan_to_num( np.log( 1 - p[i,j] ) ) ), MAX_ENTROPY)

 return total_entropy / p.size
```

# 顺序网络的实现

现在，让我们实现最后一个类，它将多个密集层和 softmax 层对象组合成一个连贯的前馈顺序神经网络。这将作为另一个类实现，它将包含其他类。让我们首先编写构造函数，我们将能够在此处设置最大批量大小，这将影响分配给该网络使用的内存量–我们将在列表变量`network_mem`中存储一些分配的内存，用于每个层的权重和输入/输出。我们还将在列表网络中存储`DenseLayer`和`SoftmaxLayer`对象，并在`network_summary`中存储 NN 中各层的信息。请注意，我们还可以在这里设置一些训练参数，包括增量、用于梯度下降的流的数量（我们将在后面看到），以及训练次数。

我们还可以在开始时看到另一个输入，称为层。在这里，我们可以通过描述每个层来指示 NN 的构造，构造函数将通过迭代层的每个元素并调用`add_layer`方法来创建该层，我们将在下面实现该方法：

```py
class SequentialNetwork:
 def __init__(self, layers=None, delta=None, stream = None, max_batch_size=32, max_streams=10, epochs = 10):

 self.network = []
 self.network_summary = []
 self.network_mem = []

 if stream is not None:
  self.stream = stream
 else:
  self.stream = drv.Stream()

 if delta is None:
  delta = 0.0001

 self.delta = delta
 self.max_batch_size=max_batch_size
 self.max_streams = max_streams
 self.epochs = epochs

 if layers is not None:
  for layer in layers:
   add_layer(self, layer)
```

现在，让我们实现`add_layer`方法。我们将使用字典数据类型将有关层的所有相关信息传递给顺序网络，包括层类型（密集、softmax 等）、输入/输出数量、权重和偏差。这会将适当的对象和信息附加到对象的网络和`network_summary`列表变量中，并将`gpuarray`对象适当分配到`network_mem`列表中：

```py
def add_layer(self, layer):
 if layer['type'] == 'dense':
  if len(self.network) == 0:
   num_inputs = layer['num_inputs']
  else:
   num_inputs = self.network_summary[-1][2]

  num_outputs = layer['num_outputs']
  sigmoid = layer['sigmoid']
  relu = layer['relu']
  weights = layer['weights']
  b = layer['bias']

  self.network.append(DenseLayer(num_inputs=num_inputs, num_outputs=num_outputs, sigmoid=sigmoid, relu=relu, weights=weights, b=b))
  self.network_summary.append( ('dense', num_inputs, num_outputs))

  if self.max_batch_size > 1:
   if len(self.network_mem) == 0:
self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][1]), dtype=np.float32))
 self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32 ) ) 
 else:
 if len(self.network_mem) == 0:
 self.network_mem.append( gpuarray.empty( (self.network_summary[-1][1], ), dtype=np.float32 ) )
 self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32 ) ) 

 elif layer['type'] == 'softmax':

  if len(self.network) == 0:
   raise Exception("Error! Softmax layer can't be first!")

  if self.network_summary[-1][0] != 'dense':
   raise Exception("Error! Need a dense layer before a softmax layer!")

  num = self.network_summary[-1][2]
  self.network.append(SoftmaxLayer(num=num))
  self.network_summary.append(('softmax', num, num))

  if self.max_batch_size > 1:
   self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32)) 
  else:
   self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32))
```

# 推理方法的实现

现在，我们将在`SequentialNetwork`类中添加两种推理方法，即预测特定输入的输出。第一种方法我们只调用`predict`，最终用户将使用它。在培训过程中，我们必须根据部分层的部分结果进行预测，为此我们将采用另一种方法，称为`partial_predict`。

让我们从实现*预测*开始。这将获取两个输入—一个一维或二维 NumPy 数组形式的样本集合，可能还有一个用户定义的 CUDA 流。我们将首先对样本进行一些类型检查和格式化（此处称为`x`），记住样本将按行存储：

```py
def predict(self, x, stream=None):

 if stream is None:
  stream = self.stream

 if type(x) != np.ndarray:
  temp = np.array(x, dtype = np.float32)
  x = temp

 if(x.size == self.network_mem[0].size):
  self.network_mem[0].set_async(x, stream=stream)
 else:

  if x.size > self.network_mem[0].size:
   raise Exception("Error: batch size too large for input.")

  x0 = np.zeros((self.network_mem[0].size,), dtype=np.float32)
  x0[0:x.size] = x.ravel()
  self.network_mem[0].set_async(x0.reshape( self.network_mem[0].shape), stream=stream)

 if(len(x.shape) == 2):
  batch_size = x.shape[0]
 else:
  batch_size = 1
```

现在，让我们执行实际的推断步骤。我们只需迭代整个神经网络，在每一层上执行`eval_`：

```py
for i in xrange(len(self.network)):
 self.network[i].eval_(x=self.network_mem[i], y= self.network_mem[i+1], batch_size=batch_size, stream=stream)
```

现在我们将提取 NN 的最终输出，即 GPU，并将其返回给用户。如果`x`中的样本数实际上小于最大批量，我们将在返回前对输出数组进行适当切片：

```py
y = self.network_mem[-1].get_async(stream=stream)

if len(y.shape) == 2:
 y = y[0:batch_size, :]

return y
```

现在，完成后，让我们实现`partial_predict`。让我们简要讨论一下这背后的想法。当我们在训练过程中，我们将评估一组样本，然后看看向每个权重和偏差分别添加*δ*的细微变化将如何影响输出。为了节省时间，我们可以计算每个层的输出并将其存储在给定的样本集合中，然后仅重新计算更改权重的层以及所有后续层的输出。我们将很快更深入地了解其背后的想法，但目前，我们可以这样实施：

```py
def partial_predict(self, layer_index=None, w_t=None, b_t=None, partial_mem=None, stream=None, batch_size=None, delta=None):

 self.network[layer_index].eval_(x=self.network_mem[layer_index], y = partial_mem[layer_index+1], batch_size=batch_size, stream = stream, w_t=w_t, b_t=b_t, delta=delta)

 for i in xrange(layer_index+1, len(self.network)):
  self.network[i].eval_(x=partial_mem[i], y =partial_mem[i+1], batch_size=batch_size, stream = stream)
```

# 梯度下降

现在，我们将以**批量随机梯度下降（BSGD）**的形式全面实现我们的神经网络的训练方法。让我们逐字思考这意味着什么。**批次**表示该训练算法一次对一组训练样本进行操作，而不是同时对所有样本进行操作，**随机**表示每个批次都是随机选择的。**梯度**意味着我们将使用微积分中的梯度，这里是损失函数中每个权重和偏差的导数集合。最后，**下降**意味着我们试图通过*减去*梯度，反复对权重和偏差进行细微更改，从而减少损失函数

Remember from calculus that the gradient of a point always points in the direction of the greatest *increase*, with its opposite direction being that of the greatest *decrease*. Since we want a *decrease*, we subtract the gradient.

现在我们将在`SequentialNetwork`类中实现 BSGD 作为`bsgd`方法。让我们逐一检查`bsgd`的输入参数：

*   `training`将是训练样本的二维 NumPy 数组
*   `labels`将是对应于每个训练样本的 NN 的最后一层的期望输出
*   `delta`将指示我们应将衍生工具计算的权重增加多少
*   `max_streams`将指示 BSGD 将在其上执行计算的最大并发 CUDA 流数
*   `batch_size`将表明我们希望批量多大，我们将在每次重量更新时计算损失函数
*   `epochs`将指示我们对当前样本集的顺序进行了多少次洗牌，分成多个批次，然后在上执行 BSGD
*   `training_rate`将指示我们使用梯度计算更新权重和偏差的速率

我们将像往常一样开始此方法，并执行一些检查和类型转换，将 CUDA 流对象集合设置到 Python 列表中，并在另一个列表中分配一些额外需要的 GPU 内存：

```py
def bsgd(self, training=None, labels=None, delta=None, max_streams = None, batch_size = None, epochs = 1, training_rate=0.01):

 training_rate = np.float32(training_rate)

 training = np.float32(training)
 labels = np.float32(labels)

 if( training.shape[0] != labels.shape[0] ):
  raise Exception("Number of training data points should be same as labels!")

 if max_streams is None:
  max_streams = self.max_streams

 if epochs is None:
 epochs = self.epochs

 if delta is None:
 delta = self.delta

 streams = []
 bgd_mem = []

 # create the streams needed for training
 for _ in xrange(max_streams):
  streams.append(drv.Stream())
  bgd_mem.append([])

 # allocate memory for each stream
 for i in xrange(len(bgd_mem)):
  for mem_bank in self.network_mem:
   bgd_mem[i].append( gpuarray.empty_like(mem_bank) )
```

现在，我们可以开始训练了。我们将首先对每个`epoch`进行整个 BSGD 的迭代，对每个历元的整个数据集执行随机洗牌。我们还将向终端打印一些信息，以便用户在培训过程中有一些状态更新：

```py
num_points = training.shape[0]

if batch_size is None:
 batch_size = self.max_batch_size

index = range(training.shape[0])

for k in xrange(epochs): 

 print '-----------------------------------------------------------'
 print 'Starting training epoch: %s' % k
 print 'Batch size: %s , Total number of training samples: %s' % (batch_size, num_points)
 print '-----------------------------------------------------------'

 all_grad = []

 np.random.shuffle(index)
```

现在，我们将创建一个循环，循环遍历无序数据集中的每个批。我们从计算当前批次的熵开始，我们也会打印出来。如果用户看到熵减少，他们就会知道梯度下降在这里起作用：

```py
for r in xrange(int(np.floor(training.shape[0]/batch_size))):

 batch_index = index[r*batch_size:(r+1)*batch_size] 

 batch_training = training[batch_index, :]
 batch_labels = labels[batch_index, :]

 batch_predictions = self.predict(batch_training)

 cur_entropy = cross_entropy(predictions=batch_predictions, ground_truth=batch_labels)

 print 'entropy: %s' % cur_entropy
```

现在，我们将迭代 NN 的每个密集层，计算整个权重和偏差集的梯度。我们将在*展平*（一维）数组中存储这些权重和偏差的导数，这将对应于我们 CUDA 内核中的`w_t`和`b_t`索引，它们也是展平的。由于我们将有多个流为不同的权重处理不同的输出，因此我们将使用 Python 队列容器来存储此批处理中尚未处理的权重和偏差集：然后我们可以将此容器顶部的值弹出到下一个可用流（我们将这些存储为元组，其中第一个元素特别指示这是权重还是偏差）：

```py
for i in xrange(len(self.network)):

 if self.network_summary[i][0] != 'dense':
  continue

 all_weights = Queue()

 grad_w = np.zeros((self.network[i].weights.size,), dtype=np.float32)
 grad_b = np.zeros((self.network[i].b.size,), dtype=np.float32)

 for w in xrange( self.network[i].weights.size ):
  all_weights.put( ('w', np.int32(w) ) )

 for b in xrange( self.network[i].b.size ):
  all_weights.put(('b', np.int32(b) ) )
```

现在，我们需要迭代每个权重和偏差，我们可以使用一个`while`循环来检查我们刚刚设置的`queue`对象是否为空。我们将设置另一个队列`stream_weights`，它将帮助我们组织每个流已处理的权重和偏差。在适当设置权重和偏差输入后，我们现在可以使用当前流和相应的 GPU 内存阵列来使用`partial_predict`：

Notice that we already performed a `predict` for this batch of samples to calculate the entropy, so we are now able to perform `partial_predict` on this batch, provided we are careful about which memory and layers we use.

```py
while not all_weights.empty():

 stream_weights = Queue()

 for j in xrange(max_streams):

  if all_weights.empty():
    break

  wb = all_weights.get()

  if wb[0] == 'w':
   w_t = wb[1]
   b_t = None
  elif wb[0] == 'b':
   b_t = wb[1]
   w_t = None

  stream_weights.put( wb )

  self.partial_predict(layer_index=i, w_t=w_t, b_t=b_t, partial_mem=bgd_mem[j], stream=streams[j], batch_size=batch_size, delta=delta)
```

我们只计算了一小部分权重和偏差变化的输出预测。我们必须计算每个的熵，然后将导数的值存储在平坦数组中：

```py
for j in xrange(max_streams):

 if stream_weights.empty():
  break

 wb = stream_weights.get()

 w_predictions = bgd_mem[j][-1].get_async(stream=streams[j])

 w_entropy = cross_entropy(predictions=w_predictions[ :batch_size,:], ground_truth=batch_labels)

 if wb[0] == 'w':
  w_t = wb[1]
  grad_w[w_t] = -(w_entropy - cur_entropy) / delta

 elif wb[0] == 'b':
  b_t = wb[1]
  grad_b[b_t] = -(w_entropy - cur_entropy) / delta
```

我们现在已经完成了`while`循环。一旦我们到达这个层的外部，我们就会知道我们已经计算了这个特定层的所有权重和偏差的导数。在我们迭代到下一层之前，我们将把当前权重和偏差集的梯度计算值附加到`all_grad`列表中。同时，我们还将把展开的权重列表重塑为原始形状：

```py
all_grad.append([np.reshape(grad_w,self.network[i].weights.shape) , grad_b])
```

在我们完成对每一层的迭代之后，我们可以对这批神经网络的权重和偏差进行优化。请注意，如果`training_rate`变量远小于`1`，这将降低权重更新的速度：

```py
for i in xrange(len(self.network)):
 if self.network_summary[i][0] == 'dense':
  new_weights = self.network[i].weights.get()
  new_weights += training_rate*all_grad[i][0]
  new_bias = self.network[i].b.get()
  new_bias += training_rate*all_grad[i][1]
  self.network[i].weights.set(new_weights)
  self.network[i].b.set(new_bias)
```

我们已经完全实现了一个（非常简单的）基于 GPU 的 DNN！

# 调整和规范化数据

在我们开始训练和测试我们全新的神经网络之前，我们需要退一步，谈谈**条件化**和**规范化**数据。NNs 极易受到数值误差的影响，尤其是当输入在规模上存在较大差异时。这可以通过适当的**调节**我们的训练数据来缓解；这意味着，对于输入样本中的每个点，我们将计算所有样本中每个点的平均值和方差，然后减去平均值并除以每个样本中每个点的标准偏差，然后将其输入 NN 进行训练或推断（预测）。这种方法称为 n**规范化**。让我们组合一个小 Python 函数，它可以为我们实现这一点：

```py
def condition_data(data, means=None, stds=None):

 if means is None:
  means = np.mean(data, axis=0)

 if stds is None:
  stds = np.std(data, axis = 0)

 conditioned_data = data.copy()
 conditioned_data -= means
 conditioned_data /= stds

 return (conditioned_data, means, stds)
```

# 虹膜数据集

现在，我们将为一个现实问题构建我们自己的 DNN：基于花瓣测量的花类型分类。为此，我们将与著名的*Iris 数据集*合作。该数据集存储为逗号分隔值（CSV）文本文件，每行包含四个不同的数值（花瓣测量值），然后是花朵类型（这里有三个类-*虹彩花*、*虹彩花*和*虹彩花*）。我们现在将设计一个小的 DNN，它将根据这个集合对虹膜的类型进行分类。

Before we continue, please download the Iris dataset and put it into your working directory.  This is available from the UC Irvine Machine Learning repository, which can be found here: [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).

首先，我们将把这个文件处理成适当的数据数组，用于培训和验证我们的 DNN。让我们先打开我们的主要功能；我们需要将花的名称翻译成 DNN 可以输出的实际类，因此让我们制作一个小词典，为每个类提供相应的标签。我们还将设置一些空列表来存储我们的培训数据和标签：

```py
if __name__ == '__main__':
 to_class = { 'Iris-setosa' : [1,0,0] , 'Iris-versicolor' : [0,1,0], 'Iris-virginica' : [0,0,1]}

 iris_data = []
 iris_labels = []
```

现在，让我们从 CSV 文件中读取。我们将使用前面导入的 Python 的`csv`模块中的`reader`函数：

```py
with open('C:/Users/btuom/examples/9/iris.data', 'rb') as csvfile:
 csvreader = csv.reader(csvfile, delimiter=',')
 for row in csvreader:
  newrow = []
  if len(row) != 5:
   break
  for i in range(4):
   newrow.append(row[i])
  iris_data.append(newrow)
  iris_labels.append(to_class[row[4]])
```

现在，我们将随机洗牌数据，并使用其中三分之二的样本作为训练数据。其余三分之一将用于测试（验证）数据：

```py
iris_len = len(iris_data)
shuffled_index = list(range(iris_len))
np.random.shuffle(shuffled_index)

iris_data = np.float32(iris_data)
iris_labels = np.float32(iris_labels)
iris_data = iris_data[shuffled_index, :]
iris_labels = iris_labels[shuffled_index,:]

t_len = (2*iris_len) // 3

iris_train = iris_data[:t_len, :]
label_train = iris_labels[:t_len, :]

iris_test = iris_data[t_len:,:]
label_test = iris_labels[t_len:, :]
```

现在，我们终于可以开始构建 DNN 了！首先，让我们创建一个`SequentialNetwork`对象。我们将`max_batch_size`设置为`32`：

```py
sn = SequentialNetwork( max_batch_size=32 )
```

现在，让我们创建 NN。这将包括四个密集层（两个隐藏层）和一个 softmax 层。我们将增加每一层的神经元数量，直到最后一层，最后一层只有三个输出（每个类一个）。每层神经元数量的增加使我们能够捕捉数据的一些微妙之处：

```py

sn.add_layer({'type' : 'dense', 'num_inputs' : 4, 'num_outputs' : 10, 'relu': True, 'sigmoid': False, 'weights' : None, 'bias' : None} ) 
sn.add_layer({'type' : 'dense', 'num_inputs' : 10, 'num_outputs' : 15, 'relu': True, 'sigmoid': False, 'weights': None, 'bias' : None} ) 
sn.add_layer({'type' : 'dense', 'num_inputs' : 15, 'num_outputs' : 20, 'relu': True, 'sigmoid': False, 'weights': None, 'bias' : None} ) 
sn.add_layer({'type' : 'dense', 'num_inputs' : 20, 'num_outputs' : 3, 'relu': True, 'sigmoid': False, 'weights': None , 'bias': None } ) 
sn.add_layer({'type' : 'softmax'})
```

现在，我们将调整我们的训练数据，并使用我们刚刚实现的 BSGD 方法开始训练。我们将把`batch_size`设置为`16`、`max_streams`设置为`10`、`epochs`的数量设置为 100、`delta`设置为 0.0001、`training_rate`设置为 1——这些将是几乎任何现代 GPU 都可以接受的参数。我们还将在培训过程中确定时间，这可能相当耗时：

```py
ctrain, means, stds = condition_data(iris_train)

t1 = time()
sn.bsgd(training=ctrain, labels=label_train, batch_size=16, max_streams=20, epochs=100 , delta=0.0001, training_rate=1)
training_time = time() - t1
```

现在，我们的 DNN 接受了全面培训。我们已准备好开始验证过程！让我们设置一个名为`hits`的 Python 变量来计算正确分类的总数。我们还需要调整验证/测试数据。还有一件事，我们通过对应于 DNN 的 softmax 层的最大值的索引来确定类。我们可以使用 NumPy 的`argmax`函数来检查这是否为我们提供了正确的分类，如下所示：

```py
hits = 0
ctest, _, _ = condition_data(iris_test, means=means, stds=stds)
for i in range(ctest.shape[0]):
 if np.argmax(sn.predict(ctest[i,:])) == np.argmax( label_test[i,:]):
  hits += 1
```

现在，我们准备检查 DNN 的实际工作情况。让我们输出精度和总训练时间：

```py
print 'Percentage Correct Classifications: %s' % (float(hits ) / ctest.shape[0])
print 'Total Training Time: %s' % training_time
```

现在，我们完成了。我们现在可以用 Python 和 CUDA 完全实现 DNN 了！一般来说，对于这个特定的问题，您可以期望精度在 80%-97%之间，在任何 Pascal 级别的 GPU 上训练时间为 10-20 分钟

The code for this chapter is available in the `deep_neural_network.py` file, under the appropriate directory in this book's GitHub repository.

# 总结

在本章中，我们首先给出了人工神经网络的定义，并向您展示了如何将单个 ANs 组合成密集的层，这些层组合成一个完整的深层神经网络。然后，我们在 CUDA-C 中实现了一个密集层，并创建了相应的 Python 包装器类。我们还包括在密集层的输出上添加 ReLU 和 sigmoid 层的功能。我们了解了使用 softmax 层（用于分类问题）的定义和动机，然后在 CUDA-C 和 Python 中实现了它。最后，我们实现了一个 Python 类，这样我们就可以从前面的类中构建一个顺序前馈 DNN；我们实现了一个交叉熵损失函数，然后将其用于梯度下降的损失函数中，以训练 DNN 中的权重和偏差。最后，我们使用我们的实现在真实数据集上构造、训练和测试 DNN。

我们现在对我们的 CUDA 编程能力充满信心，因为我们可以编写自己的基于 GPU 的 DNN！在接下来的两章中，我们将继续学习一些非常高级的材料，我们将研究如何为编译后的 CUDA 代码编写自己的接口，以及 NVIDIA GPU 的一些非常技术性的细节。

# 问题

1.  假设您构造了一个 DNN，在对它进行训练之后，它只产生垃圾。检查后，您会发现所有权重和偏差都是巨大的数字或 NaN。问题可能是什么？
2.  用一个小的`training_rate`值列出一个可能的问题。
3.  请说出一个可能存在的问题，该问题的`training_rate`值较大。
4.  假设我们想要训练一个 DNN，它将为一个动物的图像分配多个标签（“粘的”、“毛茸茸的”、“红色”、“棕色”等等）。我们应该在 DNN 的末尾使用 sigmoid 或 softmax 层吗？
5.  假设我们想要将一个动物的图像分类为猫或狗。我们使用的是 sigmoid 还是 softmax？
6.  如果我们减少批量大小，在梯度下降训练期间，权重和偏差会有或多或少的更新吗？