["```py\nfrom __future__ import print_function\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\nfrom datetime import datetime as dt\nfrom datetime import timedelta\n```", "```py\nif __name__ == '__main__':\n    parser = ArgumentParser(\n        description=__description__,\n        formatter_class=ArgumentDefaultsHelpFormatter,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"date_value\", help=\"Raw date value to parse\")\n    parser.add_argument(\"source\", help=\"Source format of date\",\n                        choices=ParseDate.get_supported_formats())\n    parser.add_argument(\"type\", help=\"Data type of input value\",\n                        choices=('number', 'hex'), default='int')\n    args = parser.parse_args()\n\n    date_parser = ParseDate(args.date_value, args.source, args.type)\n    date_parser.run()\n    print(date_parser.timestamp)\n```", "```py\nclass ParseDate(object):\n    def __init__(self, date_value, source, data_type):\n        self.date_value = date_value\n        self.source = source\n        self.data_type = data_type\n        self.timestamp = None\n```", "```py\n    def run(self):\n        if self.source == 'unix-epoch':\n            self.parse_unix_epoch()\n        elif self.source == 'unix-epoch-ms':\n            self.parse_unix_epoch(True)\n        elif self.source == 'windows-filetime':\n            self.parse_windows_filetime()\n```", "```py\n    @classmethod\n    def get_supported_formats(cls):\n        return ['unix-epoch', 'unix-epoch-ms', 'windows-filetime']\n```", "```py\n    def parse_unix_epoch(self, milliseconds=False):\n        if self.data_type == 'hex':\n            conv_value = int(self.date_value)\n            if milliseconds:\n                conv_value = conv_value / 1000.0\n        elif self.data_type == 'number':\n            conv_value = float(self.date_value)\n            if milliseconds:\n                conv_value = conv_value / 1000.0\n        else:\n            print(\"Unsupported data type '{}' provided\".format(\n                self.data_type))\n            sys.exit('1')\n\n        ts = dt.fromtimestamp(conv_value)\n        self.timestamp = ts.strftime('%Y-%m-%d %H:%M:%S.%f')\n```", "```py\n    def parse_windows_filetime(self):\n        if self.data_type == 'hex':\n            microseconds = int(self.date_value, 16) / 10.0\n        elif self.data_type == 'number':\n            microseconds = float(self.date_value) / 10\n        else:\n            print(\"Unsupported data type '{}' provided\".format(\n                self.data_type))\n            sys.exit('1')\n\n        ts = dt(1601, 1, 1) + timedelta(microseconds=microseconds)\n        self.timestamp = ts.strftime('%Y-%m-%d %H:%M:%S.%f')\n```", "```py\nfrom __future__ import print_function\nfrom argparse import ArgumentParser, FileType\nimport re\nimport shlex\nimport logging\nimport sys\nimport csv\n\nlogger = logging.getLogger(__file__)\n```", "```py\niis_log_format = [\n    (\"date\", re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")),\n    (\"time\", re.compile(r\"\\d\\d:\\d\\d:\\d\\d\")),\n    (\"s-ip\", re.compile(\n        r\"((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}\")),\n    (\"cs-method\", re.compile(\n        r\"(GET)|(POST)|(PUT)|(DELETE)|(OPTIONS)|(HEAD)|(CONNECT)\")),\n    (\"cs-uri-stem\", re.compile(r\"([A-Za-z0-1/\\.-]*)\")),\n    (\"cs-uri-query\", re.compile(r\"([A-Za-z0-1/\\.-]*)\")),\n    (\"s-port\", re.compile(r\"\\d*\")),\n    (\"cs-username\", re.compile(r\"([A-Za-z0-1/\\.-]*)\")),\n    (\"c-ip\", re.compile(\n        r\"((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}\")),\n    (\"cs(User-Agent)\", re.compile(r\".*\")),\n    (\"sc-status\", re.compile(r\"\\d*\")),\n    (\"sc-substatus\", re.compile(r\"\\d*\")),\n    (\"sc-win32-status\", re.compile(r\"\\d*\")),\n    (\"time-taken\", re.compile(r\"\\d*\"))\n]\n```", "```py\nif __name__ == '__main__':\n    parser = ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument('iis_log', help=\"Path to IIS Log\",\n                        type=FileType('r'))\n    parser.add_argument('csv_report', help=\"Path to CSV report\")\n    parser.add_argument('-l', help=\"Path to processing log\",\n                        default=__name__ + '.log')\n    args = parser.parse_args()\n\n    logger.setLevel(logging.DEBUG)\n    msg_fmt = logging.Formatter(\"%(asctime)-15s %(funcName)-10s \"\n                                \"%(levelname)-8s %(message)s\")\n\n    strhndl = logging.StreamHandler(sys.stdout)\n    strhndl.setFormatter(fmt=msg_fmt)\n    fhndl = logging.FileHandler(args.log, mode='a')\n    fhndl.setFormatter(fmt=msg_fmt)\n\n    logger.addHandler(strhndl)\n    logger.addHandler(fhndl)\n\n    logger.info(\"Starting IIS Parsing \")\n    logger.debug(\"Supplied arguments: {}\".format(\", \".join(sys.argv[1:])))\n    logger.debug(\"System \" + sys.platform)\n    logger.debug(\"Version \" + sys.version)\n    main(args.iis_log, args.csv_report, logger)\n    logger.info(\"IIS Parsing Complete\")\n```", "```py\ndef main(iis_log, report_file, logger):\n    parsed_logs = []\n    for raw_line in iis_log:\n        line = raw_line.strip()\n        log_entry = {}\n        if line.startswith(\"#\") or len(line) == 0:\n            continue\n        if '\\\"' in line:\n            line_iter = shlex.shlex(line_iter)\n        else:\n            line_iter = line.split(\" \")\n```", "```py\n        for count, split_entry in enumerate(line_iter):\n            col_name, col_pattern = iis_log_format[count]\n            if col_pattern.match(split_entry):\n                log_entry[col_name] = split_entry\n            else:\n                logger.error(\"Unknown column pattern discovered. \"\n                             \"Line preserved in full below\")\n                logger.error(\"Unparsed Line: {}\".format(line))\n\n        parsed_logs.append(log_entry)\n```", "```py\n    logger.info(\"Parsed {} lines\".format(len(parsed_logs)))\n\n    cols = [x[0] for x in iis_log_format]\n    logger.info(\"Creating report file: {}\".format(report_file))\n    write_csv(report_file, cols, parsed_logs)\n    logger.info(\"Report created\")\n```", "```py\ndef write_csv(outfile, fieldnames, data):\n    with open(outfile, 'w', newline=\"\") as open_outfile:\n        csvfile = csv.DictWriter(open_outfile, fieldnames)\n        csvfile.writeheader()\n        csvfile.writerows(data)\n```", "```py\npip install splunk-sdk==1.6.2\n```", "```py\nfrom __future__ import print_function\nfrom argparse import ArgumentParser, ArgumentError\nfrom argparse import ArgumentDefaultsHelpFormatter\nimport splunklib.client as client\nimport splunklib.results as results\nimport os\nimport sys\nimport csv\n\nif sys.version_info.major != 2:\n    print(\"Invalid python version. Must use Python 2 due to splunk api \"\n          \"library\")\n```", "```py\nif __name__ == '__main__':\n    parser = ArgumentParser(\n        description=__description__,\n        formatter_class=ArgumentDefaultsHelpFormatter,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument('action', help=\"Action to run\",\n                        choices=['index', 'query', 'export'])\n    parser.add_argument('--index-name', help=\"Name of splunk index\",\n                        required=True)\n    parser.add_argument('--config',\n                        help=\"Place where login details are stored.\"\n                        \" Should have the username on the first line and\"\n                        \" the password on the second.\"\n                        \" Please Protect this file!\",\n                        default=os.path.expanduser(\"~/.splunk_py.ini\"))\n```", "```py\n    parser.add_argument('--file', help=\"Path to file\")\n    parser.add_argument('--query', help=\"Splunk query to run or sid of \"\n                        \"existing query to export\")\n```", "```py\n    parser.add_argument(\n        '--cols',\n        help=\"Speficy columns to export. comma seperated list\",\n        default='_time,date,time,sc_status,c_ip,s_ip,cs_User_Agent')\n    parser.add_argument('--host', help=\"hostname of server\",\n                        default=\"localhost\")\n    parser.add_argument('--port', help=\"help\", default=\"8089\")\n    args = parser.parse_args()\n```", "```py\n    with open(args.config, 'r') as open_conf:\n        username, password = [x.strip() for x in open_conf.readlines()]\n    conn_dict = {'host': args.host, 'port': int(args.port),\n                 'username': username, 'password': password}\n    del(username)\n    del(password)\n    service = client.connect(**conn_dict)\n    del(conn_dict)\n\n    if len(service.apps) == 0:\n        print(\"Login likely unsuccessful, cannot find any applications\")\n        sys.exit()\n```", "```py\n    cols = args.cols.split(\",\")\n    spelunking = Spelunking(service, args.action, args.index_name, cols)\n```", "```py\n    if spelunking.action == 'index':\n        if 'file' not in vars(args):\n            ArgumentError('--file parameter required')\n            sys.exit()\n        else:\n            spelunking.file = os.path.abspath(args.file)\n\n    elif spelunking.action == 'export':\n        if 'file' not in vars(args):\n            ArgumentError('--file parameter required')\n            sys.exit()\n        if 'query' not in vars(args):\n            ArgumentError('--query parameter required')\n            sys.exit()\n        spelunking.file = os.path.abspath(args.file)\n        spelunking.sid = args.query\n\n    elif spelunking.action == 'query':\n        if 'query' not in vars(args):\n            ArgumentError('--query parameter required')\n            sys.exit()\n        else:\n            spelunking.query = \"search index={} {}\".format(args.index_name,\n                                                           args.query)\n\n    else:\n        ArgumentError('Unknown action required')\n        sys.exit()\n\n    spelunking.run()\n```", "```py\nclass Spelunking(object):\n    def __init__(self, service, action, index_name, cols):\n        self.service = service\n        self.action = action\n        self.index = index_name\n        self.file = None\n        self.query = None\n        self.sid = None\n        self.job = None\n        self.cols = cols\n```", "```py\n    def run(self):\n        index_obj = self.get_or_create_index()\n        if self.action == 'index':\n            self.index_data(index_obj)\n        elif self.action == 'query':\n            self.query_index()\n        elif self.action == 'export':\n            self.export_report()\n        return\n```", "```py\n    def get_or_create_index(self):\n        # Create a new index\n        if self.index not in self.service.indexes:\n            return service.indexes.create(self.index)\n        else:\n            return self.service.indexes[self.index]\n```", "```py\n    def index_data(self, splunk_index):\n        splunk_index.upload(self.file)\n```", "```py\n    def query_index(self):\n        self.query = self.query + \"| fields + \" + \", \".join(self.cols)\n        self.job = self.service.jobs.create(self.query, rf=self.cols)\n        self.sid = self.job.sid\n        print(\"Query job {} created. will expire in {} seconds\".format(\n            self.sid, self.job['ttl']))\n```", "```py\n    def export_report(self):\n        job_obj = None\n        for j in self.service.jobs:\n            if j.sid == self.sid:\n                job_obj = j\n\n        if job_obj is None:\n            print(\"Job SID {} not found. Did it expire?\".format(self.sid))\n            sys.exit()\n\n        if not job_obj.is_ready():\n            print(\"Job SID {} is still processing. \"\n                  \"Please wait to re-run\".format(self.sir))\n```", "```py\n        export_data = []\n        job_results = job_obj.results(rf=self.cols)\n        for result in results.ResultsReader(job_results):\n            export_data.append(result)\n\n        self.write_csv(self.file, self.cols, export_data)\n```", "```py\n    @staticmethod\n    def write_csv(outfile, fieldnames, data):\n        with open(outfile, 'wb') as open_outfile:\n            csvfile = csv.DictWriter(open_outfile, fieldnames,\n                                     extrasaction=\"ignore\")\n            csvfile.writeheader()\n            csvfile.writerows(data)\n```", "```py\nfrom __future__ import print_function\nfrom argparse import ArgumentParser, FileType\nfrom datetime import datetime\nimport csv\n```", "```py\nif __name__ == '__main__':\n    parser = ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"daily_out\", help=\"Path to daily.out file\",\n                        type=FileType('r'))\n    parser.add_argument(\"output_report\", help=\"Path to csv report\")\n    args = parser.parse_args()\n\n    processor = ProcessDailyOut(args.daily_out)\n    parsed_events = processor.run()\n    write_csv(args.output_report, processor.report_columns, parsed_events)\n```", "```py\nclass ProcessDailyOut(object):\n    def __init__(self, daily_out):\n        self.daily_out = daily_out\n        self.disk_status_columns = [\n            'Filesystem', 'Size', 'Used', 'Avail', 'Capacity', 'iused',\n            'ifree', '%iused', 'Mounted on']\n        self.report_columns = ['event_date', 'event_tz'] + \\\n            self.disk_status_columns\n```", "```py\n    def run(self):\n        event_lines = []\n        parsed_events = []\n        for raw_line in self.daily_out:\n            line = raw_line.strip()\n            if line == '-- End of daily output --':\n                parsed_events += self.process_event(event_lines)\n                event_lines = []\n            else:\n                event_lines.append(line)\n        return parsed_events\n```", "```py\n    def process_event(self, event_lines):\n        section_header = \"\"\n        section_data = []\n        event_data = {}\n        for line in event_lines:\n            if line.endswith(\":\"):\n                if len(section_data) > 0:\n                    event_data[section_header] = section_data\n                    section_data = []\n                    section_header = \"\"\n\n                section_header = line.strip(\":\")\n```", "```py\n            elif line.count(\":\") == 2:\n                try:\n                    split_line = line.split()\n                    timezone = split_line[4]\n                    date_str = \" \".join(split_line[:4] + [split_line[-1]])\n                    try:\n                        date_val = datetime.strptime(\n                            date_str, \"%a %b %d %H:%M:%S %Y\")\n                    except ValueError:\n                        date_val = datetime.strptime(\n                            date_str, \"%a %b %d %H:%M:%S %Y\")\n                    event_data[\"event_date\"] = [date_val, timezone]\n                    section_data = []\n                    section_header = \"\"\n                except ValueError:\n                    section_data.append(line)\n                except IndexError:\n                    section_data.append(line)\n```", "```py\n            else:\n                if len(line):\n                    section_data.append(line)\n```", "```py\n        return self.process_disk(event_data.get(\"Disk status\", []),\n                                 event_data.get(\"event_date\", []))\n```", "```py\n    def process_disk(self, disk_lines, event_dates):\n        if len(disk_lines) == 0:\n            return {}\n\n        processed_data = []\n        for line_count, line in enumerate(disk_lines):\n            if line_count == 0:\n                continue\n            prepped_lines = [x for x in line.split(\" \")\n                             if len(x.strip()) != 0]\n```", "```py\n            disk_info = {\n                \"event_date\": event_dates[0],\n                \"event_tz\": event_dates[1]\n            }\n            for col_count, entry in enumerate(prepped_lines):\n                curr_col = self.disk_status_columns[col_count]\n                if \"/Volumes/\" in entry:\n                    disk_info[curr_col] = \" \".join(\n                        prepped_lines[col_count:])\n                    break\n                disk_info[curr_col] = entry.strip()\n```", "```py\n            processed_data.append(disk_info)\n        return processed_data\n```", "```py\ndef write_csv(outfile, fieldnames, data):\n    with open(outfile, 'w', newline=\"\") as open_outfile:\n        csvfile = csv.DictWriter(open_outfile, fieldnames)\n        csvfile.writeheader()\n        csvfile.writerows(data)\n```", "```py\nfrom __future__ import print_function\nfrom axiom import *\nfrom datetime import datetime\n```", "```py\nclass DailyOutArtifact(Artifact):\n    def __init__(self):\n        self.AddHunter(DailyOutHunter())\n\n    def GetName(self):\n        return 'daily.out parser'\n```", "```py\n    def CreateFragments(self):\n        self.AddFragment('Snapshot Date - LocalTime (yyyy-mm-dd)',\n                         Category.DateTimeLocal, FragmentType.DateTime)\n        self.AddFragment('Snapshot Timezone', Category.None,\n                         FragmentType.String)\n        self.AddFragment('Volume Name',\n                         Category.None, FragmentType.String)\n        self.AddFragment('Filesystem Mount',\n                         Category.None, FragmentType.String)\n        self.AddFragment('Volume Size',\n                         Category.None, FragmentType.String)\n        self.AddFragment('Volume Used',\n                         Category.None, FragmentType.String)\n        self.AddFragment('Percentage Used',\n                         Category.None, FragmentType.String)\n```", "```py\nclass DailyOutHunter(Hunter):\n    def __init__(self):\n        self.Platform = Platform.Computer\n\n    def Register(self, registrar):\n        registrar.RegisterFileName('daily.out')\n```", "```py\n    def Hunt(self, context):\n        temp_daily_out = open(context.Searchable.FileCopy, 'r')\n\n        processor = ProcessDailyOut(temp_daily_out)\n        parsed_events = processor.run()\n```", "```py\n        for entry in parsed_events:\n            hit = Hit()\n            hit.AddValue(\n                \"Snapshot Date - LocalTime (yyyy-mm-dd)\",\n                entry['event_date'].strftime(\"%Y-%m-%d %H:%M:%S\"))\n            hit.AddValue(\"Snapshot Timezone\", entry['event_tz'])\n            hit.AddValue(\"Volume Name\", entry['Mounted on'])\n            hit.AddValue(\"Filesystem Mount\", entry[\"Filesystem\"])\n            hit.AddValue(\"Volume Size\", entry['Size'])\n            hit.AddValue(\"Volume Used\", entry['Used'])\n            hit.AddValue(\"Percentage Used\", entry['Capacity'])\n            self.PublishHit(hit)\n```", "```py\n        if temp_daily_out is not None:\n            temp_daily_out.close()\n```", "```py\nRegisterArtifact(DailyOutArtifact())\n```", "```py\npip install yara-python==3.6.3\n```", "```py\nfrom __future__ import print_function\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\nimport os\nimport csv\nimport yara\n```", "```py\nif __name__ == '__main__':\n    parser = ArgumentParser(\n        description=__description__,\n        formatter_class=ArgumentDefaultsHelpFormatter,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\n        'yara_rules',\n        help=\"Path to Yara rule to scan with. May be file or folder path.\")\n    parser.add_argument(\n        'path_to_scan',\n        help=\"Path to file or folder to scan\")\n    parser.add_argument(\n        '--output',\n        help=\"Path to output a CSV report of scan results\")\n    args = parser.parse_args()\n\n    main(args.yara_rules, args.path_to_scan, args.output)\n```", "```py\ndef main(yara_rules, path_to_scan, output):\n    if os.path.isdir(yara_rules):\n        yrules = yara.compile(yara_rules)\n    else:\n        yrules = yara.compile(filepath=yara_rules)\n```", "```py\n    if os.path.isdir(path_to_scan):\n        match_info = process_directory(yrules, path_to_scan)\n    else:\n        match_info = process_file(yrules, path_to_scan)\n```", "```py\n    columns = ['rule_name', 'hit_value', 'hit_offset', 'file_name',\n               'rule_string', 'rule_tag']\n\n    if output is None:\n        write_stdout(columns, match_info)\n    else:\n        write_csv(output, columns, match_info)\n```", "```py\ndef process_directory(yrules, folder_path):\n    match_info = []\n    for root, _, files in os.walk(folder_path):\n        for entry in files:\n            file_entry = os.path.join(root, entry)\n            match_info += process_file(yrules, file_entry)\n    return match_info\n```", "```py\ndef process_file(yrules, file_path):\n    match = yrules.match(file_path)\n    match_info = []\n    for rule_set in match:\n        for hit in rule_set.strings:\n            match_info.append({\n                'file_name': file_path,\n                'rule_name': rule_set.rule,\n                'rule_tag': \",\".join(rule_set.tags),\n                'hit_offset': hit[0],\n                'rule_string': hit[1],\n                'hit_value': hit[2]\n            })\n    return match_info\n```", "```py\ndef write_stdout(columns, match_info):\n    for entry in match_info:\n        for col in columns:\n            print(\"{}: {}\".format(col, entry[col]))\n        print(\"=\" * 30)\n```", "```py\ndef write_csv(outfile, fieldnames, data):\n    with open(outfile, 'w', newline=\"\") as open_outfile:\n        csvfile = csv.DictWriter(open_outfile, fieldnames)\n        csvfile.writeheader()\n        csvfile.writerows(data)\n```"]