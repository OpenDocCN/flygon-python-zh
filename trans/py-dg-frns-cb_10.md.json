["```py\npip install unicodecsv==0.14.1\n```", "```py\nfrom __future__ import print_function\nimport argparse\nfrom datetime import datetime, timedelta\nimport os\nimport pytsk3\nimport pyewf\nimport struct\nimport sys\nimport unicodecsv as csv\nfrom utility.pytskutil import TSKUtil\n```", "```py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"EVIDENCE_FILE\", help=\"Evidence file path\")\n    parser.add_argument(\"TYPE\", help=\"Type of Evidence\",\n                        choices=(\"raw\", \"ewf\"))\n    parser.add_argument(\"OUTPUT_CSV\", help=\"Path to write output csv\")\n    parser.add_argument(\"-d\", help=\"Prefetch directory to scan\",\n                        default=\"/WINDOWS/PREFETCH\")\n    args = parser.parse_args()\n\n    if os.path.exists(args.EVIDENCE_FILE) and \\\n            os.path.isfile(args.EVIDENCE_FILE):\n        main(args.EVIDENCE_FILE, args.TYPE, args.OUTPUT_CSV, args.d)\n    else:\n        print(\"[-] Supplied input file {} does not exist or is not a \"\n              \"file\".format(args.EVIDENCE_FILE))\n        sys.exit(1)\n```", "```py\ndef main(evidence, image_type, output_csv, path):\n    # Create TSK object and query path for prefetch files\n    tsk_util = TSKUtil(evidence, image_type)\n    prefetch_dir = tsk_util.query_directory(path)\n    prefetch_files = None\n    if prefetch_dir is not None:\n        prefetch_files = tsk_util.recurse_files(\n            \".pf\", path=path, logic=\"endswith\")\n```", "```py\n    if prefetch_files is None:\n        print(\"[-] No .pf files found\")\n        sys.exit(2)\n\n    print(\"[+] Identified {} potential prefetch files\".format(\n          len(prefetch_files)))\n    prefetch_data = []\n    for hit in prefetch_files:\n        prefetch_file = hit[2]\n        pf_version = check_signature(prefetch_file)\n```", "```py\ndef check_signature(prefetch_file):\n    version, signature = struct.unpack(\n        \"<2i\", prefetch_file.read_random(0, 8))\n\n    if signature == 1094927187:\n        return version\n    else:\n        return None\n```", "```py\n        if pf_version is None:\n            continue\n\n        pf_name = hit[0]\n        if pf_version == 17:\n            parsed_data = parse_pf_17(prefetch_file, pf_name)\n            parsed_data.append(os.path.join(path, hit[1].lstrip(\"//\")))\n            prefetch_data.append(parsed_data)\n```", "```py\ndef parse_pf_17(prefetch_file, pf_name):\n    # Parse Windows XP, 2003 Prefetch File\n    create = convert_unix(prefetch_file.info.meta.crtime)\n    modify = convert_unix(prefetch_file.info.meta.mtime)\n```", "```py\ndef convert_unix(ts):\n    if int(ts) == 0:\n        return \"\"\n    return datetime.utcfromtimestamp(ts)\n\ndef convert_filetime(ts):\n    if int(ts) == 0:\n        return \"\"\n    return datetime(1601, 1, 1) + timedelta(microseconds=ts / 10)\n```", "```py\n    pf_size, name, vol_info, vol_entries, vol_size, filetime, \\\n        count = struct.unpack(\"<i60s32x3iq16xi\",\n                              prefetch_file.read_random(12, 136))\n\n    name = name.decode(\"utf-16\", \"ignore\").strip(\"/x00\").split(\"/x00\")[0]\n```", "```py\n    vol_name_offset, vol_name_length, vol_create, \\\n        vol_serial = struct.unpack(\"<2iqi\",\n                                   prefetch_file.read_random(vol_info, 20))\n\n    vol_serial = hex(vol_serial).lstrip(\"0x\")\n    vol_serial = vol_serial[:4] + \"-\" + vol_serial[4:]\n\n    vol_name = struct.unpack(\n        \"<{}s\".format(2 * vol_name_length),\n        prefetch_file.read_random(vol_info + vol_name_offset,\n                                  vol_name_length * 2)\n    )[0]\n\n    vol_name = vol_name.decode(\"utf-16\", \"ignore\").strip(\"/x00\").split(\n        \"/x00\")[0]\n\n    return [\n        pf_name, name, pf_size, create,\n        modify, convert_filetime(filetime), count, vol_name,\n        convert_filetime(vol_create), vol_serial\n    ]\n```", "```py\n        elif pf_version == 23:\n            print(\"[-] Windows Vista / 7 PF file {} -- unsupported\".format(\n                pf_name))\n            continue\n        elif pf_version == 26:\n            print(\"[-] Windows 8 PF file {} -- unsupported\".format(\n                pf_name))\n            continue\n        elif pf_version == 30:\n            print(\"[-] Windows 10 PF file {} -- unsupported\".format(\n                pf_name))\n            continue\n```", "```py\n        else:\n            print(\"[-] Signature mismatch - Name: {}\\nPath: {}\".format(\n                hit[0], hit[1]))\n            continue\n\n    write_output(prefetch_data, output_csv)\n```", "```py\ndef write_output(data, output_csv):\n    print(\"[+] Writing csv report\")\n    with open(output_csv, \"wb\") as outfile:\n        writer = csv.writer(outfile)\n        writer.writerow([\n            \"File Name\", \"Prefetch Name\", \"File Size (bytes)\",\n            \"File Create Date (UTC)\", \"File Modify Date (UTC)\",\n            \"Prefetch Last Execution Date (UTC)\",\n            \"Prefetch Execution Count\", \"Volume\", \"Volume Create Date\",\n            \"Volume Serial\", \"File Path\"\n        ])\n        writer.writerows(data)\n```", "```py\n./synclibs.sh\n./autogen.sh\nsudo python setup.py install \n```", "```py\nfrom __future__ import print_function\nimport argparse\nimport unicodecsv as csv\nimport os\nimport pytsk3\nimport pyewf\nimport pyevt\nimport pyevtx\nimport sys\nfrom utility.pytskutil import TSKUtil\n```", "```py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"EVIDENCE_FILE\", help=\"Evidence file path\")\n    parser.add_argument(\"TYPE\", help=\"Type of Evidence\",\n                        choices=(\"raw\", \"ewf\"))\n    parser.add_argument(\"LOG_NAME\",\n                        help=\"Event Log Name (SecEvent.Evt, SysEvent.Evt, \"\n                             \"etc.)\")\n    parser.add_argument(\"-d\", help=\"Event log directory to scan\",\n                        default=\"/WINDOWS/SYSTEM32/WINEVT\")\n    parser.add_argument(\"-f\", help=\"Enable fuzzy search for either evt or\"\n                        \" evtx extension\", action=\"store_true\")\n    args = parser.parse_args()\n\n    if os.path.exists(args.EVIDENCE_FILE) and \\\n            os.path.isfile(args.EVIDENCE_FILE):\n        main(args.EVIDENCE_FILE, args.TYPE, args.LOG_NAME, args.d, args.f)\n    else:\n        print(\"[-] Supplied input file {} does not exist or is not a \"\n              \"file\".format(args.EVIDENCE_FILE))\n        sys.exit(1)\n```", "```py\ndef main(evidence, image_type, log, win_event, fuzzy):\n    # Create TSK object and query event log directory for Windows XP\n    tsk_util = TSKUtil(evidence, image_type)\n    event_dir = tsk_util.query_directory(win_event)\n    if event_dir is not None:\n        if fuzzy is True:\n            event_log = tsk_util.recurse_files(log, path=win_event)\n        else:\n            event_log = tsk_util.recurse_files(\n                log, path=win_event, logic=\"equal\")\n```", "```py\n        if event_log is not None:\n            event_data = []\n            for hit in event_log:\n                event_file = hit[2]\n                temp_evt = write_file(event_file)\n```", "```py\ndef write_file(event_file):\n    with open(event_file.info.name.name, \"w\") as outfile:\n        outfile.write(event_file.read_random(0, event_file.info.meta.size))\n    return event_file.info.name.name\n```", "```py\n                if pyevt.check_file_signature(temp_evt):\n                    evt_log = pyevt.open(temp_evt)\n                    print(\"[+] Identified {} records in {}\".format(\n                        evt_log.number_of_records, temp_evt))\n                    for i, record in enumerate(evt_log.records):\n                        strings = \"\"\n                        for s in record.strings:\n                            if s is not None:\n                                strings += s + \"\\n\"\n\n                        event_data.append([\n                            i, hit[0], record.computer_name,\n                            record.user_security_identifier,\n                            record.creation_time, record.written_time,\n                            record.event_category, record.source_name,\n                            record.event_identifier, record.event_type,\n                            strings, \"\",\n                            os.path.join(win_event, hit[1].lstrip(\"//\"))\n                        ])\n```", "```py\n                elif pyevtx.check_file_signature(temp_evt):\n                    evtx_log = pyevtx.open(temp_evt)\n                    print(\"[+] Identified {} records in {}\".format(\n                          evtx_log.number_of_records, temp_evt))\n                    for i, record in enumerate(evtx_log.records):\n                        strings = \"\"\n                        for s in record.strings:\n                            if s is not None:\n                                strings += s + \"\\n\"\n\n                        event_data.append([\n                            i, hit[0], record.computer_name,\n                            record.user_security_identifier, \"\",\n                            record.written_time, record.event_level,\n                            record.source_name, record.event_identifier,\n                            \"\", strings, record.xml_string,\n                            os.path.join(win_event, hit[1].lstrip(\"//\"))\n                        ])\n```", "```py\n                else:\n                    print(\"[-] {} not a valid event log. Removing temp \"\n                          \"file...\".format(temp_evt))\n                    os.remove(temp_evt)\n                    continue\n            write_output(event_data)\n        else:\n            print(\"[-] {} Event log not found in {} directory\".format(\n                log, win_event))\n            sys.exit(3)\n\n    else:\n        print(\"[-] Win XP Event Log Directory {} not found\".format(\n            win_event))\n        sys.exit(2)\n```", "```py\ndef write_output(data):\n    output_name = \"parsed_event_logs.csv\"\n    print(\"[+] Writing {} to current working directory: {}\".format(\n          output_name, os.getcwd()))\n    with open(output_name, \"wb\") as outfile:\n        writer = csv.writer(outfile)\n\n        writer.writerow([\n            \"Index\", \"File name\", \"Computer Name\", \"SID\",\n            \"Event Create Date\", \"Event Written Date\",\n            \"Event Category/Level\", \"Event Source\", \"Event ID\",\n            \"Event Type\", \"Data\", \"XML Data\", \"File Path\"\n        ])\n\n        writer.writerows(data)\n```", "```py\n./synclibs.sh\n./autogen.sh\nsudo python setup.py install \n```", "```py\nfrom __future__ import print_function\nimport argparse\nfrom datetime import datetime, timedelta\nimport os\nimport pytsk3\nimport pyewf\nimport pymsiecf\nimport sys\nimport unicodecsv as csv\nfrom utility.pytskutil import TSKUtil\n```", "```py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"EVIDENCE_FILE\", help=\"Evidence file path\")\n    parser.add_argument(\"TYPE\", help=\"Type of Evidence\",\n                        choices=(\"raw\", \"ewf\"))\n    parser.add_argument(\"-d\", help=\"Index.dat directory to scan\",\n                        default=\"/USERS\")\n    args = parser.parse_args()\n\n    if os.path.exists(args.EVIDENCE_FILE) and os.path.isfile(\n            args.EVIDENCE_FILE):\n        main(args.EVIDENCE_FILE, args.TYPE, args.d)\n    else:\n        print(\"[-] Supplied input file {} does not exist or is not a \"\n              \"file\".format(args.EVIDENCE_FILE))\n        sys.exit(1)\n```", "```py\ndef main(evidence, image_type, path):\n    # Create TSK object and query for Internet Explorer index.dat files\n    tsk_util = TSKUtil(evidence, image_type)\n    index_dir = tsk_util.query_directory(path)\n    if index_dir is not None:\n        index_files = tsk_util.recurse_files(\"index.dat\", path=path,\n                                             logic=\"equal\")\n```", "```py\n        if index_files is not None:\n            print(\"[+] Identified {} potential index.dat files\".format(\n                  len(index_files)))\n            index_data = []\n            for hit in index_files:\n                index_file = hit[2]\n                temp_index = write_file(index_file)\n```", "```py\ndef write_file(index_file):\n    with open(index_file.info.name.name, \"w\") as outfile:\n        outfile.write(index_file.read_random(0, index_file.info.meta.size))\n    return index_file.info.name.name\n```", "```py\n                if pymsiecf.check_file_signature(temp_index):\n                    index_dat = pymsiecf.open(temp_index)\n                    print(\"[+] Identified {} records in {}\".format(\n                        index_dat.number_of_items, temp_index))\n                    for i, record in enumerate(index_dat.items):\n                        try:\n                            data = record.data\n                            if data is not None:\n                                data = data.rstrip(\"\\x00\")\n```", "```py\n                        except AttributeError:\n                            if isinstance(record, pymsiecf.redirected):\n                                index_data.append([\n                                    i, temp_index, \"\", \"\", \"\", \"\", \"\",\n                                    record.location, \"\", \"\", record.offset,\n                                    os.path.join(path, hit[1].lstrip(\"//\"))\n                                ])\n\n                            elif isinstance(record, pymsiecf.leak):\n                                index_data.append([\n                                    i, temp_index, record.filename, \"\",\n                                    \"\", \"\", \"\", \"\", \"\", \"\", record.offset,\n                                    os.path.join(path, hit[1].lstrip(\"//\"))\n                                ])\n\n                            continue\n```", "```py\n                        index_data.append([\n                            i, temp_index, record.filename,\n                            record.type, record.primary_time,\n                            record.secondary_time,\n                            record.last_checked_time, record.location,\n                            record.number_of_hits, data, record.offset,\n                            os.path.join(path, hit[1].lstrip(\"//\"))\n                        ])\n```", "```py\n                else:\n                    print(\"[-] {} not a valid index.dat file. Removing \"\n                          \"temp file..\".format(temp_index))\n                    os.remove(\"index.dat\")\n                    continue\n\n            os.remove(\"index.dat\")\n            write_output(index_data)\n        else:\n            print(\"[-] Index.dat files not found in {} directory\".format(\n                path))\n            sys.exit(3)\n\n    else:\n        print(\"[-] Directory {} not found\".format(win_event))\n        sys.exit(2)\n```", "```py\ndef write_output(data):\n    output_name = \"Internet_Indexdat_Summary_Report.csv\"\n    print(\"[+] Writing {} with {} parsed index.dat files to current \"\n          \"working directory: {}\".format(output_name, len(data),\n                                         os.getcwd()))\n    with open(output_name, \"wb\") as outfile:\n        writer = csv.writer(outfile)\n        writer.writerow([\"Index\", \"File Name\", \"Record Name\",\n                         \"Record Type\", \"Primary Date\", \"Secondary Date\",\n                         \"Last Checked Date\", \"Location\", \"No. of Hits\",\n                         \"Record Data\", \"Record Offset\", \"File Path\"])\n        writer.writerows(data)\n```", "```py\n./synclibs.sh\n./autogen.sh\nsudo python setup.py install \n```", "```py\nfrom __future__ import print_function\nimport argparse\nfrom datetime import datetime, timedelta\nimport os\nimport pytsk3\nimport pyewf\nimport pyvshadow\nimport sys\nimport unicodecsv as csv\nfrom utility import vss\nfrom utility.pytskutil import TSKUtil\nfrom utility import pytskutil\n```", "```py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"EVIDENCE_FILE\", help=\"Evidence file path\")\n    parser.add_argument(\"OUTPUT_CSV\",\n                        help=\"Output CSV with VSS file listing\")\n    args = parser.parse_args()\n```", "```py\n    directory = os.path.dirname(args.OUTPUT_CSV)\n    if not os.path.exists(directory) and directory != \"\":\n        os.makedirs(directory)\n\n    if os.path.exists(args.EVIDENCE_FILE) and \\\n            os.path.isfile(args.EVIDENCE_FILE):\n        main(args.EVIDENCE_FILE, args.OUTPUT_CSV)\n    else:\n        print(\"[-] Supplied input file {} does not exist or is not a \"\n              \"file\".format(args.EVIDENCE_FILE))\n        sys.exit(1)\n\n```", "```py\ndef main(evidence, output):\n    # Create TSK object and query path for prefetch files\n    tsk_util = TSKUtil(evidence, \"raw\")\n    img_vol = tsk_util.return_vol()\n    if img_vol is not None:\n        for part in img_vol:\n            if tsk_util.detect_ntfs(img_vol, part):\n                print(\"Exploring NTFS Partition for VSS\")\n                explore_vss(evidence, part.start * img_vol.info.block_size,\n                            output)\n    else:\n        print(\"[-] Must be a physical preservation to be compatible \"\n              \"with this script\")\n        sys.exit(2)\n```", "```py\ndef explore_vss(evidence, part_offset, output):\n    vss_volume = pyvshadow.volume()\n    vss_handle = vss.VShadowVolume(evidence, part_offset)\n    vss_count = vss.GetVssStoreCount(evidence, part_offset)\n    if vss_count > 0:\n        vss_volume.open_file_object(vss_handle)\n        vss_data = []\n        for x in range(vss_count):\n            print(\"Gathering data for VSC {} of {}\".format(x, vss_count))\n            vss_store = vss_volume.get_store(x)\n            image = vss.VShadowImgInfo(vss_store)\n            vss_data.append(pytskutil.openVSSFS(image, x))\n\n        write_csv(vss_data, output)\n```", "```py\ndef write_csv(data, output):\n    if data == []:\n        print(\"[-] No output results to write\")\n        sys.exit(3)\n\n    print(\"[+] Writing output to {}\".format(output))\n    if os.path.exists(output):\n        append = True\n    with open(output, \"ab\") as csvfile:\n        csv_writer = csv.writer(csvfile)\n        headers = [\"VSS\", \"File\", \"File Ext\", \"File Type\", \"Create Date\",\n                   \"Modify Date\", \"Change Date\", \"Size\", \"File Path\"]\n        if not append:\n            csv_writer.writerow(headers)\n        for result_list in data:\n            csv_writer.writerows(result_list)\n```", "```py\n./synclibs.sh\n./autogen.sh\nsudo python setup.py install \n```", "```py\nfrom __future__ import print_function\nimport argparse\nfrom datetime import datetime, timedelta\nimport os\nimport pytsk3\nimport pyewf\nimport pyesedb\nimport struct\nimport sys\nimport unicodecsv as csv\nfrom utility.pytskutil import TSKUtil\n```", "```py\nTABLE_LOOKUP = {\n    \"{973F5D5C-1D90-4944-BE8E-24B94231A174}\": \"Network Data Usage\",\n    \"{D10CA2FE-6FCF-4F6D-848E-B2E99266FA86}\": \"Push Notifications\",\n    \"{D10CA2FE-6FCF-4F6D-848E-B2E99266FA89}\": \"Application Resource Usage\",\n    \"{DD6636C4-8929-4683-974E-22C046A43763}\": \"Network Connectivity Usage\",\n    \"{FEE4E14F-02A9-4550-B5CE-5FA2DA202E37}\": \"Energy Usage\"}\n\nAPP_ID_LOOKUP = {}\n```", "```py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"EVIDENCE_FILE\", help=\"Evidence file path\")\n    parser.add_argument(\"TYPE\", help=\"Type of Evidence\",\n                        choices=(\"raw\", \"ewf\"))\n    args = parser.parse_args()\n\n    if os.path.exists(args.EVIDENCE_FILE) and os.path.isfile(\n            args.EVIDENCE_FILE):\n        main(args.EVIDENCE_FILE, args.TYPE)\n    else:\n        print(\"[-] Supplied input file {} does not exist or is not a \"\n              \"file\".format(args.EVIDENCE_FILE))\n        sys.exit(1)\n```", "```py\ndef main(evidence, image_type):\n    # Create TSK object and query for Internet Explorer index.dat files\n    tsk_util = TSKUtil(evidence, image_type)\n    path = \"/Windows/System32/sru\"\n    srum_dir = tsk_util.query_directory(path)\n    if srum_dir is not None:\n        srum_files = tsk_util.recurse_files(\"SRUDB.dat\", path=path,\n                                            logic=\"equal\")\n```", "```py\n        if srum_files is not None:\n            print(\"[+] Identified {} potential SRUDB.dat file(s)\".format(\n                len(srum_files)))\n            for hit in srum_files:\n                srum_file = hit[2]\n                srum_tables = {}\n                temp_srum = write_file(srum_file)\n```", "```py\ndef write_file(srum_file):\n    with open(srum_file.info.name.name, \"w\") as outfile:\n        outfile.write(srum_file.read_random(0, srum_file.info.meta.size))\n    return srum_file.info.name.name\n```", "```py\n                if pyesedb.check_file_signature(temp_srum):\n                    srum_dat = pyesedb.open(temp_srum)\n                    print(\"[+] Process {} tables within database\".format(\n                        srum_dat.number_of_tables))\n                    for table in srum_dat.tables:\n                        if table.name != \"SruDbIdMapTable\":\n                            continue\n                        global APP_ID_LOOKUP\n                        for entry in table.records:\n                            app_id = entry.get_value_data_as_integer(1)\n                            try:\n                                app = entry.get_value_data(2).replace(\n                                    \"\\x00\", \"\")\n                            except AttributeError:\n                                app = \"\"\n                            APP_ID_LOOKUP[app_id] = app\n```", "```py\n                    for table in srum_dat.tables:\n                        t_name = table.name\n                        print(\"[+] Processing {} table with {} records\"\n                              .format(t_name, table.number_of_records))\n                        srum_tables[t_name] = {\"columns\": [], \"data\": []}\n                        columns = [x.name for x in table.columns]\n                        srum_tables[t_name][\"columns\"] = columns\n```", "```py\n                        for entry in table.records:\n                            data = []\n                            for x in range(entry.number_of_values):\n                                data.append(convert_data(\n                                    entry.get_value_data(x), columns[x],\n                                    entry.get_column_type(x))\n                                )\n                            srum_tables[t_name][\"data\"].append(data)\n                        write_output(t_name, srum_tables)\n\n                else:\n                    print(\"[-] {} not a valid SRUDB.dat file. Removing \"\n                          \"temp file...\".format(temp_srum))\n                    os.remove(temp_srum)\n                    continue\n\n        else:\n            print(\"[-] SRUDB.dat files not found in {} \"\n                  \"directory\".format(path))\n            sys.exit(3)\n\n    else:\n        print(\"[-] Directory {} not found\".format(path))\n        sys.exit(2)\n```", "```py\ndef convert_data(data, column, col_type):\n    if data is None:\n        return \"\"\n    elif column == \"AppId\":\n        return APP_ID_LOOKUP[struct.unpack(\"<i\", data)[0]]\n    elif col_type == 0:\n        return \"\"\n    elif col_type == 1:\n        if data == \"*\":\n            return True\n        else:\n            return False\n    elif col_type == 2:\n        return struct.unpack(\"<B\", data)[0]\n    elif col_type == 3:\n        return struct.unpack(\"<h\", data)[0]\n    elif col_type == 4:\n        return struct.unpack(\"<i\", data)[0]\n    elif col_type == 6:\n        return struct.unpack(\"<f\", data)[0]\n    elif col_type == 7:\n        return struct.unpack(\"<d\", data)[0]\n```", "```py\n    elif col_type == 8:\n        return convert_ole(struct.unpack(\"<q\", data)[0])\n    elif col_type in [5, 9, 10, 12, 13, 16]:\n        return data\n    elif col_type == 11:\n        return data.replace(\"\\x00\", \"\")\n    elif col_type == 14:\n        return struct.unpack(\"<I\", data)[0]\n    elif col_type == 15:\n        if column in [\"EventTimestamp\", \"ConnectStartTime\"]:\n            return convert_filetime(struct.unpack(\"<q\", data)[0])\n        else:\n            return struct.unpack(\"<q\", data)[0]\n    elif col_type == 17:\n        return struct.unpack(\"<H\", data)[0]\n    else:\n        return data\n```", "```py\ndef convert_filetime(ts):\n    if str(ts) == \"0\":\n        return \"\"\n    try:\n        dt = datetime(1601, 1, 1) + timedelta(microseconds=ts / 10)\n    except OverflowError:\n        return ts\n    return dt\n```", "```py\ndef convert_ole(ts):\n    ole = struct.unpack(\">d\", struct.pack(\">Q\", ts))[0]\n    try:\n        dt = datetime(1899, 12, 30, 0, 0, 0) + timedelta(days=ole)\n    except OverflowError:\n        return ts\n    return dt\n```", "```py\ndef write_output(table, data):\n    if len(data[table][\"data\"]) == 0:\n        return\n    if table in TABLE_LOOKUP:\n        output_name = TABLE_LOOKUP[table] + \".csv\"\n    else:\n        output_name = \"SRUM_Table_{}.csv\".format(table)\n    print(\"[+] Writing {} to current working directory: {}\".format(\n        output_name, os.getcwd()))\n    with open(output_name, \"wb\") as outfile:\n        writer = csv.writer(outfile)\n        writer.writerow(data[table][\"columns\"])\n        writer.writerows(data[table][\"data\"])\n```"]