- en: Getting Started with Scraping
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始爬取
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Setting up a Python development environment
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置Python开发环境
- en: Scraping Python.org with Requests and Beautiful Soup
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Requests和Beautiful Soup爬取Python.org
- en: Scraping Python.org with urllib3 and Beautiful Soup
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用urllib3和Beautiful Soup爬取Python.org
- en: Scraping Python.org with Scrapy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scrapy爬取Python.org
- en: Scraping Python.org with Selenium and PhantomJs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Selenium和PhantomJs爬取Python.org
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The amount of data available on the web is consistently growing both in quantity
    and in form.  Businesses require this data to make decisions, particularly with
    the explosive growth of machine learning tools which require large amounts of
    data for training.  Much of this data is available via Application Programming
    Interfaces, but at the same time a lot of valuable data is still only available
    through the process of web scraping.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 网上可用的数据量在数量和形式上都在持续增长。企业需要这些数据来做决策，特别是随着需要大量数据进行训练的机器学习工具的爆炸式增长。很多数据可以通过应用程序编程接口获得，但同时也有很多有价值的数据仍然只能通过网页抓取获得。
- en: This chapter will focus on several fundamentals of setting up a scraping environment
    and performing basic requests for data with several of the tools of the trade. 
    Python is the programing language of choice for this book, as well as amongst
    many who build systems to perform scraping.  It is an easy to use programming
    language which has a very rich ecosystem of tools for many tasks.  If you program
    in other languages, you will find it easy to pick up and you may never go back!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍设置爬取环境的几个基本原理，并使用行业工具进行基本数据请求。Python是本书的首选编程语言，也是许多进行爬取系统构建的人的首选语言。它是一种易于使用的编程语言，拥有丰富的工具生态系统，适用于许多任务。如果您使用其他语言进行编程，您会发现很容易上手，也许永远不会回头！
- en: Setting up a Python development environment
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Python开发环境
- en: If you have not used Python before, it is important to have a working development
    environment. The recipes in this book will be all in Python and be a mix of interactive
    examples, but primarily implemented as scripts to be interpreted by the Python
    interpreter. This recipe will show you how to set up an isolated development environment
    with `virtualenv` and manage project dependencies with `pip` . We also get the
    code for the book and install it into the Python virtual environment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您以前没有使用过Python，拥有一个可用的开发环境是很重要的。本书中的示例将全部使用Python，并且是交互式示例的混合，但主要是作为脚本实现，由Python解释器解释。这个示例将向您展示如何使用`virtualenv`设置一个隔离的开发环境，并使用`pip`管理项目依赖。我们还会获取本书的代码并将其安装到Python虚拟环境中。
- en: Getting ready
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will exclusively be using Python 3.x, and specifically in my case 3.6.1. 
    While Mac and Linux normally have Python version 2 installed, and Windows systems
    do not. So it is likely that in any case that Python 3 will need to be installed. 
    You can find references for Python installers at www.python.org.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专门使用Python 3.x，特别是在我的情况下是3.6.1。虽然Mac和Linux通常已安装了Python 2版本，而Windows系统没有安装。因此很可能需要安装Python
    3。您可以在www.python.org找到Python安装程序的参考资料。
- en: You can check Python's version with `python --version`
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`python --version`检查Python的版本
- en: '![](assets/e9039d11-8e50-44c6-8204-3199ae5d7b1e.png)`pip` comes installed with
    Python 3.x, so we will omit instructions on its installation.  Additionally, all
    command line examples in this book are run on a Mac.  For Linux users the commands
    should be identical.  On Windows, there are alternate commands (like dir instead
    of ls), but these alternatives will not be covered.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/e9039d11-8e50-44c6-8204-3199ae5d7b1e.png)`pip`已经随Python 3.x一起安装，因此我们将省略其安装说明。此外，本书中的所有命令行示例都在Mac上运行。对于Linux用户，命令应该是相同的。在Windows上，有替代命令（如dir而不是ls），但这些替代命令将不会被涵盖。'
- en: How to do it...
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: We will be installing a number of packages with `pip`.  These packages are installed
    into a Python environment.  There often can be version conflicts with other packages,
    so a good practice for following along with the recipes in the book will be to
    create a new virtual Python environment where the packages we will use will be
    ensured to work properly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`pip`安装许多包。这些包将被安装到一个Python环境中。通常可能会与其他包存在版本冲突，因此在跟着本书的示例进行操作时，一个很好的做法是创建一个新的虚拟Python环境，确保我们将使用的包能够正常工作。
- en: 'Virtual Python environments are managed with the `virtualenv` tool.  This can
    be installed with the following command:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟Python环境是用`virtualenv`工具管理的。可以用以下命令安装它：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can use `virtualenv`.  But before that let''s briefly look at `pip`.
    This command installs Python packages from PyPI, a package repository with literally
    10''s of thousands of packages.  We just saw using the install subcommand to pip,
    which ensures a package is installed.  We can also see all currently installed
    packages with `pip list`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`virtualenv`。但在那之前，让我们简要地看一下`pip`。这个命令从PyPI安装Python包，PyPI是一个拥有成千上万个包的包存储库。我们刚刚看到了使用pip的install子命令，这可以确保一个包被安装。我们也可以用`pip
    list`来查看当前安装的所有包：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I've truncated to the first few lines as there are quite a few.  For me there
    are 222 packages installed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我截取了前几行，因为安装了很多包。对我来说，安装了222个包。
- en: Packages can also be uninstalled using `pip uninstall` followed by the package
    name.  I'll leave it to you to give it a try.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用`pip uninstall`命令卸载包。我留给您去尝试一下。
- en: 'Now back to `virtualenv`. Using `virtualenv` is very simple.  Let''s use it
    to create an environment and install the code from github. Let''s walk through
    the steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到`virtualenv`。使用`virtualenv`非常简单。让我们用它来创建一个环境并安装来自github的代码。让我们一步步走过这些步骤：
- en: Create a directory to represent the project and enter the directory.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个代表项目的目录并进入该目录。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Initialize a virtual environment folder named env:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个名为env的虚拟环境文件夹：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This creates an env folder.  Let's take a look at what was installed.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将创建一个env文件夹。让我们看看安装了什么。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: New we activate the virtual environment.  This command uses the content in the
    `env` folder to configure Python. After this all python activities are relative
    to this virtual environment.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们激活虚拟环境。这个命令使用`env`文件夹中的内容来配置Python。之后所有的python活动都是相对于这个虚拟环境的。
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can check that python is indeed using this virtual environment with the
    following command:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令检查python是否确实使用了这个虚拟环境：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With our virtual environment created, let's clone the books sample code and
    take a look at its structure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们创建的虚拟环境，让我们克隆书籍的示例代码并看看它的结构。
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This created a `PythonWebScrapingCookbook` directory.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个`PythonWebScrapingCookbook`目录。
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let's change into it and examine the content.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们切换到它并检查内容。
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There are two directories.  Most the the Python code is is the `py` directory.
    `www` contains some web content that we will use from time-to-time using a local
    web server.  Let''s look at the contents of the `py` directory:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个目录。大部分Python代码都在`py`目录中。`www`包含一些我们将使用的网络内容，我们将使用本地web服务器不时地访问它。让我们看看`py`目录的内容：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Code for each chapter is in the numbered folder matching the chapter (there
    is no code for chapter 2 as it is all interactive Python).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个章节的代码都在与章节匹配的编号文件夹中（第2章没有代码，因为它都是交互式Python）。
- en: 'Note that there is a `modules` folder.  Some of the recipes throughout the
    book use code in those modules.  Make sure that your Python path points to this
    folder.  On Mac and Linux you can sets this in your `.bash_profile` file (and
    environments variables dialog on Windows):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有一个`modules`文件夹。本书中的一些食谱使用这些模块中的代码。确保你的Python路径指向这个文件夹。在Mac和Linux上，你可以在你的`.bash_profile`文件中设置这一点（在Windows上是在环境变量对话框中）：
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The contents in each folder generally follows a numbering scheme matching the
    sequence of the recipe in the chapter.  The following is the contents of the chapter
    6 folder:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件夹中的内容通常遵循与章节中食谱顺序相匹配的编号方案。以下是第6章文件夹的内容：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the recipes I'll state that we'll be using the script in `<chapter directory>`/`<recipe
    filename>`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在食谱中，我会说明我们将使用`<章节目录>`/`<食谱文件名>`中的脚本。
- en: Congratulations, you've now got a Python environment configured with the books
    code!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你现在已经配置了一个带有书籍代码的Python环境！
- en: 'Now just the be complete, if you want to get out of the Python virtual environment,
    you can exit using the following command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你想退出Python虚拟环境，你可以使用以下命令退出：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And checking which python we can see it has switched back:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 检查一下python，我们可以看到它已经切换回来了：
- en: '[PRE14]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: I won't be using the virtual environment for the rest of the book. When you
    see command prompts they will be either of the form "<directory> $" or simply
    "$".
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在本书的其余部分使用虚拟环境。当你看到命令提示时，它们将是以下形式之一"<目录> $"或者简单的"$"。
- en: Now let's move onto doing some scraping.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始爬取一些数据。
- en: Scraping Python.org with Requests and Beautiful Soup
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Requests和Beautiful Soup从Python.org上爬取数据
- en: In this recipe we will install Requests and Beautiful Soup and scrape some content
    from www.python.org.  We'll install both of the libraries and get some basic familiarity
    with them.  We'll come back to them both in subsequent chapters and dive deeper
    into each.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将安装Requests和Beautiful Soup，并从www.python.org上爬取一些内容。我们将安装这两个库，并对它们有一些基本的了解。在随后的章节中，我们将深入研究它们。
- en: Getting ready...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好了...
- en: 'In this recipe, we will scrape the upcoming Python events from [https://www.python.org/events/pythonevents](https://www.python.org/events/pythonevents).
    The following is an an example of `The Python.org Events Page` (it changes frequently,
    so your experience will differ):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将从[https://www.python.org/events/pythonevents](https://www.python.org/events/pythonevents)中爬取即将到来的Python事件。以下是`Python.org事件页面`的一个示例（它经常更改，所以你的体验会有所不同）：
- en: '![](assets/c4caf889-b8fa-4f5e-87dc-d6d78921bddb.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/c4caf889-b8fa-4f5e-87dc-d6d78921bddb.png)'
- en: 'We will need to ensure that Requests and Beautiful Soup are installed.  We
    can do that with the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保Requests和Beautiful Soup已安装。我们可以使用以下命令来安装：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How to do it...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Now let's go and learn to scrape a couple events. For this recipe we will start
    by using interactive python.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们去学习一下爬取一些事件。对于这个食谱，我们将开始使用交互式python。
- en: 'Start it with the `ipython` command:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用`ipython`命令启动它：
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next we import Requests
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来导入Requests
- en: '[PRE17]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We now use requests to make a GET HTTP request for the following url: [https://www.python.org/events/python-events/](https://www.python.org/events/python-events/)
    by making a `GET` request:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在使用requests来对以下url进行GET HTTP请求：[https://www.python.org/events/python-events/](https://www.python.org/events/python-events/)，通过进行`GET`请求：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: That downloaded the page content but it is stored in our requests object req. 
    We can retrieve the content using the `.text` property.  This prints the first
    200 characters.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这下载了页面内容，但它存储在我们的requests对象req中。我们可以使用`.text`属性检索内容。这打印了前200个字符。
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We now have the raw HTML of the page.  We can now use beautiful soup to parse
    the HTML and retrieve the event data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了页面的原始HTML。我们现在可以使用beautiful soup来解析HTML并检索事件数据。
- en: First import Beautiful Soup
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入Beautiful Soup
- en: '[PRE20]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now we create a `BeautifulSoup` object and pass it the HTML.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们创建一个`BeautifulSoup`对象并传递HTML。
- en: '[PRE21]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now we tell Beautiful Soup to find the main `<ul>` tag for the recent events,
    and then to get all the `<li>` tags below it.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们告诉Beautiful Soup找到最近事件的主要`<ul>`标签，然后获取其下的所有`<li>`标签。
- en: '[PRE22]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And finally we can loop through each of the `<li>` elements, extracting the
    event details, and print each to the console:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以循环遍历每个`<li>`元素，提取事件详情，并将每个打印到控制台：
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This entire example is available in the `01/01_events_with_requests.py` script
    file.  The following is its content and it pulls together all of what we just
    did step by step:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 整个示例都在`01/01_events_with_requests.py`脚本文件中可用。以下是它的内容，它逐步汇总了我们刚刚做的所有内容：
- en: '[PRE24]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can run this using the following command from the terminal:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在终端中使用以下命令运行它：
- en: '[PRE25]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: 'We will dive into details of both Requests and Beautiful Soup in the next chapter,
    but for now let''s just summarize a few key points about how this works.  The
    following important points about Requests:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章节详细介绍Requests和Beautiful Soup，但现在让我们总结一下关于它的一些关键点。关于Requests的一些重要点：
- en: Requests is used to execute HTTP requests.  We used it to make a GET verb request
    of the URL for the events page.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Requests用于执行HTTP请求。我们用它来对事件页面的URL进行GET请求。
- en: The Requests object holds the results of the request.  This is not only the
    page content, but also many other items about the result such as HTTP status codes
    and headers.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Requests对象保存了请求的结果。不仅包括页面内容，还有很多其他关于结果的项目，比如HTTP状态码和头部信息。
- en: Requests is used only to get the page, it does not do an parsing.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Requests仅用于获取页面，不进行解析。
- en: We use Beautiful Soup to do the parsing of the HTML and also the finding of
    content within the HTML.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Beautiful Soup来解析HTML和在HTML中查找内容。
- en: 'To understand how this worked, the content of the page has the following HTML
    to start the Upcoming Events section:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这是如何工作的，页面的内容具有以下HTML来开始“即将到来的事件”部分：
- en: '![](assets/9c3b8d5a-57e7-4cab-b868-b2362f805cc8.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9c3b8d5a-57e7-4cab-b868-b2362f805cc8.png)'
- en: 'We used the power of Beautiful Soup to:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用Beautiful Soup的强大功能：
- en: Find the `<ul>` element representing the section, which is found by looking
    for a `<ul>` with the a `class` attribute that has a value of `list-recent-events`.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到代表该部分的`<ul>`元素，通过查找具有值为`list-recent-events`的`class`属性的`<ul>`来找到。
- en: From that object, we find all the `<li>` elements.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从该对象中，我们找到所有`<li>`元素。
- en: 'Each of these `<li>` tags represent a different event.  We iterate over each
    of those making a dictionary from the event data found in child HTML tags:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`<li>`标签代表一个不同的事件。我们遍历每一个，从子HTML标签中找到事件数据，制作一个字典：
- en: The name is extracted from the `<a>` tag that is a child of the `<h3>` tag
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名称从`<h3>`标签的子标签`<a>`中提取
- en: The location is the text content of the `<span>` with a class of `event-location`
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置是具有`event-location`类的`<span>`的文本内容
- en: And the time is extracted from the `datetime` attribute of the <time> tag.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间是从`<time>`标签的`datetime`属性中提取的。
- en: Scraping Python.org in urllib3 and Beautiful Soup
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用urllib3和Beautiful Soup爬取Python.org
- en: In this recipe we swap out the use of requests for another library `urllib3`.
    This is another common library for retrieving data from URLs and for other functions
    involving URLs such as parsing of the parts of the actual URL and handling various
    encodings.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用requests替换为另一个库`urllib3`。这是另一个常见的用于从URL检索数据以及处理URL的各个部分和处理各种编码的库。
- en: Getting ready...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作...
- en: 'This recipe requires `urllib3` installed.  So install it with `pip`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方需要安装`urllib3`。所以用`pip`安装它：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How to do it...
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The recipe is implemented in `01/02_events_with_urllib3.py`.  The code is the
    following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码在`01/02_events_with_urllib3.py`中实现。代码如下：
- en: '[PRE27]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The run it with the python interpreter.  You will get identical output to the
    previous recipe.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python解释器运行它。你将得到与前一个配方相同的输出。
- en: How it works
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理
- en: 'The only difference in this recipe is how we fetch the resource:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方唯一的区别是我们如何获取资源：
- en: '[PRE28]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Unlike `Requests`, `urllib3` doesn't apply header encoding automatically. The
    reason why the code snippet works in the preceding example is because BS4 handles
    encoding beautifully.  But you should keep in mind that encoding is an important
    part of scraping. If you decide to use your own framework or use other libraries,
    make sure encoding is well handled.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与`Requests`不同，`urllib3`不会自动应用头部编码。前面示例中代码片段能够工作的原因是因为BS4能够很好地处理编码。但你应该记住编码是爬取的一个重要部分。如果你决定使用自己的框架或其他库，请确保编码得到很好的处理。
- en: There's more...
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Requests and urllib3 are very similar in terms of capabilities. it is generally
    recommended to use Requests when it comes to making HTTP requests. The following
    code example illustrates a few advanced features:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Requests和urllib3在功能方面非常相似。一般建议在进行HTTP请求时使用Requests。以下代码示例说明了一些高级功能：
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Scraping Python.org with Scrapy
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapy爬取Python.org
- en: '**Scrapy** is a very popular open source Python scraping framework for extracting
    data. It was originally designed for only scraping, but it is has also evolved
    into a powerful web crawling solution.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scrapy**是一个非常流行的开源Python爬虫框架，用于提取数据。它最初是为了爬取而设计的，但它也发展成了一个强大的网络爬虫解决方案。'
- en: In our previous recipes, we used Requests and urllib2 to fetch data and Beautiful
    Soup to extract data. Scrapy offers all of these functionalities with many other
    built-in modules and extensions. It is also our tool of choice when it comes to
    scraping with Python.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的配方中，我们使用Requests和urllib2来获取数据，使用Beautiful Soup来提取数据。Scrapy提供了所有这些功能以及许多其他内置模块和扩展。在使用Python进行爬取时，这也是我们的首选工具。
- en: 'Scrapy offers a number of powerful features that are worth mentioning:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy提供了一些强大的功能值得一提：
- en: Built-in extensions to make HTTP requests and handle compression, authentication,
    caching, manipulate user-agents, and HTTP headers
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置扩展，用于进行HTTP请求和处理压缩、认证、缓存、操作用户代理和HTTP头部
- en: Built-in support for selecting and extracting data with selector languages such
    as CSS and XPath, as well as support for utilizing regular expressions for selection
    of content and links
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置支持使用选择器语言（如CSS和XPath）选择和提取数据，以及支持利用正则表达式选择内容和链接
- en: Encoding support to deal with languages and non-standard encoding declarations
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码支持以处理语言和非标准编码声明
- en: Flexible APIs to reuse and write custom middleware and pipelines, which provide
    a clean and easy way to implement tasks such as automatically downloading assets
    (for example, images or media) and storing data in storage such as file systems,
    S3, databases, and others
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵活的API，可以重用和编写自定义中间件和管道，提供了一种干净简单的方式来执行任务，比如自动下载资源（例如图片或媒体）并将数据存储在文件系统、S3、数据库等中
- en: Getting ready...
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作...
- en: There are several means of creating a scraper with Scrapy.  One is a programmatic
    pattern where we create the crawler and spider in our code.  It is also possible
    to configure a Scrapy project from templates or generators and then run the scraper
    from the command line using the `scrapy` command.  This book will follow the programmatic
    pattern as it contains the code in a single file more effectively.  This will
    help when we are putting together specific, targeted, recipes with Scrapy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以使用Scrapy创建一个爬虫。一种是编程模式，我们在代码中创建爬虫和爬虫。还可以从模板或生成器配置一个Scrapy项目，然后使用`scrapy`命令从命令行运行爬虫。本书将遵循编程模式，因为它可以更有效地将代码放在一个文件中。这将有助于我们在使用Scrapy时组合特定的、有针对性的配方。
- en: This isn't necessarily a better way of running a Scrapy scraper than using the
    command line execution, just one that is a design decision for this book.  Ultimately
    this book is not about Scrapy (there are other books on just Scrapy), but more
    of an exposition on various things you may need to do when scraping, and in the
    ultimate creation of a functional scraper as a service in the cloud.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不一定是比使用命令行执行Scrapy爬虫更好的方式，只是这本书的设计决定。最终，这本书不是关于Scrapy的（有其他专门讲Scrapy的书），而是更多地阐述了在爬取时可能需要做的各种事情，以及在云端创建一个功能齐全的爬虫服务。
- en: How to do it...
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The script for this recipe is `01/03_events_with_scrapy.py`. The following
    is the code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的脚本是`01/03_events_with_scrapy.py`。以下是代码：
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following runs the script and shows the output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是运行脚本并显示输出的过程：
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The same result but with another tool.  Let's go take a quick review of how
    this works.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用另一个工具得到相同的结果。让我们快速回顾一下它是如何工作的。
- en: How it works
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: We will get into some details about Scrapy in later chapters, but let's just
    go through this code quick to get a feel how it is accomplishing this scrape. 
    Everything in Scrapy revolves around creating a **spider**.  Spiders crawl through
    pages on the Internet based upon rules that we provide.  This spider only processes
    one single page, so it's not really much of a spider.  But it shows the pattern
    we will use through later Scrapy examples.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中详细介绍Scrapy，但让我们快速浏览一下这段代码，以了解它是如何完成这个爬取的。Scrapy中的一切都围绕着创建**spider**。蜘蛛根据我们提供的规则在互联网上爬行。这个蜘蛛只处理一个单独的页面，所以它并不是一个真正的蜘蛛。但它展示了我们将在后面的Scrapy示例中使用的模式。
- en: The spider is created with a class definition that derives from one of the Scrapy
    spider classes.  Ours derives from the `scrapy.Spider` class.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫是通过一个类定义创建的，该类继承自Scrapy爬虫类之一。我们的类继承自`scrapy.Spider`类。
- en: '[PRE33]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Every spider is given a `name`, and also one or more `start_urls` which tell
    it where to start the crawling.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 每个爬虫都有一个`name`，还有一个或多个`start_urls`，告诉它从哪里开始爬行。
- en: 'This spider has a field to store all the events that we find:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个爬虫有一个字段来存储我们找到的所有事件：
- en: '[PRE34]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The spider then has a method names parse which will be called for every page
    the spider collects.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，爬虫有一个名为parse的方法，它将被调用来处理爬虫收集到的每个页面。
- en: '[PRE35]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The implementation of this method uses and XPath selection to get the events
    from the page (XPath is the built in means of navigating HTML in Scrapy). It them
    builds the `event_details` dictionary object similarly to the other examples,
    and then adds it to the `found_events` list.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的实现使用了XPath选择器来从页面中获取事件（XPath是Scrapy中导航HTML的内置方法）。它构建了`event_details`字典对象，类似于其他示例，然后将其添加到`found_events`列表中。
- en: The remaining code does the programmatic execution of the Scrapy crawler.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的代码执行了Scrapy爬虫的编程执行。
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: It starts with the creation of a CrawlerProcess which does the actual  crawling
    and a lot of other tasks.  We pass it a LOG_LEVEL of ERROR to prevent the voluminous
    Scrapy output.  Change this to DEBUG and re-run it to see the difference.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 它从创建一个CrawlerProcess开始，该过程执行实际的爬行和许多其他任务。我们传递了一个ERROR的LOG_LEVEL来防止大量的Scrapy输出。将其更改为DEBUG并重新运行以查看差异。
- en: Next we tell the crawler process to use our Spider implementation.  We get the
    actual spider object from that crawler so that we can get the items when the crawl
    is complete.  And then we kick of the whole thing by calling `process.start()`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们告诉爬虫进程使用我们的Spider实现。我们从爬虫中获取实际的蜘蛛对象，这样当爬取完成时我们就可以获取项目。然后我们通过调用`process.start()`来启动整个过程。
- en: When the crawl is completed we can then iterate and print out the items that
    were found.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当爬取完成后，我们可以迭代并打印出找到的项目。
- en: '[PRE37]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This example really didn't touch any of the power of Scrapy.  We will look more
    into some of the more advanced features later in the book.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子并没有涉及到Scrapy的任何强大功能。我们将在本书的后面更深入地了解一些更高级的功能。
- en: Scraping Python.org with Selenium and PhantomJS
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Selenium和PhantomJS来爬取Python.org
- en: This recipe will introduce Selenium and PhantomJS, two frameworks that are very
    different from the frameworks in the previous recipes. In fact, Selenium and PhantomJS
    are often used in functional/acceptance testing. We want to demonstrate these
    tools as they offer unique benefits from the scraping perspective. Several that
    we will look at later in the book are the ability to fill out forms, press buttons,
    and wait for dynamic JavaScript to be downloaded and executed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方将介绍Selenium和PhantomJS，这两个框架与之前的配方中的框架非常不同。实际上，Selenium和PhantomJS经常用于功能/验收测试。我们想展示这些工具，因为它们从爬取的角度提供了独特的好处。我们将在本书的后面看到一些，比如填写表单、按按钮和等待动态JavaScript被下载和执行的能力。
- en: 'Selenium itself is a programming language neutral framework. It offers a number
    of programming language bindings, such as Python, Java, C#, and PHP (amongst others).
    The framework also provides many components that focus on testing. Three commonly
    used components are:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Selenium本身是一个与编程语言无关的框架。它提供了许多编程语言绑定，如Python、Java、C#和PHP（等等）。该框架还提供了许多专注于测试的组件。其中三个常用的组件是：
- en: IDE for recording and replaying tests
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于录制和重放测试的IDE
- en: Webdriver, which actually launches a web browser (such as Firefox, Chrome, or
    Internet Explorer) by sending commands and sending the results to the selected
    browser
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Webdriver实际上启动了一个Web浏览器（如Firefox、Chrome或Internet Explorer），通过发送命令并将结果发送到所选的浏览器来运行脚本
- en: A grid server executes tests with a web browser on a remote server. It can run
    multiple test cases in parallel.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格服务器在远程服务器上执行带有Web浏览器的测试。它可以并行运行多个测试用例。
- en: Getting ready
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First we need to install Selenium.  We do this with our trusty `pip`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装Selenium。我们可以使用我们信赖的`pip`来完成这个过程：
- en: '[PRE38]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This installs the Selenium Client Driver for Python (the language bindings).
    You can find more information on it at [https://github.com/SeleniumHQ/selenium/blob/master/py/docs/source/index.rst](https://github.com/SeleniumHQ/selenium/blob/master/py/docs/source/index.rst)
    if you want to in the future.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装Python的Selenium客户端驱动程序（语言绑定）。如果你将来想要了解更多信息，可以在[https://github.com/SeleniumHQ/selenium/blob/master/py/docs/source/index.rst](https://github.com/SeleniumHQ/selenium/blob/master/py/docs/source/index.rst)找到更多信息。
- en: For this recipe we also need to have the driver for Firefox in the directory
    (it's named `geckodriver`).  This file is operating system specific.  I've included
    the file for Mac in the folder. To get other versions, visit [https://github.com/mozilla/geckodriver/releases](https://github.com/mozilla/geckodriver/releases).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们还需要在目录中有Firefox的驱动程序（名为`geckodriver`）。这个文件是特定于操作系统的。我已经在文件夹中包含了Mac的文件。要获取其他版本，请访问[https://github.com/mozilla/geckodriver/releases](https://github.com/mozilla/geckodriver/releases)。
- en: 'Still, when running this sample you may get the following error:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当运行这个示例时，你可能会遇到以下错误：
- en: '[PRE39]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: If you do, put the geckodriver file somewhere on your systems PATH, or add the
    `01` folder to your path. Oh, and you will need to have Firefox installed.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你这样做了，将geckodriver文件放在系统的PATH中，或者将`01`文件夹添加到你的路径中。哦，你还需要安装Firefox。
- en: Finally, it is required to have PhantomJS installed.  You can download and find
    installation instructions at: [http://phantomjs.org/](http://phantomjs.org/)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要安装PhantomJS。你可以在[http://phantomjs.org/](http://phantomjs.org/)下载并找到安装说明。
- en: How to do it...
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: The script for this recipe is `01/04_events_with_selenium.py`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的脚本是`01/04_events_with_selenium.py`。
- en: 'The following is the code:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是代码：
- en: '[PRE40]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'And run the script with Python.  You will see familiar output:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后用Python运行脚本。你会看到熟悉的输出：
- en: '[PRE41]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: During this process, Firefox will pop up and open the page. We have reused the
    previous recipe and adopted Selenium.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，Firefox将弹出并打开页面。我们重用了之前的配方并采用了Selenium。
- en: '![](assets/05feca6d-bf9f-4938-9cb7-1392310dc374.png)The Window Popped up by
    Firefox'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/05feca6d-bf9f-4938-9cb7-1392310dc374.png)Firefox弹出的窗口'
- en: How it works
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理
- en: 'The primary difference in this recipe is the following code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的主要区别在于以下代码：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This gets the Firefox driver and uses it to get the content of the specified
    URL.  This works by starting Firefox and automating it to go the the page, and
    then Firefox returns the page content to our app.  This is why Firefox popped
    up.  The other difference is that to find things we need to call `find_element_by_xpath`
    to search the resulting HTML.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本获取了Firefox驱动程序，并使用它来获取指定URL的内容。这是通过启动Firefox并自动化它去到页面，然后Firefox将页面内容返回给我们的应用程序。这就是为什么Firefox弹出的原因。另一个区别是，为了找到东西，我们需要调用`find_element_by_xpath`来搜索结果的HTML。
- en: There's more...
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: PhantomJS, in many ways, is very similar to Selenium. It has fast and native
    support for various web standards, with features such as DOM handling, CSS selector,
    JSON, Canvas, and SVG. It is often used in web testing, page automation, screen
    capturing, and network monitoring.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，PhantomJS与Selenium非常相似。它对各种Web标准有快速和本地支持，具有DOM处理、CSS选择器、JSON、Canvas和SVG等功能。它经常用于Web测试、页面自动化、屏幕捕捉和网络监控。
- en: 'There is one key difference between Selenium and PhantomJS: PhantomJS is **headless**
    and uses WebKit.  As we saw, Selenium opens and automates a browser.  This is
    not very good if we are in a continuous integration or testing environment where
    the browser is not installed, and where we also don''t want thousands of browser
    windows or tabs being opened.  Being headless, makes this faster and more efficient.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Selenium和PhantomJS之间有一个关键区别：PhantomJS是**无头**的，使用WebKit。正如我们所看到的，Selenium打开并自动化浏览器。如果我们处于一个连续集成或测试环境中，浏览器没有安装，我们也不希望打开成千上万个浏览器窗口或标签，那么这并不是很好。无头浏览器使得这一切更快更高效。
- en: 'The example for PhantomJS is in the `01/05_events_with_phantomjs.py` file. 
    There is a single one line change:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: PhantomJS的示例在`01/05_events_with_phantomjs.py`文件中。只有一行代码需要更改：
- en: '[PRE43]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: And running the script results in similar output to the Selenium / Firefox example,
    but without a browser popping up and also it takes less time to complete.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本会产生与Selenium/Firefox示例类似的输出，但不会弹出浏览器，而且完成时间更短。
