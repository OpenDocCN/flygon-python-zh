- en: Principles of Algorithm Design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法设计原则
- en: Why do we study algorithm design? There are, of course, many reasons, and our
    motivations for learning something is very much dependent on our own circumstances.
    There are, without a doubt, important professional reasons for being interested
    in algorithm design. Algorithms are the foundation of all computing. We can think
    of a computer as being a piece of hardware, with a hard drive, memory chips, processors,
    and so on. However, the essential component, the thing that, if missing, would
    render modern technology impossible, is algorithms. Let's learn more about it
    in the upcoming sections.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要学习算法设计？当然有很多原因，我们学习某些东西的动机很大程度上取决于我们自己的情况。对于对算法设计感兴趣有重要专业原因。算法是所有计算的基础。我们可以将计算机视为一台硬件，带有硬盘、内存芯片、处理器等。然而，如果缺少的是算法，现代技术将不可能存在。让我们在接下来的章节中了解更多。
- en: 'In this chapter, we will look at the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: An introduction to algorithms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法简介
- en: Recursion and backtracking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归和回溯
- en: Big O notation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大O符号
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We will need to install the `matplotlib` library with Python to plot the diagram
    in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用Python安装`matplotlib`库来绘制本章的图表。
- en: 'It can be installed on Ubuntu/Linux by running the following commands on the
    terminal:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在终端上运行以下命令在Ubuntu/Linux上安装：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can also use the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用以下内容：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To install `matplotlib` on Windows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上安装`matplotlib`：
- en: If Python is already installed on the Windows operating system, `matplotlib`
    can be obtained from the following link to install it on Windows: [https://github.com/matplotlib/matplotlib/downloads](https://github.com/matplotlib/matplotlib/downloads)
    or [https://matplotlib.org](https://matplotlib.org).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Python已经安装在Windows操作系统上，可以从以下链接获取`matplotlib`并在Windows上安装：[https://github.com/matplotlib/matplotlib/downloads](https://github.com/matplotlib/matplotlib/downloads)
    或 [https://matplotlib.org](https://matplotlib.org)。
- en: Code files for this chapter can be found at: [https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-Second-Edition/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-Second-Edition/tree/master/Chapter03).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在以下链接找到：[https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-Second-Edition/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-Second-Edition/tree/master/Chapter03)。
- en: An introduction to algorithms
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法简介
- en: The theoretical foundation of algorithms, in the form of the Turing machine,
    was established several decades before digital logic circuits could actually implement
    such a machine. The Turing machine is essentially a mathematical model that, using
    a predefined set of rules, translates a set of inputs into a set of outputs. The
    first implementations of Turing machines were mechanical and the next generation
    may likely see digital logic circuits replaced by quantum circuits or something
    similar. Regardless of the platform, algorithms play a central predominant role.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的理论基础，以图灵机的形式，是在数字逻辑电路实际上能够实现这样的机器的几十年前建立的。图灵机本质上是一个数学模型，它使用预定义的一组规则，将一组输入转换为一组输出。图灵机的第一批实现是机械的，下一代可能会看到数字逻辑电路被量子电路或类似的东西所取代。无论平台如何，算法都起着中心主导作用。
- en: 'Another aspect is the effect algorithms have on technological innovation. As
    an obvious example, consider the page rank search algorithm, a variation of which
    the Google Search engine is based on. Using this and similar algorithms allows
    researchers, scientists, technicians, and others to quickly search through vast
    amounts of information extremely quickly. This has a massive effect on the rate
    at which new research can be carried out, new discoveries made, and new innovative
    technologies developed. An algorithm is a sequential set of instructions to execute
    a particular task. They are very important, as we can break a complex problem
    into a smaller one to prepare simple steps to execute a big problem—that is the
    most important part of algorithms. A good algorithm is key for an efficient program
    to solve a specific problem. The study of algorithms is also important because
    it trains us to think very specifically about certain problems. It can help to
    increase our problem-solving abilities by isolating the components of a problem
    and defining relationships between these components. In summary, there are some
    important reasons for studying algorithms:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 算法对技术创新的影响是另一个方面。显而易见的例子是页面排名搜索算法，Google搜索引擎就是基于其变体。使用这些和类似的算法允许研究人员、科学家、技术人员等快速搜索大量信息。这对新研究的速度、新发现的速度以及新的创新技术的开发速度产生了巨大影响。算法是执行特定任务的顺序指令集。它们非常重要，因为我们可以将一个复杂的问题分解为一个小问题，以准备执行一个大问题的简单步骤——这是算法最重要的部分。一个好的算法是解决特定问题的高效程序的关键。学习算法也很重要，因为它训练我们对某些问题进行非常具体的思考。它可以通过隔离问题的组成部分并定义这些组成部分之间的关系来增加我们的问题解决能力。总之，学习算法有一些重要原因：
- en: They are essential for computer science and *intelligent* systems
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们对计算机科学和*智能*系统至关重要
- en: They are important in many other domains (computational biology, economics,
    ecology, communications, ecology, physics, and so on)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在许多其他领域中很重要（计算生物学、经济学、生态学、通信、生态学、物理等）
- en: They play a role in technology innovation
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在技术创新中发挥作用
- en: They improve problem-solving and analytical thinking
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们改进问题解决和分析思维
- en: 'There are mainly two important aspects to solve a given problem. Firstly, we
    need an efficient mechanism to store, manage, and retrieve the data, which is
    important to solve a problem (this comes under data structures); secondly, we
    require an efficient algorithm which is a finite set of instructions to solve
    that problem. Thus, the study of data structures and algorithms is key to solving
    any problem using computer programs. An efficient algorithm should have the following
    characteristics:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 解决给定问题主要有两个重要方面。首先，我们需要一个有效的机制来存储、管理和检索数据，这对解决问题很重要（这属于数据结构）；其次，我们需要一个有效的算法，这是一组有限的指令来解决问题。因此，研究数据结构和算法对使用计算机程序解决任何问题至关重要。有效的算法应具有以下特征：
- en: It should be as specific as possible
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该尽可能具体
- en: It should have each instruction properly defined
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法的每个指令都应该被正确定义
- en: There should not be any ambiguous instruction
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不应该有任何模糊的指令
- en: All the instructions of the algorithm should be executable in a finite amount
    of time and in a finite number of steps
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法的所有指令都应该在有限的时间内和有限的步骤内可执行
- en: It should have clear input and output to solve the problem
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该有清晰的输入和输出来解决问题
- en: Each instruction of the algorithm should be important in solving the given problem
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法的每个指令在解决给定问题时都很重要
- en: Algorithms, in their simplest form, are just a sequence of actions—a list of
    instructions. It may just be a linear construct of the form do *x*, then do *y*,
    then do *z*, then finish. However, to make things more useful we add clauses to
    the effect of do *x* then do *y*; in Python, these are if-else statements. Here,
    the future course of action is dependent on some conditions; say the state of
    a data structure. To this, we also add the operation, iteration, the while, and
    the for statements. Expanding our algorithmic literacy further, we add recursion.
    Recursion can often achieve the same results as iteration, however, they are fundamentally
    different. A recursive function calls itself, applying the same function to progressively
    smaller inputs. The input of any recursive step is the output of the previous
    recursive step.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在其最简单的形式中只是一系列操作 - 一系列指令。它可能只是一个形式为do *x*，然后do *y*，然后do *z*，然后完成的线性构造。然而，为了使事情更有用，我们添加了类似于do *x*然后do *y*的子句；在Python中，这些是if-else语句。在这里，未来的行动取决于某些条件；比如数据结构的状态。为此，我们还添加了操作、迭代、while和for语句。扩展我们的算法素养，我们添加了递归。递归通常可以实现与迭代相同的结果，但它们在根本上是不同的。递归函数调用自身，将相同的函数应用于逐渐减小的输入。任何递归步骤的输入是前一个递归步骤的输出。
- en: Algorithm design paradigms
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法设计范式
- en: 'In general, we can discern three broad approaches to algorithm design. They
    are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们可以分辨出三种算法设计的广泛方法。它们是：
- en: Divide and conquer
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分而治之
- en: Greedy algorithms
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪婪算法
- en: Dynamic programming
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态规划
- en: As the name suggests, the divide and conquer paradigm involves breaking a problem
    into smaller simple sub-problems, and then solving these sub-problems, and finally,
    combining the results to obtain a global optimal solution. This is a very common
    and natural problem-solving technique, and is, arguably, the most commonly used
    approach to algorithm design. For example, merge sort is an algorithm to sort
    a list of n natural numbers increasingly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，分而治之范式涉及将问题分解为较小的简单子问题，然后解决这些子问题，最后将结果组合以获得全局最优解。这是一种非常常见和自然的问题解决技术，可以说是算法设计中最常用的方法。例如，归并排序是一种对n个自然数列表进行递增排序的算法。
- en: In this algorithm, we divide the list iteratively in equal parts until each
    sub-list contains one element, and then we combine these sub-lists to create a
    new list in a sorted order. We will be discussing merge sort in more detail later
    in this section/chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，我们迭代地将列表分成相等的部分，直到每个子列表包含一个元素，然后我们将这些子列表组合在一起，以排序顺序创建一个新列表。我们将在本节/章节后面更详细地讨论归并排序。
- en: 'Some examples of divide and conquer algorithm paradigms are as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 分而治之算法范式的一些例子如下：
- en: Binary search
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分搜索
- en: Merge sort
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归并排序
- en: Quick sort
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速排序
- en: Karatsuba algorithm for fast multiplication
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karatsuba算法用于快速乘法
- en: Strassen's matrix multiplication
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯特拉森矩阵乘法
- en: Closest pair of points
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最接近的点对
- en: 'Greedy algorithms often involve optimization and combinatorial problems. In
    greedy algorithms, the objective is to obtain the best optimum solution from many
    possible solutions in each step, and we try to get the local optimum solution
    which may eventually lead us to obtain the overall optimum solution. Generally,
    greedy algorithms are used for optimization problems. Here are many popular standard
    problems where we can use greedy algorithms to obtain the optimum solution:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪算法通常涉及优化和组合问题。在贪婪算法中，目标是在每一步中从许多可能的解决方案中获得最佳的最优解，并且我们试图获得局部最优解，这可能最终导致我们获得整体最优解。通常，贪婪算法用于优化问题。以下是许多流行的标准问题，我们可以使用贪婪算法来获得最优解：
- en: Kruskal's minimum spanning tree
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克鲁斯卡尔最小生成树
- en: Dijkstra's shortest path
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迪杰斯特拉最短路径
- en: Knapsack problem
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 背包问题
- en: Prim's minimal spanning tree algorithm
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普林姆最小生成树算法
- en: Travelling salesman problem
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旅行推销员问题
- en: Greedy algorithms often involve optimization and combinatorial problems; the
    classic example is to apply the greedy algorithm to the traveling salesperson
    problem, where a greedy approach always chooses the closest destination first.
    This shortest-path strategy involves finding the best solution to a local problem
    in the hope that this will lead to a global solution.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪算法通常涉及优化和组合问题；经典的例子是将贪婪算法应用于旅行推销员问题，其中贪婪方法总是首先选择最近的目的地。这种最短路径策略涉及找到局部问题的最佳解决方案，希望这将导致全局解决方案。
- en: Another classic example is to apply the greedy algorithm   to the traveling
    salesperson problem; it is an NP-hard problem. In this problem, a greedy approach
    always chooses the closest unvisited city first from the current city; in this
    way, we are not sure that we get the best solution, but we surely get an optimal
    solution. This shortest-path strategy involves finding the best solution to a
    local problem in the hope that this will lead to a global solution.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个经典的例子是将贪婪算法应用于旅行推销员问题；这是一个NP难问题。在这个问题中，贪婪方法总是首先选择当前城市中最近的未访问城市；这样，我们不能确定我们得到了最佳解决方案，但我们肯定得到了一个最优解。这种最短路径策略涉及在希望这将导致全局解决方案的情况下找到局部问题的最佳解决方案。
- en: The dynamic programming approach is useful when our sub-problems overlap. This
    is different from divide and conquer. Rather than breaking our problem into independent
    sub-problems, with dynamic programming, intermediate results are cached and can
    be used in subsequent operations. Like divide and conquer, it uses recursion;
    however, dynamic programming allows us to compare results at different stages.
    This can have a performance advantage over the divide and conquer for some problems
    because it is often quicker to retrieve a previously calculated result from memory
    rather than having to recalculate it. Dynamic programming also uses recursion to
    solve the problems. For example, the matrix chain multiplication problem can be
    solved using dynamic programming. The matrix chain multiplication problem determines
    the best effective way to multiply the matrices when a sequence of matrices is
    given, it finds the order of multiplication that requires the minimum number of
    operations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划方法在我们的子问题重叠时非常有用。这与分治法不同。与将问题分解为独立子问题不同，动态规划中间结果被缓存并可以在后续操作中使用。与分治法一样，它使用递归；然而，动态规划允许我们在不同阶段比较结果。这对于某些问题来说可能比分治法具有性能优势，因为通常从内存中检索先前计算的结果比重新计算要快。动态规划也使用递归来解决问题。例如，矩阵链乘法问题可以使用动态规划来解决。矩阵链乘法问题确定了在给定一系列矩阵时，最有效的矩阵相乘的顺序，它找到需要最少操作次数的乘法顺序。
- en: 'For example, let''s look at three matrices—*P*, *Q*, and *R*. To compute the
    multiplication of these three matrices, we have many possible choices (because
    the matrix multiplication is associative), such as *(PQ)R = P(QR)*. So, if the
    sizes of these matrices are—*P* is a 20 × 30, *Q* is 30 × 45, *R* is 45 x 50,
    then, the number of multiplications for *(PQ)R* and *P(QR)* will be:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看三个矩阵——*P*、*Q*和*R*。要计算这三个矩阵的乘法，我们有许多可能的选择（因为矩阵乘法是可结合的），比如*(PQ)R = P(QR)*。因此，如果这些矩阵的大小是——*P*是20×30，*Q*是30×45，*R*是45×50，那么*(PQ)R*和*P(QR)*的乘法次数将是：
- en: '*(PQ)R* = 20 x 30 x 45 + 20 x 45 x 50 = 72,000'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(PQ)R* = 20 x 30 x 45 + 20 x 45 x 50 = 72,000'
- en: '*P(QR)* =  20 x 30 x 50 + 30 x 45 x 50 = 97,500'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(QR)* =  20 x 30 x 50 + 30 x 45 x 50 = 97,500'
- en: 'It can be observed from this example that if we multiply using the first option,
    then we would need 72,000 multiplications, which is less when compared to the
    second option/ This is shown in the following code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子可以看出，如果我们使用第一个选项进行乘法，那么我们需要72,000次乘法，与第二个选项相比要少。这在以下代码中显示：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Chapter 13](fff4acae-cc26-4b4c-a6d1-454703fa9e67.xhtml), *Design Techniques
    and Strategies*, presents a more detailed discussion on the algorithm design strategy.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[第13章](fff4acae-cc26-4b4c-a6d1-454703fa9e67.xhtml)，*设计技术和策略*，对算法设计策略进行了更详细的讨论。'
- en: Recursion and backtracking
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归和回溯
- en: 'Recursion is particularly useful for divide and conquer problems; however,
    it can be difficult to understand exactly what is happening, since each recursive
    call is itself spinning off other recursive calls. A recursive function can be
    in an infinite loop, therefore, it is required that each recursive function adhere
    to some properties. At the core of a recursive function are two types of cases:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 递归对于分治问题特别有用；然而，确切地了解发生了什么可能很困难，因为每个递归调用本身都会产生其他递归调用。递归函数可能会陷入无限循环，因此需要每个递归函数都遵守一些属性。递归函数的核心是两种类型的情况：
- en: '**Base cases**: These tell the recursion when to terminate, meaning the recursion
    will be stopped once the base condition is met'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基本情况**：这些告诉递归何时终止，意味着一旦满足基本条件，递归将停止'
- en: '**Recursive cases**: The function calls itself and we progress towards achieving
    the base criteria'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归情况**：函数调用自身，我们朝着实现基本条件的目标前进'
- en: 'A simple problem that naturally lends itself to a recursive solution is calculating
    factorials. The recursive factorial algorithm defines two cases: the base case
    when *n* is zero (the terminating condition), and the recursive case when *n* is
    greater than zero (the call of the function itself). A typical implementation
    is the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然适合递归解决方案的简单问题是计算阶乘。递归阶乘算法定义了两种情况：当*n*为零时的基本情况（终止条件），以及当*n*大于零时的递归情况（函数本身的调用）。一个典型的实现如下：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To calculate the factorial of `4`, we require four recursive calls plus the
    initial parent call. On each recursion, a copy of the method variables is stored
    in memory. Once the method returns it is removed from memory. The following is
    a way we can visualize this process:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算`4`的阶乘，我们需要四次递归调用加上初始父调用。在每次递归中，方法变量的副本都存储在内存中。一旦方法返回，它就会从内存中删除。以下是我们可以可视化这个过程的一种方式：
- en: 'It may not necessarily be clear if recursion or iteration is a better solution
    to a particular problem; after all, they both repeat a series of operations and
    both are very well-suited to divide and conquer approaches and to algorithm design.
    Iteration churns away until the problem is done with. Recursion breaks the problem
    down into smaller and smaller chunks and then combines the results. Iteration
    is often easier for programmers, because control stays local to a loop, whereas
    recursion can more closely represent mathematical concepts such as factorials.
    Recursive calls are stored in memory, whereas iterations are not. This creates
    a trade-off between processor cycles and memory usage, so choosing which one to
    use may depend on whether the task is processor or memory intensive. The following
    table outlines the key differences between recursion and iteration:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 递归或迭代哪个更好的解决方案可能并不清楚；毕竟，它们都重复一系列操作，并且都非常适合分治方法和算法设计。迭代一直进行，直到问题解决为止。递归将问题分解成越来越小的块，然后将结果组合起来。迭代对程序员来说通常更容易，因为控制保持在循环内部，而递归可以更接近表示阶乘等数学概念。递归调用存储在内存中，而迭代不是。这在处理器周期和内存使用之间产生了一种权衡，因此选择使用哪种可能取决于任务是处理器密集型还是内存密集型。以下表格概述了递归和迭代之间的主要区别：
- en: '| **Recursion** | **Iteration** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **递归** | **迭代** |'
- en: '| The function calls itself. | A set of instructions are executed repeatedly
    in the loop. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 函数调用自身。 | 一组指令在循环中重复执行。 |'
- en: '| It stops when the termination condition is met. | It stops execution when the loop
    condition is met. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 当满足终止条件时停止。 | 当满足循环条件时停止执行。 |'
- en: '| Infinite recursive calls may give an error related to stack overflow. | An
    infinite iteration will run indefinitely until the hardware is powered. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 无限递归调用可能会导致与堆栈溢出相关的错误。 | 无限迭代将无限运行，直到硬件断电。 |'
- en: '| Each recursive call needs memory space. | Each iteration does not require
    memory storage. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 每个递归调用都需要内存空间。 | 每次迭代不需要内存存储。 |'
- en: '| The code size, in general, is comparatively smaller. | The code size, in
    general, is comparatively smaller. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 代码大小一般来说相对较小。 | 代码大小一般来说相对较小。 |'
- en: '| Recursion is generally slower than iteration. | It is faster as it does not
    require a stack. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 递归通常比迭代慢。 | 它更快，因为不需要栈。 |'
- en: Backtracking
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回溯
- en: Backtracking is a form of recursion that is particularly useful for types of
    problems such as traversing tree structures, where we are presented with a number
    of options for each node, from which we must choose one. Subsequently, we are
    presented with a different set of options, and depending on the series of choices
    made, either a goal state or a dead end is reached. If it is the latter, we must
    backtrack to a previous node and traverse a different branch. Backtracking is
    a divide and conquer method for exhaustive searching. Importantly, backtracking
    **prunes** branches that cannot give a result.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 回溯是一种特别适用于遍历树结构等类型问题的递归形式，其中对于每个节点我们有许多选项可供选择。随后，我们会得到一组不同的选项，根据所做的选择系列，会达到一个目标状态或者一个死胡同。如果是后者，我们必须回溯到先前的节点并遍历不同的分支。回溯是一种用于穷举搜索的分治方法。重要的是，回溯**修剪**了无法给出结果的分支。
- en: 'An example of backtracking is given next. Here, we have used a recursive approach
    to generate all the possible arrangements of a given string, `s`, of a given length, `n`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面给出了回溯的一个例子。在这里，我们使用了递归方法来生成给定字符串 `s` 的所有可能排列，长度为 `n`：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This generates the following output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '![](Images/933bcc38-2e75-47b4-917e-7d5ee731f5b7.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/933bcc38-2e75-47b4-917e-7d5ee731f5b7.png)'
- en: Notice the double list compression and the two recursive calls within this comprehension.
    This recursively concatenates each element of the initial sequence, returned when
    *n* =1, with each element of the string generated in the previous recursive call.
    In this sense, it is *backtracking* to uncover previously ungenerated combinations.
    The final string that is returned is all *n* letter combinations of the initial
    string.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个推导中的双重列表压缩和两个递归调用。这递归地连接了初始序列的每个元素，当 *n* =1 时返回，与先前递归调用生成的字符串的每个元素。在这个意义上，它是
    *回溯*，以揭示先前未生成的组合。返回的最终字符串是初始字符串的所有 *n* 个字母组合。
- en: Divide and conquer – long multiplication
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分治——长乘法
- en: For recursion to be more than just a clever trick, we need to understand how
    to compare it to other approaches, such as iteration, and to understand when its
    use will lead to a faster algorithm. An iterative algorithm that we are all familiar
    with is the procedure we learned in primary math classes, and is used to multiply
    two large numbers. That is long multiplication. If you remember, long multiplication
    involved iterative multiplying and carry operations followed by a shifting and
    addition operation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使递归不仅仅是一个巧妙的技巧，我们需要了解如何将其与其他方法进行比较，例如迭代，并了解何时使用它将导致更快的算法。我们都熟悉的迭代算法是我们在小学数学课上学到的程序，用于将两个大数相乘。那就是长乘法。如果你记得的话，长乘法涉及迭代乘法和进位操作，然后是移位和加法操作。
- en: Our aim here is to examine ways to measure how efficient this procedure is and
    attempt to answer the question—is this the most efficient procedure we can use
    for multiplying two large numbers together?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是检查如何衡量这个过程的效率，并尝试回答这个问题——这是我们用来将两个大数相乘的最有效的过程吗？
- en: 'In the following diagram, we can see that multiplying two four-digit numbers
    together requires 16 multiplication operations, and we can generalize and say
    that an *n* digit number requires, approximately, *n*^(*2*) multiplication operations:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到将两个四位数相乘需要 16 次乘法运算，我们可以概括地说，一个 *n* 位数需要大约 *n*^(*2*) 次乘法运算：
- en: '![](Images/0bc9b9a7-2672-436c-b651-f1d56260339c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/0bc9b9a7-2672-436c-b651-f1d56260339c.png)'
- en: This method of analyzing algorithms, in terms of the number of computational
    primitives such as multiplication and addition, is important because it gives
    us a way to understand the relationship between the time it takes to complete
    a certain computation and the size of the input to that computation. In particular,
    we want to know what happens when the input, the number of digits, *n*, is very
    large. This topic, called **asymptotic analysis**, or **time complexity**, is
    essential to our study of algorithms and we will revisit it often during this
    chapter and the rest of this book.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以计算原语的数量，如乘法和加法，来分析算法的方法很重要，因为它为我们提供了一种理解完成某个计算所需的时间与该计算的输入大小之间关系的方法。特别是，我们想知道当输入，即数字的位数*n*非常大时会发生什么。这个主题被称为**渐近分析**或**时间复杂度**，对我们研究算法至关重要，在本章和本书的其余部分我们将经常回顾这个主题。
- en: The recursive approach
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归方法
- en: 'It turns out that in the case of long multiplication the answer is yes, there
    are in fact several algorithms for multiplying large numbers that require less
    operations. One of the most well-known alternatives to long multiplication is
    the **Karatsuba algorithm**, first published in 1962\. This takes a fundamentally
    different approach: rather than iteratively multiplying single-digit numbers,
    it recursively carries out multiplication operations on progressively smaller
    inputs. Recursive programs call themselves on smaller subsets of the input. The
    first step in building a recursive algorithm is to decompose a large number into
    several smaller numbers. The most natural way to do this is to simply split the
    number into two halves, the first half of most-significant digits, and a second
    half of least-significant digits. For example, our four-digit number, 2345, becomes
    a pair of two-digit numbers, 23 and 45\. We can write a more general decomposition
    of any two *n* digit numbers, *x*, and *y* using the following, where *m* is any
    positive integer less than *n*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，在长乘法的情况下，答案是肯定的，实际上有几种算法可以减少操作次数。其中最著名的替代长乘法的算法之一是**Karatsuba算法**，首次发表于1962年。这采用了一种基本不同的方法：而不是迭代地相乘单个数字，它在逐渐减小的输入上递归地进行乘法运算。递归程序在输入的较小子集上调用自身。构建递归算法的第一步是将一个大数分解为几个较小的数。这样做的最自然的方式是将数字分成两半，前半部分是最高有效数字，后半部分是最低有效数字。例如，我们的四位数2345变成了一对两位数23和45。我们可以使用以下更一般的分解来写出任意两个*n*位数*x*和*y*的分解，其中*m*是小于*n*的任意正整数：
- en: '![](Images/a3c99940-655a-414f-81bf-3f12983cecde.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/a3c99940-655a-414f-81bf-3f12983cecde.png)
- en: '![](Images/b3b0c275-29d8-4f19-a38d-f3b935180c21.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/b3b0c275-29d8-4f19-a38d-f3b935180c21.png)
- en: 'So now we can rewrite our multiplication problem *x*, *y* as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将我们的乘法问题*x*，*y*重写如下：
- en: '![](Images/feea5e6e-0ba8-42af-820e-c61603ca563e.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/feea5e6e-0ba8-42af-820e-c61603ca563e.png)
- en: 'When we expand, we get the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们展开时，我们得到以下结果：
- en: '![](Images/31fa81b7-71ca-471f-a4a5-8d92229fc993.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/31fa81b7-71ca-471f-a4a5-8d92229fc993.png)
- en: 'More conveniently, we can write it like this (equation 3.1):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 更方便的是，我们可以这样写（方程3.1）：
- en: '![](Images/3eecd139-882e-4cf6-bfb1-1d89f560583b.png)                       
      ... (3.1)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/3eecd139-882e-4cf6-bfb1-1d89f560583b.png)                       
      ... (3.1)
- en: 'Where:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在哪里：
- en: '![](Images/d46fac6b-adc9-4d6e-aeb8-efdf3c003ddd.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/d46fac6b-adc9-4d6e-aeb8-efdf3c003ddd.png)
- en: It should be pointed out that this suggests a recursive approach to multiplying
    two numbers since this procedure does itself involve multiplication. Specifically,
    the products *ac*, *ad*, *bc*, and *bd* all involve numbers smaller than the input
    number and so it is conceivable that we could apply the same operation as a partial
    solution to the overall problem. This algorithm, so far, consists of four recursive
    multiplication steps and it is not immediately clear if it will be faster than
    the classic long multiplication approach.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 应该指出，这表明了一种递归方法来乘两个数字，因为这个过程本身涉及乘法。具体来说，乘积*ac*、*ad*、*bc*和*bd*都涉及比输入数字小的数字，因此我们可以将相同的操作应用为整体问题的部分解决方案。到目前为止，这个算法包括四个递归乘法步骤，目前还不清楚它是否比经典的长乘法方法更快。
- en: 'What we have discussed so far in regards to the recursive approach to multiplication,
    has been well-known to mathematicians since the late nineteenth century. The Karatsuba
    algorithm improves on this by making the following observation. We really only
    need to know three quantities: *z*[*2*]= *ac*, *z*[*1*]*=ad +bc*, and *z*[*0*]=
    *bd* to solve equation 3.1\. We need to know the values of *a*, *b*, *c*, and
    *d* only in so far as they contribute to the overall sum and products involved
    in calculating the quantities *z*[*2*], *z*[*1*], and *z*[*0*]. This suggests
    the possibility that perhaps we can reduce the number of recursive steps. It turns
    out that this is indeed the situation.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所讨论的关于递归方法的乘法，自19世纪末以来就为数学家所熟知。Karatsuba算法通过以下观察改进了这一点。我们实际上只需要知道三个量：*z*[*2*]=
    *ac*，*z*[*1*]*=ad +bc*，和*z*[*0*]= *bd*来解方程3.1。我们只需要知道*a*、*b*、*c*和*d*的值，因为它们对计算涉及的总和和乘积有贡献。这表明或许我们可以减少递归步骤的数量。事实证明，情况确实如此。
- en: 'Since the products *ac* and *bd* are already in their simplest form, it seems
    unlikely that we can eliminate these calculations. We can, however, make the following
    observation:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于乘积*ac*和*bd*已经处于最简形式，看来我们无法消除这些计算。然而，我们可以做出以下观察：
- en: '![](Images/88959a91-37a3-4a23-93c4-083e43baa17a.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/88959a91-37a3-4a23-93c4-083e43baa17a.png)
- en: 'When we subtract the quantities *ac* and *bd*, which we have calculated in
    the previous recursive step, we get the quantity we need, namely (*ad + bc*):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们减去我们在上一个递归步骤中计算的量*ac*和*bd*时，我们得到我们需要的量，即(*ad + bc*)：
- en: '![](Images/4d502318-8937-4c95-a555-5be65f889dce.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/4d502318-8937-4c95-a555-5be65f889dce.png)
- en: 'This shows that we can indeed compute the sum of *ad + bc* without separately
    computing each of the individual quantities. In summary, we can improve on equation
    3.1 by reducing four recursive steps to three. These three steps are as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们确实可以计算*ad + bc*的和，而不必分别计算每个单独的数量。总之，我们可以通过将四个递归步骤减少到三个来改进方程3.1。这三个步骤如下：
- en: Recursively calculate *ac*
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归计算*ac*
- en: Recursively calculate *bd*
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归计算*bd*
- en: Recursively calculate (*a + b*)(*c + d*) and subtract *ac* and *bd*
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归计算（*a + b*）（*c + d*）并减去*ac*和*bd*
- en: 'The following example shows a Python implementation of the Karatsuba algorithm.
    In the following code, initially, we see if any one of the given numbers is less
    than 10, then there is no need to run recursive functions. Next, we identify the
    number of digits in the larger value, and add one if the number of digits is odd.
    Finally, we recursively call the function three times to calculate *ac*, *bd*,
    and (*a + d*)(*c + d*). The following code prints the multiplication of any two
    digits; for example, it prints `4264704` for the multiplication of `1234` and
    `3456`. The implementation of the Karatsuba algorithm is:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了Karatsuba算法的Python实现。在以下代码中，最初，我们检查给定数字中是否有任何一个小于10，然后就不需要运行递归函数。接下来，我们确定较大值的数字位数，并在数字位数为奇数时加一。最后，我们递归调用函数三次来计算*ac*、*bd*和（*a
    + d*）（*c + d*）。以下代码打印任意两个数字的乘积；例如，它打印出`4264704`来表示`1234`和`3456`的乘积。Karatsuba算法的实现如下：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Runtime analysis
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行时间分析
- en: The performance of an algorithm is generally measured by the size of its input
    data (**n**) and the time and the memory space used by the algorithm. **Time**
    required is measured by the key operations to be performed by the algorithm (such
    as comparison operations), whereas the space requirements of an algorithm is measured
    by the storage needed to store the variables, constants, and instructions during
    the execution of the program. The space requirements of an algorithm may also
    change dynamically during execution as it depends on variable size, which is to
    be decided at runtime, such as dynamic memory allocation, memory stacks, and so
    on.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的性能通常由其输入数据的大小（**n**）以及算法使用的时间和内存空间来衡量。所需的**时间**由算法执行的关键操作（如比较操作）来衡量，而算法的空间需求则由在程序执行期间存储变量、常量和指令所需的存储空间来衡量。算法的空间需求在执行期间也可能动态变化，因为它取决于变量大小，这在运行时决定，例如动态内存分配、内存堆栈等。
- en: The running time required by an algorithm depends on the input size; as the
    input size (**n**) increases, the runtime also increases. For example, a sorting
    algorithm will have more running time to sort the list of input size 5,000 as
    compared to the other list of input size 50\. Therefore, it is clear that to compute
    the time complexity, the input size is important. Further, for a specific input,
    the running time depends on the key operations to be executed in the algorithm.
    For example, the key operation for a sorting algorithm is a **comparison operation**
    that will take most of the time as compared to assignment or any other operation.
    The more is the number of key operations to be executed, the longer it will take
    to run the algorithm.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 算法所需的运行时间取决于输入大小；随着输入大小（**n**）的增加，运行时间也会增加。例如，对于输入大小为5,000的列表，排序算法将需要更多的运行时间来排序，而对于输入大小为50的列表，运行时间较短。因此，可以清楚地看出，要计算时间复杂度，输入大小是重要的。此外，对于特定输入，运行时间取决于算法中要执行的关键操作。例如，对于排序算法，关键操作是**比较操作**，它将占用大部分时间，而不是赋值或其他任何操作。要执行的关键操作越多，运行算法所需的时间就越长。
- en: 'It should be noted that an important aspect to algorithm design is gauging
    the efficiency both in terms of space (memory) and time (number of operations).
    It should be mentioned that an identical metric is used to measure an algorithm''s
    memory performance. There are a number of ways we could, conceivably, measure
    runtime and probably the most obvious way is to simply measure the total time
    taken by the algorithm. The major problem with this approach is that the time
    taken for an algorithm to run is very much dependent on the hardware it is run
    on. A platform-independent way to gauge an algorithm''s runtime is to count the
    number of operations involved. However, this is also problematic as there is no
    definitive way to quantify an operation. This is dependent on the programming
    language, the coding style, and how we decide to count operations. We can use
    this idea, though, of counting operations, if we combine it with the expectation
    that as the size of the input increases the runtime will increase in a specific
    way. That is, there is a mathematical relationship between *n*, the size of the
    input, and the time it takes for the algorithm to run. There are essentially three
    things that characterize an algorithm''s runtime performance; these can be described
    as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，算法设计的一个重要方面是评估效率，无论是在空间（内存）还是时间（操作次数）方面。应该提到的是，用于衡量算法内存性能的度量标准与衡量算法运行时间的度量标准相同。我们可以以多种方式来衡量运行时间，最明显的方式可能是简单地测量算法所需的总时间。这种方法的主要问题在于算法运行所需的时间非常依赖于其运行的硬件。衡量算法运行时间的一个与平台无关的方法是计算所涉及的操作次数。然而，这也是有问题的，因为没有明确的方法来量化一个操作。这取决于编程语言、编码风格以及我们决定如何计算操作。然而，如果我们将这种计算操作的想法与一个期望相结合，即随着输入大小的增加，运行时间将以特定方式增加，我们就可以使用这个想法。也就是说，输入大小**n**和算法运行时间之间存在数学关系。基本上有三个因素决定了算法的运行时间性能；它们可以描述如下：
- en: Worst-case complexity is the upper-bound complexity; it is the maximum running
    time required for an algorithm to execute. In this case, the key operations would
    be executed the maximum number of times.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最坏情况复杂度是上界复杂度；它是算法执行所需的最大运行时间。在这种情况下，关键操作将执行最大次数。
- en: Best-case complexity is the lower-bound complexity; it is the minimum running
    time required for an algorithm to execute. In this case, the key operations would
    be executed the minimum number of times.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳情况复杂度是下界复杂度；这是算法执行所需的最小运行时间。在这种情况下，关键操作将执行最少次数。
- en: Average-case complexity is the average running time required for an algorithm
    to execute.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均情况复杂度是算法执行所需的平均运行时间。
- en: Worst-case analysis is useful because it gives us a tight upper bound that our
    algorithm is guaranteed not to exceed. Ignoring small constant factors, and lower-order
    terms, is really just about ignoring the things that, at large values of the input
    size, *n*, do not contribute, in a large degree, to the overall run time. Not
    only does this make our work mathematically easier, but it also allows us to focus
    on the things that are having the most impact on performance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最坏情况分析是有用的，因为它给出了我们的算法保证不会超过的严格上界。忽略小的常数因子和低阶项，实际上就是忽略那些在输入规模较大时对总运行时间没有很大贡献的事物。这不仅使我们的工作在数学上更容易，而且还使我们能够专注于对性能影响最大的事物。
- en: We saw with the Karatsuba algorithm that the number of multiplication operations
    increased to the square of the size, *n*, of the input. If we have a four-digit
    number the number of multiplication operations is 16; an eight-digit number requires
    64 operations. Typically, though, we are not really interested in the behavior
    of an algorithm at small values of *n*, so we most often ignore factors that increase
    at slower rates, say linearly with *n*. This is because at high values of *n*,
    the operations that increase the fastest as we increase *n *will dominate.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Karatsuba算法中看到，乘法操作的数量增加到输入大小*n*的平方。如果我们有一个四位数，乘法操作的数量是16；一个八位数需要64次操作。通常，我们实际上并不关心算法在*n*的小值时的行为，所以我们经常忽略随着*n*线性增加的因子。这是因为在较大的*n*值时，随着*n*的增加，增长最快的操作将占主导地位。
- en: 'We will explain this in more detail with an example: the merge sort algorithm.
    Sorting is the subject of [Chapter 10](3b546628-5e98-41b9-a0a8-066c907061c3.xhtml),
    *Sorting*, however, as a precursor and as a useful way to learn about runtime
    performance, we will introduce merge sort here.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个示例来更详细地解释这个归并排序算法。排序是[第10章](3b546628-5e98-41b9-a0a8-066c907061c3.xhtml)的主题，*排序*，然而，作为一个前导和了解运行时性能的有用方式，我们将在这里介绍归并排序。
- en: The merge sort algorithm is a classic algorithm developed over 60 years ago.
    It is still used widely in many of the most popular sorting libraries. It is relatively
    simple and efficient. It is a recursive algorithm that uses a divide and conquer
    approach. This involves breaking the problem into smaller sub-problems, recursively
    solving them, and then somehow combining the results. Merge sort is one of the
    most obvious demonstrations of the divide and conquer paradigm.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 归并排序算法是60多年前开发的经典算法。它仍然广泛应用于许多最流行的排序库中。它相对简单而高效。它是一种使用分而治之方法的递归算法。这涉及将问题分解为更小的子问题，递归地解决它们，然后以某种方式组合结果。归并排序是分而治之范式最明显的演示之一。
- en: 'The merge sort algorithm consists of three simple steps:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 归并排序算法由三个简单的步骤组成：
- en: Recursively sort the left half of the input array
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归地对输入数组的左半部分进行排序
- en: Recursively sort the right half of the input array
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归地对输入数组的右半部分进行排序
- en: Merge two sorted sub-arrays into one
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个排序好的子数组合并成一个
- en: 'A typical problem is sorting a list of numbers into a numerical order. Merge
    sort works by splitting the input into two halves and working on each half in
    parallel. We can illustrate this process schematically with the following diagram:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 典型问题是将一组数字按数字顺序排序。归并排序通过将输入分成两半，并同时处理每一半来工作。我们可以用以下图表来形象地说明这个过程：
- en: 'Here is the Python code for the merge sort algorithm:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是归并排序算法的Python代码：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We run this program for the following results:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行这个程序得到以下结果：
- en: '![](Images/d2f8424e-f06e-47ff-ac59-5a0d32a0f06f.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/d2f8424e-f06e-47ff-ac59-5a0d32a0f06f.png)'
- en: 'The problem that we are interested in is how we determine the runtime performance,
    that is, what is the rate of growth in the time it takes for the algorithm to
    complete relative to the size of *n*? To understand this a bit better, we can
    map each recursive call onto a tree structure. Each node in the tree is a recursive
    call working on progressively smaller sub-problems:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的问题是如何确定运行时性能，也就是说，算法完成所需的时间与*n*的大小相关的增长率是多少？为了更好地理解这一点，我们可以将每个递归调用映射到一个树结构上。树中的每个节点都是递归调用，处理逐渐变小的子问题：
- en: '![](Images/40e4b2b0-3c6a-4630-8f5d-643da4b7210c.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/40e4b2b0-3c6a-4630-8f5d-643da4b7210c.png)'
- en: Each invocation of merge sort subsequently creates two recursive calls, so we
    can represent this with a binary tree. Each of the child nodes receives a subset
    of the input. Ultimately, we want to know the total time it takes for the algorithm
    to complete relative to the size of *n*. To begin with, we can calculate the amount
    of work and the number of operations at each level of the tree.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 每次调用归并排序都会随后创建两个递归调用，因此我们可以用二叉树来表示这一点。每个子节点都接收输入的一个子集。最终，我们想知道算法完成所需的总时间与*n*的大小相关。首先，我们可以计算树的每个级别的工作量和操作数量。
- en: Focusing on the runtime analysis, at level one, the problem is split into two
    *n*/2 sub-problems; at level two, there are four *n*/4 subproblems, and so on.
    The question is, when does the recursion bottom out, that is, when does it reach
    its base case? This is simply when the array is either zero or one.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 关注运行时分析，在第一级，问题分成两个*n*/2个子问题；在第二级，有四个*n*/4个子问题，依此类推。问题是，递归何时结束，也就是说，何时达到基本情况？这只是当数组要么是零要么是一时。
- en: The number of recursive levels is exactly the number of times you need to divide
    *n* by two until you get a number that is at most one. This is precisely the definition
    of log2\. Since we are counting the initial recursive call as level zero, the
    total number of levels is log[2]*n* + 1.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 递归级别的数量恰好是将*n*除以二直到得到最多为一的数字的次数。这恰好是log2的定义。由于我们将初始递归调用计为级别零，总级别数为log[2]*n*
    + 1。
- en: Let's just pause to refine our definitions. So far, we have been describing
    the number of elements in our input by the letter *n*. This refers to the number
    of elements in the first level of the recursion, that is, the length of the initial
    input. We are going to need to differentiate between the size of the input at
    subsequent recursive levels. For this, we will use the letter *m* or specifically
    *m*[*j*] for the length of the input at recursive level *j.*
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，重新定义一下。到目前为止，我们一直用字母*n*来描述输入中的元素数量。这指的是递归的第一级中的元素数量，也就是初始输入的长度。我们需要区分后续递归级别的输入大小。为此，我们将使用字母*m*，或者特别是*m*[*j*]来表示递归级别*j*的输入长度。
- en: Also, there are a few details we have overlooked, and I am sure you are beginning
    to wonder about. For example, what happens when *m*/2 is not an integer, or when
    we have duplicates in our input array? It turns out that this does not have an
    important impact on our analysis here; we will revisit some of the finer details
    of the merge sort algorithm in [Chapter 12](fff4acae-cc26-4b4c-a6d1-454703fa9e67.xhtml),
    *Design Techniques and Strategies*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些细节我们忽略了，我相信你也开始好奇了。例如，当*m*/2不是整数时会发生什么，或者当我们的输入数组中有重复元素时会发生什么？事实证明，这对我们的分析并没有重要影响；我们将在《第12章设计技术和策略》中重新审视归并排序算法的一些细节。
- en: The advantage of using a recursion tree to analyze algorithms is that we can
    calculate the work done at each level of the recursion. How we define this work
    is simply by the total number of operations and this, of course, is related to
    the size of the input. It is important to measure and compare the performance
    of algorithms in a platform-independent way. The actual runtime will, of course,
    be dependent on the hardware on which it is run. Counting the number of operations
    is important because it gives us a metric that is directly related to an algorithm's
    performance, independent of the platform.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用递归树来分析算法的优势在于我们可以计算每个递归级别的工作量。我们定义这个工作量就是总操作次数，这当然与输入的大小有关。以平台无关的方式来测量和比较算法的性能是很重要的。实际运行时间当然取决于其运行的硬件。计算操作次数很重要，因为它给了我们一个与算法性能直接相关的度量，而不受平台的影响。
- en: In general, since each invocation of merge sort is making two recursive calls,
    the number of calls is doubling at each level. At the same time, each of these
    calls is working on an input that is half of its parents. We can formalize this
    and say that for level *j*, where *j* is an integer *0, 1, 2 ... log[2]n*, there
    are two sub-problems each of size *n/2^j*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，由于归并排序的每次调用都会进行两次递归调用，所以调用次数在每个级别都会翻倍。与此同时，每个调用都在处理其父级别一半大小的输入。我们可以形式化地说，在第*j*级，其中*j*是整数*0,
    1, 2 ... log[2]n*，有两个大小为*n/2^j*的子问题。
- en: To calculate the total number of operations, we need to know the number of operations
    encompassed by a single merge of two sub-arrays. Let's count the number of operations
    in the previous Python code. What we are interested in is all the code after the
    two recursive calls have been made. Firstly, we have the three assignment operations.
    This is followed by three `while` loops. In the first loop, we have an if-else
    statement and within each of our two operations, a comparison followed by an assignment.
    Since there are only one of these sets of operations within the if-else statements,
    we can count this block of code as two operations carried out *m* times. This
    is followed by two `while` loops with an assignment operation each. This makes
    a total of *4m + 3* operations for each recursion of merge sort.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算总操作次数，我们需要知道合并两个子数组所包含的操作次数。让我们来数一下之前Python代码中的操作次数。我们感兴趣的是在进行两次递归调用之后的所有代码。首先，我们有三个赋值操作。然后是三个`while`循环。在第一个循环中，我们有一个if-else语句，在每个操作中，都有一个比较，然后是一个赋值。由于在if-else语句中只有一个这样的操作集，我们可以将这段代码计算为每次递归执行两次的操作。接下来是两个`while`循环，每个循环都有一个赋值操作。这使得每次归并排序递归的总操作次数为*4m
    + 3*。
- en: Since *m* must be at least one, the upper bound for the number of operations
    is 7*m*. It has to be said that this has no pretence at being an exact number.
    We could, of course, decide to count operations in a different way. We have not
    counted the increment operations or any of the housekeeping operations; however,
    this is not so important as we are more concerned with the rate of growth of the
    runtime with respect to *n* at high values of *n*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*m*至少必须为一，操作次数的上限是7*m*。必须指出，这并不是一个精确的数字。当然，我们可以决定以不同的方式计算操作次数。我们没有计算增量操作或任何维护操作；然而，在高值的*n*下，这并不重要，因为我们更关心运行时间相对于*n*的增长率。
- en: This may seem a little daunting since each call of a recursive call itself spins
    off more recursive calls, and seemingly explodes exponentially. The key fact that
    makes this manageable is that as the number of recursive calls doubles, the size
    of each subproblem halves. These two opposing forces cancel out nicely, as we
    can demonstrate.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有点令人生畏，因为每次递归调用本身都会产生更多的递归调用，似乎呈指数级增长。使这一切变得可控的关键事实是，随着递归调用次数翻倍，每个子问题的大小减半。这两股相反的力量得到了很好的抵消，我们可以证明这一点。
- en: 'To calculate the maximum number of operations at each level of the recursion
    tree we simply multiply the number of subproblems by the number of operations
    in each subproblem as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算递归树每个级别的最大操作次数，我们只需将子问题的数量乘以每个子问题的操作次数，如下所示：
- en: '![](Images/f963cdb9-6851-4a7b-9ee5-a7437f7c0f5c.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f963cdb9-6851-4a7b-9ee5-a7437f7c0f5c.png)'
- en: Importantly, this shows that, because the *2^j* cancels out the number of operations
    at each level is independent of the level. This gives us an upper bound to the
    number of operations carried out on each level, in this example, 7*n*. It should
    be pointed out that this includes the number of operations performed by each recursive
    call on that level, not the recursive calls made on subsequent levels. This shows
    that the work is done, as the number of recursive calls doubles with each level,
    and is exactly counterbalanced by the fact that the input size for each sub-problem
    is halved.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这表明，因为*2^j*取消了每个级别的操作数量，所以每个级别的操作数量是独立的。这给了我们每个级别执行的操作数量的上限，在这个例子中是7*n*。需要指出的是，这包括在该级别上每个递归调用执行的操作数量，而不是在后续级别上进行的递归调用。这表明工作是完成的，因为随着每个级别递归调用的数量翻倍，而每个子问题的输入大小减半，这正好抵消了这一事实。
- en: 'To find the total number of operations for a complete merge sort, we simply
    multiply the number of operations on each level by the number of levels. This
    gives us the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到完整归并排序的总操作数，我们只需将每个级别上的操作数乘以级别数。这给出了以下结果：
- en: '![](Images/e06dc99e-5e0a-4f6b-829f-2ad033c7a5ed.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/e06dc99e-5e0a-4f6b-829f-2ad033c7a5ed.png)'
- en: 'When we expand this out, we get the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们展开这个式子时，我们得到以下结果：
- en: '![](Images/d0ad8584-5760-4081-aac4-65ab789871f5.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/d0ad8584-5760-4081-aac4-65ab789871f5.png)'
- en: The key point to take from this is that there is a logarithmic component to
    the relationship between the size of the input and the total running time. If
    you remember from school mathematics, the distinguishing characteristic of the
    logarithm function is that it flattens off very quickly. As an input variable,
    *x* increases in size; the output variable *y* increases by smaller and smaller
    amounts.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以得出一个关键点，即输入大小和总运行时间之间存在对数关系。如果你还记得学校数学，对数函数的显著特点是它非常快速地变平。作为输入变量，*x*增加，输出变量*y*增加的幅度越来越小。
- en: 'For example, compare the log function to a linear function:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将对数函数与线性函数进行比较：
- en: '![](Images/a70898f7-13ed-4b6b-92d4-a71a04c40e64.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/a70898f7-13ed-4b6b-92d4-a71a04c40e64.png)'
- en: 'In the previous example, multiplying the *n*log[2] *n* component and comparing
    it to ![](Images/2b34c45d-a9f3-4b96-8893-66994aba5875.png):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，将*n*log[2] *n*分量与![](Images/2b34c45d-a9f3-4b96-8893-66994aba5875.png)进行比较：
- en: '![](Images/66071838-ae0b-4bb1-b942-01638c4ef2e2.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/66071838-ae0b-4bb1-b942-01638c4ef2e2.png)'
- en: Notice how for very low values of *n*, the time to complete, *t*, is actually
    lower for an algorithm that runs in n2 time. However, for values above about 40,
    the log function begins to dominate, flattening the output until, at the comparatively
    moderate size *n* = 100, the performance is more than twice than that of an algorithm
    running in *n*² time. Notice also that the disappearance of the constant factor,
    + 7, is irrelevant at high values of *n*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于非常低的*n*值，完成时间*t*实际上比运行时间为n2的算法更低。然而，对于大约40以上的值，对数函数开始主导，使输出变得平坦，直到相对较中等大小的*n*
    = 100时，性能比运行时间为*n*²的算法高出一倍以上。还要注意，在高*n*值时，常数因子+7的消失是无关紧要的。
- en: 'The code used to generate these graphs is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成这些图表的代码如下：
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You will need to install the `matplotlib` library, if it is not installed already,
    for this to work. Details can be found at the following address; I encourage you
    to experiment with this list comprehension expression used to generate the plots.
    For example, we could add the following `plot` statement:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尚未安装`matplotlib`库，您需要安装它才能运行。详细信息可以在以下地址找到；我鼓励您尝试使用列表推导表达式来生成图表。例如，我们可以添加以下`plot`语句：
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This gives the following output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '![](Images/4a4614bc-1fd3-49e5-ad08-f9c7ce354585.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/4a4614bc-1fd3-49e5-ad08-f9c7ce354585.png)'
- en: The preceding graph shows the difference between counting six operations or
    seven operations. We can see how the two cases diverge, and this is important
    when we are talking about the specifics of an application. However, what we are
    more interested in here is a way to characterize growth rates. We are not so much
    concerned with the absolute values, but how these values change as we increase
    *n*. In this way, we can see that the two lower curves have similar growth rates
    when compared to the top (*x*²) curve. We say that these two lower curves have
    the same **complexity class**. This is a way to understand and describe different
    runtime behaviors. We will formalize this performance metric in the next section.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了计算六次操作或七次操作的差异。我们可以看到这两种情况的分歧，这在谈论应用程序的具体情况时很重要。然而，我们在这里更感兴趣的是一种表征增长率的方法。我们不太关心绝对值，而是关心这些值随着*n*的增加而如何变化。通过这种方式，我们可以看到两条较低的曲线与顶部（*x*²）曲线相比具有相似的增长率。我们说这两条较低的曲线具有相同的**复杂度类**。这是一种理解和描述不同运行时行为的方法。我们将在下一节中正式化这个性能指标。
- en: Asymptotic analysis
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 渐近分析
- en: Asymptotic analysis of an algorithm refers to the computation of the running
    time of the algorithm. To determine which algorithm is better, given two algorithms,
    a simple approach can be to run both the programs, and the algorithm that takes
    the least time to execute for a given input is better than the other. However,
    it is possible that for a specific input, one algorithm performs better than other,
    whereas for any other input value that the algorithm may perform worse.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的渐近分析是指计算算法的运行时间。要确定哪个算法更好，给定两个算法，一个简单的方法是运行两个程序，对于给定的输入，执行时间最短的算法比另一个更好。然而，可能对于特定的输入，一个算法比另一个更好，而对于算法可能表现更差的任何其他输入值。
- en: 'In asymptotic analysis, we compare two algorithms with respect to input size
    rather than the actual runtime, and we measure how the time taken increases with
    the increase in input size. This is depicted with the following code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在渐近分析中，我们比较两个算法的输入大小而不是实际运行时间，并测量随着输入大小的增加，所需时间的增加情况。这通过以下代码表示：
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Assuming that the size of the array is `n`, and *T(n)* is the total number
    of key operations required to perform a linear search, the key operation in this
    example is the comparison. Let''s consider the linear search as an example to
    understand the worst case, average-case, and best-case complexity:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数组的大小为`n`，*T(n)*是执行线性搜索所需的关键操作总数，这个例子中的关键操作是比较。让我们以线性搜索为例来理解最坏情况、平均情况和最佳情况的复杂性：
- en: '**Worst-case analysis**: We consider the upper-bound running time, that is,
    the maximum time to be taken by the algorithm. In the linear search, the worst
    case happens when the element to be searched is found in the last comparison or
    not found in the list. In this case, there will be a maximum number of comparisons
    and that will be the total number of elements in the array. Therefore, the worst-case
    time complexity is Θ(n).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最坏情况分析**：我们考虑上界运行时间，即算法所需的最长时间。在线性搜索中，最坏情况发生在要搜索的元素在最后一次比较中被找到或者在列表中未找到。在这种情况下，将会有最大数量的比较，即数组中的元素总数。因此，最坏情况的时间复杂度是Θ(n)。'
- en: '**Average-case analysis**: In this analysis, we consider all the possible cases
    where the element can be found in the list, and then, we compute the average running
    time complexity. For example, in the linear search, the number of comparisons
    at all the positions would be *1* if the element to be searched was found at *0th*
    index, and similarly, the number of comparisons would be 2, 3, and so forth, up
    to *n* respectively for elements found at *1, 2, 3, … (n-1)* index positions*.*
    Thus the average time complexity can defined as `average-case complexity= (1+2+3…n)/n
    = n(n+1)/2`.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均情况分析**：在这种分析中，我们考虑元素可能在列表中被找到的所有可能情况，然后计算平均运行时间复杂度。例如，在线性搜索中，如果要搜索的元素在*0*索引处找到，那么所有位置的比较次数将为*1*，类似地，对于在*1,
    2, 3, … (n-1)*索引位置找到的元素，比较次数将分别为2, 3，直到*n*。因此，平均时间复杂度可以定义为`average-case complexity=
    (1+2+3…n)/n = n(n+1)/2`。'
- en: '**Best-case analysis**: Best-case running time complexity is the minimum time
    needed for an algorithm to run; it is the lower-bound running time. In a linear
    search, the best case would be if the element to be searched is found in the first
    comparison. In this example, it is clear that the best-case time complexity is
    not dependent upon how long the list is. So, the best-case time complexity would
    be *Θ(1)*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最佳情况分析**：最佳情况的运行时间复杂度是算法运行所需的最短时间；它是下界运行时间。在线性搜索中，最佳情况是要搜索的元素在第一次比较中被找到。在这个例子中，很明显最佳情况的时间复杂度不取决于列表的长度。因此，最佳情况的时间复杂度将是*Θ(1)*。'
- en: Generally, we use worst-case analysis to analyze an algorithm as it provides
    us with the upper bound on the running time, whereas best-case anaylsis is the
    least important as it provides us with the lower bound—that is, a minimum time
    required for an algorithm. Furthermore, the computation of average-case analysis
    is very difficult.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们使用最坏情况分析来分析算法，因为它为我们提供了运行时间的上界，而最佳情况分析是最不重要的，因为它为我们提供了算法所需的最小时间的下界。此外，计算平均情况分析非常困难。
- en: To calculate each of these, we need to know the upper and lower bounds. We have
    looked at a way to represent an algorithm's runtime using mathematical expressions,
    essentially adding and multiplying operations. To use asymptotic analysis, we
    simply create two expressions, one each for the best and worst cases.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这些情况，我们需要知道上界和下界。我们已经看到了用数学表达式表示算法运行时间的方法，基本上是添加和乘法操作。使用渐近分析，我们只需创建两个表达式，分别用于最佳和最坏情况。
- en: Big O notation
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大O符号
- en: 'The letter O in big *O* notation stands for order, in recognition that rates
    of growth are defined as the order of a function. It measures the worst-case running
    time complexity, that is, the maximum time to be taken by the algorithm. We say
    that one function *T*(*n*) is a big O of another function, *F*(*n*), and we define
    this as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 大O符号中的O代表order，意味着增长率被定义为函数的阶。它衡量最坏情况的运行时间复杂度，即算法所需的最长时间。我们说一个函数*T*(*n*)是另一个函数*F*(*n*)的大O，我们定义如下：
- en: '![](Images/ca98e7da-6adf-45dd-bb3d-2d818e74f5b1.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ca98e7da-6adf-45dd-bb3d-2d818e74f5b1.png)'
- en: 'The function, *g*(*n*), of the input size, *n*, is based on the observation
    that for all sufficiently large values of *n*, *g*(*n*) is bounded above by a
    constant multiple of *f*(*n*). The objective is to find the smallest rate of growth
    that is less than or equal to *f*(*n*). We only care what happens at higher values
    of *n*. The variable *n**0 *represents the threshold below which the rate of growth
    is not important. The function *T(n)* represents the **tight upper bound** F(n).
    In the following plot, we can see that *T*(*n*) = *n*^(*2*) + 500 = *O*(*n*^(*2*)),
    with *C* = 2 and *n*[*0*] being approximately 23:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小*n*的函数*g*(*n*)基于这样的观察：对于所有足够大的*n*值，*g*(*n*)都受到*f*(*n*)的常数倍的上界限制。目标是找到小于或等于*f*(*n*)的增长率最小的增长率。我们只关心在较高的*n*值发生的情况。变量*n**0*表示增长率不重要的阈值以下。函数*T(n)*表示**紧密上界**F(n)。在下图中，我们可以看到*T*(*n*)
    = *n*^(*2*) + 500 = *O*(*n*^(*2*))，其中*C* = 2，*n*[*0*]约为23：
- en: '![](Images/34695609-21eb-4181-ad54-229cd55006af.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/34695609-21eb-4181-ad54-229cd55006af.png)'
- en: You will also see the notation *f*(*n*) = *O*(*g*(*n*)). This describes the
    fact that *O*(*g*(*n*)) is really a set of functions that includes all functions
    with the same or smaller rates of growth than *f*(n). For example, *O*(*n*^(*2*))
    also includes the functions *O(n)*, *O(nlogn)*, and so on. Let's consider another
    example.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会看到符号*f*(*n*) = *O*(*g*(*n*))。这描述了*O*(*g*(*n*))实际上是一个包含所有增长速度与*f*(n)相同或更小的函数的集合。例如，*O*(*n*^(*2*))也包括函数*O(n)*，*O(nlogn)*等。让我们考虑另一个例子。
- en: The big O time complexity for the function `f(x)= 19n log[2]n  +56 ` is *O(nlogn)*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`f(x)= 19n log[2]n  +56 `的大O时间复杂度为*O(nlogn)*。
- en: 'In the following table, we list the most common growth rates in order from
    lowest to highest. We sometimes call these growth rates the **time complexity**
    of a function, or the complexity class of a function:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们按照从低到高的顺序列出了最常见的增长率。我们有时将这些增长率称为函数的**时间复杂度**或函数的复杂度类：
- en: '| **Complexity class** | **Name** | **Example operations** |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **复杂度类** | **名称** | **示例操作** |'
- en: '| *O(1)* | Constant | append, get item, set item. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: 常数 | 常数 | 追加，获取项目，设置项目。
- en: '| *O(logn)* | Logarithmic | Finding an element in a sorted array. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: 对数 | 对数 | 在排序数组中查找元素。
- en: '| *O(n)* | Linear | copy, insert, delete, iteration. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: 线性 | 线性 | 复制，插入，删除，迭代。
- en: '| *nLogn* | Linear-logarithmic | Sort a list, merge-sort. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: 线性对数 | 线性对数 | 对列表进行排序，归并排序。
- en: '| *n*^(*2*) | Quadratic | Find the shortest path between two nodes in a graph.
    Nested loops. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: 二次 | 二次 | 在图中两个节点之间找到最短路径。嵌套循环。
- en: '| *n*^(*3*) | Cubic | Matrix multiplication. |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: 三次 | 三次 | 矩阵乘法。
- en: '| 2^(*n*) | Exponential | **Towers of Hanoi** problem, backtracking. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: 指数 | 指数 | 汉诺塔问题，回溯。
- en: Composing complexity classes
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合复杂度类
- en: Normally, we need to find the total running time of a number of basic operations.
    It turns out that we can combine the complexity classes of simple operations to
    find the complexity class of more complex, combined operations. The goal is to
    analyze the combined statements in a function or method to understand the total
    time complexity of executing several operations. The simplest way to combine two
    complexity classes is to add them. This occurs when we have two sequential operations.
    For example, consider the two operations of inserting an element into a list and
    then sorting that list. We can see that inserting an item occurs in *O(n)* time
    and sorting is in *O(nlogn)* time. We can write the total time complexity as *O(n
    + nlogn)*, that is, we bring the two functions inside the *O(...)*. We are only
    interested in the highest-order term, so this leaves us with just *O(nlogn)*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要找到一系列基本操作的总运行时间。事实证明，我们可以组合简单操作的复杂度类来找到更复杂的组合操作的复杂度类。目标是分析函数或方法中的组合语句，以了解执行多个操作的总时间复杂度。组合两个复杂度类的最简单方法是将它们相加。当我们有两个连续的操作时就会发生这种情况。例如，考虑将元素插入列表然后对该列表进行排序的两个操作。我们可以看到插入项目需要*O(n)*时间，排序需要*O(nlogn)*时间。我们可以将总时间复杂度写为*O(n
    + nlogn)*，也就是说，我们将两个函数放在*O(...)*中。我们只对最高阶项感兴趣，因此这让我们只剩下*O(nlogn)*。
- en: 'If we repeat an operation, for example, in a `while` loop, then we multiply
    the complexity class by the number of times the operation is carried out. If an
    operation with time complexity *O(f(n))* is repeated *O(n)* times then we multiply
    the two complexities:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重复一个操作，例如在`while`循环中，那么我们将复杂度类乘以操作执行的次数。如果一个时间复杂度为*O(f(n))*的操作重复执行*O(n)*次，那么我们将两个复杂度相乘：
- en: '![](Images/4e850cfb-5448-44c5-976f-747b075798b0.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/4e850cfb-5448-44c5-976f-747b075798b0.png)'
- en: 'For example, suppose the function `f(...)` has a time complexity of *O(n²)*
    and it is executed *n *times in a `while` loop, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设函数`f(...)`的时间复杂度为*O(n²)*，并且在`while`循环中执行了*n*次，如下所示：
- en: '[PRE10]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The time complexity of this loop then becomes *O(n²) * O(n) = O(n * n²) = O(n³)*.
    Here we are simply multiplying the time complexity of the operation by the number
    of times this operation executes. The running time of a loop is at most the running
    time of the statements inside the loop multiplied by the number of iterations.
    A single nested loop, that is, one loop nested inside another loop, will run in
    *n*2 time assuming both loops run `n` times, as demonstrated in the following
    example:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个循环的时间复杂度变为*O(n²) * O(n) = O(n * n²) = O(n³)*。在这里，我们只是将操作的时间复杂度乘以这个操作执行的次数。循环的运行时间最多是循环内部语句的运行时间乘以迭代次数。一个单独的嵌套循环，也就是一个循环嵌套在另一个循环中，假设两个循环都运行n次，将在n²时间内运行，就像下面的例子中演示的那样：
- en: '[PRE11]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Each statement is a constant, *c*, executed *nn* times, so we can express the
    running time as the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 每个语句都是一个常数*c*，执行*nn*次，因此我们可以将运行时间表示为以下形式：
- en: '![](Images/252c1e30-173c-4958-923c-369eb1bc0fcb.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/252c1e30-173c-4958-923c-369eb1bc0fcb.png)'
- en: 'For consecutive statements within nested loops, we add the time complexities
    of each statement and multiply by the number of times the statement executed,
    for example:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于嵌套循环中的连续语句，我们将每个语句的时间复杂度相加，然后乘以语句执行的次数，例如：
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This can be written as `c[0] +c[1 ]n + cn^(2 )= O(n²)`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以写成`c[0] +c[1 ]n + cn^(2 )= O(n²)`。
- en: 'We can define (base 2) logarithmic complexity, reducing the size of the problem
    by ½, in constant time. For example, consider the following snippet:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义（以2为底）对数复杂度，将问题的大小减少一半，以常数时间。例如，考虑以下代码片段：
- en: '[PRE13]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Notice that i is doubling on each iteration; if we run this with *n* = 10 we
    see that it prints out four numbers: 2, 4, 8, and 16\. If we double *n* we see
    it prints out five numbers. With each subsequent doubling of *n*, the number of
    iterations is only increased by one. If we assume *k* iterations, we can write
    this as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意i在每次迭代时都会加倍；如果我们以*n*=10运行它，我们会看到它打印出四个数字：2，4，8和16。如果我们将*n*加倍，我们会看到它打印出五个数字。随着*n*的每次加倍，迭代次数只增加了一个。如果我们假设*k*次迭代，我们可以将其写成如下形式：
- en: '![](Images/b31931aa-a8aa-451c-b30c-b8590dfcf074.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/b31931aa-a8aa-451c-b30c-b8590dfcf074.png)'
- en: '![](Images/7ed3d882-ac80-4bc2-93b7-5c00efc6350d.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/7ed3d882-ac80-4bc2-93b7-5c00efc6350d.png)'
- en: '![](Images/8ab8d784-60cc-4838-9816-7f1c9269a84e.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/8ab8d784-60cc-4838-9816-7f1c9269a84e.png)'
- en: From this, we can conclude that the total time = ***O**(log(n))*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由此可得，总时间 = ***O**(log(n))*。
- en: Although big O is the most used notation involved in asymptotic analysis, there
    are two other related notations that should be briefly mentioned. They are Omega
    notation and Theta notation.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大O符号是渐近分析中最常用的符号，但还有两个相关的符号应该简要提到。它们是Omega符号和Theta符号。
- en: Omega notation (Ω)
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Omega符号（Ω）
- en: 'Omega notation describes tight lower bound on algorithms, similar to big O
    notation which describes a tight upper bound. Omega notation computes the best-case
    running time complexity of the algorithm. It provides the highest rate of growth
    *T(n)* which is less than or equal to the given algorithm. It can be computed
    as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Omega符号描述了算法的严格下界，类似于大O符号描述了严格的上界。Omega符号计算算法的最佳运行时间复杂度。它提供了最高的增长率*T(n)*，它小于或等于给定算法。它可以计算如下：
- en: '![](Images/69e1d0c6-1542-4193-be5e-5e43ebcbc465.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/69e1d0c6-1542-4193-be5e-5e43ebcbc465.png)'
- en: Theta notation (ϴ )
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Theta符号（ϴ）
- en: 'It is often the case where both the upper and lower bounds of a given function
    are the same and the purpose of Theta notation is to determine if this is the
    case. The definition is as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，给定函数的上界和下界是相同的，Theta符号的目的是确定是否是这种情况。定义如下：
- en: '![](Images/391dc9e6-d17d-40c8-a353-f80e5b7576e9.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/391dc9e6-d17d-40c8-a353-f80e5b7576e9.png)'
- en: Although Omega and Theta notations are required to completely describe growth
    rates, the most practically useful is big O notation and this is the one you will
    see most often.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Omega和Theta符号需要完全描述增长率，但最实用的是大O符号，这是你经常看到的符号。
- en: Amortized analysis
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摊销分析
- en: Often we are not so interested in the time complexity of individual operations;
    we are more interested in the average running time of sequences of operations.
    This is called amortized analysis. It is different from average-case analysis,
    which we will discuss shortly, in that we make no assumptions regarding the data
    distribution of input values. It does, however, take into account the state change
    of data structures. For example, if a list is sorted, any subsequent find operations
    should be quicker. The amortized analysis considers the state change of data structures
    because it analyzes sequences of operations, rather than simply aggregating single
    operations.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们对单个操作的时间复杂度不太感兴趣；我们更关心操作序列的平均运行时间。这就是摊销分析。它与平均情况分析不同，我们将很快讨论，因为我们对输入值的数据分布没有任何假设。然而，它考虑了数据结构的状态变化。例如，如果列表已排序，则任何后续的查找操作应该更快。摊销分析考虑了数据结构的状态变化，因为它分析操作序列，而不仅仅是聚合单个操作。
- en: Amortized analysis describes an upper bound on the runtime of the algorithm;
    it imposes an additional cost on each operation in the algorithm. The additional
    considered cost of a sequence may be cheaper as compared to the initial expensive
    operation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 摊销分析描述了算法运行时间的上界；它对算法中的每个操作施加了额外的成本。序列的额外考虑成本可能比初始昂贵的操作要便宜。
- en: When we have a small number of expensive operations, such as sorting, and lots
    of cheaper operations such as lookups, standard worst-case analysis can lead to
    overly pessimistic results, since it assumes that each lookup must compare each
    element in the list until a match is found. We should take into account that once
    we sort the list we can make subsequent find operations cheaper.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有少量昂贵的操作，比如排序，和大量更便宜的操作，比如查找时，标准的最坏情况分析可能会导致过于悲观的结果，因为它假设每次查找都必须比较列表中的每个元素直到找到匹配项。我们应该考虑到一旦我们对列表进行排序，我们可以使后续的查找操作变得更便宜。
- en: 'So far in our runtime analysis, we have assumed that the input data was completely
    random and have only looked at the effect the size of the input has on the runtime.
    There are two other common approaches to algorithm analysis; they are:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们的运行时分析中，我们假设输入数据是完全随机的，并且只关注输入大小对运行时间的影响。算法分析还有另外两种常见的方法，它们是：
- en: Average-case analysis
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均情况分析
- en: Benchmarking
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试
- en: Average-case analysis will find the average running time which is based on some
    assumptions regarding the relative frequencies of various input values. Using
    real-world data, or data that replicates the distribution of real-world data,
    is many times on a particular data distribution and the average running time is
    calculated.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 平均情况分析将找到基于对各种输入值的相对频率的一些假设的平均运行时间。使用真实世界的数据，或者复制真实世界数据的分布的数据，往往是基于特定数据分布的，然后计算平均运行时间。
- en: Benchmarking is simply having an agreed set of typical inputs that are used
    to measure performance. Both benchmarking and average-time analysis rely on having
    some domain knowledge. We need to know what the typical or expected datasets are.
    Ultimately, we will try to find ways to improve performance by fine-tuning to
    a very specific application setting.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试就是简单地有一组约定的典型输入，用于衡量性能。基准测试和平均时间分析都依赖于一些领域知识。我们需要知道典型或预期的数据集是什么。最终，我们将尝试通过微调到一个非常特定的应用设置来提高性能。
- en: Let's look at a straightforward way to benchmark an algorithm's runtime performance.
    This can be done by simply timing how long the algorithm takes to complete given
    various input sizes. As we mentioned earlier, this way of measuring runtime performance
    is dependent on the hardware that it is run on. Obviously, faster processors will
    give better results, however, the relative growth rates as we increase the input
    size will retain characteristics of the algorithm itself rather than the hardware
    it is run on. The absolute time values will differ between hardware (and software)
    platforms; however, their relative growth will still be bound by the time complexity
    of the algorithm.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一种简单的方法来衡量算法的运行时间性能。这可以通过简单地计时算法完成给定各种输入大小所需的时间来完成。正如我们之前提到的，这种衡量运行时间性能的方式取决于它运行的硬件。显然，更快的处理器会给出更好的结果，然而，随着输入大小的增加，它们的相对增长率将保留算法本身的特征，而不是运行在硬件上。绝对时间值将在硬件（和软件）平台之间有所不同；然而，它们的相对增长仍将受到算法的时间复杂度的限制。
- en: 'Let''s take a simple example of a nested loop. It should be fairly obvious
    that the time complexity of this algorithm is *O(n²)* since for each *n* iterations
    in the outer loop there are also *n* iterations in the interloop. For example,
    our simple nested for loop consists of a simple statement executed on the inner
    loop:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个嵌套循环的简单例子来说明。很明显，这个算法的时间复杂度是*O(n²)*，因为在外部循环的每个*n*次迭代中，内部循环也有*n*次迭代。例如，我们简单的嵌套for循环包含在内部循环中执行的一个简单语句：
- en: '[PRE14]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The following code is a simple test function that runs the `nest` function with
    increasing values of `n`. With each iteration, we calculate the time this function
    takes to complete using the `timeit.timeit` function. The `timeit` function, in
    this example, takes three arguments, a string representation of the function to
    be timed, a `setup` function that imports the `nest` function, and an `int` parameter
    that indicates the number of times to execute the main statement.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是一个简单的测试函数，它使用不断增加的`n`值运行`nest`函数。在每次迭代中，我们使用`timeit.timeit`函数计算这个函数完成所需的时间。`timeit`函数在这个例子中接受三个参数，一个表示要计时的函数的字符串表示，一个导入`nest`函数的`setup`函数，以及一个`int`参数，表示执行主语句的次数。
- en: 'Since we are interested in the time the `nest` function takes to complete relative
    to the input size, `n`, it is sufficient, for our purposes, to call the `nest`
    function once on each iteration. The following function returns a list of the
    calculated runtimes for each value of `n`:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对`nest`函数完成所需的时间与输入大小`n`感兴趣，对于我们的目的来说，每次迭代调用`nest`函数一次就足够了。以下函数返回每个`n`值的计算运行时间的列表：
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the following code, we run the `test2` function and graph the results, together
    with the appropriately scaled `n²` function, for comparison, represented by the
    dashed line:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们运行`test2`函数并绘制结果，以及适当缩放的`n²`函数进行比较，用虚线表示：
- en: '[PRE16]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This gives the following results:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下结果：
- en: '![](Images/0f25b101-61b9-454e-9adb-9f8774b28063.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/0f25b101-61b9-454e-9adb-9f8774b28063.png)'
- en: As we can see, this gives us pretty much what we expect. It should be remembered
    that this represents both the performance of the algorithm itself as well as the
    behavior of underlying software and hardware platforms, as indicated by both the
    variability in the measured runtime and the relative magnitude of the runtime.
    Obviously, a faster processor will result in faster runtimes, and also performance
    will be affected by other running processes, memory constraints, clock speed,
    and so on.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，这基本上符合我们的预期。应该记住，这既代表了算法本身的性能，也代表了底层软件和硬件平台的行为，正如测量运行时间的变化和运行时间的相对大小所指示的那样。显然，更快的处理器会导致更快的运行时间，而且性能也会受到其他运行进程、内存限制、时钟速度等的影响。
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have looked at a general overview of algorithm design. Importantly,
    we studied a platform-independent way to measure an algorithm's performance. We
    looked at some different approaches to algorithmic problems. We looked at a way
    to recursively multiply large numbers and also a recursive approach for merge
    sort. We learned how to use backtracking for exhaustive search and generating
    strings. We also introduced the idea of benchmarking and a simple platform-dependent
    way to measure runtime.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经对算法设计进行了一般性概述。重要的是，我们研究了一种独立于平台的算法性能衡量方法。我们研究了一些不同的算法问题解决方法。我们研究了一种递归相乘大数的方法，也研究了归并排序的递归方法。我们学习了如何使用回溯进行穷举搜索和生成字符串。我们还介绍了基准测试的概念以及一种简单的依赖于平台的衡量运行时间的方法。
- en: In the following chapters, we will revisit many of these ideas with reference
    to specific data structures. In the next chapter, we will discuss linked lists
    and other pointer structures.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将参考特定的数据结构重新讨论这些想法。在下一章中，我们将讨论链表和其他指针结构。
