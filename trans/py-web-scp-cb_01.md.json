["```py\n~ $ pip install virtualenv\nCollecting virtualenv\n Using cached virtualenv-15.1.0-py2.py3-none-any.whl\nInstalling collected packages: virtualenv\nSuccessfully installed virtualenv-15.1.0\n```", "```py\n~ $ pip list\nalabaster (0.7.9)\namqp (1.4.9)\nanaconda-client (1.6.0)\nanaconda-navigator (1.5.3)\nanaconda-project (0.4.1)\naniso8601 (1.3.0)\n```", "```py\n~ $ mkdir pywscb\n~ $ cd pywscb\n```", "```py\npywscb $ virtualenv env\nUsing base prefix '/Users/michaelheydt/anaconda'\nNew python executable in /Users/michaelheydt/pywscb/env/bin/python\ncopying /Users/michaelheydt/anaconda/bin/python => /Users/michaelheydt/pywscb/env/bin/python\ncopying /Users/michaelheydt/anaconda/bin/../lib/libpython3.6m.dylib => /Users/michaelheydt/pywscb/env/lib/libpython3.6m.dylib\nInstalling setuptools, pip, wheel...done.\n```", "```py\npywscb $ ls -la env\ntotal 8\ndrwxr-xr-x 6  michaelheydt staff 204 Jan 18 15:38 .\ndrwxr-xr-x 3  michaelheydt staff 102 Jan 18 15:35 ..\ndrwxr-xr-x 16 michaelheydt staff 544 Jan 18 15:38 bin\ndrwxr-xr-x 3  michaelheydt staff 102 Jan 18 15:35 include\ndrwxr-xr-x 4  michaelheydt staff 136 Jan 18 15:38 lib\n-rw-r--r-- 1  michaelheydt staff 60 Jan 18 15:38  pip-selfcheck.json\n```", "```py\npywscb $ source env/bin/activate\n(env) pywscb $\n```", "```py\n(env) pywscb $ which python\n/Users/michaelheydt/pywscb/env/bin/python\n```", "```py\n(env) pywscb $ git clone https://github.com/PacktBooks/PythonWebScrapingCookbook.git\n Cloning into 'PythonWebScrapingCookbook'...\n remote: Counting objects: 420, done.\n remote: Compressing objects: 100% (316/316), done.\n remote: Total 420 (delta 164), reused 344 (delta 88), pack-reused 0\n Receiving objects: 100% (420/420), 1.15 MiB | 250.00 KiB/s, done.\n Resolving deltas: 100% (164/164), done.\n Checking connectivity... done.\n```", "```py\n(env) pywscb $ ls -l\n total 0\n drwxr-xr-x 9 michaelheydt staff 306 Jan 18 16:21 PythonWebScrapingCookbook\n drwxr-xr-x 6 michaelheydt staff 204 Jan 18 15:38 env\n```", "```py\n(env) PythonWebScrapingCookbook $ ls -l\n total 0\n drwxr-xr-x 15 michaelheydt staff 510 Jan 18 16:21 py\n drwxr-xr-x 14 michaelheydt staff 476 Jan 18 16:21 www\n```", "```py\n(env) py $ ls -l\n total 0\n drwxr-xr-x 9  michaelheydt staff 306 Jan 18 16:21 01\n drwxr-xr-x 25 michaelheydt staff 850 Jan 18 16:21 03\n drwxr-xr-x 21 michaelheydt staff 714 Jan 18 16:21 04\n drwxr-xr-x 10 michaelheydt staff 340 Jan 18 16:21 05\n drwxr-xr-x 14 michaelheydt staff 476 Jan 18 16:21 06\n drwxr-xr-x 25 michaelheydt staff 850 Jan 18 16:21 07\n drwxr-xr-x 14 michaelheydt staff 476 Jan 18 16:21 08\n drwxr-xr-x 7  michaelheydt staff 238 Jan 18 16:21 09\n drwxr-xr-x 7  michaelheydt staff 238 Jan 18 16:21 10\n drwxr-xr-x 9  michaelheydt staff 306 Jan 18 16:21 11\n drwxr-xr-x 8  michaelheydt staff 272 Jan 18 16:21 modules\n```", "```py\nexport PYTHONPATH=\"/users/michaelheydt/dropbox/packt/books/pywebscrcookbook/code/py/modules\"\nexport PYTHONPATH\n```", "```py\n(env) py $ ls -la 06\n total 96\n drwxr-xr-x 14 michaelheydt staff 476 Jan 18 16:21 .\n drwxr-xr-x 14 michaelheydt staff 476 Jan 18 16:26 ..\n -rw-r--r-- 1  michaelheydt staff 902 Jan 18 16:21  01_scrapy_retry.py\n -rw-r--r-- 1  michaelheydt staff 656 Jan 18 16:21  02_scrapy_redirects.py\n -rw-r--r-- 1  michaelheydt staff 1129 Jan 18 16:21 03_scrapy_pagination.py\n -rw-r--r-- 1  michaelheydt staff 488 Jan 18 16:21  04_press_and_wait.py\n -rw-r--r-- 1  michaelheydt staff 580 Jan 18 16:21  05_allowed_domains.py\n -rw-r--r-- 1  michaelheydt staff 826 Jan 18 16:21  06_scrapy_continuous.py\n -rw-r--r-- 1  michaelheydt staff 704 Jan 18 16:21  07_scrape_continuous_twitter.py\n -rw-r--r-- 1  michaelheydt staff 1409 Jan 18 16:21 08_limit_depth.py\n -rw-r--r-- 1  michaelheydt staff 526 Jan 18 16:21  09_limit_length.py\n -rw-r--r-- 1  michaelheydt staff 1537 Jan 18 16:21 10_forms_auth.py\n -rw-r--r-- 1  michaelheydt staff 597 Jan 18 16:21  11_file_cache.py\n -rw-r--r-- 1  michaelheydt staff 1279 Jan 18 16:21 12_parse_differently_based_on_rules.py\n```", "```py\n(env) py $ deactivate\n py $\n```", "```py\npy $ which python\n /Users/michaelheydt/anaconda/bin/python\n```", "```py\npywscb $ pip install requests\nDownloading/unpacking requests\n Downloading requests-2.18.4-py2.py3-none-any.whl (88kB): 88kB downloaded\nDownloading/unpacking certifi>=2017.4.17 (from requests)\n Downloading certifi-2018.1.18-py2.py3-none-any.whl (151kB): 151kB downloaded\nDownloading/unpacking idna>=2.5,<2.7 (from requests)\n Downloading idna-2.6-py2.py3-none-any.whl (56kB): 56kB downloaded\nDownloading/unpacking chardet>=3.0.2,<3.1.0 (from requests)\n Downloading chardet-3.0.4-py2.py3-none-any.whl (133kB): 133kB downloaded\nDownloading/unpacking urllib3>=1.21.1,<1.23 (from requests)\n Downloading urllib3-1.22-py2.py3-none-any.whl (132kB): 132kB downloaded\nInstalling collected packages: requests, certifi, idna, chardet, urllib3\nSuccessfully installed requests certifi idna chardet urllib3\nCleaning up...\npywscb $ pip install bs4\nDownloading/unpacking bs4\n Downloading bs4-0.0.1.tar.gz\n Running setup.py (path:/Users/michaelheydt/pywscb/env/build/bs4/setup.py) egg_info for package bs4\n```", "```py\n$ ipython\nPython 3.6.1 |Anaconda custom (x86_64)| (default, Mar 22 2017, 19:25:17)\nType \"copyright\", \"credits\" or \"license\" for more information.\nIPython 5.1.0 -- An enhanced Interactive Python.\n? -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp -> Python's own help system.\nobject? -> Details about 'object', use 'object??' for extra details.\nIn [1]:\n```", "```py\nIn [1]: import requests\n```", "```py\nIn [2]: url = 'https://www.python.org/events/python-events/'\nIn [3]: req = requests.get(url)\n```", "```py\nreq.text[:200]\nOut[4]: '<!doctype html>\\n<!--[if lt IE 7]> <html class=\"no-js ie6 lt-ie7 lt-ie8 lt-ie9\"> <![endif]-->\\n<!--[if IE 7]> <html class=\"no-js ie7 lt-ie8 lt-ie9\"> <![endif]-->\\n<!--[if IE 8]> <h'\n```", "```py\nIn [5]: from bs4 import BeautifulSoup\n```", "```py\nIn [6]: soup = BeautifulSoup(req.text, 'lxml')\n```", "```py\nIn [7]: events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n```", "```py\nIn [13]: for event in events:\n ...: event_details = dict()\n ...: event_details['name'] = event_details['name'] = event.find('h3').find(\"a\").text\n ...: event_details['location'] = event.find('span', {'class', 'event-location'}).text\n ...: event_details['time'] = event.find('time').text\n ...: print(event_details)\n ...:\n{'name': 'PyCascades 2018', 'location': 'Granville Island Stage, 1585 Johnston St, Vancouver, BC V6H 3R9, Canada', 'time': '22 Jan. \u2013 24 Jan. 2018'}\n{'name': 'PyCon Cameroon 2018', 'location': 'Limbe, Cameroon', 'time': '24 Jan. \u2013 29 Jan. 2018'}\n{'name': 'FOSDEM 2018', 'location': 'ULB Campus du Solbosch, Av. F. D. Roosevelt 50, 1050 Bruxelles, Belgium', 'time': '03 Feb. \u2013 05 Feb. 2018'}\n{'name': 'PyCon Pune 2018', 'location': 'Pune, India', 'time': '08 Feb. \u2013 12 Feb. 2018'}\n{'name': 'PyCon Colombia 2018', 'location': 'Medellin, Colombia', 'time': '09 Feb. \u2013 12 Feb. 2018'}\n{'name': 'PyTennessee 2018', 'location': 'Nashville, TN, USA', 'time': '10 Feb. \u2013 12 Feb. 2018'}\n```", "```py\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_upcoming_events(url):\n    req = requests.get(url)\n\n    soup = BeautifulSoup(req.text, 'lxml')\n\n    events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n\n    for event in events:\n        event_details = dict()\n        event_details['name'] = event.find('h3').find(\"a\").text\n        event_details['location'] = event.find('span', {'class', 'event-location'}).text\n        event_details['time'] = event.find('time').text\n        print(event_details)\n\nget_upcoming_events('https://www.python.org/events/python-events/')\n```", "```py\n$ python 01_events_with_requests.py\n{'name': 'PyCascades 2018', 'location': 'Granville Island Stage, 1585 Johnston St, Vancouver, BC V6H 3R9, Canada', 'time': '22 Jan. \u2013 24 Jan. 2018'}\n{'name': 'PyCon Cameroon 2018', 'location': 'Limbe, Cameroon', 'time': '24 Jan. \u2013 29 Jan. 2018'}\n{'name': 'FOSDEM 2018', 'location': 'ULB Campus du Solbosch, Av. F. D. Roosevelt 50, 1050 Bruxelles, Belgium', 'time': '03 Feb. \u2013 05 Feb. 2018'}\n{'name': 'PyCon Pune 2018', 'location': 'Pune, India', 'time': '08 Feb. \u2013 12 Feb. 2018'}\n{'name': 'PyCon Colombia 2018', 'location': 'Medellin, Colombia', 'time': '09 Feb. \u2013 12 Feb. 2018'}\n{'name': 'PyTennessee 2018', 'location': 'Nashville, TN, USA', 'time': '10 Feb. \u2013 12 Feb. 2018'}\n```", "```py\n$ pip install urllib3\nCollecting urllib3\n Using cached urllib3-1.22-py2.py3-none-any.whl\nInstalling collected packages: urllib3\nSuccessfully installed urllib3-1.22\n```", "```py\nimport urllib3\nfrom bs4 import BeautifulSoup\n\ndef get_upcoming_events(url):\n    req = urllib3.PoolManager()\n    res = req.request('GET', url)\n\n    soup = BeautifulSoup(res.data, 'html.parser')\n\n    events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n\n    for event in events:\n        event_details = dict()\n        event_details['name'] = event.find('h3').find(\"a\").text\n        event_details['location'] = event.find('span', {'class', 'event-location'}).text\n        event_details['time'] = event.find('time').text\n        print(event_details)\n\nget_upcoming_events('https://www.python.org/events/python-events/')\n```", "```py\nreq = urllib3.PoolManager()\nres = req.request('GET', url)\n```", "```py\nimport requests\n\n# builds on top of urllib3's connection pooling\n# session reuses the same TCP connection if \n# requests are made to the same host\n# see https://en.wikipedia.org/wiki/HTTP_persistent_connection for details\nsession = requests.Session()\n\n# You may pass in custom cookie\nr = session.get('http://httpbin.org/get', cookies={'my-cookie': 'browser'})\nprint(r.text)\n# '{\"cookies\": {\"my-cookie\": \"test cookie\"}}'\n\n# Streaming is another nifty feature\n# From http://docs.python-requests.org/en/master/user/advanced/#streaming-requests\n# copyright belongs to reques.org\nr = requests.get('http://httpbin.org/stream/20', stream=True) \n```", "```py\nfor line in r.iter_lines():\n  # filter out keep-alive new lines\n  if line:\n     decoded_line = line.decode('utf-8')\n     print(json.loads(decoded_line))\n```", "```py\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass PythonEventsSpider(scrapy.Spider):\n    name = 'pythoneventsspider'    start_urls = ['https://www.python.org/events/python-events/',]\n    found_events = []\n\n    def parse(self, response):\n        for event in response.xpath('//ul[contains(@class, \"list-recent-events\")]/li'):\n            event_details = dict()\n            event_details['name'] = event.xpath('h3[@class=\"event-title\"]/a/text()').extract_first()\n            event_details['location'] = event.xpath('p/span[@class=\"event-location\"]/text()').extract_first()\n            event_details['time'] = event.xpath('p/time/text()').extract_first()\n            self.found_events.append(event_details)\n\nif __name__ == \"__main__\":\n    process = CrawlerProcess({ 'LOG_LEVEL': 'ERROR'})\n    process.crawl(PythonEventsSpider)\n    spider = next(iter(process.crawlers)).spider\n    process.start()\n\n    for event in spider.found_events: print(event)\n```", "```py\n~ $ python 03_events_with_scrapy.py\n{'name': 'PyCascades 2018', 'location': 'Granville Island Stage, 1585 Johnston St, Vancouver, BC V6H 3R9, Canada', 'time': '22 Jan. \u2013 24 Jan. '}\n{'name': 'PyCon Cameroon 2018', 'location': 'Limbe, Cameroon', 'time': '24 Jan. \u2013 29 Jan. '}\n{'name': 'FOSDEM 2018', 'location': 'ULB Campus du Solbosch, Av. F. D. Roosevelt 50, 1050 Bruxelles, Belgium', 'time': '03 Feb. \u2013 05 Feb. '}\n{'name': 'PyCon Pune 2018', 'location': 'Pune, India', 'time': '08 Feb. \u2013 12 Feb. '}\n{'name': 'PyCon Colombia 2018', 'location': 'Medellin, Colombia', 'time': '09 Feb. \u2013 12 Feb. '}\n{'name': 'PyTennessee 2018', 'location': 'Nashville, TN, USA', 'time': '10 Feb. \u2013 12 Feb. '}\n{'name': 'PyCon Pakistan', 'location': 'Lahore, Pakistan', 'time': '16 Dec. \u2013 17 Dec. '}\n{'name': 'PyCon Indonesia 2017', 'location': 'Surabaya, Indonesia', 'time': '09 Dec. \u2013 10 Dec. '}\n```", "```py\nclass PythonEventsSpider(scrapy.Spider):\n    name = 'pythoneventsspider'    start_urls = ['https://www.python.org/events/python-events/',]\n```", "```py\n    found_events = []\n```", "```py\ndef parse(self, response):\n        for event in response.xpath('//ul[contains(@class, \"list-recent-events\")]/li'):\n            event_details = dict()\n            event_details['name'] = event.xpath('h3[@class=\"event-title\"]/a/text()').extract_first()\n            event_details['location'] = event.xpath('p/span[@class=\"event-location\"]/text()').extract_first()\n            event_details['time'] = event.xpath('p/time/text()').extract_first()\n            self.found_events.append(event_details)\n```", "```py\n    process = CrawlerProcess({ 'LOG_LEVEL': 'ERROR'})\n    process.crawl(PythonEventsSpider)\n    spider = next(iter(process.crawlers)).spider\n    process.start()\n```", "```py\n    for event in spider.found_events: print(event)\n```", "```py\n~ $ pip install selenium\nCollecting selenium\n Downloading selenium-3.8.1-py2.py3-none-any.whl (942kB)\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 952kB 236kB/s\nInstalling collected packages: selenium\nSuccessfully installed selenium-3.8.1\n```", "```py\nFileNotFoundError: [Errno 2] No such file or directory: 'geckodriver'\n```", "```py\nfrom selenium import webdriver\n\ndef get_upcoming_events(url):\n    driver = webdriver.Firefox()\n    driver.get(url)\n\n    events = driver.find_elements_by_xpath('//ul[contains(@class, \"list-recent-events\")]/li')\n\n    for event in events:\n        event_details = dict()\n        event_details['name'] = event.find_element_by_xpath('h3[@class=\"event-title\"]/a').text\n        event_details['location'] = event.find_element_by_xpath('p/span[@class=\"event-location\"]').text\n        event_details['time'] = event.find_element_by_xpath('p/time').text\n        print(event_details)\n\n    driver.close()\n\nget_upcoming_events('https://www.python.org/events/python-events/')\n```", "```py\n~ $ python 04_events_with_selenium.py\n{'name': 'PyCascades 2018', 'location': 'Granville Island Stage, 1585 Johnston St, Vancouver, BC V6H 3R9, Canada', 'time': '22 Jan. \u2013 24 Jan.'}\n{'name': 'PyCon Cameroon 2018', 'location': 'Limbe, Cameroon', 'time': '24 Jan. \u2013 29 Jan.'}\n{'name': 'FOSDEM 2018', 'location': 'ULB Campus du Solbosch, Av. F. D. Roosevelt 50, 1050 Bruxelles, Belgium', 'time': '03 Feb. \u2013 05 Feb.'}\n{'name': 'PyCon Pune 2018', 'location': 'Pune, India', 'time': '08 Feb. \u2013 12 Feb.'}\n{'name': 'PyCon Colombia 2018', 'location': 'Medellin, Colombia', 'time': '09 Feb. \u2013 12 Feb.'}\n{'name': 'PyTennessee 2018', 'location': 'Nashville, TN, USA', 'time': '10 Feb. \u2013 12 Feb.'}\n```", "```py\ndriver = webdriver.Firefox()\ndriver.get(url)\n```", "```py\ndriver = webdriver.PhantomJS('phantomjs')\n```"]