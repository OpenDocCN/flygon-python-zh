- en: Log-Based Artifact Recipes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于日志的工件配方
- en: 'The following recipes are covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下配方：
- en: About time
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于时间
- en: Parsing IIS weblogs with RegEx
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RegEx解析IIS weblogs
- en: Going spelunking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去探险
- en: Interpreting the daily out log
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释每日日志
- en: Adding `daily.out` parsing to Axiom
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`daily.out`解析添加到Axiom
- en: Scanning for indicators with YARA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用YARA扫描指标
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'These days it is not uncommon to encounter modern systems equipped with some
    form of event or activity monitoring software. This software may be implemented
    to assist with security, debugging, or compliance requirements. Whatever the situation,
    this veritable treasure trove of information can be, and commonly is, leveraged
    in all types of cyber investigations. A common issue with log analysis can be
    the huge amount of data one is required to sift through for the subset of interest.
    Through the recipes in this chapter, we will explore various logs with great evidentiary
    value and demonstrate ways to quickly process and review them. Specifically, we
    will cover:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天，遇到配备某种形式的事件或活动监控软件的现代系统并不罕见。这种软件可能被实施以协助安全、调试或合规要求。无论情况如何，这些宝贵的信息宝库通常被广泛利用于各种类型的网络调查。日志分析的一个常见问题是需要筛选出感兴趣的子集所需的大量数据。通过本章的配方，我们将探索具有很大证据价值的各种日志，并演示快速处理和审查它们的方法。具体来说，我们将涵盖：
- en: Converting different timestamp formats (UNIX, FILETIME, and so on) to human-readable
    formats
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不同的时间戳格式（UNIX、FILETIME等）转换为人类可读的格式
- en: Parsing web server access logs from an IIS platform
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析来自IIS平台的Web服务器访问日志
- en: Ingesting, querying, and exporting logs with Splunk's Python API
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Splunk的Python API摄取、查询和导出日志
- en: Extracting drive usage information from macOS `daily.out` logs
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从macOS的`daily.out`日志中提取驱动器使用信息
- en: Executing our `daily.out` log parser from Axiom
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Axiom执行我们的`daily.out`日志解析器
- en: A bonus recipe for identifying files of interest with YARA rules
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用YARA规则识别感兴趣的文件的奖励配方
- en: Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)下载本章的代码包。
- en: About time
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于时间
- en: 'Recipe Difficulty: Easy'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 配方难度：简单
- en: 'Python Version: 2.7 or 3.5'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7或3.5
- en: 'Operating System: Any'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任何
- en: One important element of any good log file is the timestamp. This value conveys
    the date and time of the activity or event noted in the log. These date values
    can come in many formats and may be represented as numbers or hexadecimal values.
    Outside of logs, different files and artifacts store dates in different manners,
    even if the data type remains the same. A common differentiating factor is the
    epoch value, which is the date that the format counts time from. A common epoch
    is January 1, 1970, though other formats count from January 1, 1601\. Another
    factor that differs between formats is the interval used for counting. While it
    is common to see formats that count seconds or milliseconds, some formats count
    blocks of time, such as the number of 100-nanoseconds since the epoch. Because
    of this, the recipe developed here can take the raw datetime input and provide
    a formatted timestamp as its output.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 任何良好日志文件的一个重要元素是时间戳。这个值传达了日志中记录的活动或事件的日期和时间。这些日期值可以以许多格式出现，并且可以表示为数字或十六进制值。除了日志之外，不同的文件和工件以不同的方式存储日期，即使数据类型保持不变。一个常见的区分因素是纪元值，即格式从中计算时间的日期。一个常见的纪元是1970年1月1日，尽管其他格式从1601年1月1日开始计算。在不同格式之间不同的因素是用于计数的间隔。虽然常见的是看到以秒或毫秒计数的格式，但有些格式计算时间块，比如自纪元以来的100纳秒数。因此，这里开发的配方可以接受原始日期时间输入，并将格式化的时间戳作为其输出。
- en: Getting started
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: All libraries used in this script are present in Python's standard library.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本中使用的所有库都包含在Python的标准库中。
- en: How to do it...
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To interpret common date formats in Python, we perform the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Python中解释常见的日期格式，我们执行以下操作：
- en: Set up arguments to take the raw date value, the source of the date, and the
    data type.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置参数以获取原始日期值、日期来源和数据类型。
- en: Develop a class that provides a common interface for data across different date
    formats.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一个为不同日期格式提供通用接口的类。
- en: Support processing of Unix epoch values and Microsoft `FILETIME` dates.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 支持处理Unix纪元值和Microsoft的`FILETIME`日期。
- en: How it works...
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We begin by importing libraries for argument handling and parsing dates. Specifically,
    we need the `datetime` class from the `datetime` library to read the raw date
    values and the `timedelta` class to specify timestamp offsets.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入用于处理参数和解析日期的库。具体来说，我们需要从`datetime`库中导入`datetime`类来读取原始日期值，以及`timedelta`类来指定时间戳偏移量。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This recipe's command-line handler takes three positional arguments, `date_value`,
    `source`, and `type`, which represent the date value to process, the source of
    date value (UNIX, FILETIME, and so on), and the type (integer or hexadecimal value),
    respectively. We use the choices keyword for the source and type arguments to
    limit the options the user can supply. Notice, that the source argument uses a
    custom `get_supported_formats()` function rather than a predefined list of supported
    date formats. We then take these arguments and initiate an instance of the `ParseDate`
    class and call the `run()` method to handle the conversion process before printing
    its `timestamp` attribute to the console.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序接受三个位置参数，`date_value`、`source`和`type`，分别代表要处理的日期值、日期值的来源（UNIX、FILETIME等）和类型（整数或十六进制值）。我们使用`choices`关键字来限制用户可以提供的选项。请注意，源参数使用自定义的`get_supported_formats()`函数，而不是预定义的受支持日期格式列表。然后，我们获取这些参数并初始化`ParseDate`类的一个实例，并调用`run()`方法来处理转换过程，然后将其`timestamp`属性打印到控制台。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s look at how the `ParseDate` class works. By using a class, we can easily
    extend and implement this code in other scripts. From the command-line arguments,
    we accept arguments for the date value, date source, and the value type. These
    values and the output variable, `timestamp`, are defined in the `__init__` method:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`ParseDate`类是如何工作的。通过使用一个类，我们可以轻松地扩展和在其他脚本中实现这段代码。从命令行参数中，我们接受日期值、日期源和值类型的参数。这些值和输出变量`timestamp`在`__init__`方法中被定义：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `run()` method is the controller, much like the `main()` function of many
    of our recipes, and selects the correct method to call based on the date source.
    This allows us to easily extend the class and add new support with ease. In this
    version, we only support three date types: Unix epoch second, Unix epoch millisecond,
    and Microsoft''s FILETIME. To reduce the number of methods we would need to write,
    we will design the Unix epoch method to handle both second - and millisecond -
    formatted timestamps.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`run（）`方法是控制器，很像我们许多食谱中的`main（）`函数，并根据日期源选择要调用的正确方法。这使我们能够轻松扩展类并轻松添加新的支持。在这个版本中，我们只支持三种日期类型：Unix纪元秒，Unix纪元毫秒和Microsoft的FILETIME。为了减少我们需要编写的方法数量，我们将设计Unix纪元方法来处理秒和毫秒格式的时间戳。'
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To help those wanting to use this library in the future, we add a method for
    viewing what formats are supported. By using the `@classmethod` decorator, we
    expose this function without needing to initialize the class first. This is the
    reason we can use the `get_supported_formats()` method in the command-line handler.
    Just remember to update this as new features are added!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助未来想要使用这个库的人，我们添加了一个查看支持的格式的方法。通过使用`@classmethod`装饰器，我们可以在不需要先初始化类的情况下公开这个函数。这就是我们可以在命令行处理程序中使用`get_supported_formats（）`方法的原因。只需记住在添加新功能时更新它！
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `parse_unix_epoch()` method handles processing Unix epoch time. We specify
    an optional argument, `milliseconds`, to switch this method between processing
    second and millisecond values. First we must determine if the data type is `"hex"`
    or `"number"`. If it is `"hex"`, we convert it to an integer and if it is a `"number"`
    we convert it to a float. If we do not recognize or support the data type for
    this method, such as a `string`, we throw an error to the user and exit the script.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse_unix_epoch（）`方法处理处理Unix纪元时间。我们指定一个可选参数`milliseconds`，以在处理秒和毫秒值之间切换此方法。首先，我们必须确定数据类型是`“hex”`还是`“number”`。如果是`“hex”`，我们将其转换为整数，如果是`“number”`，我们将其转换为浮点数。如果我们不认识或不支持此方法的数据类型，比如`string`，我们向用户抛出错误并退出脚本。'
- en: After converting the value, we evaluate if this should be treated as a millisecond
    value and, if so, divide it by `1,000` before handling it further. Following this,
    we use the `fromtimestamp()` method of the `datetime` class to convert the number
    to a `datetime` object. Lastly, we format this date to a human-readable format
    and store this string in the `timestamp` property.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换值后，我们评估是否应将其视为毫秒值，如果是，则在进一步处理之前将其除以`1,000`。随后，我们使用`datetime`类的`fromtimestamp（）`方法将数字转换为`datetime`对象。最后，我们将这个日期格式化为人类可读的格式，并将这个字符串存储在`timestamp`属性中。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `parse_windows_filetime()` class method handles the `FILETIME` format, commonly
    stored as a hex value. Using a similar block of code as before, we convert the
    `"hex"` or `"number"` value into a Python object and raise an error for any other
    provided formats. The one difference is that we divide the date value by `10`
    rather than `1,000` before processing them further.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse_windows_filetime（）`类方法处理`FILETIME`格式，通常存储为十六进制值。使用与之前相似的代码块，我们将`“hex”`或`“number”`值转换为Python对象，并对任何其他提供的格式引发错误。唯一的区别是在进一步处理之前，我们将日期值除以`10`而不是`1,000`。'
- en: 'While in the previous method the `datetime` library handled the epoch offset,
    we need to handle this offset separately this time. Using the `timedelta` class,
    we specify the millisecond value and add that to a `datetime` object representing
    the FILETIME format''s epoch. The resulting `datetime` object is now ready for
    us to format and output for the user:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的方法中，`datetime`库处理了纪元偏移，这次我们需要单独处理这个偏移。使用`timedelta`类，我们指定毫秒值，并将其添加到代表FILETIME格式纪元的`datetime`对象中。现在得到的`datetime`对象已经准备好供我们格式化和输出给用户了：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When we run this script, we can provide a timestamp and see the converted value
    in an easy-to-read format, as shown here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个脚本时，我们可以提供一个时间戳，并以易于阅读的格式查看转换后的值，如下所示：
- en: '![](../images/00076.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00076.jpeg)'
- en: There's more...
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们提供了一个或多个建议如下：
- en: Add support for other types of timestamps (OLE, WebKit, and so on)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为其他类型的时间戳（OLE，WebKit等）添加支持
- en: Add time zone support through `pytz`
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`pytz`添加时区支持
- en: Handle the formatting of hard-to-read dates with `dateutil`
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`dateutil`处理难以阅读的日期格式
- en: Parsing IIS web logs with RegEx
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RegEx解析IIS Web日志
- en: 'Recipe Difficulty: Medium'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：中等
- en: 'Python Version: 3.5'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：3.5
- en: 'Operating System: Any'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任何
- en: Logs from web servers are very useful for generating user statistics, providing
    us with insightful information about the devices used and the geographical locations
    of the visitors. They also provide clarification to examiners looking for users
    attempting to exploit the web server or otherwise unauthorized use. While these
    logs store important details, they do so in a manner inconvenient to analyze efficiently.
    If you were to attempt to do so manually, the field names are specified at the
    top of the file and would require you to remember the order of the fields as you
    read through the text file. Fortunately, there is a better way. Using the following
    script, we show how to iterate through each line, map the values to the fields,
    and create a spreadsheet of properly displayed results - making it much easier
    to quickly analyze the dataset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Web服务器的日志对于生成用户统计信息非常有用，为我们提供了有关使用的设备和访问者的地理位置的深刻信息。它们还为寻找试图利用Web服务器或未经授权使用的用户的审查人员提供了澄清。虽然这些日志存储了重要的细节，但以一种不便于高效分析的方式进行。如果您尝试手动分析，字段名称被指定在文件顶部，并且在阅读文本文件时需要记住字段的顺序。幸运的是，有更好的方法。使用以下脚本，我们展示了如何遍历每一行，将值映射到字段，并创建一个正确显示结果的电子表格
    - 使得快速分析数据集变得更加容易。
- en: Getting started
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: All libraries used in this script are present in Python's standard library.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本中使用的所有库都包含在Python的标准库中。
- en: How to do it...
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To properly form this recipe, we need to take the following steps:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确制作这个配方，我们需要采取以下步骤：
- en: Accept arguments for an input log file and output CSV file.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受输入日志文件和输出CSV文件的参数。
- en: Define regular expression patterns for each of the log's columns.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为日志的每一列定义正则表达式模式。
- en: Iterate through each line in the log and prepare each line in a manner that
    we can parse individual elements and handle quoted space characters.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历日志中的每一行，并以一种我们可以解析单独元素并处理带引号的空格字符的方式准备每一行。
- en: Validate and map each value to its respective column.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证并将每个值映射到其相应的列。
- en: Write mapped columns and values to a spreadsheet report.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将映射的列和值写入电子表格报告。
- en: How it works...
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We begin with by importing libraries for argument handling and logging, followed
    by the built-in libraries we need to parse and validate the log information. These
    include the `re` regular expression library and `shlex` lexical analyzer library.
    We also include `sys` and `csv` for handling the output of log messages and reports.
    We initialize the recipe's logging object by calling the `getLogger()` method.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入用于处理参数和日志记录的库，然后是我们需要解析和验证日志信息的内置库。这些包括`re`正则表达式库和`shlex`词法分析器库。我们还包括`sys`和`csv`来处理日志消息和报告的输出。我们通过调用`getLogger()`方法初始化了该配方的日志对象。
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Following the imports, we define patterns for the fields we will parse from
    the logs. This information may vary a bit between logs, though the patterns expressed
    here should cover most elements in a log.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入之后，我们为从日志中解析的字段定义模式。这些信息在日志之间可能会有所不同，尽管这里表达的模式应该涵盖日志中的大多数元素。
- en: You may need to add, remove, or reorder some of the patterns defined as follows
    to properly parse the IIS log you are working with. These patterns should cover
    the common elements found in IIS logs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要添加、删除或重新排序以下定义的模式，以正确解析您正在使用的IIS日志。这些模式应该涵盖IIS日志中常见的元素。
- en: We build these patterns as a list of tuples called `iis_log_format`, where the
    first tuple element is the column name and the second is the regular expression
    pattern to validate the expected content. By using a regular expression pattern,
    we can define a set of rules that the data must follow in order to be valid. It
    is critical that these columns are expressed in the order they appear in the log;
    otherwise, the code won't be able to properly map values to columns.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些模式构建为名为`iis_log_format`的元组列表，其中第一个元组元素是列名，第二个是用于验证预期内容的正则表达式模式。通过使用正则表达式模式，我们可以定义数据必须遵循的一组规则以使其有效。这些列必须按它们在日志中出现的顺序来表达，否则代码将无法正确地将值映射到列。
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This recipe's command-line handler takes two positional arguments, `iis_log`
    and `csv_report`, which represent the IIS log to process and the desired CSV path,
    respectively. Additionally, this recipe also accepts an optional argument, `l`,
    specifying the output path for the recipe's log file.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的命令行处理程序接受两个位置参数，`iis_log`和`csv_report`，分别表示要处理的IIS日志和所需的CSV路径。此外，此配方还接受一个可选参数`l`，指定配方日志文件的输出路径。
- en: Next, we initialize the recipe's logging utility and configure it for console
    and file-based logging. This is important as we should note in a formal manner
    when we fail to parse a line for the user. In this manner, if something fails
    they shouldn't be working under the mistaken assumption that all lines were parsed
    successfully and are displayed in the resulting CSV spreadsheet. We also want
    to record runtime messages, including the version of the script and the supplied
    arguments. At this point, we are ready to call the `main()` function and kick
    off the script. Refer to the logging recipe in [Chapter 1](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe),
    *Essential Scripting and File Information Recipes* for a more detailed explanation
    of setting up a logging object.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化了该配方的日志实用程序，并为控制台和基于文件的日志记录进行了配置。这一点很重要，因为我们应该以正式的方式注意到当我们无法为用户解析一行时。通过这种方式，如果出现问题，他们不应该在错误的假设下工作，即所有行都已成功解析并显示在生成的CSV电子表格中。我们还希望记录运行时消息，包括脚本的版本和提供的参数。在这一点上，我们准备调用`main()`函数并启动脚本。有关设置日志对象的更详细解释，请参阅[第1章](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe)中的日志配方，*基本脚本和文件信息配方*。
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `main()` function handles the bulk of the logic in this script. We create
    a list, `parsed_logs`, to store the parsed lines before iterating over the lines
    in the log file. Inside the `for` loop, we strip the line and create a storage
    dictionary, `log_entry`, for the record. We speed up our processing, and prevent
    errors in column matching, by skipping lines beginning with the comment (or pound)
    character or if the line is empty.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`main（）`函数处理了脚本中大部分的逻辑。我们创建一个列表`parsed_logs`，用于在迭代日志文件中的行之前存储解析后的行。在`for`循环中，我们剥离行并创建一个存储字典`log_entry`，以存储记录。通过跳过以注释（或井号）字符开头的行或者行为空，我们加快了处理速度，并防止了列匹配中的错误。'
- en: While IIS logs are stored as space-delimited values, they use double quotes
    to escape strings that contain spaces. For example, a `useragent` string is a
    single value but generally, contains one or more spaces. Using the `shlex` module,
    we can parse the line with the `shlex()` method, and handle quote escaped spaces
    automatically by delimiting the data correctly on space values. This library can
    slow down processing, so we only use it on lines containing a double-quote character.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然IIS日志存储为以空格分隔的值，但它们使用双引号来转义包含空格的字符串。例如，`useragent`字符串是一个单一值，但通常包含一个或多个空格。使用`shlex`模块，我们可以使用`shlex（）`方法解析带有双引号的空格的行，并通过正确地在空格值上分隔数据来自动处理引号转义的空格。这个库可能会减慢处理速度，因此我们只在包含双引号字符的行上使用它。
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With the line properly delimited, we use the `enumerate` function to step through
    each element in the record and extract the corresponding column name and pattern.
    Using the pattern, we call the `match()` method on the value and, if it matches,
    create an entry in the `log_entry` dictionary. If the value doesn't match the
    pattern, we log an error and provide the whole line in the log file. After iterating
    through each of the columns, we append the record dictionary to the initial list
    of parsed log records and repeat this process for the remaining lines.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将行正确分隔后，我们使用`enumerate`函数逐个遍历记录中的每个元素，并提取相应的列名和模式。使用模式，我们在值上调用`match（）`方法，如果匹配，则在`log_entry`字典中创建一个条目。如果值不匹配模式，我们记录一个错误，并在日志文件中提供整行。在遍历每个列后，我们将记录字典附加到初始解析日志记录列表，并对剩余行重复此过程。
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once all lines have been processed, we print a status message to the console
    prior to preparing for the `write_csv()` method. We use a simple list comprehension
    expression to extract the first element of each tuple, which represents a column
    name, within the `iis_log_format` list. With the columns extracted, let's look
    at the report writer.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完所有行后，我们在准备`write_csv（）`方法之前向控制台打印状态消息。我们使用一个简单的列表推导表达式来提取`iis_log_format`列表中每个元组的第一个元素，这代表一个列名。有了提取的列，让我们来看看报告编写器。
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The report writer creates a CSV file using the methods we have previously explored.
    Since we stored the lines as a list of dictionaries, we can easily create the
    report with four lines of code using the `csv.DictWriter` class.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 报告编写器使用我们之前探讨过的方法创建一个CSV文件。由于我们将行存储为字典列表，我们可以使用`csv.DictWriter`类的四行代码轻松创建报告。
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When we look at the CSV report generated by the script, we see the following
    fields in the sample output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看脚本生成的CSV报告时，我们会在样本输出中看到以下字段：
- en: '![](../images/00077.jpeg)![](../images/00078.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00077.jpeg)![](../images/00078.jpeg)'
- en: There's more...
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. Here is a recommendation:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。以下是一个建议：
- en: While we can define regex patterns as seen at the start of the script, we can
    make our lives easier using regular expression management libraries instead. One
    example is the `grok` library, which is used to create variable names for patterns.
    This allows us to organize and extend patterns with ease, as we can express them
    by name instead of a string value. This library is used by other platforms, such
    as the ELK stack, for management and implementation of regular expressions.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然我们可以像在脚本开头看到的那样定义正则表达式模式，但我们可以使用正则表达式管理库来简化我们的生活。一个例子是`grok`库，它用于为模式创建变量名。这使我们能够轻松地组织和扩展模式，因为我们可以按名称而不是字符串值来表示它们。这个库被其他平台使用，比如ELK堆栈，用于管理和实现正则表达式。
- en: Going spelunking
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行洞穴探险
- en: 'Recipe Difficulty: Medium'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 菜谱难度：中等
- en: 'Python Version: 2.7'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating System: Any'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任何
- en: Log files can quickly become quite sizable due to the level of detail and time
    frame preserved. As you may have noticed, the CSV report from the prior recipe
    can easily become too large for our spreadsheet application to open or browse
    efficiently. Rather than analyzing this data in a spreadsheet, one alternative
    would be to load the data into a database.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于保存的详细级别和时间范围，日志文件很快就会变得相当庞大。正如您可能已经注意到的那样，先前菜谱的CSV报告很容易变得过大，以至于我们的电子表格应用程序无法有效地打开或浏览。与其在电子表格中分析这些数据，一个替代方法是将数据加载到数据库中。
- en: '**Splunk** is a platform that incorporates a NoSQL database with an ingestion
    and query engine, making it a powerful analysis tool. Its database operates in
    a manner like Elasticsearch or MongoDB, permitting the storage of documents or
    structured records. Because of this, we do not need to provide records with a
    consistent key-value mapping to store them in the database. This is what makes
    NoSQL databases so useful for log analysis, as log formats can be variable depending
    on the event type.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**Splunk**是一个将NoSQL数据库与摄取和查询引擎结合在一起的平台，使其成为一个强大的分析工具。它的数据库的操作方式类似于Elasticsearch或MongoDB，允许存储文档或结构化记录。因此，我们不需要为了将记录存储在数据库中而提供具有一致键值映射的记录。这就是使NoSQL数据库对于日志分析如此有用的原因，因为日志格式可能根据事件类型而变化。'
- en: In this recipe, we learn to index the CSV report from the previous recipe into
    Splunk, allowing us to interact with the data inside the platform. We also design
    the script to run queries against the dataset and to export the resulting subset
    of data responsive to the query to a CSV file. These processes are handled in
    separate stages so we can independently query and export data as needed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们学习将上一个步骤的CSV报告索引到Splunk中，从而可以在平台内部与数据交互。我们还设计脚本来针对数据集运行查询，并将响应查询的结果子集导出到CSV文件。这些过程分别处理，因此我们可以根据需要独立查询和导出数据。
- en: Getting started
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: This recipe requires the installation of the third-party library `splunk-sdk`.
    All other libraries used in this script are present in Python's standard library.
    Additionally, we must install Splunk on the host operating system and, due to
    limitations of the `splunk-sdk` library, run the script using Python 2.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤需要安装第三方库`splunk-sdk`。此脚本中使用的所有其他库都包含在Python的标准库中。此外，我们必须在主机操作系统上安装Splunk，并且由于`splunk-sdk`库的限制，必须使用Python
    2来运行脚本。
- en: To install Splunk, we need to navigate to [Splunk.com](https://www.splunk.com/),
    fill out the form, and select the Splunk Enterprise free trial download. This
    enterprise trial allows us to practice with the API and gives us the ability to
    upload 500 MB per day. Once we have downloaded the application, we need to launch
    it to configure the application. While there are a lot of configurations we could
    change, launch it with the defaults, for now, to keep things simple and focus
    on the API. In doing so, the default address for the server will be `localhost:8000`.
    By navigating to this address in a browser, we can log in for the first time,
    set up accounts and (*please do this*) change the administrator password.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Splunk，我们需要转到[Splunk.com](https://www.splunk.com/)，填写表格，并选择Splunk Enterprise免费试用下载。这个企业试用版允许我们练习API，并且可以每天上传500MB。下载应用程序后，我们需要启动它来配置应用程序。虽然有很多配置可以更改，但现在使用默认配置启动，以保持简单并专注于API。这样做后，服务器的默认地址将是`localhost:8000`。通过在浏览器中导航到这个地址，我们可以首次登录，设置账户和（*请执行此操作*）更改管理员密码。
- en: The default username and password for a new Splunk install is *admin* and *changeme*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 新安装的Splunk的默认用户名和密码是*admin*和*changeme*。
- en: 'With the Splunk instance active, we can now install the API library. This library
    handles the conversion from the REST API into Python objects. At the time of writing
    of this book, the Splunk API is only available in Python 2\. The `splunk-sdk`
    library can be installed with `pip`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在Splunk实例激活后，我们现在可以安装API库。这个库处理从REST API到Python对象的转换。在撰写本书时，Splunk API只能在Python
    2中使用。`splunk-sdk`库可以使用`pip`安装：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: To learn more about the `splunk-sdk` library, visit [http://dev.splunk.com/python](http://dev.splunk.com/python).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`splunk-sdk`库的信息，请访问[http://dev.splunk.com/python](http://dev.splunk.com/python)。
- en: How to do it...
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Now that the environment is properly configured, we can begin to develop the
    code. This script will index new data to Splunk, run queries on that data, and
    export subsets of data responsive to our queries to a CSV file. To accomplish
    this, we need to:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在环境已经正确配置，我们可以开始开发代码。这个脚本将新数据索引到Splunk，对该数据运行查询，并将响应我们查询的数据子集导出到CSV文件。为了实现这一点，我们需要：
- en: Develop a robust argument-handling interface allowing the user to specify these
    options.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一个强大的参数处理接口，允许用户指定这些选项。
- en: Build a class to handle operations with the various properties' methods.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个处理各种属性方法的操作类。
- en: Create methods to handle the process of indexing new data and creating the index
    for data storage.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建处理索引新数据和创建数据存储索引的过程的方法。
- en: Set up methods for running Splunk queries in a manner that allows for informative
    reports.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立运行Splunk查询的方法，以便生成信息丰富的报告。
- en: Provide a mechanism for exporting reports to a CSV format.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一种将报告导出为CSV格式的机制。
- en: How it works...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We begin by importing the required libraries for this script, including the
    newly installed `splunklib`. To prevent unnecessary errors arising due to user
    ignorance, we use the `sys` library to determine the version of Python executing
    the script and raise an error if it is not Python 2.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入此脚本所需的库，包括新安装的`splunklib`。为了防止由于用户无知而引起不必要的错误，我们使用`sys`库来确定执行脚本的Python版本，并在不是Python
    2时引发错误。
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The next logical block to develop is the recipe's command-line argument handler.
    As we have many options and operations to execute in this code, we need to spend
    some extra time on this section. And because this code is class based, we must
    set up some additional logic in this section.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个逻辑块是开发步骤的命令行参数处理程序。由于这段代码有很多选项和操作需要在代码中执行，我们需要在这一部分花费一些额外的时间。而且因为这段代码是基于类的，所以我们必须在这一部分设置一些额外的逻辑。
- en: 'This recipe''s command-line handler takes one positional input, `action`, which
    represents the action to run (index, query, or export). This recipe also supports
    seven optional arguments: `index`, `config`, `file`, `query`, `cols`, `host`,
    and `port`. Let''s start looking at what all of these options do.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤的命令行处理程序接受一个位置输入`action`，表示要运行的操作（索引、查询或导出）。此步骤还支持七个可选参数：`index`、`config`、`file`、`query`、`cols`、`host`和`port`。让我们开始看看所有这些选项都做什么。
- en: The `index` argument, which is actually a required argument, is used to specify
    the name of the Splunk index to ingest, query, or export the data from. This can
    be an existing or new `index` name. The `config` parameter refers to the configuration
    file containing your username and password for the Splunk instance. This file,
    as described in the argument's help, should be protected and stored outside of
    the location where the code is executed. In an enterprise environment, you may
    need to further protect these credentials.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`index`参数实际上是一个必需的参数，用于指定要从中摄取、查询或导出数据的Splunk索引的名称。这可以是现有的或新的`index`名称。`config`参数是指包含Splunk实例的用户名和密码的配置文件。如参数帮助中所述，此文件应受保护并存储在代码执行位置之外。在企业环境中，您可能需要进一步保护这些凭据。'
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `file` parameter will be used to provide a path to the `file` to `index`
    into the platform or be used to specify the filename to write the exported `query`
    data to. For example, we will use the `file` parameter to point to the CSV spreadsheet
    we wish to ingest from the previous recipe. The `query` parameter also serves
    a dual purpose, it can be used to run a query from Splunk or to specify a query
    ID to export as CSV. This means that the `index` and `query` actions require only
    one of these parameters, but the `export` action requires both.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`file`参数将用于提供要`index`到平台的文件的路径，或用于指定要将导出的`query`数据写入的文件名。例如，我们将使用`file`参数指向我们希望从上一个配方中摄取的CSV电子表格。`query`参数也具有双重作用，它可以用于从Splunk运行查询，也可以用于指定要导出为CSV的查询ID。这意味着`index`和`query`操作只需要其中一个参数，但`export`操作需要两个参数。'
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The last block of arguments allows the user to modify default properties of
    the recipe. The `cols` argument, for example, can be used to specify what columns
    from the source data to export and in what order. As we will be querying and exporting
    IIS logs, we already know what columns are available and are of interest to us.
    You may want to specify alternative default columns based on what type of data
    is being explored. Our last two arguments include the `host` and `port` parameters,
    each defaulting to a local server but can be configured to allow you to interact
    with alternate instances.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一组参数允许用户修改配方的默认属性。例如，`cols`参数可用于指定从源数据中导出的列及其顺序。由于我们将查询和导出IIS日志，因此我们已经知道可用的列，并且对我们感兴趣。您可能希望根据正在探索的数据类型指定替代默认列。我们的最后两个参数包括`host`和`port`参数，每个参数默认为本地服务器，但可以配置为允许您与替代实例进行交互。
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: With our arguments specified, we can parse them and verify that all requirements
    are met prior to executing the recipe. First, we must open and read the `config`
    file containing the authentication credentials, where the `username` is on the
    first line and the `password` is on the second line. Using this information, we
    create a dictionary, `conn_dict`, containing the login details and server location.
    This dictionary is passed to the `splunklib` `client.connect()` method. Notice
    how we delete, using the `del()` method, the variables containing this sensitive
    information. While the username and password are still accessible through the
    `service` object, we want to limit the number of areas in which those details
    are stored. Following the creation of the `service` variable, we test if any applications
    are installed in Splunk, as by default there is at least one, and use that as
    a test of successful authentication.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 确定了我们的参数后，我们可以解析它们并在执行配方之前验证所有要求是否满足。首先，我们必须打开并读取包含身份验证凭据的`config`文件，其中`username`在第一行，`password`在第二行。使用这些信息，我们创建一个包含登录详细信息和服务器位置的字典`conn_dict`，并将该字典传递给`splunklib`的`client.connect()`方法。请注意，我们使用`del()`方法删除包含这些敏感信息的变量。虽然用户名和密码仍然可以通过`service`对象访问，但我们希望限制存储这些详细信息的区域数量。在创建`service`变量后，我们测试是否在Splunk中安装了任何应用程序，因为默认情况下至少有一个应用程序，并将其用作成功验证的测试。
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We continue processing the supplied arguments by converting the columns into
    a list and creating the `Spelunking` class instance. To initialize the class,
    we must supply it the `service` variable, the action to take, the index name,
    and the columns. Using this, our class instance is now ready for use.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续处理提供的参数，将列转换为列表并创建`Spelunking`类实例。要初始化该类，我们必须向其提供`service`变量、要执行的操作、索引名称和列。使用这些信息，我们的类实例现在已经准备就绪。
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we use a series of `if-elif-else` statements to handle the three various
    actions we expect to encounter. If the user supplied the `index` action, we first
    confirm that the optional `file` parameter is present, raising an error if it
    is not. If we do find it, we assign the value to the corresponding property of
    the `Spelunking` class instance. This type of logic is repeated for the `query`
    and `export` actions, confirming that they also were used with the correct optional
    arguments. Notice how we assign the absolute path of the file for the class using
    the `os.path.abspath()` function. This allows `splunklib` to find the correct
    file on the system. And, in what perhaps may be the longest argument handling
    section in the book, we have completed the requisite logic and can now call the
    class `run()` method to kick off the processing for the specific action requested.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用一系列`if-elif-else`语句来处理我们预期遇到的三种不同操作。如果用户提供了`index`操作，我们首先确认可选的`file`参数是否存在，如果不存在则引发错误。如果我们找到它，我们将该值分配给`Spelunking`类实例的相应属性。对于`query`和`export`操作，我们重复这种逻辑，确认它们也使用了正确的可选参数。请注意，我们使用`os.path.abspath()`函数为类分配文件的绝对路径。这允许`splunklib`在系统上找到正确的文件。也许这是本书中最长的参数处理部分，我们已经完成了必要的逻辑，现在可以调用类的`run()`方法来启动特定操作的处理。
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: With the arguments now behind us, let's dive into the class responsible for
    handling the operations requested by the user. This class takes four arguments,
    including the `service` variable, the `action` specified by the user, the Splunk
    index name, and the columns to use. All other properties are set to `None` and,
    as seen in the previous code block, will be appropriately initialized at the time
    of execution if they were supplied. This is done to limit the number of arguments
    required by the class and to handle the situations where certain properties are
    unused. All of these properties are initialized at the start of our class to ensure
    we have assigned default values.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在参数已经在我们身后，让我们深入研究负责处理用户请求操作的类。这个类有四个参数，包括`service`变量，用户指定的`action`，Splunk索引名称和要使用的列。所有其他属性都设置为`None`，如前面的代码块所示，如果它们被提供，将在执行时适当地初始化。这样做是为了限制类所需的参数数量，并处理某些属性未使用的情况。所有这些属性都在我们的类开始时初始化，以确保我们已经分配了默认值。
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `run()` method is responsible for obtaining the `index` object from the
    Splunk instance using the `get_or_create_index()` method. It also checks which
    action was specified at the command-line and calls the corresponding class instance
    method.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()`方法负责使用`get_or_create_index()`方法从Splunk实例获取`index`对象。它还检查在命令行指定了哪个动作，并调用相应的类实例方法。'
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `get_or_create_index()` method, as the name suggests, first tests whether
    the specified index exists and makes a connection to it, or creates a new index
    if none is found by that name. Since this information is stored in the `indexes`
    property of the `service` variable as a dictionary-like object, we can easily
    test for the existence of the index by name.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_or_create_index()`方法，顾名思义，首先测试指定的索引是否存在，并连接到它，或者如果没有找到该名称的索引，则创建一个新的索引。由于这些信息存储在`service`变量的`indexes`属性中，作为一个类似字典的对象，我们可以很容易地通过名称测试索引的存在。'
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: To ingest the data from a file, such as a CSV file, we can use a one-line statement
    to send information to the instance in the `index_data()` method. This method
    uses the `upload()` method of the `splunk_index` object to send the file to Splunk
    for ingestion. While a CSV file is a simplistic example of how we can import data,
    we could also use some of the logic from the previous recipe to read the raw log
    into the Splunk instance without the intermediate CSV step. For that, we would
    want to use a different method of the `index` object that would allow us to send
    each parsed event individually.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要从文件中摄取数据，比如CSV文件，我们可以使用一行语句将信息发送到`index_data()`方法中的实例。这个方法使用`splunk_index`对象的`upload()`方法将文件发送到Splunk进行摄取。虽然CSV文件是一个简单的例子，说明我们可以如何导入数据，但我们也可以使用前面的方法从原始日志中读取数据到Splunk实例，而不需要中间的CSV步骤。为此，我们希望使用`index`对象的不同方法，允许我们逐个发送每个解析的事件。
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `query_index()` method is a little more involved, as we first need to modify
    the query provided from the user. As seen in the following snippet, we need to
    add the columns specified by the user to the initial query. This will make fields
    not used in the query available during the export stage. Following this modification,
    we create a new job in the Splunk system with the `service.jobs.create()` method
    and record the query SID. This SID will be used in the exporting phase to export
    the results of the specific query job. We print this information, along with the
    time before the job expires from the Splunk instance. By default, this time-to-live
    value is `300` seconds, or five minutes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`query_index()`方法涉及更多，因为我们首先需要修改用户提供的查询。如下面的片段所示，我们需要将用户指定的列添加到初始查询中。这将使在导出阶段未使用的字段在查询中可用。在修改后，我们使用`service.jobs.create()`方法在Splunk系统中创建一个新的作业，并记录查询SID。这个SID将在导出阶段用于导出特定查询作业的结果。我们打印这些信息，以及作业在Splunk实例中到期之前的时间。默认情况下，这个生存时间值是`300`秒，或五分钟。'
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As previously alluded to, the `export_report()` method uses the SID mentioned
    in the prior method to check whether the job is complete and to retrieve the data
    for export. in order to do this, we iterate through the available jobs, and if
    ours is not present, raise a warning. If the job is found, but the `is_ready()`
    method returns `False`, the job is still processing and not ready to export results.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，`export_report()`方法使用前面方法中提到的SID来检查作业是否完成，并检索要导出的数据。为了做到这一点，我们遍历可用的作业，如果我们的作业不存在，则发出警告。如果找到作业，但`is_ready()`方法返回`False`，则作业仍在处理中，尚未准备好导出结果。
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If the job passes these two tests, we extract the data from Splunk and write
    it to a CSV file using the `write_csv()` method. Before we can do that, we need
    to initialize a list to store the job results. Next, we retrieve the results,
    specifying the columns of interest, and read this raw data into the `job_results`
    variable. Luckily, `splunklib` provides a `ResultsReader` that converts the `job_results`
    variable into a list of dictionaries. We iterate through this list and append
    each of these dictionaries to the `export_data` list. Finally, we provide the
    file path, column names, and dataset to export to the CSV writer.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果作业通过了这两个测试，我们从Splunk中提取数据，并使用`write_csv()`方法将其写入CSV文件。在这之前，我们需要初始化一个列表来存储作业结果。接下来，我们检索结果，指定感兴趣的列，并将原始数据读入`job_results`变量。幸运的是，`splunklib`提供了一个`ResultsReader`，它将`job_results`变量转换为一个字典列表。我们遍历这个列表，并将每个字典附加到`export_data`列表中。最后，我们提供文件路径、列名和要导出到CSV写入器的数据集。
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `write_csv()` method in this class is a `@staticmethod`. This decorator
    allows us to use a generalized method in the class without needing to specify
    an instance. This method will no doubt look familiar to those used elsewhere in
    the book, where we open the output file, create a `DictWriter` object, then write
    the column headers and data to file.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类中的`write_csv()`方法是一个`@staticmethod`。这个装饰器允许我们在类中使用一个通用的方法，而不需要指定一个实例。这个方法无疑会让那些在本书的其他地方使用过的人感到熟悉，我们在那里打开输出文件，创建一个`DictWriter`对象，然后将列标题和数据写入文件。
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In our hypothetical use case, the first stage will be to index the data contained
    in the CSV spreadsheet from the previous recipe. As seen in the following snippet,
    we supply the CSV file from our previous recipe and add it to the Splunk index.
    Next, we look for all entries where the user agent is an iPhone. Finally, the
    last stage involves taking the output from the query and creating a CSV report.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的假设用例中，第一阶段将是索引前一个食谱中CSV电子表格中的数据。如下片段所示，我们提供了前一个食谱中的CSV文件，并将其添加到Splunk索引中。接下来，我们寻找所有用户代理为iPhone的条目。最后，最后一个阶段涉及从查询中获取输出并创建一个CSV报告。
- en: '![](../images/00079.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00079.jpeg)'
- en: 'With these three commands successfully executed, we can open and review the
    filtered output:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 成功执行这三个命令后，我们可以打开并查看过滤后的输出：
- en: '![](../images/00080.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00080.jpeg)'
- en: There's more...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    as shown here:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们提供了一个或多个建议，如下所示：
- en: The Splunk API for Python (and in general) has many other features. Additionally,
    more advanced querying techniques can be used to generate data that we can manipulate
    into graphics for technical and non-technical end users alike. Learn more about
    the many features that the Splunk API affords you.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python的Splunk API（以及一般）还有许多其他功能。此外，还可以使用更高级的查询技术来生成我们可以将其转换为技术和非技术最终用户的图形的数据。了解更多Splunk
    API提供的许多功能。
- en: Interpreting the daily.out log
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释daily.out日志
- en: 'Recipe Difficulty: Medium'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：中等
- en: 'Python Version: 3.5'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：3.5
- en: 'Operating System: Any'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任意
- en: Operating system logs generally reflect events for software, hardware, and services
    on the system. These details can assist us in our investigations as we look into
    an event, such as the use of removable devices. One example of a log that can
    prove useful in identifying this activity is `daily.out` log found on macOS systems.
    This log records a lot of information, including what drives are connected to
    the machine and the amount of storage available and used daily. While we can also
    learn about shutdown times, network states, and other information from this log,
    we will focus on drive usage over time.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统日志通常反映系统上软件、硬件和服务的事件。这些细节可以在我们调查事件时帮助我们，比如可移动设备的使用。一个可以证明在识别这种活动中有用的日志的例子是在macOS系统上找到的`daily.out`日志。这个日志记录了大量信息，包括连接到机器上的驱动器以及每天可用和已使用的存储量。虽然我们也可以从这个日志中了解关机时间、网络状态和其他信息，但我们将专注于随时间的驱动器使用情况。
- en: Getting started
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: All libraries used in this script are present in Python's standard library.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本中使用的所有库都包含在Python的标准库中。
- en: How to do it...
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This script will leverage the following steps:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本将利用以下步骤：
- en: Set up arguments to accept the log file and a path to write the report.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置参数以接受日志文件和写入报告的路径。
- en: Build a class that handles the parsing of the log's various sections.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个处理日志各个部分解析的类。
- en: Create a method to extract the relevant section and pass it for further processing.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个提取相关部分并传递给进一步处理的方法。
- en: Extract disk information from these sections.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这些部分提取磁盘信息。
- en: Create a CSV writer to export the extracted details.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个CSV写入器来导出提取的细节。
- en: How it works...
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We begin by importing libraries necessary for argument handling, date interpretation,
    and the writing spreadsheets. One of the great things about processing text files
    in Python is that you rarely need a third-party library.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库来处理参数、解释日期和写入电子表格。在Python中处理文本文件的一个很棒的地方是你很少需要第三方库。
- en: '[PRE30]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This recipe's command-line handler accepts two positional arguments, `daily_out`
    and `output_report`, which represent the path to the daily.out log file and the
    desired output path for the CSV spreadsheet, respectively. Notice how we pass
    an open file object for processing through the `argparse.FileType` class. Following
    this, we initialize the `ProcessDailyOut` class with the log file and call the
    `run()` method and store the returned results in the `parsed_events` variable.
    We then call the `write_csv()` method to write the results to a spreadsheet in
    the desired output directory using defined columns from the `processor` class
    object.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱的命令行处理程序接受两个位置参数，`daily_out`和`output_report`，分别代表daily.out日志文件的路径和CSV电子表格的期望输出路径。请注意，我们通过`argparse.FileType`类传递一个打开的文件对象进行处理。随后，我们用日志文件初始化`ProcessDailyOut`类，并调用`run()`方法，并将返回的结果存储在`parsed_events`变量中。然后我们调用`write_csv()`方法，使用`processor`类对象中定义的列将结果写入到所需输出目录中的电子表格中。
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the `ProcessDailyOut` class, we set up the properties supplied by the user
    and define the columns used for the report. Notice how we add two different sets
    of columns: the `disk_status_columns` and the `report_columns`. The `report_columns`
    are simply the `disk_status_columns` with two additional fields to identify the
    entry date and time zone.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ProcessDailyOut`类中，我们设置了用户提供的属性，并定义了报告中使用的列。请注意，我们添加了两组不同的列：`disk_status_columns`和`report_columns`。`report_columns`只是`disk_status_columns`，再加上两个额外的字段来标识条目的日期和时区。
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `run()` method begins by iterating over the provided log file. After stripping
    whitespace characters from the start and end of each line, we validate the content
    to identify breaks in sections. The `"-- End of daily output --"` string breaks
    each entry in the log file. Each entry contains several sections of data broken
    up by new lines. For this reason, we must use several blocks of code to split
    and process each section separately.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()`方法首先遍历提供的日志文件。在从每行的开头和结尾去除空白字符后，我们验证内容以识别部分中断。`"-- End of daily output
    --"`字符串中断了日志文件中的每个条目。每个条目包含几个由新行分隔的数据部分。因此，我们必须使用几个代码块来分割和处理每个部分。'
- en: In this loop, we gather all lines from a single event and pass it to the `process_event()`
    method and append the processed results to the `parsed_events` list that is eventually
    returned.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个循环中，我们收集来自单个事件的所有行，并将其传递给`process_event()`方法，并将处理后的结果追加到最终返回的`parsed_events`列表中。
- en: '[PRE33]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the `process_event()` method, we will define variables that will allow us
    to split sections of an event for further processing. To better understand this
    next segment of code, please take a moment to review the following example of
    an event:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在`process_event()`方法中，我们将定义变量，以便我们可以分割事件的各个部分以进行进一步处理。为了更好地理解代码的下一部分，请花一点时间查看以下事件的示例：
- en: '![](../images/00081.jpeg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00081.jpeg)'
- en: Within this event, we can see that the first element is the date value and time
    zone, followed by a series of subsections. Each of the subsection headers is a
    line that ends with a colon; we use this to split the various data elements within
    this file, as seen in the following code. We create a dictionary, `event_data`,
    using the section headers as a key and their content, if present, as the value
    before further processing each subsection.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在此事件中，我们可以看到第一个元素是日期值和时区，后面是一系列子部分。每个子部分标题都是以冒号结尾的行；我们使用这一点来拆分文件中的各种数据元素，如下面的代码所示。我们使用部分标题作为键，其内容（如果存在）作为值，然后进一步处理每个子部分。
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If the section header line does not end with a colon, we check if there are
    exactly two colons in the line. If so, we try to validate this line as a date
    value. To handle this date format with built-in libraries, we need to extract
    the time zone separately from the rest of the date as there is a known bug in
    versions of Python 3 with parsing time zones with the `%Z` formatter. For the
    curious, more information on this bug can be found at [https://bugs.python.org/issue22377](https://bugs.python.org/issue22377).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果部分标题行不以冒号结尾，我们检查行中是否恰好有两个冒号。如果是这样，我们尝试将此行验证为日期值。为了处理此日期格式，我们需要从日期的其余部分单独提取时区，因为已知Python
    3版本在解析带有`%Z`格式化程序的时区时存在已知错误。对于感兴趣的人，可以在[https://bugs.python.org/issue22377](https://bugs.python.org/issue22377)找到有关此错误的更多信息。
- en: To separate the time zone from the date value, we delimit the string on the
    space value, place the time zone value (element `4` in this example) in its own
    variable, then join the remaining time values into a new string that we can parse
    with the `datetime` library. This may raise an `IndexError` if the string does
    not have a minimum of `5` elements or a `ValueError` if the `datetime` format
    string is invalid. If either of these error types are not raised, we assign the
    date to the `event_data` dictionary. If we do receive either of these errors,
    the line will be appended to the `section_data` list and the next loop iteration
    will continue. This is important as a line may contain two colons and not be a
    date value, and so we wouldn't want to then disregard the line by removing it
    from the script's consideration.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将时区与日期值分开，我们在空格值上分隔字符串，在这个示例中将时区值（元素`4`）放入自己的变量中，然后将剩余的时间值连接成一个新的字符串，我们可以使用`datetime`库解析。如果字符串没有至少`5`个元素，可能会引发`IndexError`，或者如果`datetime`格式字符串无效，可能会引发`ValueError`。如果没有引发这两种错误类型，我们将日期分配给`event_data`字典。如果我们收到这些错误中的任何一个，该行将附加到`section_data`列表中，并且下一个循环迭代将继续。这很重要，因为一行可能包含两个冒号，但不是日期值，所以我们不希望将其从脚本的考虑中移除。
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The last piece of this conditional appends any line that has content to the
    `section_data` variable for further processing as needed. This prevents blank
    lines from finding their way in and allows us to capture all information between
    two section headers.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此条件的最后一部分将任何具有内容的行附加到`section_data`变量中，以根据需要进行进一步处理。这可以防止空白行进入，并允许我们捕获两个部分标题之间的所有信息。
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We close this function by calling to any subsection processors. At this time,
    we only handle the disk information subsection, with the `process_disk()` method,
    though one could develop code to extract other values of interest. This method
    accepts as its input the event information and the event date. The disk information
    is returned as a list of processed disk information elements which we return to
    the `run()` method and add the values to the processed event list.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用任何子部分处理器来关闭此函数。目前，我们只处理磁盘信息子部分，使用`process_disk()`方法，尽管可以开发代码来提取其他感兴趣的值。此方法接受事件信息和事件日期作为其输入。磁盘信息作为处理过的磁盘信息元素列表返回，我们将其返回给`run()`方法，并将值添加到处理过的事件列表中。
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: To process a disk subsection, we iterate through each of the lines, if there
    are any, and extract the relevant event information. The `for` loop starts by
    checking the iteration number and skipping row zero as it contains the data's
    column headers. For any other line, we use list comprehension and split the line
    on a single space, strip whitespace, and filter out any fields that are blank.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理磁盘子部分，我们遍历每一行，如果有的话，并提取相关的事件信息。`for`循环首先检查迭代号，并跳过行零，因为它包含数据的列标题。对于任何其他行，我们使用列表推导式，在单个空格上拆分行，去除空白，并过滤掉任何空字段。
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Next, we initialize a dictionary, `disk_info`, that holds the event information
    with the date and time zone details for this snapshot. The `for` loop uses the
    `enumerate()` function to map values to their column names. If the column name
    contains `"/Volumes/"` (the standard mount point for drive volumes), we will join
    the remainder of the split items. This ensures that volumes with spaces in their
    names are preserved appropriately.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化一个名为`disk_info`的字典，其中包含了此快照的日期和时区详细信息。`for`循环使用`enumerate()`函数将值映射到它们的列名。如果列名包含`"/Volumes/"`（驱动器卷的标准挂载点），我们将连接剩余的拆分项。这样可以确保保留具有空格名称的卷。
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The innermost `for` loop ends by appending the disk information to the `processed_data`
    list. Once all lines in the disk section have been processed, we return the `processed_data`
    list to the parent function.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最内层的`for`循环通过将磁盘信息附加到`processed_data`列表来结束。一旦磁盘部分中的所有行都被处理，我们就将`processed_data`列表返回给父函数。
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Lastly, we briefly touch on the `write_csv()` method, which uses the `DictWriter`
    class to open the file and write the header rows and the content to the CSV file.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要介绍了`write_csv()`方法，它使用`DictWriter`类来打开文件并将标题行和内容写入CSV文件。
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'When we run this script, we can see the extracted details in the CSV report.
    An example of this output is shown here:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个脚本时，我们可以在CSV报告中看到提取出的细节。这里展示了这个输出的一个例子：
- en: '![](../images/00082.jpeg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00082.jpeg)'
- en: Adding daily.out parsing to Axiom
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将daily.out解析添加到Axiom
- en: 'Recipe Difficulty: Easy'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 教程难度：简单
- en: 'Python Version: 2.7'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating System: Any'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任意
- en: Using the code we just developed to parse macOS `daily.out` logs, we add this
    functionality into Axiom, developed by *Magnet Forensics*, for the automatic extraction
    of these events. As Axiom supports the processing of forensic images and loose
    files, we can either provide it a full acquisition or just an export of the `daily.out`
    log for this example. Through the API made available by this tool, we can access
    and process files found by its engine and return results for review directly within
    Axiom.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们刚刚开发的代码来解析macOS的`daily.out`日志，我们将这个功能添加到Axiom中，由*Magnet Forensics*开发，用于自动提取这些事件。由于Axiom支持处理取证镜像和松散文件，我们可以提供完整的获取或只是`daily.out`日志的导出作为示例。通过这个工具提供的API，我们可以访问和处理其引擎发现的文件，并直接在Axiom中返回审查结果。
- en: Getting started
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: The Magnet Forensics team developed an API for both Python and XML to add support
    for creating custom artifacts within Axiom. The Python API, at of the writing
    of this book, is only available through `IronPython` running Python version 2.7\.
    While we have developed our code outside of this platform, we can easily integrate
    it into Axiom following the steps laid out in this recipe. We used Axiom version
    1.1.3.5726 to test and develop this recipe.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Magnet Forensics团队开发了一个API，用于Python和XML，以支持在Axiom中创建自定义artifact。截至本书编写时，Python
    API仅适用于运行Python版本2.7的`IronPython`。虽然我们在这个平台之外开发了我们的代码，但我们可以按照本教程中的步骤轻松地将其集成到Axiom中。我们使用了Axiom版本1.1.3.5726来测试和开发这个教程。
- en: We first need to install Axiom in a Windows instance and ensure that our code
    is stable and portable. Additionally, our code needs to be sandbox friendly. The
    Axiom sandbox limits the use of third-party libraries and access to some Python
    modules and functions that could cause code to interact with the system outside
    of the application. For this reason, we designed our `daily.out` parser to only
    use built-in libraries that are safe in the sandbox to demonstrate the ease of
    develop with these custom artifacts.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要在Windows实例中安装Axiom，并确保我们的代码稳定且可移植。此外，我们的代码需要在沙盒中运行。Axiom沙盒限制了对第三方库的使用以及对可能导致代码与应用程序外部系统交互的一些Python模块和函数的访问。因此，我们设计了我们的`daily.out`解析器，只使用在沙盒中安全的内置库，以演示使用这些自定义artifact的开发的便利性。
- en: How to do it...
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To develop and implement a custom artifact, we will need to:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要开发和实现自定义artifact，我们需要：
- en: Install Axiom in on a Windows machine.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Windows机器上安装Axiom。
- en: Import the script we developed.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们开发的脚本。
- en: Create the `Artifact` class and define the parser metadata and columns.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`Artifact`类并定义解析器元数据和列。
- en: Develop the `Hunter` class to handle the artifact processing and result reporting.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发`Hunter`类来处理artifact处理和结果报告。
- en: How it works...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: For this script, we import the `axiom` library and the datetime library. Notice,
    we have removed the previous `argparse` and `csv` imports are they are unnecessary
    here.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个脚本，我们导入了`axiom`库和datetime库。请注意，我们已经删除了之前的`argparse`和`csv`导入，因为它们在这里是不必要的。
- en: '[PRE42]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next, we must paste in the `ProcessDailyOut` class from the prior recipe, not
    including the `write_csv` or argument handling code, to use in this script. Since
    the current version of the API does not allow imports, we have to bundle all the
    code we need into a single script. To save pages and avoid redundancy, we will
    omit the code block in this section (though it exists as you'd expect in the code
    file bundled with this chapter).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须粘贴前一个教程中的`ProcessDailyOut`类，不包括`write_csv`或参数处理代码，以在这个脚本中使用。由于当前版本的API不允许导入，我们必须将所有需要的代码捆绑到一个单独的脚本中。为了节省页面并避免冗余，我们将在本节中省略代码块（尽管它在本章附带的代码文件中存在）。
- en: The next class is the `DailyOutArtifact`, a subclass of the `Artifact` class
    provided by the Axiom API. We call the `AddHunter()` method, providing our (not
    yet shown) `hHunter` class, before defining the plugin's name within the `GetName()`
    method.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个类是`DailyOutArtifact`，它是Axiom API提供的`Artifact`类的子类。在定义插件的名称之前，我们调用`AddHunter()`方法，提供我们的（尚未显示的）`hHunter`类。
- en: '[PRE43]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The last method of this class, `CreateFragments()`, specifies how to handle
    a single entry of the processed daily.out log results. A fragment, with respect
    to the Axiom API, is the term used to describe a single entry of an artifact.
    This code block allows us to add custom column names and assign the proper categories
    and data types for those columns. The categories include date, location, and other
    special values defined by the tool. The majority of columns for our artifact will
    be in the `None` category, as they don't display a specific kind of information.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的最后一个方法`CreateFragments()`指定了如何处理已处理的daily.out日志结果的单个条目。就Axiom API而言，片段是用来描述artifact的单个条目的术语。这段代码允许我们添加自定义列名，并为这些列分配适当的类别和数据类型。这些类别包括日期、位置和工具定义的其他特殊值。我们artifact的大部分列将属于`None`类别，因为它们不显示特定类型的信息。
- en: 'One important categorical difference is `DateTimeLocal` versus `DateTime`:
    the `DateTime` will present the date as a UTC value to the user, so we need to
    be conscious about selecting the proper date category. Because we extracted the
    time zone from the daily.out log entries, we use the `DateTimeLocal` category
    in this recipe. The `FragmentType` property is a string for all of the values,
    as the class does not convert values from strings into another data type.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的分类区别是`DateTimeLocal`与`DateTime`：`DateTime`将日期呈现为UTC值呈现给用户，因此我们需要注意选择正确的日期类别。因为我们从daily.out日志条目中提取了时区，所以在这个示例中我们使用`DateTimeLocal`类别。`FragmentType`属性是所有值的字符串，因为该类不会将值从字符串转换为其他数据类型。
- en: '[PRE44]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The next class is our `Hunter`. This parent class is used to run the processing
    code and, as you will see, specifies the platform and content that will be provided
    to the plugin by the Axiom engine. In this instance, we only want to run this
    against the computer platform and a file that goes by a single name. The `RegisterFileName()`
    method is one of several options for specifying what files will be requested by
    the plugin. We can also use regular expressions or file extensions to select the
    files we would like to process.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的类是我们的`Hunter`。这个父类用于运行处理代码，并且正如你将看到的，指定了将由Axiom引擎提供给插件的平台和内容。在这种情况下，我们只想针对计算机平台和一个单一名称的文件运行。`RegisterFileName()`方法是指定插件将请求哪些文件的几种选项之一。我们还可以使用正则表达式或文件扩展名来选择我们想要处理的文件。
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `Hunt()` method is where the magic happens. To start, we get a temporary
    path where the file can be read within the sandbox and assign it to the `temp_daily_out`
    variable. With this open file, we hand the file object to the `ProcessDailyOut`
    class and use the `run()` method to parse the file, just like in the last recipe.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hunt()`方法是魔法发生的地方。首先，我们获取一个临时路径，在沙箱内可以读取文件，并将其分配给`temp_daily_out`变量。有了这个打开的文件，我们将文件对象交给`ProcessDailyOut`类，并使用`run()`方法解析文件，就像上一个示例中一样。'
- en: '[PRE46]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'After gathering the parsed event information, we are ready to "publish" the
    data to the software and display it to the user. In the `for` loop, we first initiate
    a `Hit()` object to add data to a new fragment using the `AddValue()` method.
    Once we have assigned the event values to a hit, we publish the hit to the platform
    with the `PublishHit()` method and continue the loop until all parsed events have
    been published:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了解析的事件信息之后，我们准备将数据“发布”到软件并显示给用户。在`for`循环中，我们首先初始化一个`Hit()`对象，使用`AddValue()`方法向新片段添加数据。一旦我们将事件值分配给了一个hit，我们就使用`PublishHit()`方法将hit发布到平台，并继续循环直到所有解析的事件都被发布：
- en: '[PRE47]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The last bit of code checks to see if the file is not `None` and will close
    it if so. This is the end of the processing code, which may be called again if
    another `daily.out` file is discovered on the system!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分代码检查文件是否不是`None`，如果是，则关闭它。这是处理代码的结尾，如果在系统上发现另一个`daily.out`文件，可能会再次调用它！
- en: '[PRE48]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The last line registers our hard work with Axiom's engine to ensure it is included
    and called by the framework.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行注册了我们的辛勤工作到Axiom的引擎，以确保它被框架包含和调用。
- en: '[PRE49]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'To use the newly developed artifact in Axiom, we need to take a few more steps
    to import and run the code against an image. First, we need to launch Axiom Process.
    This is where we will load, select, and run the artifact against the provided
    evidence. Under the Tools menu, we select the Manage custom artifacts option:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Axiom中使用新开发的工件，我们需要采取一些步骤来导入并针对图像运行代码。首先，我们需要启动Axiom Process。这是我们将加载、选择并针对提供的证据运行工件的地方。在工具菜单下，我们选择管理自定义工件选项：
- en: '![](../images/00083.jpeg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00083.jpeg)'
- en: 'Within the Manage custom artifacts window, we will see any existing custom
    artifacts and can import new ones as seen here:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理自定义工件窗口中，我们将看到任何现有的自定义工件，并可以像这样导入新的工件：
- en: '![](../images/00084.jpeg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00084.jpeg)'
- en: 'We will add our custom artifact and the updated Manage custom artifacts window
    should show the name of the artifact:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将添加我们的自定义工件，更新的管理自定义工件窗口应该显示工件的名称：
- en: '![](../images/00085.jpeg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00085.jpeg)'
- en: 'Now we can press OKAY and continue through Axiom, adding the evidence and configuring
    our processing options. When we reach the COMPUTER ARTIFACTS selection, we want
    to confirm that the custom artifact is selected to run. It probably goes without
    saying: we should only run this artifact if the machine is running macOS or has
    a macOS partition on it:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以按下确定并继续进行Axiom，添加证据并配置我们的处理选项。当我们到达计算机工件选择时，我们要确认选择运行自定义工件。可能不用说：我们应该只在机器运行macOS或者在其上有macOS分区时运行这个工件：
- en: '![](../images/00086.jpeg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00086.jpeg)'
- en: 'After completing the remaining configuration options, we can start processing
    the evidence. With processing complete, we run Axiom Examine to review the processed
    results. As seen in the following screenshot, we can navigate to the CUSTOM pane
    of the artifact review and see the parsed columns from the plugin! These columns
    can be sorted and exported using the standard options in Axiom, without any additional
    code on our part:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 完成剩余的配置选项后，我们可以开始处理证据。处理完成后，我们运行Axiom Examine来查看处理结果。如下截图所示，我们可以导航到工件审查的自定义窗格，并看到插件解析的列！这些列可以使用Axiom中的标准选项进行排序和导出，而无需我们额外的代码：
- en: '![](../images/00087.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00087.jpeg)'
- en: Scanning for indicators with YARA
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用YARA扫描指示器
- en: 'Recipe Difficulty: Medium'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 配方难度：中等
- en: 'Python Version: 3.5'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：3.5
- en: 'Operating System: Any'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任何
- en: As a bonus section, we will leverage the powerful **Yet Another Recursive Algorithm**
    (**YARA**) regular-expression engine to scan for files of interest and indicators
    of compromise. YARA is a pattern-matching utility designed for use in malware
    identification and incident response. Many tools use this engine as the backbone
    for identification of likely malicious files. Through this recipe, we learn how
    to take YARA rules, compile them, and match them across one or more folders or
    files. While we will not cover the steps required to form a YARA rule, one can
    learn more about the process from their documentation at [http://yara.readthedocs.io/en/latest/writingrules.html](http://yara.readthedocs.io/en/latest/writingrules.html).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个额外的部分，我们将利用强大的**Yet Another Recursive Algorithm**（**YARA**）正则表达式引擎来扫描感兴趣的文件和妥协指标。YARA是一种用于恶意软件识别和事件响应的模式匹配实用程序。许多工具使用此引擎作为识别可能恶意文件的基础。通过这个示例，我们学习如何获取YARA规则，编译它们，并在一个或多个文件夹或文件中进行匹配。虽然我们不会涵盖形成YARA规则所需的步骤，但可以从他们的文档中了解更多关于这个过程的信息[http://yara.readthedocs.io/en/latest/writingrules.html](http://yara.readthedocs.io/en/latest/writingrules.html)。
- en: Getting started
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'This recipe requires the installation of the third-party library `yara`. All
    other libraries used in this script are present in Python''s standard library.
    This library can be installed with `pip`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例需要安装第三方库`yara`。此脚本中使用的所有其他库都包含在Python的标准库中。可以使用`pip`安装此库：
- en: '[PRE50]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: To learn more about the `yara-python` library, visit [https://yara.readthedocs.io/en/latest/](https://yara.readthedocs.io/en/latest/).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`yara-python`库的信息，请访问[https://yara.readthedocs.io/en/latest/](https://yara.readthedocs.io/en/latest/)。
- en: We can also use projects such as YaraRules ([http://yararules.com](http://yararules.com))
    and use pre-built rules from the industry and VirusShare ([http://virusshare.com](http://virusshare.com))
    to use real malware samples for analysis.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用项目如YaraRules ([http://yararules.com](http://yararules.com))，并使用行业和VirusShare
    ([http://virusshare.com](http://virusshare.com))的预构建规则来使用真实的恶意软件样本进行分析。
- en: How to do it...
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This script has four main developmental steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本有四个主要的开发步骤：
- en: Set up and compile YARA rules.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置和编译YARA规则。
- en: Scan a single file.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描单个文件。
- en: Iterate through directories to process individual files.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历目录以处理单个文件。
- en: Export results to CSV.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果导出到CSV。
- en: How it works...
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This script imports the required libraries to handle argument parsing, file
    and folder iteration, writing CSV spreadsheets, and the `yara` library to compile
    and scan for the YARA rules.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本导入所需的库来处理参数解析、文件和文件夹迭代、编写CSV电子表格，以及`yara`库来编译和扫描YARA规则。
- en: '[PRE51]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This recipe's command-line handler accepts two positional arguments, `yara_rules`
    and `path_to_scan`, which represent the path to the YARA rules and the file or
    folder to scan, respectively. This recipe also accepts one optional argument,
    `output`, which, if supplied, writes the results of the scan to a spreadsheet
    as opposed to the console. Lastly, we pass these values to the `main()` method.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的命令行处理程序接受两个位置参数，`yara_rules`和`path_to_scan`，分别表示YARA规则的路径和要扫描的文件或文件夹。此示例还接受一个可选参数`output`，如果提供，将扫描结果写入电子表格而不是控制台。最后，我们将这些值传递给`main()`方法。
- en: '[PRE52]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In the `main()` function, we accept the path to the `yara` rules, the files
    or folders to scan, and the output file (if any). Since the `yara` rules can be
    a file or directory, we use the `ios.isdir()` method to determine if we use the
    `compile()` method on a whole directory or, if the input is a file, pass it to
    the method using the `filepath` keyword. The `compile()` method reads the rule
    file or files and creates an object that we can match against objects we scan.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main()`函数中，我们接受`yara`规则的路径、要扫描的文件或文件夹以及输出文件（如果有）。由于`yara`规则可以是文件或目录，我们使用`ios.isdir()`方法来确定我们是否在整个目录上使用`compile()`方法，或者如果输入是一个文件，则使用`filepath`关键字将其传递给该方法。`compile()`方法读取规则文件或文件并创建一个我们可以与我们扫描的对象进行匹配的对象。
- en: '[PRE53]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Once the rules are compiled, we perform a similar `if-else` statement to process
    the path to scan. If the input to scan is a directory, we pass it to the `process_directory()`
    function and, otherwise, we use the `process_file()` method. Both take the compiled
    YARA rules and the path to scan and return a list of dictionaries containing any
    matches.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦规则被编译，我们执行类似的`if-else`语句来处理要扫描的路径。如果要扫描的输入是一个目录，我们将其传递给`process_directory()`函数，否则，我们使用`process_file()`方法。两者都使用编译后的YARA规则和要扫描的路径，并返回包含任何匹配项的字典列表。
- en: '[PRE54]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: As you may guess, we will ultimately convert this list of dictionaries to a
    CSV report if the output path was specified, using the columns we define in the
    `columns` list. However, if the output argument is `None`, we write this data
    to the console in a different format instead.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，如果指定了输出路径，我们最终将把这个字典列表转换为CSV报告，使用我们在`columns`列表中定义的列。然而，如果输出参数是`None`，我们将以不同的格式将这些数据写入控制台。
- en: '[PRE55]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The `process_directory()` function essentially iterates through a directory
    and passes each file to the `process_file()` function. This decreases the amount
    of redundant code in the script. Each processed entry that is returned is added
    to the `match_info` list, as the returned object is a list. Once we have processed
    each file, we return the complete list of results to the parent function.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_directory()`函数本质上是遍历目录并将每个文件传递给`process_file()`函数。这减少了脚本中冗余代码的数量。返回的每个处理过的条目都被添加到`match_info`列表中，因为返回的对象是一个列表。一旦我们处理了每个文件，我们将完整的结果列表返回给父函数。'
- en: '[PRE56]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The `process_file()` method uses with the `match()` method of the `yrules` object.
    The returned match object is an iterable containing one or more hits against the
    rules. From the hit, we can extract the rule name, any tags, the offset in the
    file, the string value of the rule, and the string value of the hit. This information,
    plus the file path, will form an entry in the report. Collectively, this information
    is useful in identifying whether the hit is a false positive or is of significance.
    It can also be helpful when fine-tuning YARA rules to ensure only relevant results
    are presented for review.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_file()` 方法使用了 `yrules` 对象的 `match()` 方法。返回的匹配对象是一个可迭代的对象，包含了一个或多个与规则匹配的结果。从匹配结果中，我们可以提取规则名称、任何标签、文件中的偏移量、规则的字符串值以及匹配结果的字符串值。这些信息加上文件路径将形成报告中的一条记录。总的来说，这些信息对于确定匹配结果是误报还是重要的非常有用。在微调
    YARA 规则以确保只呈现相关结果进行审查时也非常有帮助。'
- en: '[PRE57]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: To `write_stdout()` function reports match information to the console if the
    user does not specify an output file. We iterate through each entry in the `match_info`
    list and print each column name and its value from the `match_info` dictionary
    in a colon-delimited, newline-separated format. After each entry, we print `30`
    equals signs to visually separate the entries from each other.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_stdout()` 函数如果用户没有指定输出文件，则将匹配信息报告到控制台。我们遍历 `match_info` 列表中的每个条目，并以冒号分隔、换行分隔的格式打印出
    `match_info` 字典中的每个列名及其值。在每个条目之后，我们打印 `30` 个等号来在视觉上将条目分隔开。'
- en: '[PRE58]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The `write_csv()` method follows the standard convention, using the `DictWriter`
    class to write the headers and all of the data into the sheet. Notice how this
    function is adjusted to handle CSV writing in Python 3, using the `'w'` mode and
    `newline` parameter.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_csv()` 方法遵循标准约定，使用 `DictWriter` 类来写入标题和所有数据到表格中。请注意，这个函数已经调整为在 Python
    3 中处理 CSV 写入，使用了 `''w''` 模式和 `newline` 参数。'
- en: '[PRE59]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Using this code, we can provide the appropriate arguments at the command-line
    and generate a report of any matches. The following screenshot shows the custom
    rules for detecting Python files and keyloggers:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这段代码，我们可以在命令行提供适当的参数，并生成任何匹配的报告。以下截图显示了用于检测 Python 文件和键盘记录器的自定义规则：
- en: '![](../images/00088.jpeg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00088.jpeg)'
- en: 'These rules are shown in the output CSV report, or console if a report is not
    specified, as seen here:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则显示在输出的 CSV 报告中，如果没有指定报告，则显示在控制台中，如下所示：
- en: '![](../images/00089.jpeg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00089.jpeg)'
