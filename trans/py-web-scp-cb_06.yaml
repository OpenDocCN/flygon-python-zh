- en: Scraping Challenges and Solutions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取挑战和解决方案
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Retrying failed page downloads
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重试失败的页面下载
- en: Supporting page redirects
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持页面重定向
- en: Waiting for content to be available in Selenium
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待Selenium中的内容可用
- en: Limiting crawling to a single domain
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将爬行限制为单个域
- en: Processing infinitely scrolling pages
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理无限滚动页面
- en: Controlling the depth of a crawl
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制爬行的深度
- en: Controlling the length of a crawl
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制爬行的长度
- en: Handling paginated websites
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理分页网站
- en: Handling forms and form-based authorization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理表单和基于表单的授权
- en: Handling basic authorization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理基本授权
- en: Preventing bans by scraping via proxies
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过代理防止被禁止爬取
- en: Randomizing user agents
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机化用户代理
- en: Caching responses
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存响应
- en: Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Developing a reliable scraper is never easy, there are so many *what ifs* that
    we need to take into account. What if the website goes down? What if the response
    returns unexpected data? What if your IP is throttled or blocked? What if authentication
    is required? While we can never predict and cover all *what ifs*, we will discuss
    some common traps, challenges, and workarounds.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 开发可靠的爬虫从来都不容易，我们需要考虑很多*假设*。如果网站崩溃了怎么办？如果响应返回意外数据怎么办？如果您的IP被限制或阻止了怎么办？如果需要身份验证怎么办？虽然我们永远无法预测和涵盖所有*假设*，但我们将讨论一些常见的陷阱、挑战和解决方法。
- en: 'Note that several of the recipes require access to a website that I have provided
    as a Docker container. They require more logic than the simple, static site we
    used in earlier chapters. Therefore, you will need to pull and run a Docker container
    using the following Docker commands:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，其中几个配方需要访问我提供的作为Docker容器的网站。它们需要比我们在早期章节中使用的简单静态站点更多的逻辑。因此，您需要使用以下Docker命令拉取和运行Docker容器：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Retrying failed page downloads
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重试失败的页面下载
- en: 'Failed page requests can be easily handled by Scrapy using retry middleware.
    When installed, Scrapy will attempt retries when receiving the following HTTP
    error codes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用重试中间件，Scrapy可以轻松处理失败的页面请求。安装后，Scrapy将在接收以下HTTP错误代码时尝试重试：
- en: '`[500, 502, 503, 504, 408]`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`[500, 502, 503, 504, 408]`'
- en: 'The process can be further configured using the following parameters:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下参数进一步配置该过程：
- en: '`RETRY_ENABLED` (True/False - default is True)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RETRY_ENABLED`（True/False-默认为True）'
- en: '`RETRY_TIMES` (# of times to retry on any errors - default is 2)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RETRY_TIMES`（在任何错误上重试的次数-默认为2）'
- en: '`RETRY_HTTP_CODES` (a list of HTTP error codes which should be retried - default
    is [500, 502, 503, 504, 408])'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RETRY_HTTP_CODES`（应该重试的HTTP错误代码列表-默认为[500, 502, 503, 504, 408]）'
- en: How to do it
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'The `06/01_scrapy_retry.py` script demonstrates how to configure Scrapy for
    retries. The script file contains the following configuration for Scrapy:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`06/01_scrapy_retry.py`脚本演示了如何配置Scrapy进行重试。脚本文件包含了以下Scrapy的配置：'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How it works
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: Scrapy will pick up the configuration for retries as specified when the spider
    is run. When encountering errors, Scrapy will retry up to three times before giving
    up.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy在运行蜘蛛时会根据指定的重试配置进行重试。在遇到错误时，Scrapy会在放弃之前最多重试三次。
- en: Supporting page redirects
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持页面重定向
- en: 'Page redirects in Scrapy are handled using redirect middleware, which is enabled
    by default. The process can be further configured using the following parameters:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy中的页面重定向是使用重定向中间件处理的，默认情况下已启用。可以使用以下参数进一步配置该过程：
- en: '`REDIRECT_ENABLED`: (True/False - default is True)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`REDIRECT_ENABLED`：（True/False-默认为True）'
- en: '`REDIRECT_MAX_TIMES`: (The maximum number of redirections to follow for any
    single request - default is 20)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`REDIRECT_MAX_TIMES`：（对于任何单个请求要遵循的最大重定向次数-默认为20）'
- en: How to do it
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'The script in `06/02_scrapy_redirects.py` demonstrates how to configure Scrapy
    to handle redirects. This configures a maximum of two redirects for any page. Running
    the script reads the NASA sitemap and crawls that content. This contains a large
    number of redirects, many of which are redirects from HTTP to HTTPS versions of
    URLs. There will be a lot of output, but here are a few lines demonstrating the
    output:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`06/02_scrapy_redirects.py`脚本演示了如何配置Scrapy来处理重定向。这为任何页面配置了最多两次重定向。运行该脚本会读取NASA站点地图并爬取内容。其中包含大量重定向，其中许多是从HTTP到HTTPS版本的URL的重定向。输出会很多，但以下是一些演示输出的行：'
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This particular URL was processed after one redirection, from an HTTP to an
    HTTPS version of the URL. The list defines all of the URLs that were involved
    in the redirection.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此特定URL在重定向后被处理，从URL的HTTP版本重定向到HTTPS版本。该列表定义了涉及重定向的所有URL。
- en: 'You will also be able to see where redirection exceeded the specified level
    (2) in the output pages.  The following is one example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您还将能够看到输出页面中重定向超过指定级别（2）的位置。以下是一个例子：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'The spider is defined as the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 蜘蛛被定义为以下内容：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is identical to our previous NASA sitemap based crawler, with the addition
    of one line printing the `redirect_urls`. In any call to `parse`, this metadata
    will contain all redirects that occurred to get to this page.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们之前基于NASA站点地图的爬虫相同，只是增加了一行打印`redirect_urls`。在对`parse`的任何调用中，此元数据将包含到达此页面所发生的所有重定向。
- en: 'The crawling process is configured with the following code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 爬行过程使用以下代码进行配置：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Redirect is enabled by default, but this sets the maximum number of redirects
    to 2 instead of the default of 20.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 重定向默认已启用，但这将将最大重定向次数设置为2，而不是默认值20。
- en: Waiting for content to be available in Selenium
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等待Selenium中的内容可用
- en: A common problem with dynamic web pages is that even after the whole page has
    loaded, and hence the `get()` method in Selenium has returned, there still may
    be content that we need to access later as there are outstanding Ajax requests
    from the page that are still pending completion. An example of this is needing
    to click a button, but the button not being enabled until all data has been loaded
    asyncronously to the page after loading.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 动态网页的一个常见问题是，即使整个页面已经加载完成，因此Selenium中的`get()`方法已经返回，仍然可能有我们需要稍后访问的内容，因为页面上仍有未完成的Ajax请求。这个的一个例子是需要点击一个按钮，但是在加载页面后，直到所有数据都已异步加载到页面后，按钮才被启用。
- en: 'Take the following page as an example: [http://the-internet.herokuapp.com/dynamic_loading/2](http://the-internet.herokuapp.com/dynamic_loading/2).
    This page finishes loading very quickly and presents us with a Start button:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下页面为例：[http://the-internet.herokuapp.com/dynamic_loading/2](http://the-internet.herokuapp.com/dynamic_loading/2)。这个页面加载非常快，然后呈现给我们一个开始按钮：
- en: '![](assets/08dd65a4-9018-4136-9bb9-0b7f74e17aff.png)The Start button presented
    on screen'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/08dd65a4-9018-4136-9bb9-0b7f74e17aff.png)屏幕上呈现的开始按钮'
- en: 'When pressing the button, we are presented with a progress bar for five seconds:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 按下按钮后，我们会看到一个进度条，持续五秒：
- en: '![](assets/530d1355-e1cc-4551-ab0f-9374c029b03e.png)The status bar while waiting'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/530d1355-e1cc-4551-ab0f-9374c029b03e.png)等待时的状态栏'
- en: And when this is completed, we are presented with Hello World!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个完成后，我们会看到Hello World！
- en: '![](assets/d37ade1b-f857-4a5f-a7ce-5157238e9e09.png)After the page is completely
    rendered'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d37ade1b-f857-4a5f-a7ce-5157238e9e09.png)页面完全渲染后'
- en: Now suppose we want to scrape this page to get the content that is exposed only
    after the button is pressed and after the wait? How do we do this?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想要爬取这个页面，以获取只有在按下按钮并等待后才暴露的内容？我们该怎么做？
- en: How to do it
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: We can do this using Selenium. We will use two features of Selenium. The first
    is the ability to click on page elements. The second is the ability to wait until
    an element with a specific ID is available on the page.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Selenium来做到这一点。我们将使用Selenium的两个功能。第一个是点击页面元素的能力。第二个是等待直到页面上具有特定ID的元素可用。
- en: 'First, we get the button and click it. The button''s HTML is the following:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们获取按钮并点击它。按钮的HTML如下：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When the button is pressed and the load completes, the following HTML is added
    to the document:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当按下按钮并加载完成后，以下HTML将被添加到文档中：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will use the Selenium driver to find the Start button, click it, and then
    wait until a `div` with an ID of `'finish'` is available. Then we get that element
    and return the text in the enclosed `<h4>` tag.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用Selenium驱动程序来查找开始按钮，点击它，然后等待直到`div`中的ID为`'finish'`可用。然后我们获取该元素并返回封闭的`<h4>`标签中的文本。
- en: 'You can try this by running `06/03_press_and_wait.py`.  It''s output will be
    the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行`06/03_press_and_wait.py`来尝试这个。它的输出将是以下内容：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now let's see how it worked.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看它是如何工作的。
- en: How it works
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'Let us break down the explanation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下解释：
- en: 'We start by importing the required items from Selenium:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从Selenium中导入所需的项目：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we load the driver and the page:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们加载驱动程序和页面：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With the page loaded, we can retrieve the button:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 页面加载后，我们可以检索按钮：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And then we can click the button:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以点击按钮：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next we create a `WebDriverWait` object:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来我们创建一个`WebDriverWait`对象：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With this object, we can request Selenium''s UI wait for certain events. This
    also sets a maximum wait of 10 seconds. Now using this, we can wait until we meet
    a criterion; that an element is identifiable using the following XPath:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这个对象，我们可以请求Selenium的UI等待某些事件。这也设置了最长等待10秒。现在使用这个，我们可以等到我们满足一个标准；使用以下XPath可以识别一个元素：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When this completes, we can retrieve the h4 element and get its enclosing text:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当这完成后，我们可以检索h4元素并获取其封闭文本：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Limiting crawling to a single domain
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制爬行到单个域
- en: We can inform Scrapy to limit the crawl to only pages within a specified set
    of domains. This is an important task, as links can point to anywhere on the web,
    and we often want to control where crawls end up going. Scrapy makes this very
    easy to do. All that needs to be done is setting the `allowed_domains` field of
    your scraper class.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以通知Scrapy将爬行限制在指定域内的页面。这是一个重要的任务，因为链接可以指向网页的任何地方，我们通常希望控制爬行的方向。Scrapy使这个任务非常容易。只需要设置爬虫类的`allowed_domains`字段即可。 '
- en: How to do it
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: The code for this example is `06/04_allowed_domains.py`. You can run the script
    with your Python interpreter. It will execute and generate a ton of output, but
    if you keep an eye on it, you will see that it only processes pages on nasa.gov.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的代码是`06/04_allowed_domains.py`。您可以使用Python解释器运行脚本。它将执行并生成大量输出，但如果您留意一下，您会发现它只处理nasa.gov上的页面。
- en: How it works
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'The code is the same as previous NASA site crawlers except that we include
    `allowed_domains=[''nasa.gov'']`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 代码与之前的NASA网站爬虫相同，只是我们包括`allowed_domains=['nasa.gov']`：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The NASA site is fairly consistent with staying within its root domain, but
    there are occasional links to other sites such as content on boeing.com. This
    code will prevent moving to those external sites.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: NASA网站在其根域内保持相当一致，但偶尔会有指向其他网站的链接，比如boeing.com上的内容。这段代码将阻止转移到这些外部网站。
- en: Processing infinitely scrolling pages
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理无限滚动页面
- en: Many websites have replaced "previous/next" pagination buttons with an infinite
    scrolling mechanism. These websites use this technique to load more data when
    the user has reached the bottom of the page. Because of this, strategies for crawling
    by following the "next page" link fall apart.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 许多网站已经用无限滚动机制取代了“上一页/下一页”分页按钮。这些网站使用这种技术在用户到达页面底部时加载更多数据。因此，通过点击“下一页”链接进行爬行的策略就会崩溃。
- en: While this would seem to be a case for using browser automation to simulate
    the scrolling, it's actually quite easy to figure out the web pages' Ajax requests
    and use those for crawling instead of the actual page. Let's look at `spidyquotes.herokuapp.com/scroll`
    as an example.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这似乎是使用浏览器自动化来模拟滚动的情况，但实际上很容易找出网页的Ajax请求，并使用这些请求来爬取，而不是实际页面。让我们以`spidyquotes.herokuapp.com/scroll`为例。
- en: Getting ready
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Open [http://spidyquotes.herokuapp.com/scroll](http://spidyquotes.herokuapp.com/scroll)
    in your browser. This page will load additional content when you scroll to the
    bottom of the page:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中打开[http://spidyquotes.herokuapp.com/scroll](http://spidyquotes.herokuapp.com/scroll)。当你滚动到页面底部时，页面将加载更多内容：
- en: '![](assets/5aedcf3b-b3dd-4e67-8328-7093d70c2db4.png)Screenshot of the quotes
    to scrape'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/5aedcf3b-b3dd-4e67-8328-7093d70c2db4.png)要抓取的引用的屏幕截图'
- en: 'Once the page is open, go into your developer tools and select the network
    panel. Then, scroll to the bottom of the page. You will see new content in the
    network panel:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 页面打开后，进入开发者工具并选择网络面板。然后，滚动到页面底部。您将在网络面板中看到新内容：
- en: '![](assets/b8f8c31d-706b-4f11-bab8-901263f7fdfc.png)Screenshot of the developer
    tools options'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/b8f8c31d-706b-4f11-bab8-901263f7fdfc.png)开发者工具选项的屏幕截图'
- en: 'When we click on one of the links, we can see the following JSON:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们点击其中一个链接时，我们可以看到以下JSON：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is great because all we need to do is continually generate requests to
    `/api/quotes?page=x`, increasing `x` until the `has_next` tag exists in the reply
    document. If there are no more pages, then this tag will not be in the document.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这很棒，因为我们只需要不断生成对`/api/quotes?page=x`的请求，增加`x`直到回复文档中存在`has_next`标签。如果没有更多页面，那么这个标签将不在文档中。
- en: How to do it
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'The `06/05_scrapy_continuous.py` file contains a Scrapy agent, which crawls
    this set of pages. Run it with your Python interpreter and you will see output
    similar to the following (the following is multiple excerpts from the output):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`06/05_scrapy_continuous.py`文件包含一个Scrapy代理，它爬取这组页面。使用Python解释器运行它，你将看到类似以下的输出（以下是输出的多个摘录）：'
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: When this gets to page 10 it will stop as it will see that there is no next
    page flag set in the content.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当它到达第10页时，它将停止，因为它会看到内容中没有设置下一页标志。
- en: How it works
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'Let''s walk through the spider to see how this works. The spider starts with
    the following definition of the start URL:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过蜘蛛来看看这是如何工作的。蜘蛛从以下开始URL的定义开始：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The parse method then prints the response and also parses the JSON into the
    data variable:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后解析方法打印响应，并将JSON解析为数据变量：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then it loops through all the items in the quotes element of the JSON objects.
    For each item, it yields a new Scrapy item back to the Scrapy engine:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它循环遍历JSON对象的引用元素中的所有项目。对于每个项目，它将向Scrapy引擎产生一个新的Scrapy项目：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It then checks to see if the data JSON variable has a `''has_next''` property,
    and if so it gets the next page and yields a new request back to Scrapy to parse
    the next page:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它检查数据JSON变量是否具有`'has_next'`属性，如果有，它将获取下一页并向Scrapy产生一个新的请求来解析下一页：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: There's more...
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'It is also possible to process infinite, scrolling pages using Selenium. The
    following code is in `06/06_scrape_continuous_twitter.py`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用Selenium处理无限滚动页面。以下代码在`06/06_scrape_continuous_twitter.py`中：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output would be similar to the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE24]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This code starts by loading the page from Twitter. The call to `.get()` will
    return when the page is fully loaded. The `scrollHeight` is then retrieved, and
    the program scrolls to that height and waits for a moment for the new content
    to load. The `scrollHeight` of the browser is retrieved again, and if different
    than `last_height`, it will loop and continue processing. If the same as `last_height`,
    no new content has loaded and you can then continue on and retrieve the HTML for
    the completed page.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先从Twitter加载页面。调用`.get()`将在页面完全加载时返回。然后检索`scrollHeight`，程序滚动到该高度并等待新内容加载片刻。然后再次检索浏览器的`scrollHeight`，如果与`last_height`不同，它将循环并继续处理。如果与`last_height`相同，则没有加载新内容，然后您可以继续检索已完成页面的HTML。
- en: Controlling the depth of a crawl
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制爬取的深度
- en: The depth of a crawl can be controlled using Scrapy `DepthMiddleware` middleware.
    The depth middleware limits the number of follows that Scrapy will take from any
    given link. This option can be useful for controlling how deep you go into a particular
    crawl. This is also used to keep a crawl from going on too long, and useful if
    you know that the content you are crawling for is located within a certain number
    of degrees of separation from the pages at the start of your crawl.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Scrapy的`DepthMiddleware`中间件来控制爬取的深度。深度中间件限制了Scrapy从任何给定链接获取的跟随数量。这个选项对于控制你深入到特定爬取中有多有用。这也用于防止爬取过长，并且在你知道你要爬取的内容位于从爬取开始的页面的一定数量的分离度内时非常有用。
- en: How to do it
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'The depth control middleware is installed in the middleware pipeline by default. An
    example of depth limiting is contained in the `06/06_limit_depth.py` script. This
    script crawls the static site provided with the source code on port 8080, and
    allows you to configure the depth limit. This site consists of three levels: 0,
    1, and 2, and has three pages at each level. The files are named `CrawlDepth<level><pagenumber>.html`.
    Page 1 on each level links to the other two pages on the same level, as well as
    to the first page on the next level. Links to higher levels end at level 2\. This
    structure is great for examining how depth processing is handled in Scrapy.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 深度控制中间件默认安装在中间件管道中。深度限制的示例包含在`06/06_limit_depth.py`脚本中。该脚本爬取源代码提供的端口8080上的静态站点，并允许您配置深度限制。该站点包括三个级别：0、1和2，并且每个级别有三个页面。文件名为`CrawlDepth<level><pagenumber>.html`。每个级别的第1页链接到同一级别的其他两页，以及下一级别的第1页。到达第2级的链接结束。这种结构非常适合检查Scrapy中如何处理深度处理。
- en: How it works
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'The limiting of depth can be performed by setting the `DEPTH_LIMIT` parameter:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 深度限制可以通过设置`DEPTH_LIMIT`参数来执行：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A depth limit of 1 means we will only crawl one level, which means it will
    process the URLs specified in `start_urls`, and then any URLs found within those
    pages. With `DEPTH_LIMIT` we get the following output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 深度限制为1意味着我们只会爬取一层，这意味着它将处理`start_urls`中指定的URL，然后处理这些页面中找到的任何URL。使用`DEPTH_LIMIT`我们得到以下输出：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The crawl starts with `CrawlDepth0-1.html`. That page has two lines, one to
    `CrawlDepth0-2.html` and one to `CrawlDepth1-1.html`. They are then requested
    to be parsed. Considering that the start page is at depth 0, those pages are at
    depth 1, the limit of our depth. Therefore, we will see those two pages being
    parsed. However, note that all the links from those two pages, although requesting
    to be parsed, are then ignored by Scrapy as they are at depth 2, which exceeds
    the specified limit.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 爬取从`CrawlDepth0-1.html`开始。该页面有两行，一行到`CrawlDepth0-2.html`，一行到`CrawlDepth1-1.html`。然后请求解析它们。考虑到起始页面在深度0，这些页面在深度1，是我们深度的限制。因此，我们将看到这两个页面被解析。但是，请注意，这两个页面的所有链接，虽然请求解析，但由于它们在深度2，超出了指定的限制，因此被Scrapy忽略。
- en: 'Now change the depth limit to 2:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将深度限制更改为2：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output then becomes as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后输出变成如下：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that the three pages previously ignored with `DEPTH_LIMIT` set to 1 are
    now parsed. And now, links found at that depth, such as for the page `CrawlDepth1-3.html`,
    are now ignored as their depth exceeds 2.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，之前被忽略的三个页面，当`DEPTH_LIMIT`设置为1时，现在被解析了。现在，这个深度下找到的链接，比如`CrawlDepth1-3.html`页面的链接，现在被忽略了，因为它们的深度超过了2。
- en: Controlling the length of a crawl
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制爬取的长度
- en: The length of a crawl, in terms of number of pages that can be parsed, can be
    controlled with the `CLOSESPIDER_PAGECOUNT` setting.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 爬取的长度，即可以解析的页面数量，可以通过`CLOSESPIDER_PAGECOUNT`设置来控制。
- en: How to do it
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'We will be using the script in `06/07_limit_length.py`. The script and scraper
    are the same as the NASA sitemap crawler with the addition of the following configuration
    to limit the number of pages parsed to 5:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`06/07_limit_length.py`中的脚本。该脚本和爬虫与NASA站点地图爬虫相同，只是增加了以下配置来限制解析的页面数量为5：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'When this is run, the following output will be generated (interspersed in the
    logging output):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行时，将生成以下输出（在日志输出中交错）：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: Note that we set the page limit to 5, but the example actually parsed 7 pages. 
    The value for `CLOSESPIDER_PAGECOUNT` should be considered a value that Scrapy
    will do as a minimum, but which may be exceeded by a small amount.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将页面限制设置为5，但实际示例解析了7页。`CLOSESPIDER_PAGECOUNT`的值应被视为Scrapy将至少执行的值，但可能会略微超出。
- en: Handling paginated websites
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理分页网站
- en: Pagination breaks large sets of content into a number of pages. Normally, these
    pages have a previous/next page link for the user to click. These links can generally
    be found with XPath or other means and then followed to get to the next page (or
    previous). Let's examine how to traverse across pages with Scrapy. We'll look
    at a hypothetical example of crawling the results of an automated internet search.
    The techniques directly apply to many commercial sites with search capabilities,
    and are easily modified for those situations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 分页将大量内容分成多个页面。通常，这些页面有一个供用户点击的上一页/下一页链接。这些链接通常可以通过XPath或其他方式找到，然后跟随以到达下一页（或上一页）。让我们来看看如何使用Scrapy遍历页面。我们将看一个假设的例子，爬取自动互联网搜索结果。这些技术直接适用于许多具有搜索功能的商业网站，并且很容易修改以适应这些情况。
- en: Getting ready
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will demonstrate handling pagination with an example that crawls a set of
    pages from the website in the provided container.  This website models five pages
    with previous and next links on each page, along with some embedded data within
    each page that we will extract.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将演示如何处理分页，示例将从提供的容器网站中爬取一组页面。该网站模拟了五个页面，每个页面上都有上一页和下一页的链接，以及每个页面中的一些嵌入数据，我们将提取这些数据。
- en: 'The first page of the set can be seen at `http://localhost:5001/pagination/page1.html`. 
    The following image shows this page open, and we are inspecting the Next button:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个集合的第一页可以在`http://localhost:5001/pagination/page1.html`中看到。以下图片显示了这个页面的打开情况，我们正在检查下一页按钮：
- en: '![](assets/2c9af8d6-9b76-47e9-965e-1875830119d4.png)Inspecting the Next button'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2c9af8d6-9b76-47e9-965e-1875830119d4.png)检查下一页按钮'
- en: 'There are two parts of the page that are of interest. The first is the link
    for the Next button. It''s a fairly common practice that this link has a class
    that identifies the link as being for the next page. We can use that info to find
    this link. In this case, we can find it using the following XPath:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 页面有两个感兴趣的部分。第一个是下一页按钮的链接。这个链接通常有一个类来标识链接作为下一页的链接。我们可以使用这个信息来找到这个链接。在这种情况下，我们可以使用以下XPath找到它：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The second item of interest is actually retrieving the data we want from the
    page. On these pages, this is identified by a `<div>` tag with a `class="data"`
    attribute. These pages only have one data item, but in this example of crawling
    the pages resulting in a search, we will pull multiple items.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个感兴趣的部分实际上是从页面中检索我们想要的数据。在这些页面上，这是由具有`class="data"`属性的`<div>`标签标识的。这些页面只有一个数据项，但在这个搜索结果页面爬取的示例中，我们将提取多个项目。
- en: Now let's go and actually run a scraper for these pages.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实际运行这些页面的爬虫。
- en: How to do it
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'There is a script named `06/08_scrapy_pagination.py`. Run this script with
    Python and there will be a lot of output from Scrapy, most of which will be the
    standard Scrapy debugging output. However, within that output you will see that
    we extracted the data items on all five pages:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个名为`06/08_scrapy_pagination.py`的脚本。用Python运行此脚本，Scrapy将输出大量内容，其中大部分将是标准的Scrapy调试输出。但是，在这些输出中，您将看到我们提取了所有五个页面上的数据项：
- en: '[PRE32]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: How it works
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: 'The code begins with the definition of `CrawlSpider` and the start URL:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 代码从定义`CrawlSpider`和起始URL开始：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then the rules field is defined, which informs Scrapy how to parse each page
    to look for links. This code uses the XPath discussed earlier to find the Next
    link in the page. Scrapy will use this rule on every page to find the next page
    to process, and will queue that request for processing after the current page.
    For each page that is found, the callback parameter informs Scrapy which method
    to call for processing, in this case `parse_result_page`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后定义了规则字段，它告诉Scrapy如何解析每个页面以查找链接。此代码使用前面讨论的XPath来查找页面中的下一个链接。Scrapy将使用此规则在每个页面上查找下一个要处理的页面，并将该请求排队等待处理。对于找到的每个页面，回调参数告诉Scrapy调用哪个方法进行处理，在本例中是`parse_result_page`：
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'A single list variable named `all_items` is declared to hold all the items
    we find:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 声明了一个名为`all_items`的单个列表变量来保存我们找到的所有项目：
- en: '[PRE35]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then the `parse_start_url` method is defined. Scrapy will call this to parse
    the initial URL in the crawl. The function simply defers that processing to `parse_result_page`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后定义了`parse_start_url`方法。Scrapy将调用此方法来解析爬行中的初始URL。该函数简单地将处理推迟到`parse_result_page`：
- en: '[PRE36]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `parse_result_page` method then uses XPath to find the text inside of the
    `<h1>` tag within the `<div class="data">` tag. It then appends that text to the
    `all_items` list:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`parse_result_page`方法使用XPath来查找`<div class="data">`标签中`<h1>`标签内的文本。然后将该文本附加到`all_items`列表中：
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Upon the crawl being completed, the `closed()` method is called and writes
    out the content of the `all_items` field:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 爬行完成后，将调用`closed()`方法并写出`all_items`字段的内容：
- en: '[PRE38]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The crawler is run using Python as a script using the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python作为脚本运行爬虫，如下所示：
- en: '[PRE39]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note the use of the `CLOSESPIDER_PAGECOUNT` property being set to `10`. This
    exceeds the number of pages on this site, but in many (or most) cases there will
    likely be thousands of pages in a search result. It's a good practice to stop
    after an appropriate number of pages. This is good behavior a crawler, as the
    relevance of items to your search drops dramatically after a few pages, so crawling
    beyond the first few pages has greatly diminishing returns and it's generally
    best to stop after a few pages.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`CLOSESPIDER_PAGECOUNT`属性被设置为`10`。这超过了该网站上的页面数量，但在许多（或大多数）情况下，搜索结果可能会有数千个页面。在适当数量的页面后停止是一个很好的做法。这是爬虫的良好行为，因为在前几页之后，与您的搜索相关的项目的相关性急剧下降，因此在前几页之后继续爬行会大大减少回报，通常最好在几页后停止。
- en: There's more...
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'As mentioned at the start of the recipe, this is easy to modify for various
    automatic searches on various content sites. This practice can push the limits
    of acceptable use, so it has been generalized here. But for more actual examples,
    visit my blog at: `www.smac.io`.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在本教程开始时提到的，这很容易修改为在各种内容网站上进行各种自动搜索。这种做法可能会推动可接受使用的极限，因此这里进行了泛化。但是，要获取更多实际示例，请访问我的博客：`www.smac.io`。
- en: Handling forms and forms-based authorization
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理表单和基于表单的授权
- en: We are often required to log into a site before we can crawl its content. This
    is usually done through a form where we enter a user name and password, press
    *Enter*, and then granted access to previously hidden content. This type of form
    authentication is often called cookie authorization, as when we authorize, the
    server creates a cookie that it can use to verify that you have signed in.  Scrapy
    respects these cookies, so all we need to do is somehow automate the form during
    our crawl.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要在爬取网站内容之前登录网站。这通常是通过一个表单完成的，我们在其中输入用户名和密码，按*Enter*，然后获得以前隐藏的内容的访问权限。这种类型的表单认证通常称为cookie授权，因为当我们授权时，服务器会创建一个cookie，用于验证您是否已登录。Scrapy尊重这些cookie，所以我们所需要做的就是在爬行过程中自动化表单。
- en: Getting ready
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will crawl a page in the containers web site at the following URL: `http://localhost:5001/home/secured`. 
    On this page, and links from that page, there is content we would like to scrape.
    However, this page is blocked by a login. When opening the page in a browser,
    we are presented with the following login form, where we can enter `darkhelmet`
    as the user name and `vespa` as the password:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在容器网站的页面上爬行以下URL：`http://localhost:5001/home/secured`。在此页面上，以及从该页面链接出去的页面，有我们想要抓取的内容。但是，此页面被登录阻止。在浏览器中打开页面时，我们会看到以下登录表单，我们可以在其中输入`darkhelmet`作为用户名，`vespa`作为密码：
- en: '![](assets/518cef3e-91c8-47a3-b978-020504dcc4ca.png)Username and password credentials
    are entered'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/518cef3e-91c8-47a3-b978-020504dcc4ca.png)输入用户名和密码凭证'
- en: Upon pressing *Enter* we are authenticated and taken to our originally desired
    page.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 按下*Enter*后，我们将得到验证，并被带到最初想要的页面。
- en: There's not a great deal of content there, but the message is enough to verify
    that we have logged in, and our scraper knows that too.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 那里没有太多的内容，但这条消息足以验证我们已经登录，我们的爬虫也知道这一点。
- en: How to do it
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'We proceed with the recipe as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下步骤进行：
- en: 'If you examine the HTML for the sign-in page, you will have noticed the following
    form code:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您检查登录页面的HTML，您会注意到以下表单代码：
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To get the form processors in Scrapy to work, we will need the IDs of the username
    and password fields in this form. They are `Username` and `Password` respectively.
    Now we can create a spider using this information. This spider is in the script
    file, `06/09_forms_auth.py`. The spider definition starts with the following:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使Scrapy中的表单处理器工作，我们需要该表单中用户名和密码字段的ID。它们分别是`Username`和`Password`。现在我们可以使用这些信息创建一个蜘蛛。这个蜘蛛在脚本文件`06/09_forms_auth.py`中。蜘蛛定义以以下内容开始：
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We define two fields in the class, `login_user` and `login_pass`, to hold the
    username we want to use. The crawl will also start at the specified URL.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在类中定义了两个字段`login_user`和`login_pass`，用于保存我们要使用的用户名。爬行也将从指定的URL开始。
- en: 'The `parse` method is then changed to examine if the page contains a login
    form. This is done by using XPath to see if there is an input form of type password
    and with an `id` of `Password`:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后更改`parse`方法以检查页面是否包含登录表单。这是通过使用XPath来查看页面是否有一个类型为密码的输入表单，并且具有`id`为`Password`的方式来完成的：
- en: '[PRE42]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If that field is found, we then return a `FormRequest` to Scrapy, generated
    using its `from_response` method:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果找到了该字段，我们将返回一个`FormRequest`给Scrapy，使用它的`from_response`方法生成：
- en: '[PRE43]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This function is passed the response, and then a dictionary specifying the
    IDs of fields that need data inserted along with those values. A callback is then
    defined to be executed after this FormRequest is executed by Scrapy, and to which
    is passed the content of the resulting form:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个函数接收响应，然后是一个指定需要插入数据的字段的ID的字典，以及这些值。然后定义一个回调函数，在Scrapy执行这个FormRequest后执行，并将生成的表单内容传递给它：
- en: '[PRE44]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This callback simply looks for the words `This page is secured`, which are
    only returned if the login is successful. When running this successfully, we will
    see the following output from our scraper''s print statements:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个回调函数只是寻找单词`This page is secured`，只有在登录成功时才会返回。当成功运行时，我们将从我们的爬虫的打印语句中看到以下输出：
- en: '[PRE45]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: How it works
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: When you create a `FormRequest`, your are instructing Scrapy to construct a
    form POST request on behalf of your process, using the data in the specified dictionary
    as the form parameters in the POST request. It constructs this request and sends
    it to the server.  Upon receipt of the answer in that POST, it calls the specified
    callback function.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建一个`FormRequest`时，您正在指示Scrapy代表您的进程构造一个表单POST请求，使用指定字典中的数据作为POST请求中的表单参数。它构造这个请求并将其发送到服务器。在收到POST的答复后，它调用指定的回调函数。
- en: There's more...
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: This technique is also useful in form entries of many other kinds, not just
    login forms. This can be used to automate, then execute, any type of HTML form
    request, such as making orders, or those used for executing search operations.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在许多其他类型的表单输入中也很有用，不仅仅是登录表单。这可以用于自动化，然后执行任何类型的HTML表单请求，比如下订单，或者用于执行搜索操作的表单。
- en: Handling basic authorization
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理基本授权
- en: 'Some websites use a form of authorization known as *basic authorization*. This
    was popular before other means of authorization, such as cookie auth or OAuth.
    It is also common on corporate intranets and some web APIs. In basic authorization,
    a header is added to the HTTP request. This header, `Authorization`, is passed
    the Basic string and then a base64 encoding of the values `<username>:<password>`. 
    So in the case of darkhelmet, this header would look as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一些网站使用一种称为*基本授权*的授权形式。这在其他授权方式（如cookie授权或OAuth）出现之前很流行。它也常见于企业内部网络和一些Web API。在基本授权中，一个头部被添加到HTTP请求中。这个头部，`Authorization`，传递了Basic字符串，然后是值`<username>:<password>`的base64编码。所以在darkhelmet的情况下，这个头部会如下所示：
- en: '[PRE46]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Note that this is no more secure than sending it in plain-text, (although when
    performed over HTTPS it is secure.) However, for the most part, is has been subsumed
    for more robust authorization forms, and even cookie authorization allows for
    more complex features such as claims:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这并不比以明文发送更安全（尽管通过HTTPS执行时是安全的）。然而，大部分情况下，它已经被更健壮的授权表单所取代，甚至cookie授权允许更复杂的功能，比如声明：
- en: How to do it
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'Supporting basic auth in Scrapy is straightforward. To get this to work for
    a spider and a given site the spider is crawling, simply define the `http_user`,
    `http_pass`, and `name` fields in your scraper. The following demonstrates:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scrapy中支持基本授权是很简单的。要使其对爬虫和爬取的特定网站起作用，只需在您的爬虫中定义`http_user`，`http_pass`和`name`字段。以下是示例：
- en: '[PRE47]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: How it works
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: When the spider crawls any pages on the given site specified by the name, it
    will use the values of `http_user` and `http_pass` to construct the appropriate
    header.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当爬虫爬取由名称指定的网站上的任何页面时，它将使用`http_user`和`http_pass`的值来构造适当的标头。
- en: There's more...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Note, this task is performed by the `HttpAuthMiddleware` module of Scrapy. More
    info on basic authorization is also available at:[ https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication](https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个任务是由Scrapy的`HttpAuthMiddleware`模块执行的。有关基本授权的更多信息也可以在[https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication](https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication)上找到。
- en: Preventing bans by scraping via proxies
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过代理来防止被屏蔽
- en: Sometimes you may get blocked by a site that your are scraping because you are
    identified as a scraper, and sometimes this happens because the webmaster sees
    the scrape requests coming from a uniform IP, at which point they simply block
    access to that IP.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候您可能会因为被识别为爬虫而被屏蔽，有时候是因为网站管理员看到来自统一IP的爬取请求，然后他们会简单地屏蔽对该IP的访问。
- en: To help prevent this problem, it is possible to use proxy randomization middleware
    within Scrapy. There exists a library, `scrapy-proxies`, which implements a proxy
    randomization feature.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助防止这个问题，可以在Scrapy中使用代理随机化中间件。存在一个名为`scrapy-proxies`的库，它实现了代理随机化功能。
- en: Getting ready
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can get `scrapy-proxies` from GitHub at [https://github.com/aivarsk/scrapy-proxies](https://github.com/aivarsk/scrapy-proxies)
    or by installing it using `pip install scrapy_proxies`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从GitHub上获取`scrapy-proxies`，网址为[https://github.com/aivarsk/scrapy-proxies](https://github.com/aivarsk/scrapy-proxies)，或者使用`pip
    install scrapy_proxies`进行安装。
- en: How to do it
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'Use of `scrapy-proxies` is done by configuration. It starts by configuring `DOWNLOADER_MIDDLEWARES`,
    and making sure they have `RetryMiddleware`, `RandomProxy`, and `HttpProxyMiddleware`
    installed. The following would be a typical configuration:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`scrapy-proxies`是通过配置完成的。首先要配置`DOWNLOADER_MIDDLEWARES`，并确保安装了`RetryMiddleware`，`RandomProxy`和`HttpProxyMiddleware`。以下是一个典型的配置：
- en: '[PRE48]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The the `PROXY_LIST` setting is configured to point to a file containing a
    list of proxies:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`PROXY_LIST`设置被配置为指向一个包含代理列表的文件：'
- en: '[PRE49]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Then, we need to let Scrapy know the `PROXY_MODE`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要让Scrapy知道`PROXY_MODE`：
- en: '[PRE50]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'If `PROXY_MODE` is `2`, then you must specify a `CUSTOM_PROXY`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`PROXY_MODE`是`2`，那么您必须指定一个`CUSTOM_PROXY`：
- en: '[PRE51]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: This configuration essentially tells Scrapy that if a request for a page fails
    with any of the `RETRY_HTTP_CODES`, and for up to `RETRY_TIMES` per URL, then
    use a proxy from within the file specified by `PROXY_LIST`, and by using the pattern
    defined by `PROXY_MODE`. With this, you can have Scrapy fail back to any number
    of proxy servers to retry the request from a different IP address and/or port.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置基本上告诉Scrapy，如果对页面的请求失败，并且每个URL最多重试`RETRY_TIMES`次中的任何一个`RETRY_HTTP_CODES`，则使用`PROXY_LIST`指定的文件中的代理，并使用`PROXY_MODE`定义的模式。通过这种方式，您可以让Scrapy退回到任意数量的代理服务器，以从不同的IP地址和/或端口重试请求。
- en: Randomizing user agents
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机化用户代理
- en: Which user agent you use can have an effect on the success of your scraper.
    Some websites will flat out refuse to serve content to specific user agents. This
    can be because the user agent is identified as a scraper that is banned, or the
    user agent is for an unsupported browser (namely Internet Explorer 6).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 您使用的用户代理可能会影响爬虫的成功。一些网站将直接拒绝为特定的用户代理提供内容。这可能是因为用户代理被识别为被禁止的爬虫，或者用户代理是不受支持的浏览器（即Internet
    Explorer 6）的用户代理。
- en: Another reason for control over the scraper is that content may be rendered
    differently by the web server depending on the specified user agent. This is currently
    common for mobile sites, but it can also be used for desktops, to do things such
    as delivering simpler content for older browsers.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对爬虫的控制另一个原因是，根据指定的用户代理，内容可能会在网页服务器上以不同的方式呈现。目前移动网站通常会这样做，但也可以用于桌面，比如为旧版浏览器提供更简单的内容。
- en: Therefore, it can be useful to set the user agent to other values than the defaults.
    Scrapy defaults to a user agent named `scrapybot`. This can be configured by using
    the `BOT_NAME` parameter. If you use Scrapy projects, Scrapy will set the agent
    to the name of your project.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将用户代理设置为默认值以外的其他值可能是有用的。Scrapy默认使用名为`scrapybot`的用户代理。可以通过使用`BOT_NAME`参数进行配置。如果使用Scrapy项目，Scrapy将把代理设置为您的项目名称。
- en: 'For more complicated schemes, there are two popular extensions that can be
    used: `scrapy-fake-agent` and `scrapy-random-useragent`.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的方案，有两个常用的扩展可以使用：`scrapy-fake-agent`和`scrapy-random-useragent`。
- en: How to do it
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'We proceed with the recipe as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下步骤进行操作：
- en: '`scrapy-fake-useragent` is available on GitHub at [https://github.com/alecxe/scrapy-fake-useragent](https://github.com/alecxe/scrapy-fake-useragent),
    and `scrapy-random-useragent` is available at [https://github.com/cnu/scrapy-random-useragent](https://github.com/cnu/scrapy-random-useragent). 
    You can include them using `pip install scrapy-fake-agent` and/or  `pip install
    scrapy-random-useragent`.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`scrapy-fake-useragent`可在GitHub上找到，网址为[https://github.com/alecxe/scrapy-fake-useragent](https://github.com/alecxe/scrapy-fake-useragent)，而`scrapy-random-useragent`可在[https://github.com/cnu/scrapy-random-useragent](https://github.com/cnu/scrapy-random-useragent)找到。您可以使用`pip
    install scrapy-fake-agent`和/或`pip install scrapy-random-useragent`来包含它们。'
- en: '`scrapy-random-useragent` will select a random user agent for each of your
    requests from a file. It is configured in two settings:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`scrapy-random-useragent`将从文件中为每个请求选择一个随机用户代理。它配置在两个设置中：'
- en: '[PRE52]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This disables the existing `UserAgentMiddleware`, and replaces it with the
    implementation provided in `RandomUserAgentMiddleware`. Then, you configure a
    reference to a file containing a list of user agent names:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将禁用现有的`UserAgentMiddleware`，并用`RandomUserAgentMiddleware`中提供的实现来替换它。然后，您需要配置一个包含用户代理名称列表的文件的引用：
- en: '[PRE53]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Once configured, each request will use a random user agent from the file.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置完成后，每个请求将使用文件中的随机用户代理。
- en: '`scrapy-fake-useragent` uses a different model. It retrieves user agents from
    an online database tracking the most common user agents in use. Configuring Scrapy
    for its use is done with the following settings:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`scrapy-fake-useragent`使用了不同的模型。它从在线数据库中检索用户代理，该数据库跟踪使用最普遍的用户代理。配置Scrapy以使用它的设置如下：'
- en: '[PRE54]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: It also has the ability to set the type of user agent used, to values such as
    mobile or desktop, to force selection of user agents in those two categories.
    This is performed using the `RANDOM_UA_TYPE` setting, which defaults to random.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它还具有设置使用的用户代理类型的能力，例如移动或桌面，以强制选择这两个类别中的用户代理。这是使用`RANDOM_UA_TYPE`设置执行的，默认为随机。
- en: If using `scrapy-fake-useragent` with any proxy middleware, then you may want
    to randomize per proxy. This can be done by setting `RANDOM_UA_PER_PROXY` to True.
    Also, you will want to set the priority of `RandomUserAgentMiddleware` to be greater
    than `scrapy-proxies`, so that the proxy is set before being handled.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果使用`scrapy-fake-useragent`与任何代理中间件，那么您可能希望对每个代理进行随机化。这可以通过将`RANDOM_UA_PER_PROXY`设置为True来实现。此外，您还需要将`RandomUserAgentMiddleware`的优先级设置为大于`scrapy-proxies`，以便在处理之前设置代理。
- en: Caching responses
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存响应
- en: Scrapy comes with the ability to cache HTTP requests. This can greatly reduce
    crawling times if pages have already been visited. By enabling the cache, Scrapy
    will store every request and response.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy具有缓存HTTP请求的能力。如果页面已经被访问过，这可以大大减少爬取时间。通过启用缓存，Scrapy将存储每个请求和响应。
- en: How to do it
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'There is a working example in the `06/10_file_cache.py` script. In Scrapy,
    caching middleware is disabled by default. To enable this cache, set `HTTPCACHE_ENABLED` to `True` and `HTTPCACHE_DIR` to
    a directory on the file system (using a relative path will create the directory
    in the project''s data folder). To demonstrate, this script runs a crawl of the
    NASA site, and caches the content. It is configured using the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`06/10_file_cache.py`脚本中有一个可用的示例。在Scrapy中，默认情况下禁用了缓存中间件。要启用此缓存，将`HTTPCACHE_ENABLED`设置为`True`，将`HTTPCACHE_DIR`设置为文件系统上的一个目录（使用相对路径将在项目的数据文件夹中创建目录）。为了演示，此脚本运行了NASA网站的爬取，并缓存了内容。它的配置如下：'
- en: '[PRE55]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We ask Scrapy to cache using files and to create a sub-directory in the current
    folder. We also instruct it to limit the crawl to roughly 500 pages. When running
    this, the crawl will take roughly a minute (depending on your internet speed),
    and there will be roughly 500 lines of output.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求Scrapy使用文件进行缓存，并在当前文件夹中创建一个子目录。我们还指示它将爬取限制在大约500页。运行此操作时，爬取将大约需要一分钟（取决于您的互联网速度），并且大约会有500行的输出。
- en: 'After the first execution, you can see that there is now a `.scrapy` folder
    in your directory that contains the cache data.  The structure will look like
    the following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次执行后，您会发现您的目录中现在有一个`.scrapy`文件夹，其中包含缓存数据。 结构将如下所示：
- en: '![](assets/5c6f662d-595c-46d0-95f7-c118ffe6afc4.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/5c6f662d-595c-46d0-95f7-c118ffe6afc4.png)'
- en: Running the script again will only take a few seconds, and will produce the
    same output/reporting of pages parsed, except that this time the content will
    come from the cache instead of HTTP requests.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行脚本只需要几秒钟，将产生相同的输出/报告已解析的页面，只是这次内容将来自缓存而不是HTTP请求。
- en: There's more...
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There are many configurations and options for caching in Scrapy. By default,
    the cache expiration, specified by `HTTPCACHE_EXPIRATION_SECS`, is set to 0\.
    0 means the cache items never expire, so once written, Scrapy will never request
    that item via HTTP again. Realistically, you will want to set this to some value
    that does expire.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scrapy中有许多缓存的配置和选项。默认情况下，由`HTTPCACHE_EXPIRATION_SECS`指定的缓存过期时间设置为0。 0表示缓存项永远不会过期，因此一旦写入，Scrapy将永远不会通过HTTP再次请求该项。实际上，您可能希望将其设置为某个会过期的值。
- en: File storage for the cache is only one of the options for caching. Items can
    also be cached in DMB and LevelDB by setting the `HTTPCACHE_STORAGE` setting to `scrapy.extensions.httpcache.DbmCacheStorage` or
    `scrapy.extensions.httpcache.LeveldbCacheStorage`, respectively. You could also
    write your own code, to store page content in another type of database or cloud
    storage if you feel so inclined.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 文件存储仅是缓存的选项之一。通过将`HTTPCACHE_STORAGE`设置为`scrapy.extensions.httpcache.DbmCacheStorage`或`scrapy.extensions.httpcache.LeveldbCacheStorage`，也可以将项目缓存在DMB和LevelDB中。如果您愿意，还可以编写自己的代码，将页面内容存储在另一种类型的数据库或云存储中。
- en: 'Finally, we come to cache policy. Scrapy comes with two policies built in:
    Dummy (the default), and RFC2616\. This can be set by changing the `HTTPCACHE_POLICY`
    setting to `scrapy.extensions.httpcache.DummyPolicy` or `scrapy.extensions.httpcache.RFC2616Policy`.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到缓存策略。Scrapy自带两种内置策略：Dummy（默认）和RFC2616。这可以通过将`HTTPCACHE_POLICY`设置更改为`scrapy.extensions.httpcache.DummyPolicy`或`scrapy.extensions.httpcache.RFC2616Policy`来设置。
- en: 'The RFC2616 policy enables HTTP cache-control awareness with operations including
    the following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: RFC2616策略通过以下操作启用HTTP缓存控制意识：
- en: Do not attempt to store responses/requests with no-store cache-control directive
    set
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要尝试存储设置了no-store缓存控制指令的响应/请求
- en: Do not serve responses from cache if no-cache cache-control directive is set
    even for fresh responses
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果设置了no-cache缓存控制指令，即使是新鲜的响应，也不要从缓存中提供响应
- en: Compute freshness lifetime from max-age cache-control directive
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从max-age缓存控制指令计算新鲜度生存期
- en: Compute freshness lifetime from Expires response header
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Expires响应标头计算新鲜度生存期
- en: Compute freshness lifetime from Last-Modified response header (heuristic used
    by Firefox)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Last-Modified响应标头计算新鲜度生存期（Firefox使用的启发式）
- en: Compute current age from Age response header
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Age响应标头计算当前年龄
- en: Compute current age from Date header
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从日期标头计算当前年龄
- en: Revalidate stale responses based on Last-Modified response header
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据Last-Modified响应标头重新验证陈旧的响应
- en: Revalidate stale responses based on ETag response header
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据ETag响应标头重新验证陈旧的响应
- en: Set Date header for any received response missing it
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为任何接收到的响应设置日期标头
- en: Support max-stale cache-control directive in requests
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持请求中的max-stale缓存控制指令
