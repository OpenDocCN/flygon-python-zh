# 评估

# 第1章，为什么要进行GPU编程？

1.  前两个`for`循环遍历每个像素，它们的输出彼此不变；因此我们可以在这两个`for`循环上并行化。第三个`for`循环计算特定像素的最终值，这是固有的递归。

1.  阿姆达尔定律没有考虑在GPU和主机之间传输内存所需的时间。

1.  512 x 512等于262,144像素。这意味着第一个GPU一次只能计算一半像素的输出，而第二个GPU可以一次计算所有像素；这意味着第二个GPU在这里将比第一个GPU快大约两倍。第三个GPU有足够的核心来一次计算所有像素，但正如我们在问题1中看到的那样，额外的核心在这里对我们没有用。因此，对于这个问题，第二个和第三个GPU的速度将是相同的。

1.  将某个代码段通用地指定为与阿姆达尔定律相关的并行化存在一个问题，即假设这段代码的计算时间在处理器数量*N*非常大时接近0。正如我们从上一个问题中看到的，情况并非如此。

1.  首先，一致使用*时间*可能很麻烦，并且可能无法找出程序的瓶颈。其次，分析器可以告诉您从Python的角度来看，所有代码的确切计算时间，因此您可以确定某些库函数或操作系统的后台活动是否有问题，而不是您的代码。

# 第2章，设置GPU编程环境

1.  不，CUDA只支持Nvidia GPU，不支持Intel HD或AMD Radeon

1.  本书只使用Python 2.7示例

1.  设备管理器

1.  lspci

1.  `free`

1.  `.run`

# 第3章，开始使用PyCUDA

1.  是的。

1.  主机/设备之间的内存传输和编译时间。

1.  你可以，但这将取决于你的GPU和CPU设置。

1.  使用C `?`运算符来执行点对点和减少操作。

1.  如果`gpuarray`对象超出范围，其析构函数将被调用，这将自动在GPU上释放它所代表的内存。

1.  `ReductionKernel`可能执行多余的操作，这可能取决于底层GPU代码的结构。*中性元素*将确保没有值因这些多余的操作而改变。

1.  我们应该将`neutral`设置为带符号32位整数的最小可能值。

# 第4章，内核，线程，块和网格

1.  试试看。

1.  并非所有线程都同时在GPU上运行。就像CPU在操作系统中在任务之间切换一样，GPU的各个核心在内核的不同线程之间切换。

1.  O(n/640 log n)，即O(n log n)。

1.  试试看。

1.  实际上，CUDA中没有内部网格级同步，只有块级（使用`__syncthreads`）。我们必须将单个块以上的任何内容与主机同步。

1.  天真：129次加法运算。高效工作：62次加法运算。

1.  同样，如果我们需要在大量块的网格上进行同步，我们不能使用`__syncthreads`。如果我们在主机上进行同步，我们还可以在每次迭代中启动更少的线程，从而为其他操作释放更多资源。

1.  在天真的并行求和的情况下，我们可能只会处理一小部分数据点，这些数据点应该等于或少于GPU核心的总数，这些核心可能适合于块的最大大小（1032）；由于单个块可以在内部同步，我们应该这样做。只有当数据点的数量远远大于GPU上可用核心的数量时，我们才应该使用高效的工作算法。

# 第5章，流，事件，上下文和并发性

1.  两者的性能都会提高；随着线程数量的增加，GPU在两种情况下都达到了峰值利用率，减少了使用流所获得的收益。

1.  是的，你可以异步启动任意数量的内核，并使用`cudaDeviceSynchronize`进行同步。

1.  打开你的文本编辑器，试试看！

1.  高标准差意味着GPU的使用不均匀，有时会过载GPU，有时会过度利用它。低标准差意味着所有启动的操作通常都运行顺利。

1.  i. 主机通常可以处理的并发线程远远少于GPU。ii. 每个线程都需要自己的CUDA上下文。GPU可能会因为上下文过多而不堪重负，因为每个上下文都有自己的内存空间，并且必须处理自己加载的可执行代码。

# 第6章，调试和分析CUDA代码

1.  在CUDA中，内存分配会自动同步。

1.  `lockstep`属性仅在大小为32或更小的单个块中有效。在这里，两个块将在没有任何`lockstep`的情况下正确分歧。

1.  在这里也会发生同样的事情。这个64线程块实际上会被分成两个32线程的warp。

1.  Nvprof可以计时单个内核启动、GPU利用率和流使用；任何主机端的分析器只会看到CUDA主机函数的启动。

1.  Printf通常更容易用于规模较小、内联内核相对较短的项目。如果你编写了一个非常复杂的CUDA内核，有数千行代码，那么你可能会想使用IDE逐行步进和调试你的内核。

1.  这告诉CUDA我们要使用哪个GPU。

1.  `cudaDeviceSynchronize`将确保相互依赖的内核启动和内存复制确实是同步的，并且它们不会在所有必要的操作完成之前启动。

# 第7章，使用Scikit-CUDA库

1.  SBLAH以S开头，因此该函数使用32位实数浮点数。ZBLEH以Z开头，这意味着它使用128位复数浮点数。

1.  提示：设置`trans = cublas._CUBLAS_OP['T']`

1.  提示：使用Scikit-CUDA包装器进行点积，`skcuda.cublas.cublasSdot`

1.  提示：建立在上一个问题的答案基础上。

1.  你可以将cuBLAS操作放在一个CUDA流中，并使用该流的事件对象来精确测量GPU上的计算时间。

1.  由于输入看起来对cuFFT来说很复杂，它将计算所有值作为NumPy。

1.  暗边是由于图像周围的零缓冲造成的。这可以通过在边缘上*镜像*图像来缓解，而不是使用零缓冲。

# 第8章，CUDA设备函数库和Thrust

1.  试试看。(它实际上比你想象的更准确。)

1.  一个应用：高斯分布可以用于向样本添加“白噪声”以增加机器学习中的数据集。

1.  不，因为它们来自不同的种子，如果我们将它们连接在一起，这些列表可能会有很强的相关性。如果我们打算将它们连接在一起，我们应该使用相同种子的子序列。

1.  试试看。

1.  提示：记住矩阵乘法可以被看作是一系列矩阵-向量乘法，而矩阵-向量乘法可以被看作是一系列点积。

1.  `Operator()`用于定义实际函数。

# 第9章，实现深度神经网络

1.  一个问题可能是我们没有对训练输入进行归一化。另一个可能是训练速率太大。

1.  使用较小的训练速率，一组权重可能会收敛得非常缓慢，或者根本不会收敛。

1.  大的训练速率可能导致一组权重过度拟合特定的批处理值或该训练集。此外，它可能导致数值溢出/下溢，就像第一个问题一样。

1.  Sigmoid。

1.  Softmax。

1.  更多更新。

# 第10章，使用已编译的GPU代码

1.  只有EXE文件将包含主机函数，但PTX和EXE都将包含GPU代码。

1.  `cuCtxDestory`。

1.  `printf`带有任意输入参数。(尝试查找`printf`原型。)

1.  使用Ctypes的`c_void_p`对象。

1.  这将允许我们使用原始名称从Ctypes链接到函数。

1.  CUDA会自动同步设备内存分配和设备/主机之间的内存复制。

# 第11章，CUDA性能优化

1.  `atomicExch`是线程安全的事实并不保证所有线程将同时执行此函数（因为在网格中不同的块可以在不同的时间执行）。

1.  大小为100的块将在多个warp上执行，除非我们使用`__syncthreads`，否则块内部将不会同步。因此，`atomicExch`可能会被多次调用。

1.  由于warp默认以锁步方式执行，并且大小为32或更小的块将使用单个warp执行，因此`__syncthreads`是不必要的。

1.  我们在warp内使用天真的并行求和，但除此之外，我们使用`atomicAdd`进行了许多求和，就像我们使用串行求和一样。虽然CUDA自动并行化了许多这些`atomicAdd`调用，但我们可以通过实现高效的并行求和来减少所需的`atomicAdd`调用总数。

1.  肯定是`sum_ker`。很明显PyCUDA的求和没有使用与我们相同的硬件技巧，因为我们在较小的数组上表现更好，但通过扩大尺寸，PyCUDA版本更好的唯一解释是它执行的加法操作更少。

# 第12章，下一步去哪里

1.  两个例子：DNA分析和物理模拟。

1.  两个例子：OpenACC，Numba。

1.  TPU仅用于机器学习操作，缺少呈现图形所需的组件。

1.  以太网。
