["```py\nIn [ ]:\n    import quandl\n\n    QUANDL_API_KEY = 'BCzkk3NDWt7H9yjzx-DY'  # Your own Quandl key here\n    quandl.ApiConfig.api_key = QUANDL_API_KEY\n\n    SYMBOLS = [\n        'AAPL','MMM', 'AXP', 'BA', 'CAT',\n        'CVX', 'CSCO', 'KO', 'DD', 'XOM',\n        'GS', 'HD', 'IBM', 'INTC', 'JNJ',\n        'JPM', 'MCD', 'MRK', 'MSFT', 'NKE',\n        'PFE', 'PG', 'UNH', 'UTX', 'TRV', \n        'VZ', 'V', 'WMT', 'WBA', 'DIS',\n    ]\n\n    wiki_symbols = ['WIKI/%s'%symbol for symbol in SYMBOLS]\n    df_components = quandl.get(\n        wiki_symbols, \n        start_date='2017-01-01', \n        end_date='2017-12-31', \n        column_index=11)\n    df_components.columns = SYMBOLS  # Renaming the columns\n```", "```py\nIn [ ]:\n    filled_df_components = df_components.fillna(method='ffill')\n    daily_df_components = filled_df_components.resample('24h').ffill()\n    daily_df_components = daily_df_components.fillna(method='bfill')\n```", "```py\n$ pip install alpha_vantage\n```", "```py\nIn [ ]:\n    \"\"\"\n    Download the all-time DJIA dataset\n    \"\"\"\n    from alpha_vantage.timeseries import TimeSeries\n\n    # Update your Alpha Vantage API key here...\n    ALPHA_VANTAGE_API_KEY = 'PZ2ISG9CYY379KLI'\n\n    ts = TimeSeries(key=ALPHA_VANTAGE_API_KEY, output_format='pandas')\n    df, meta_data = ts.get_daily_adjusted(symbol='^DJI', outputsize='full')\n```", "```py\nIn [ ]:\n    df.info()\nOut[ ]:\n    <class 'pandas.core.frame.DataFrame'>\n    Index: 4760 entries, 2000-01-03 to 2018-11-30\n    Data columns (total 8 columns):\n    1\\. open                 4760 non-null float64\n    2\\. high                 4760 non-null float64\n    3\\. low                  4760 non-null float64\n    4\\. close                4760 non-null float64\n    5\\. adjusted close       4760 non-null float64\n    6\\. volume               4760 non-null float64\n    7\\. dividend amount      4760 non-null float64\n    8\\. split coefficient    4760 non-null float64\n    dtypes: float64(8)\n    memory usage: 316.1+ KB\n```", "```py\nIn [ ]:\n    df.index\nOut[ ]:\n    Index(['2000-01-03', '2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07',\n           '2000-01-10', '2000-01-11', '2000-01-12', '2000-01-13', '2000-01-14',\n           ...\n           '2018-08-17', '2018-08-20', '2018-08-21', '2018-08-22', '2018-08-23',\n           '2018-08-24', '2018-08-27', '2018-08-28', '2018-08-29', '2018-08-30'],\n          dtype='object', name='date', length=4696)\n```", "```py\nIn [ ]:\n    import pandas as pd\n\n    # Prepare the dataframe\n    df_dji = pd.DataFrame(df['5\\. adjusted close'])\n    df_dji.columns = ['DJIA']\n    df_dji.index = pd.to_datetime(df_dji.index)\n\n    # Trim the new dataframe and resample\n    djia_2017 = pd.DataFrame(df_dji.loc['2017-01-01':'2017-12-31'])\n    djia_2017 = djia_2017.resample('24h').ffill()\n```", "```py\nIn [ ]:\n    from sklearn.decomposition import KernelPCA\n\n    fn_z_score = lambda x: (x - x.mean()) / x.std()\n\n    df_z_components = daily_df_components.apply(fn_z_score)\n    fitted_pca = KernelPCA().fit(df_z_components)\n```", "```py\nIn [ ]:\n    %matplotlib inline\n    import matplotlib.pyplot as plt\n\n    plt.rcParams['figure.figsize'] = (12,8)\n    plt.plot(fitted_pca.lambdas_)\n    plt.ylabel('Eigenvalues')\n    plt.show();\n```", "```py\nIn [ ]:\n    fn_weighted_avg = lambda x: x / x.sum()\n    weighted_values = fn_weighted_avg(fitted_pca.lambdas_)[:5]\nIn [ ]:\n    print(weighted_values)\nOut[ ]:\n    array([0.64863002, 0.13966718, 0.05558246, 0.05461861, 0.02313883])\n```", "```py\nIn [ ]:\n    weighted_values.sum()\nOut[ ]:\n    0.9216371041932268\n```", "```py\nIn [ ]:\n    import numpy as np\n\n    kernel_pca = KernelPCA(n_components=5).fit(df_z_components)\n    pca_5 = kernel_pca.transform(-daily_df_components)\n\n    weights = fn_weighted_avg(kernel_pca.lambdas_)\n    reconstructed_values = np.dot(pca_5, weights)\n\n    # Combine DJIA and PCA index for comparison\n    df_combined = djia_2017.copy()\n    df_combined['pca_5'] = reconstructed_values\n    df_combined = df_combined.apply(fn_z_score)\n    df_combined.plot(figsize=(12, 8));\n```", "```py\nIn [ ]:\n    import quandl\n\n    QUANDL_API_KEY = 'BCzkk3NDWt7H9yjzx-DY'  # Your Quandl key here\n    quandl.ApiConfig.api_key = QUANDL_API_KEY\n\n    df = quandl.get(\n        'CHRIS/CME_GC1', \n        column_index=6,\n        collapse='monthly',\n        start_date='2000-01-01')\n```", "```py\nIn [ ]:\n    df.head()\n```", "```py\nIn [ ] :\n    df_settle = df['Settle'].resample('MS').ffill().dropna()\n\n    df_rolling = df_settle.rolling(12)\n    df_mean = df_rolling.mean()\n    df_std = df_rolling.std()\n```", "```py\nIn [ ] :\n    plt.figure(figsize=(12, 8))\n    plt.plot(df_settle, label='Original')\n    plt.plot(df_mean, label='Mean')\n    plt.legend();\n```", "```py\nIn [ ] :\n    df_std.plot(figsize=(12, 8));\n```", "```py\nIn [ ]:\n    from statsmodels.tsa.stattools import adfuller\n\n    result = adfuller(df_settle)\n    print('ADF statistic: ',  result[0])\n    print('p-value:', result[1])\n\n    critical_values = result[4]\n    for key, value in critical_values.items():\n        print('Critical value (%s): %.3f' % (key, value))\nOut[ ]:\n    ADF statistic:  -1.4017828015895548\n    p-value: 0.5814211232134314\n    Critical value (1%): -3.461\n    Critical value (5%): -2.875\n    Critical value (10%): -2.574\n```", "```py\nIn [ ]:\n    import numpy as np\n\n    df_log = np.log(df_settle)\nIn [ ]:\n    df_log_ma= df_log.rolling(2).mean()\n    df_detrend = df_log - df_log_ma\n    df_detrend.dropna(inplace=True)\n\n    # Mean and standard deviation of detrended data\n    df_detrend_rolling = df_detrend.rolling(12)\n    df_detrend_ma = df_detrend_rolling.mean()\n    df_detrend_std = df_detrend_rolling.std()\n\n    # Plot\n    plt.figure(figsize=(12, 8))\n    plt.plot(df_detrend, label='Detrended')\n    plt.plot(df_detrend_ma, label='Mean')\n    plt.plot(df_detrend_std, label='Std')\n    plt.legend(loc='upper right');\n\n```", "```py\nIn [ ]:\n    from statsmodels.tsa.stattools import adfuller\n\n    result = adfuller(df_detrend)\n    print('ADF statistic: ', result[0])\n    print('p-value: %.5f' % result[1])\n\n    critical_values = result[4]\n    for key, value in critical_values.items():\n        print('Critical value (%s): %.3f' % (key, value))\nOut[ ]:\n    ADF statistic:  -17.04239232215001\n    p-value: 0.00000\n    Critical value (1%): -3.460\n    Critical value (5%): -2.874\n    Critical value (10%): -2.574\n```", "```py\nIn [ ]:\n    df_log_diff = df_log.diff(periods=3).dropna()\n\n    # Mean and standard deviation of differenced data\n    df_diff_rolling = df_log_diff.rolling(12)\n    df_diff_ma = df_diff_rolling.mean()\n    df_diff_std = df_diff_rolling.std()\n\n    # Plot the stationary data\n    plt.figure(figsize=(12, 8))\n    plt.plot(df_log_diff, label='Differenced')\n    plt.plot(df_diff_ma, label='Mean')\n    plt.plot(df_diff_std, label='Std')\n    plt.legend(loc='upper right');\n```", "```py\nIn [ ]:\n    from statsmodels.tsa.stattools import adfuller\n\n    result = adfuller(df_log_diff)\n\n    print('ADF statistic:', result[0])\n    print('p-value: %.5f' % result[1])\n\n    critical_values = result[4]\n    for key, value in critical_values.items():\n        print('Critical value (%s): %.3f' % (key, value))\nOut[ ]:\n    ADF statistic: -2.931684356800213\n    p-value: 0.04179\n    Critical value (1%): -3.462\n    Critical value (5%): -2.875\n    Critical value (10%): -2.574\n```", "```py\nIn [ ]:\n    from statsmodels.tsa.seasonal import seasonal_decompose\n\n    decompose_result = seasonal_decompose(df_log.dropna(), freq=12)\n\n    df_trend = decompose_result.trend\n    df_season = decompose_result.seasonal\n    df_residual = decompose_result.resid\n```", "```py\nIn [ ]:\n    plt.rcParams['figure.figsize'] = (12, 8)\n    fig = decompose_result.plot()\n```", "```py\nIn [ ]:\n    df_log_diff = df_residual.diff().dropna()\n\n    # Mean and standard deviation of differenced data\n    df_diff_rolling = df_log_diff.rolling(12)\n    df_diff_ma = df_diff_rolling.mean()\n    df_diff_std = df_diff_rolling.std()\n\n    # Plot the stationary data\n    plt.figure(figsize=(12, 8))\n    plt.plot(df_log_diff, label='Differenced')\n    plt.plot(df_diff_ma, label='Mean')\n    plt.plot(df_diff_std, label='Std')\n    plt.legend();\n```", "```py\nIn [ ]:\n    from statsmodels.tsa.stattools import adfuller    \n\n    result = adfuller(df_residual.dropna())\n\n    print('ADF statistic:',  result[0])\n    print('p-value: %.5f' % result[1])\n\n    critical_values = result[4]\n    for key, value in critical_values.items():\n        print('Critical value (%s): %.3f' % (key, value))\nOut[ ]:\n    ADF statistic: -6.468683205304995\n    p-value: 0.00000\n    Critical value (1%): -3.463\n    Critical value (5%): -2.876\n    Critical value (10%): -2.574\n```", "```py\nIn [ ]:\n    import itertools    \n    import warnings\n    from statsmodels.tsa.statespace.sarimax import SARIMAX\n\n    warnings.filterwarnings(\"ignore\")\n\n    def arima_grid_search(dataframe, s):\n        p = d = q = range(2)\n        param_combinations = list(itertools.product(p, d, q))\n        lowest_aic, pdq, pdqs = None, None, None\n        total_iterations = 0\n        for order in param_combinations:    \n            for (p, q, d) in param_combinations:\n                seasonal_order = (p, q, d, s)\n                total_iterations += 1\n                try:\n                    model = SARIMAX(df_settle, order=order, \n                        seasonal_order=seasonal_order, \n                        enforce_stationarity=False,\n                        enforce_invertibility=False,\n                        disp=False\n                    )\n                    model_result = model.fit(maxiter=200, disp=False)\n\n                    if not lowest_aic or model_result.aic < lowest_aic:\n                        lowest_aic = model_result.aic\n                        pdq, pdqs = order, seasonal_order\n\n                except Exception as ex:\n                    continue\n\n        return lowest_aic, pdq, pdqs \n```", "```py\nIn [ ]:\n    lowest_aic, order, seasonal_order = arima_grid_search(df_settle, 12)\nIn [ ]:\n    print('ARIMA{}x{}'.format(order, seasonal_order))\n    print('Lowest AIC: %.3f'%lowest_aic)\nOut[ ]:\n    ARIMA(0, 1, 1)x(0, 1, 1, 12)\n    Lowest AIC: 2149.636\n```", "```py\nIn [ ]:\n    model = SARIMAX(\n        df_settle,\n        order=order,\n        seasonal_order=seasonal_order,\n        enforce_stationarity=False,\n        enforce_invertibility=False,\n        disp=False\n    )\n\n    model_results = model.fit(maxiter=200, disp=False)\n    print(model_results.summary())\n```", "```py\n                                 Statespace Model Results                                 \n==========================================================================================\nDep. Variable:                             Settle   No. Observations:                  226\nModel:             SARIMAX(0, 1, 1)x(0, 1, 1, 12)   Log Likelihood               -1087.247\nDate:                            Sun, 02 Dec 2018   AIC                           2180.495\nTime:                                    17:38:32   BIC                           2190.375\nSample:                                02-01-2000   HQIC                          2184.494\n                                     - 11-01-2018                                         \nCovariance Type:                              opg                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nma.L1         -0.1716      0.044     -3.872      0.000      -0.258      -0.085\nma.S.L12      -1.0000    447.710     -0.002      0.998    -878.496     876.496\nsigma2      2854.6342   1.28e+06      0.002      0.998    -2.5e+06    2.51e+06\n===================================================================================\nLjung-Box (Q):                       67.93   Jarque-Bera (JB):                52.74\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               6.98   Skew:                            -0.34\nProb(H) (two-sided):                  0.00   Kurtosis:                         5.43\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n```", "```py\nIn [ ]:\n    model_results.plot_diagnostics(figsize=(12, 8));\n```", "```py\nIn [ ] :\n    model_results.resid.describe()\nOut[ ]:\n   count    223.000000\n    mean       0.353088\n    std       57.734027\n    min     -196.799109\n    25%      -22.036234\n    50%        3.500942\n    75%       22.872743\n    max      283.200000\n    dtype: float64\n```", "```py\nIn [ ]:\n    n = len(df_settle.index)\n    prediction = model_results.get_prediction(\n        start=n-12*5, \n        end=n+5\n    )\n    prediction_ci = prediction.conf_int()\n```", "```py\nIn [ ]:\n    print(prediction_ci.head(3))\nOut[ ]:\n                lower Settle  upper Settle\n    2017-09-01   1180.143917   1396.583325\n    2017-10-01   1204.307842   1420.747250\n    2017-11-01   1176.828881   1393.268289\n```", "```py\nIn  [ ]:\n    plt.figure(figsize=(12, 6))\n\n    ax = df_settle['2008':].plot(label='actual')\n    prediction_ci.plot(\n        ax=ax, style=['--', '--'],\n        label='predicted/forecasted')\n\n    ci_index = prediction_ci.index\n    lower_ci = prediction_ci.iloc[:, 0]\n    upper_ci = prediction_ci.iloc[:, 1]\n\n    ax.fill_between(ci_index, lower_ci, upper_ci,\n        color='r', alpha=.1)\n\n    ax.set_xlabel('Time (years)')\n    ax.set_ylabel('Prices')\n\n    plt.legend()\n    plt.show()\n```"]