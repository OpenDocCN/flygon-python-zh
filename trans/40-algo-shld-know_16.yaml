- en: Large-Scale Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模算法
- en: Large-scale algorithms  are designed to solve gigantic complex problems. The
    characterizing feature of large-scale algorithms is their need to have more than
    one execution engine due to the scale of their data and processing requirements.
    This chapter starts by discussing what types of algorithms are best suited to
    be run in parallel. Then, it discusses the issues related to parallelizing algorithms.
    Next, it presents the  **Compute Unified Device Architecture** (**CUDA**)architecture
    and discusses how a single **graphics processing unit** (**GPU**) or an array
    of GPUs can be used to accelerate the algorithms. It also discusses what changes
    need to be made to the algorithm to effectively utilize the power of the GPU.
    Finally, this chapter discusses cluster computing and discusses how Apache Spark
    creates  **Resilient Distributed Datasets**  (**RDDs**) to create an extremely
    fast parallel implementation of standard algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模算法旨在解决庞大的复杂问题。大规模算法的特征是由于其数据规模和处理要求的缘故，需要多个执行引擎。本章首先讨论了什么类型的算法最适合并行运行。然后，讨论了与并行化算法相关的问题。接下来，介绍了**计算统一设备架构**（**CUDA**）架构，并讨论了如何使用单个**图形处理单元**（**GPU**）或一组GPU来加速算法。还讨论了需要对算法进行哪些更改才能有效利用GPU的性能。最后，本章讨论了集群计算，并讨论了Apache
    Spark如何创建**弹性分布式数据集**（**RDDs**）以创建标准算法的极快并行实现。
- en: By the end of this chapter, you will be able to understand the basic strategies
    related to the design of large-scale algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够理解与设计大规模算法相关的基本策略。
- en: 'The following topics are covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Introduction to large-scale algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模算法介绍
- en: The design of parallel algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行算法的设计
- en: Algorithms to utilize the GPU
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用GPU的算法
- en: Understanding algorithms utilizing cluster computing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用集群计算理解算法
- en: How to use the GPU to run large-scale algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何利用GPU运行大规模算法
- en: How to use the power of clusters to run large-scale algorithms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何利用集群的能力运行大规模算法
- en: Let's start with the introduction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从介绍开始。
- en: Introduction to large-scale algorithms
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模算法介绍
- en: Human beings like to be challenged. For centuries, various human innovations
    have allowed us to solve really complex problems in different ways. From predicting
    the next target area of a locust attack to calculating the largest prime number,
    the methods to provide answers for complex problems around us kept on evolving.
    With the advent of the computer, we found a powerful new way to solve complex
    algorithms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人类喜欢受到挑战。几个世纪以来，各种人类创新使我们能够以不同的方式解决真正复杂的问题。从预测蝗虫袭击的下一个目标区域到计算最大的质数，为我们周围的复杂问题提供答案的方法不断发展。随着计算机的出现，我们发现了一种强大的解决复杂算法的新方法。
- en: Defining a well-designed, large-scale algorithm
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义良好的大规模算法
- en: 'A well-designed, large-scale algorithm has the following two characteristics:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 良好设计的大规模算法具有以下两个特征：
- en: It is designed to handle a huge amount of data and processing requirements using
    an available pool of resources optimally.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它旨在使用现有资源池最佳地处理大量数据和处理需求。
- en: It is scalable. As the problem becomes more complex, it can handle the complexity
    simply by provisioning more resources.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是可扩展的。随着问题变得更加复杂，它可以通过提供更多资源来处理复杂性。
- en: One of the most practical ways of implementing large-scale algorithms is by
    using the divide and conquer strategy, that is, divide the larger problem into
    smaller problems that can be solved independently of each other.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 实现大规模算法的一种最实用的方法是使用分而治之的策略，即将较大的问题分解为可以独立解决的较小问题。
- en: Terminology
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语
- en: Let's look into some of the terminology that can be used to quantify the quality
    of large-scale algorithms.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些用于量化大规模算法质量的术语。
- en: Latency
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 延迟
- en: 'Latency is the end-to-end time taken to perform a single computation. If *Compute[1]*  represents
    a single computation that starts at *t[1]*  and ends at *t[2]*,  then we can say
    the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟是执行单个计算所需的端到端时间。如果*Compute[1]*表示从*t[1]*开始到*t[2]*结束的单个计算，则我们可以说以下内容：
- en: '*Latency = t[2]-t[1]*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*延迟 = t[2]-t[1]*'
- en: Throughput
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 吞吐量
- en: In the context of parallel computing, throughput is the number of single computations
    that can be performed simultaneously. For example, if, at *t[1]*, we can perform
    four simultaneous computations, *C[1]*, *C[2]*, *C[3]*, and C[4], then the throughput
    is four.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行计算的背景下，吞吐量是可以同时执行的单个计算的数量。例如，如果在*t[1]*时，我们可以同时执行四个计算，*C[1]*，*C[2]*，*C[3]*和*C[4]*，那么吞吐量为四。
- en: Network bisection bandwidth
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络双分带宽
- en: The bandwidth between two equal parts of a network is called the  **network
    bisection bandwidth**. For distributed computing to work efficiently, this is
    the most important parameter to consider. If we do not have enough network bisection
    bandwidth, the benefits gained by the availability of multiple execution engines
    in distributed computing will be overshadowed by slow communication links.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中两个相等部分之间的带宽称为**网络双分带宽**。对于分布式计算要有效工作，这是最重要的参数。如果我们没有足够的网络双分带宽，分布式计算中多个执行引擎的可用性带来的好处将被慢速通信链路所掩盖。
- en: Elasticity
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性
- en: The ability of the infrastructure to react to a sudden increase in processing
    requirements by provisioning more resources is called elasticity.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施对突然增加的处理需求做出反应并通过提供更多资源来满足需求的能力称为弹性。
- en: The three cloud computing giants, Google, Amazon, and Microsoft can provide
    highly elastic infrastructures. Due to the gigantic size of their shared resource
    pools, there are very few companies that have the potential to match the elasticity
    of infrastructure of these three companies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 三大云计算巨头，谷歌、亚马逊和微软可以提供高度弹性的基础设施。由于它们共享资源池的巨大规模，很少有公司有潜力与这三家公司的基础设施弹性相匹敌。
- en: If the infrastructure is elastic, it can create a scalable solution to the problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果基础设施是弹性的，它可以为问题创建可扩展的解决方案。
- en: The design of parallel algorithms
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行算法的设计
- en: It is important to note that parallel algorithms are not a silver bullet. Even
    the best designed parallel architectures may not give the performance that we
    may expect. One law that is widely used to design parallel algorithms is Amdahl's
    law.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，并行算法并不是万能的。即使设计最好的并行架构也可能无法达到我们期望的性能。广泛使用的一个定律来设计并行算法是安达尔定律。
- en: Amdahl's law
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安达尔定律
- en: 'Gene Amdahl was one of the first people who studied parallel processing in
    the 1960s. He proposed Amdahl''s law, which is still applicable today and can
    become a basis to understand the various trade-offs involved when designing a
    parallel computing solution. Amdahl''s law can be explained as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Gene Amdahl是20世纪60年代研究并行处理的第一批人之一。他提出了安达尔定律，这个定律至今仍然适用，并可以成为理解设计并行计算解决方案时涉及的各种权衡的基础。安达尔定律可以解释如下：
- en: It is based on the concept that in any computing process, not all of the processes
    can be executed in parallel. There will be a sequential portion of the process
    that cannot be parallelized.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于这样一个概念，即在任何计算过程中，并非所有过程都可以并行执行。将会有一个无法并行化的顺序部分。
- en: Let's look at a particular example. Assume that we want to read a large number
    of files stored on a computer and want to train a machine learning model using
    the data found in these files.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子。假设我们想要读取存储在计算机上的大量文件，并使用这些文件中的数据训练机器学习模型。
- en: 'This whole process is called P. It is obvious that P can be divided into the
    following two subprocesses:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程称为P。很明显，P可以分为以下两个子过程：
- en: '*P1*: Scan the files in the directory, create a list of filenames that matches
    the input file, and pass it on.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P1*：扫描目录中的文件，创建与输入文件匹配的文件名列表，并传递它。'
- en: '*P2*: Read the files, create the data processing pipeline, process the files,
    and train the model.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P2*：读取文件，创建数据处理管道，处理文件并训练模型。'
- en: Conducting sequential process analysis
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行顺序过程分析
- en: 'The time to run *P* is represented by *T[seq]**(P)*. The times to run *P1*
    and *P2* are represented by *T[seq](P1)* and *T[seq](P2)*. It is obvious that,
    when running on a single node, we can observe two things:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 运行*P*的时间由*T[seq]**(P)*表示。运行*P1*和*P2*的时间由*T[seq](P1)*和*T[seq](P2)*表示。很明显，当在单个节点上运行时，我们可以观察到两件事：
- en: '*P2* cannot start running before P1 is complete. This is represented by *P1*
    -- > *P2*'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P2*在*P1*完成之前无法开始运行。这由*P1* --> *P2*表示'
- en: '*T[seq](P) = T[seq](P1) + T[seq](P2)*'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T[seq](P) = T[seq](P1) + T[seq](P2)*'
- en: 'Let''s assume that P overall takes 10 minutes to run on a single node. Out
    of these 10 minutes, P1 takes 2 minutes to run and P2 takes 8 minutes to run on
    a single node. This is shown in the following diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设P在单个节点上运行需要10分钟。在这10分钟中，P1需要2分钟运行，P2需要8分钟在单个节点上运行。如下图所示：
- en: '![](assets/28852de3-0a22-4dbc-877e-ac90073c894e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/28852de3-0a22-4dbc-877e-ac90073c894e.png)'
- en: Now the important thing to note is that *P1* is sequential in nature. We cannot
    make it faster by making it parallel. On the other hand, *P2* can easily be split
    into parallel subtasks that can run in parallel. So, we can make it run faster
    by running it in parallel.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在要注意的重要事情是*P1*的性质是顺序的。我们不能通过并行化来加快它。另一方面，*P2*可以很容易地分成可以并行运行的并行子任务。因此，我们可以通过并行运行它来加快运行速度。
- en: The major benefit of using cloud computing is the availability of a large pool
    of resources and many of them are used in parallel. The plan to use these resources
    for a given problem is called an execution plan. Amdahl's law is used comprehensively
    to identify the bottlenecks for a given problem and a pool of resources.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云计算的主要好处是拥有大量资源池，其中许多资源可以并行使用。使用这些资源解决问题的计划称为执行计划。安达尔定律被广泛用于识别给定问题和资源池的瓶颈。
- en: Conducting parallel execution analysis
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行并行执行分析
- en: 'If we want to use more than one node to speed up *P*, it will only affect *P2*
    by a factor of *s>1*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要使用多个节点加速*P*，它只会影响*P2*，乘以一个大于1的因子*s>1*：
- en: '![](assets/fa78d698-b932-406a-9a95-6bb07cebff10.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/fa78d698-b932-406a-9a95-6bb07cebff10.png)'
- en: 'The speedup of the process P can be easily calculated as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 过程P的加速可以很容易地计算如下：
- en: '![](assets/e9f45310-5be0-4c7b-b3e3-37f68fc8ba09.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e9f45310-5be0-4c7b-b3e3-37f68fc8ba09.png)'
- en: 'The ratio of the parallelizable portion of a process to its total is represented
    by *b* and is calculated as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 进程的可并行部分与其总体的比例由*b*表示，并计算如下：
- en: '![](assets/b0a3962b-be67-47a2-8678-7d948493ec94.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b0a3962b-be67-47a2-8678-7d948493ec94.png)'
- en: For example, in the preceding scenario, *b = 8/10 = 0.8.*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在前面的情景中，*b = 8/10 = 0.8*。
- en: 'Simplifying these equations will give us Amdahl''s law:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 简化这些方程将给我们安达尔定律：
- en: '![](assets/65eafd8e-1313-4267-b1c7-b65390f7e3fa.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/65eafd8e-1313-4267-b1c7-b65390f7e3fa.png)'
- en: 'Here, we have the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '*P* is the overall process.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*是整个过程。'
- en: '*b* is the ratio of the parallelizable portion of *P*.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*是*P*的可并行部分的比例。'
- en: '*s* is the speedup achieved in the parallelizable portion of *P*.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s*是在*P*的可并行部分实现的加速。'
- en: 'Let''s assume that we plan to run the process P on three parallel nodes:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们计划在三个并行节点上运行过程P：
- en: '*P1* is the sequential portion and cannot be reduced by using parallel nodes.
    It will remain at 2 seconds.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P1*是顺序部分，不能通过使用并行节点来减少。它将保持在2秒。'
- en: '*P2* now takes 3 seconds instead of 9 seconds.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P2*现在需要3秒而不是9秒。'
- en: 'So, the total time taken by process *P* is reduced to 5 seconds, as shown in
    the following diagram:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*P*的总运行时间减少到5秒，如下图所示：
- en: '![](assets/26d817e9-bdb3-4a6e-98cd-7ef4073ea5d4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/26d817e9-bdb3-4a6e-98cd-7ef4073ea5d4.png)'
- en: 'In the preceding example, we can calculate the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们可以计算以下内容：
- en: '*n[p]*  = the number of processors = 3'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n[p]* = 处理器的数量 = 3'
- en: '*b* = the parallel portion = 9/11 = 81.81%'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* = 并行部分 = 9/11 = 81.81%'
- en: '*s* = the speedup = 3'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s* = 速度提升 = 3'
- en: 'Now, let''s look at a typical graph that explains Amdahl''s law:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一个典型的图表，解释阿姆达尔定律：
- en: '![](assets/6075c179-8dae-4a80-aa66-27dd2f492939.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6075c179-8dae-4a80-aa66-27dd2f492939.png)'
- en: In the preceding diagram, we draw the graph between *s* and *n*[*p*] for different
    values of *b*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们绘制了不同*b*值的*s*和*n*[*p*]之间的图表。
- en: Understanding task granularity
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解任务粒度
- en: When we parallelize an algorithm, a larger job is divided into multiple parallel
    tasks. It is not always straightforward to find the optimal number of parallel
    tasks into which a job should be divided. If there are too few parallel tasks,
    we will not get much benefit from parallel computing. If there are too many tasks,
    then it will generate too much overhead.  This is a challenge also termed as task
    granularity.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们并行化算法时，一个更大的任务被分成多个并行任务。确定任务应该被分成的最佳并行任务数量并不总是直截了当的。如果并行任务太少，我们将无法从并行计算中获得太多好处。如果任务太多，那么将会产生太多的开销。这也是一个被称为任务粒度的挑战。
- en: Load balancing
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载平衡
- en: In parallel computing, a scheduler is responsible for selecting the resources
    to execute the tasks. Optimal load balancing is a difficult thing to achieve,
    in the absence of which, the resources are not fully utilized.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行计算中，调度程序负责选择执行任务的资源。在没有实现最佳负载平衡的情况下，资源无法充分利用。
- en: Locality issues
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部性问题
- en: In parallel processing, the movement of data should be discouraged. Whenever
    possible, instead of moving data, it should be processed locally on the node where
    it resides, otherwise, it reduces the quality of the parallelization.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行处理中，应该避免数据的移动。在可能的情况下，应该在数据所在的节点上本地处理数据，否则会降低并行化的质量。
- en: Enabling concurrent processing in Python
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中启用并发处理
- en: The easiest way to enable parallel processing in Python is by cloning a current
    process that will start a new concurrent process called the  **child process**.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中启用并行处理的最简单方法是克隆一个当前进程，这将启动一个名为**子进程**的新并发进程。
- en: Python programmers, although not biologists, have created their own process
    of cloning. Just like a cloned sheep, the clone copy is the exact copy of the
    original process.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Python程序员，虽然不是生物学家，但已经创造了他们自己的克隆过程。就像克隆的羊一样，克隆副本是原始过程的精确副本。
- en: Strategizing multi-resource processing
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 制定多资源处理策略
- en: 'Initially, large-scale algorithms were used to run on huge machines called  **supercomputers**.
    These supercomputers shared the same memory space. The resources were all local—physically
    placed in the same machine. It means that the communications between the various
    processors were very fast and they were able to share the same variable through
    the common memory space. As the systems evolved and the need to run large-scale
    algorithms grew, the supercomputers evolved into  **Distributed Shared Memory**  (**DSM**)
    where each processing node used to own a portion of the physical memory. Eventually,
    clusters were developed, which are loosely coupled and rely on message passing
    among processing nodes. For large-scale algorithms, we need to find more than
    one execution engines running in parallel to solve a complex problem:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，大规模算法是在称为**超级计算机**的巨大机器上运行的。这些超级计算机共享相同的内存空间。资源都是本地的——物理上放置在同一台机器上。这意味着各种处理器之间的通信非常快，它们能够通过共同的内存空间共享相同的变量。随着系统的发展和运行大规模算法的需求增长，超级计算机演变成了**分布式共享内存**（**DSM**），其中每个处理节点都拥有一部分物理内存。最终，发展出了松散耦合的集群，依赖处理节点之间的消息传递。对于大规模算法，我们需要找到多个并行运行的执行引擎来解决复杂的问题：
- en: '![](assets/6b9c2b35-14a6-4162-a4d8-6b6a6cd4274f.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6b9c2b35-14a6-4162-a4d8-6b6a6cd4274f.png)'
- en: 'There are three strategies to have more than one execution engine:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种策略可以拥有多个执行引擎：
- en: '**Look within**:  Exploit the resources already on the computer. Use the hundreds
    of cores of the GPU to run a large-scale algorithm.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向内寻找**：利用计算机上已有的资源。使用GPU的数百个核心来运行大规模算法。'
- en: '**Look outside**:  Use distributed computing to find more computing resources
    that can be collectively used to solve the large-scale problem at hand.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向外寻找**：使用分布式计算来寻找更多的计算资源，这些资源可以共同用于解决手头的大规模问题。'
- en: '**Hybrid strategy**: Use distributed computing  and, on each  of the nodes,
    use the GPU or an array of GPUs to expedite the running of the algorithm.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合策略**：使用分布式计算，并在每个节点上使用GPU或GPU阵列来加速算法的运行。'
- en: Introducing CUDA
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍CUDA
- en: 'GPUs have been designed originally for graphic processing. They have been designed
    to suit the needs of optimization dealing with the multimedia data of a typical
    computer. To do so, they have developed certain characteristics that differentiate
    them from CPUs. For example, they have thousands of cores as compared to the limited
    number of CPU cores. Their clock speed is much slower than a CPU. A GPU has its
    own DRAM. For example, Nvidia''s RTX 2080 has 8GB of RAM.  Note that GPUs are
    specialized processing devices and do not have general processing unit features,
    including interrupts or means of addressing devices, for example, a keyboard and
    mouse. Here is the architecture of GPUs:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: GPU最初是为图形处理而设计的。它们被设计来满足处理典型计算机的多媒体数据的优化需求。为此，它们开发了一些特性，使它们与CPU有所不同。例如，它们有成千上万的核心，而CPU核心数量有限。它们的时钟速度比CPU慢得多。GPU有自己的DRAM。例如，Nvidia的RTX
    2080有8GB的RAM。请注意，GPU是专门的处理设备，没有通用处理单元的特性，包括中断或寻址设备的手段，例如键盘和鼠标。以下是GPU的架构：
- en: '![](assets/3f8ba45d-46a9-4345-b8ff-08ef34554310.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3f8ba45d-46a9-4345-b8ff-08ef34554310.png)'
- en: Soon after GPUs became mainstream, data scientists started exploring GPUs for
    their potential to efficiently perform parallel operations. As a typical GPU has
    thousands of ALUs, it has the potential to spawn 1,000s concurrent processes.
    This makes GPUs the architecture optimized for data-parallel computation. Hence,
    algorithms that can perform parallel computations are best suited for GPUs. For
    example, an object search in a video is known to be at least 20 times faster in
    GPUs, compared to CPUs. Graph algorithms, which were discussed in [Chapter 5](051e9b32-f15f-4e88-a63a-ae3c14696492.xhtml)
    , *Graph Algorithms*, are known to run much faster on GPUs than on CPUs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: GPU成为主流后不久，数据科学家开始探索GPU在高效执行并行操作方面的潜力。由于典型的GPU具有数千个ALU，它有潜力产生数千个并发进程。这使得GPU成为优化数据并行计算的架构。因此，能够执行并行计算的算法最适合于GPU。例如，在视频中进行对象搜索，GPU的速度至少比CPU快20倍。图算法在[第5章](051e9b32-f15f-4e88-a63a-ae3c14696492.xhtml)
    *图算法*中讨论过，已知在GPU上比在CPU上运行得快得多。
- en: 'To realize the dreams of data scientists to fully utilize GPUs for algorithms,
    in 2007, Nvidia created an open source framework called CUDA, which stands for
    Compute Unified Device Architecture.  CUDA abstracts  the working of the CPU and
    GPU as a host and device respectively. The host, that is, the CPU, is responsible
    for calling the device, which is the GPU. The CUDA architecture has various layers
    of abstractions that can be presented as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现数据科学家充分利用GPU进行算法的梦想，Nvidia在2007年创建了一个名为CUDA的开源框架，全称为Compute Unified Device
    Architecture。CUDA将CPU和GPU的工作抽象为主机和设备。主机，即CPU，负责调用设备，即GPU。CUDA架构有各种抽象层，可以表示为以下形式：
- en: '![](assets/fe199de5-75f7-4fa7-8815-0eb6a9630652.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/fe199de5-75f7-4fa7-8815-0eb6a9630652.png)'
- en: Note that CUDA runs on top of Nvidia's GPUs. It needs support in the OS kernel.
    CUDA initially started with support in the Linux kernel. More recently, Windows
    is now fully supported. Then, we have the CUDA Driver API that acts as a bridge
    between the programming language API and the CUDA driver. On the top layer, we
    have support for C, C+, and Python.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，CUDA在Nvidia的GPU上运行。它需要在操作系统内核中得到支持。最近，Windows现在也得到了全面支持。然后，我们有CUDA Driver
    API，它充当编程语言API和CUDA驱动程序之间的桥梁。在顶层，我们支持C、C+和Python。
- en: Designing parallel algorithms on CUDA
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在CUDA上设计并行算法
- en: Let's look deeper into how the GPU accelerates certain processing operations.
    As we know, CPUs are designed for the sequential execution of data that results
    in significant running time for certain classes of applications. Let's look into
    the example of processing an image of a size of 1,920 x 1,200\. It can be calculated
    that there are 2,204,000 pixels to process. Sequential processing means that it
    will take a long time to process them on a traditional CPU. Modern GPUs such as
    Nvidia's Tesla are capable of spawning this unbelievable amount of 2,204,000 parallel
    threads to process the pixels. For most multimedia applications, the pixels can
    be processed independently of each other and will achieve a significant speedup.
    If we map each pixel with a thread, they can all be processed in O(1) constant
    time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解GPU如何加速某些处理操作。我们知道，CPU设计用于顺序执行数据，这导致某些类别的应用程序运行时间显著增加。让我们以处理尺寸为1,920
    x 1,200的图像为例。可以计算出有2,204,000个像素需要处理。顺序处理意味着在传统CPU上处理它们需要很长时间。像Nvidia的Tesla这样的现代GPU能够产生惊人数量的2,204,000个并行线程来处理像素。对于大多数多媒体应用程序，像素可以独立地进行处理，并且会实现显著加速。如果我们将每个像素映射为一个线程，它们都可以在O(1)常数时间内进行处理。
- en: 'But image processing is not the only application where we can use data parallelism
    to speed up the process. Data parallelism can be used in preparing data for machine
    learning libraries. In fact, the GPU can massively reduce the execution time of
    parallelizable algorithms, which include the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但图像处理并不是唯一可以利用数据并行性加速处理的应用。数据并行性可以用于为机器学习库准备数据。事实上，GPU可以大大减少可并行化算法的执行时间，包括以下内容：
- en: Mining money for bitcoins
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为比特币挖矿
- en: Large-scale simulations
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模模拟
- en: DNA analysis
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNA分析
- en: Video and photos analysis
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频和照片分析
- en: GPUs are not designed for **Single Program, Multiple Data** (**SPMD**). For
    example, if we want to calculate the hash for a block of data, it is a single
    program that cannot run in parallel. GPUs will perform slower in such scenarios.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: GPU不适用于**单程序，多数据**（**SPMD**）。例如，如果我们想要计算一块数据的哈希值，这是一个无法并行运行的单个程序。在这种情况下，GPU的性能会较慢。
- en: The code that we want to run on the GPU is marked with special CUDA keywords
    called **kernels**. These kernels are used to mark the functions that we intend
    to run on GPUs for parallel processing. Based on the kernels, the GPU compiler
    separates which code needs to run on the GPU and the CPU.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要在GPU上运行的代码使用特殊的CUDA关键字标记为**内核**。这些内核用于标记我们打算在GPU上并行处理的函数。基于这些内核，GPU编译器分离出需要在GPU和CPU上运行的代码。
- en: Using GPUs for data processing in Python
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中使用GPU进行数据处理
- en: 'GPUs are great for data processing in a multidimensional data structure. These
    data structures are inherently parallelizable. Let''s see how we can use the GPU
    for multidimensional data processing in Python:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: GPU在多维数据结构的数据处理中非常出色。这些数据结构本质上是可并行化的。让我们看看如何在Python中使用GPU进行多维数据处理：
- en: 'First, let''s import the Python packages that are needed:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入所需的Python包：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will be using a multidimensional array in NumPy, which is a traditional Python
    package that uses the CPU.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用NumPy中的多维数组，这是一个传统的使用CPU的Python包。
- en: 'Then, we create a multidimensional array using a CuPy array, which uses the
    GPU. Then, we will compare the timings:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用CuPy数组创建一个多维数组，它使用GPU。然后，我们将比较时间：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If we will run this code, it will generate the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行这段代码，它将生成以下输出：
- en: '![](assets/20d041ea-1df5-4075-898e-dd19cfe68e37.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/20d041ea-1df5-4075-898e-dd19cfe68e37.png)'
- en: Note that it took around 1.13 seconds to create this array in NumPy and around
    0.012 in CuPy, which makes the initialization of this array 92 times faster in
    the GPU.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用NumPy创建此数组大约需要1.13秒，而使用CuPy只需要大约0.012秒，这使得在GPU中初始化此数组的速度快了92倍。
- en: Cluster computing
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群计算
- en: Cluster computing is one of the ways of implementing parallel processing for
    large-scale algorithms. In cluster computing, we have multiple nodes connected
    via a very high-speed network. Large-scale algorithms are submitted as jobs. Each
    job is divided into various tasks and each task is run on a separate node.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 集群计算是实现大规模算法并行处理的一种方式。在集群计算中，我们有多个通过高速网络连接的节点。大规模算法被提交为作业。每个作业被分成各种任务，并且每个任务在单独的节点上运行。
- en: 'Apache Spark is one of the most popular ways of implementing cluster computing.
    In Apache Spark, the data is converted into distributed fault-tolerant datasets,
    which are  called  **Resilient Distributed Datasets**  (**RDDs**). RDDs are the
    core Apache Spark abstraction. They are immutable collections of elements that
    can be operated in parallel. They are split into partitions and are distributed
    across the nodes, as shown here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是实现集群计算的最流行方式之一。在Apache Spark中，数据被转换为分布式容错数据集，称为**Resilient Distributed
    Datasets**（**RDDs**）。RDDs是Apache Spark的核心抽象。它们是不可变的元素集合，可以并行操作。它们被分割成分区，并分布在节点之间，如下所示：
- en: '![](assets/88c9702a-23ec-45b0-a223-4050254b50e1.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/88c9702a-23ec-45b0-a223-4050254b50e1.png)'
- en: Through this parallel data structure, we can run algorithms in parallel.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种并行数据结构，我们可以并行运行算法。
- en: Implementing data processing in Apache Spark
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Apache Spark中实现数据处理
- en: 'Let''s see how we can create an RDD in Apache Spark and run distributed processing
    on it across the cluster:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Apache Spark中创建RDD并在整个集群上运行分布式处理：
- en: 'For this, first, we need to create a new Spark session, as follows:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，首先，我们需要创建一个新的Spark会话，如下所示：
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once we have created a Spark session, we use a CSV file for the source of the
    RDD. Then, we will run the following function—it will create an RDD that is abstracted
    as a DataFrame called `df`. The ability to abstract an RDD as a DataFrame was
    added in Spark 2.0 and this makes it easier to process the data:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们创建了一个Spark会话，我们就可以使用CSV文件作为RDD的来源。然后，我们将运行以下函数-它将创建一个被抽象为名为`df`的DataFrame的RDD。在Spark
    2.0中添加了将RDD抽象为DataFrame的功能，这使得处理数据变得更加容易：
- en: '[PRE3]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s look into the columns of the DataFrame:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看DataFrame的列：
- en: '![](assets/ffc20c87-5cdf-4c78-a235-2483f9c50655.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ffc20c87-5cdf-4c78-a235-2483f9c50655.png)'
- en: 'Next, we can create a temporary table from the DataFrame, as follows:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以从DataFrame创建一个临时表，如下所示：
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the temporary table is created, we can run SQL statements to process the
    data:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦临时表创建完成，我们就可以运行SQL语句来处理数据：
- en: '![](assets/dada91d3-357d-4523-9c0d-3fb41a6d2691.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dada91d3-357d-4523-9c0d-3fb41a6d2691.png)'
- en: An important point to note is that although it looks like a regular DataFrame,
    it is just a high-level data structure. Under the hood, it is the RDD that spreads
    data across the cluster. Similarly, when we run SQL functions, under the hood,
    they are converted into parallel transformer and reducers and they fully use the
    power of the cluster to process the code.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的重要一点是，尽管它看起来像一个常规的DataFrame，但它只是一个高级数据结构。在幕后，它是将数据分布到整个集群的RDD。同样，当我们运行SQL函数时，在幕后，它们被转换为并行转换器和减少器，并充分利用集群的能力来处理代码。
- en: The hybrid strategy
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合策略
- en: 'Increasingly, cloud computing is becoming more and more popular to run large-scale
    algorithms. This provides us with the opportunity to combine *look outside* and
    *look within* strategies. This can be done by provisioning one or more GPUs in
    multiple virtual machines, as shown in the following screenshot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的人开始使用云计算来运行大规模算法。这为我们提供了结合*向外看*和*向内看*策略的机会。这可以通过在多个虚拟机中配置一个或多个GPU来实现，如下面的屏幕截图所示：
- en: '![](assets/3c81bb15-6d1a-4f10-9bd7-a575497280e4.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3c81bb15-6d1a-4f10-9bd7-a575497280e4.png)'
- en: Making the best use of hybrid architecture is a non-trivial task. This is approached
    by first dividing the data into multiple partitions. Compute-intensive tasks that
    require less data are parallelized within each node at the GPUs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 充分利用混合架构是一项非常重要的任务。首先将数据分成多个分区。在每个节点上并行化需要较少数据的计算密集型任务在GPU上进行。
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked into the design of parallel algorithms and the design
    issues of large-scale algorithms. We looked into using parallel computing and
    GPUs to implement large-scale algorithms. We also looked into how we can use Spark
    clusters  to implement large-scale algorithms.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了并行算法的设计以及大规模算法的设计问题。我们研究了使用并行计算和GPU来实现大规模算法。我们还研究了如何使用Spark集群来实现大规模算法。
- en: In this chapter, we learned about issues related to large-scale algorithms.
    We looked into the issues related to parallelizing algorithms and the potential
    bottlenecks created in the process.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了与大规模算法相关的问题。我们研究了与并行化算法相关的问题以及在此过程中可能产生的潜在瓶颈。
- en: In the next chapter, we will look at some practical aspects of implementing
    algorithms.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨实现算法的一些实际方面。
