["```py\nimport requests\n\nclass IPtoGeo(object):\n\n    def __init__(self, ip_address):\n\n        # Initialize objects to store\n        self.latitude = ''\n        self.longitude = ''\n        self.country = ''\n        self.city = ''\n        self.ip_address = ip_address\n        self._get_location()\n\n    def _get_location(self):\n        json_request = requests.get('http://api.hostip.info/get_json.php ip=%s&position=true' % self.ip_address).json()\n\n        self.country = json_request['country_name']\n        self.country_code = json_request['country_code']\n        self.city = json_request['city']\n        self.latitude = json_request['lat']\n        self.longitude = json_request['lng']\n\nif __name__ == '__main__':\n    ip1 = IPtoGeo('8.8.8.8')\n    print(ip1.__dict__)\n```", "```py\n{'latitude': '37.402', 'longitude': '-122.078', 'country': 'UNITED STATES', 'city': 'Mountain View, CA', 'ip_address': '8.8.8.8', 'country_code': 'US'}\n```", "```py\nimport pygeoip\nimport pprint\ngi = pygeoip.GeoIP('GeoLiteCity.dat')\npprint.pprint(\"Country code: %s \" %(str(gi.country_code_by_addr('173.194.34.192'))))\npprint.pprint(\"Country code: %s \" %(str(gi.country_code_by_name('google.com'))))\npprint.pprint(\"Country name: %s \" %(str(gi.country_name_by_addr('173.194.34.192'))))\npprint.pprint(\"Country code: %s \" %(str(gi.country_name_by_name('google.com'))))\n```", "```py\ngi2 = pygeoip.GeoIP('GeoIPASNum.dat')\npprint.pprint(\"Organization by addr: %s \" %(str(gi2.org_by_addr('173.194.34.192'))))\npprint.pprint(\"Organization by name: %s \" %(str(gi2.org_by_name('google.com'))))\n```", "```py\nfor record,value in gi.record_by_addr('173.194.34.192').items():\n    print(record + \"-->\" + str(value))\n```", "```py\nimport pygeoip\n\ndef main():\n geoip_country() \n geoip_city()\n\ndef geoip_city():\n path = 'GeoLiteCity.dat'\n gic = pygeoip.GeoIP(path)\n print(gic.record_by_addr('64.233.161.99'))\n print(gic.record_by_name('google.com'))\n print(gic.region_by_name('google.com'))\n print(gic.region_by_addr('64.233.161.99'))\n\ndef geoip_country(): \n path = 'GeoIP.dat'\n gi = pygeoip.GeoIP(path)\n print(gi.country_code_by_name('google.com'))\n print(gi.country_code_by_addr('64.233.161.99'))\n print(gi.country_name_by_name('google.com'))\n print(gi.country_name_by_addr('64.233.161.99'))\n\nif __name__ == '__main__':\n main()\n```", "```py\nfrom pygeocoder import Geocoder\n\nresults = Geocoder.geocode(\"Mountain View\")\n\nprint(results.coordinates)\nprint(results.country)\nprint(results.postal_code)\nprint(results.latitude)\nprint(results.longitude)\nresults = Geocoder.reverse_geocode(results.latitude, results.longitude)\nprint(results.formatted_address)\n```", "```py\nimport socket\nfrom geolite2 import geolite2\nimport argparse\nimport json\n\nif __name__ == '__main__':\n # Commandline arguments\n parser = argparse.ArgumentParser(description='Get IP Geolocation info')\n parser.add_argument('--hostname', action=\"store\", dest=\"hostname\",required=True)\n\n# Parse arguments\n given_args = parser.parse_args()\n hostname = given_args.hostname\n ip_address = socket.gethostbyname(hostname)\n print(\"IP address: {0}\".format(ip_address))\n\n# Call geolite2\n reader = geolite2.reader()\n response = reader.get(ip_address)\n print (json.dumps(response['continent']['names']['en'],indent=4))\n print (json.dumps(response['country']['names']['en'],indent=4))\n print (json.dumps(response['location']['latitude'],indent=4))\n print (json.dumps(response['location']['longitude'],indent=4))\n print (json.dumps(response['location']['time_zone'],indent=4))\n```", "```py\nfrom PIL import Image\nfrom PIL.ExifTags import TAGS\n\nfor (i,j) in Image.open('images/image.jpg')._getexif().items():\n    print('%s = %s' % (TAGS.get(i), j))\n```", "```py\ndef get_exif_metadata(image_path):\n    exifData = {}\n    image = Image.open(image_path)\n    if hasattr(image, '_getexif'):\n        exifinfo = image._getexif()\n        if exifinfo is not None:\n            for tag, value in exifinfo.items():\n                decoded = TAGS.get(tag, tag)\n                exifData[decoded] = value\n decode_gps_info(exifData)\n return exifData\n```", "```py\ndef decode_gps_info(exif):\n    gpsinfo = {}\n    if 'GPSInfo' in exif:\n    '''\n    Raw Geo-references\n    for key in exif['GPSInfo'].keys():\n        decode = GPSTAGS.get(key,key)\n        gpsinfo[decode] = exif['GPSInfo'][key]\n    exif['GPSInfo'] = gpsinfo\n    '''\n\n     #Parse geo references.\n     Nsec = exif['GPSInfo'][2][2][0] / float(exif['GPSInfo'][2][2][1])\n     Nmin = exif['GPSInfo'][2][1][0] / float(exif['GPSInfo'][2][1][1])\n     Ndeg = exif['GPSInfo'][2][0][0] / float(exif['GPSInfo'][2][0][1])\n     Wsec = exif['GPSInfo'][4][2][0] / float(exif['GPSInfo'][4][2][1])\n     Wmin = exif['GPSInfo'][4][1][0] / float(exif['GPSInfo'][4][1][1])\n     Wdeg = exif['GPSInfo'][4][0][0] / float(exif['GPSInfo'][4][0][1])\n     if exif['GPSInfo'][1] == 'N':\n         Nmult = 1\n     else:\n         Nmult = -1\n     if exif['GPSInfo'][1] == 'E':\n         Wmult = 1\n     else:\n         Wmult = -1\n         Lat = Nmult * (Ndeg + (Nmin + Nsec/60.0)/60.0)\n         Lng = Wmult * (Wdeg + (Wmin + Wsec/60.0)/60.0)\n         exif['GPSInfo'] = {\"Lat\" : Lat, \"Lng\" : Lng}\n```", "```py\ndef findImages(url):\n    print('[+] Finding images on ' + url)\n    urlContent = requests.get(url).text\n    soup = BeautifulSoup(urlContent,'lxml')\n    imgTags = soup.findAll('img')\n    return imgTags\n\ndef downloadImage(imgTag):\n    try:\n        print('[+] Dowloading in images directory...'+imgTag['src'])\n        imgSrc = imgTag['src']\n        imgContent = urlopen(imgSrc).read()\n        imgFileName = basename(urlsplit(imgSrc)[2])\n        imgFile = open('images/'+imgFileName, 'wb')\n        imgFile.write(imgContent)\n        imgFile.close()\n        return imgFileName\n    except Exception as e:\n        print(e)\n        return ''\n```", "```py\ndef printMetadata():\n    print(\"Extracting metadata from images in images directory.........\")\n    for dirpath, dirnames, files in os.walk(\"images\"):\n    for name in files:\n        print(\"[+] Metadata for file: %s \" %(dirpath+os.path.sep+name))\n            try:\n                exifData = {}\n                exif = get_exif_metadata(dirpath+os.path.sep+name)\n                for metadata in exif:\n                print(\"Metadata: %s - Value: %s \" %(metadata, exif[metadata]))\n            except:\n                import sys, traceback\n                traceback.print_exc(file=sys.stdout)\n```", "```py\ndef main():\n    parser = optparse.OptionParser('-url <target url>')\n    parser.add_option('-u', dest='url', type='string', help='specify url address')\n    (options, args) = parser.parse_args()\n    url = options.url\n    if url == None:\n        print(parser.usage)\n        exit(0)\n    else:#find and download images and extract metadata\n       imgTags = findImages(url) print(imgTags) for imgTag in imgTags: imgFileName = downloadImage(imgTag) printMetadata()\n```", "```py\n#!usr/bin/env python\n# coding: utf-8\n\nfrom PyPDF2 import PdfFileReader, PdfFileWriter\nimport os, time, os.path, stat\n\nfrom PyPDF2.generic import NameObject, createStringObject\n\nclass bcolors:\n    OKGREEN = '\\033[92m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n\ndef get_metadata():\n  for dirpath, dirnames, files in os.walk(\"pdf\"):\n    for data in files:\n      ext = data.lower().rsplit('.', 1)[-1]\n      if ext in ['pdf']:\n        print(bcolors.OKGREEN + \"------------------------------------------------------------------------------------\")\n        print(bcolors.OKGREEN + \"[--- Metadata : \" + bcolors.ENDC + bcolors.BOLD + \"%s \" %(dirpath+os.path.sep+data) + bcolors.ENDC)\n        print(bcolors.OKGREEN + \"------------------------------------------------------------------------------------\")\n        pdf = PdfFileReader(open(dirpath+os.path.sep+data, 'rb'))\n        info = pdf.getDocumentInfo()\n\n        for metaItem in info:\n\n          print (bcolors.OKGREEN + '[+] ' + metaItem.strip( '/' ) + ': ' + bcolors.ENDC + info[metaItem])\n\n        pages = pdf.getNumPages()\n        print (bcolors.OKGREEN + '[+] Pages:' + bcolors.ENDC, pages)\n\n        layout = pdf.getPageLayout()\n        print (bcolors.OKGREEN + '[+] Layout: ' + bcolors.ENDC + str(layout))\n\n```", "```py\n        xmpinfo = pdf.getXmpMetadata()\n\n        if hasattr(xmpinfo,'dc_contributor'): print (bcolors.OKGREEN + '[+] Contributor:' + bcolors.ENDC, xmpinfo.dc_contributor)\n        if hasattr(xmpinfo,'dc_identifier'): print (bcolors.OKGREEN + '[+] Identifier:' + bcolors.ENDC, xmpinfo.dc_identifier)\n        if hasattr(xmpinfo,'dc_date'): print (bcolors.OKGREEN + '[+] Date:' + bcolors.ENDC, xmpinfo.dc_date)\n        if hasattr(xmpinfo,'dc_source'): print (bcolors.OKGREEN + '[+] Source:' + bcolors.ENDC, xmpinfo.dc_source)\n        if hasattr(xmpinfo,'dc_subject'): print (bcolors.OKGREEN + '[+] Subject:' + bcolors.ENDC, xmpinfo.dc_subject)\n        if hasattr(xmpinfo,'xmp_modifyDate'): print (bcolors.OKGREEN + '[+] ModifyDate:' + bcolors.ENDC, xmpinfo.xmp_modifyDate)\n        if hasattr(xmpinfo,'xmp_metadataDate'): print (bcolors.OKGREEN + '[+] MetadataDate:' + bcolors.ENDC, xmpinfo.xmp_metadataDate)\n        if hasattr(xmpinfo,'xmpmm_documentId'): print (bcolors.OKGREEN + '[+] DocumentId:' + bcolors.ENDC, xmpinfo.xmpmm_documentId)\n        if hasattr(xmpinfo,'xmpmm_instanceId'): print (bcolors.OKGREEN + '[+] InstanceId:' + bcolors.ENDC, xmpinfo.xmpmm_instanceId)\n        if hasattr(xmpinfo,'pdf_keywords'): print (bcolors.OKGREEN + '[+] PDF-Keywords:' + bcolors.ENDC, xmpinfo.pdf_keywords)\n        if hasattr(xmpinfo,'pdf_pdfversion'): print (bcolors.OKGREEN + '[+] PDF-Version:' + bcolors.ENDC, xmpinfo.pdf_pdfversion)\n\n        if hasattr(xmpinfo,'dc_publisher'):\n          for y in xmpinfo.dc_publisher:\n            if y:\n              print (bcolors.OKGREEN + \"[+] Publisher:\\t\" + bcolors.ENDC + y) \n\n      fsize = os.stat((dirpath+os.path.sep+data))\n      print (bcolors.OKGREEN + '[+] Size:' + bcolors.ENDC, fsize[6], 'bytes \\n\\n')\n\nget_metadata()\n```", "```py\n>>> import builtwith\n>>> builtwith.parse('http://example.webscraping.com')\n{u'javascript-frameworks': [u'jQuery', u'Modernizr', u'jQuery UI'],\nu'programming-languages': [u'Python'],\nu'web-frameworks': [u'Web2py', u'Twitter Bootstrap'],\nu'web-servers': [u'Nginx']}\n```", "```py\npython3 dumpzilla.py \"/root/.mozilla/firefox/[Your Profile.default]\"\n```", "```py\nimport sqlite3\nimport datetime\nimport optparse\n\ndef fixDate(timestamp):\n    #Chrome stores timestamps in the number of microseconds since Jan 1 1601.\n    #To convert, we create a datetime object for Jan 1 1601...\n    epoch_start = datetime.datetime(1601,1,1)\n    #create an object for the number of microseconds in the timestamp\n    delta = datetime.timedelta(microseconds=int(timestamp))\n    #and return the sum of the two.\n    return epoch_start + delta\n\nselectFromDownloads = 'SELECT target_path, referrer, start_time, end_time, received_bytes FROM downloads;'\n\ndef getMetadataHistoryFile(locationHistoryFile):\n    sql_connect = sqlite3.connect(locationHistoryFile)\n    for row in sql_connect.execute(selectFromDownloads):\n        print (\"Download:\",row[0].encode('utf-8'))\n        print (\"\\tFrom:\",str(row[1]))\n        print (\"\\tStarted:\",str(fixDate(row[2])))\n        print (\"\\tFinished:\",str(fixDate(row[3])))\n        print (\"\\tSize:\",str(row[4]))\n\ndef main():\n    parser = optparse.OptionParser('-location <target location>')\n    parser.add_option('-l', dest='location', type='string', help='specify url address')\n\n    (options, args) = parser.parse_args()\n     location = options.location\n     print(location)\n     if location == None:\n         exit(0)\n     else:\n         getMetadataHistoryFile(location)\n\nif __name__ == '__main__':\n    main()\n```"]