["```py\ndocker pull mheydt/pywebscrapecookbook\ndocker run -p 5001:5001 pywebscrapecookbook\n```", "```py\nprocess = CrawlerProcess({\n  'LOG_LEVEL': 'DEBUG',\n  'DOWNLOADER_MIDDLEWARES':\n {  \"scrapy.downloadermiddlewares.retry.RetryMiddleware\": 500\n  },\n  'RETRY_ENABLED': True,\n  'RETRY_TIMES': 3 }) process.crawl(Spider) process.start()\n```", "```py\nParsing: <200 https://www.nasa.gov/content/earth-expeditions-above/>\n['http://www.nasa.gov/content/earth-expeditions-above', 'https://www.nasa.gov/content/earth-expeditions-above']\n```", "```py\n2017-10-22 17:55:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Discarding <GET http://www.nasa.gov/topics/journeytomars/news/index.html>: max redirections reached\n```", "```py\nclass Spider(scrapy.spiders.SitemapSpider):\n  name = 'spider'\n  sitemap_urls = ['https://www.nasa.gov/sitemap.xml']    def parse(self, response):\n  print(\"Parsing: \", response)\n  print (response.request.meta.get('redirect_urls'))\n```", "```py\nprocess = CrawlerProcess({\n  'LOG_LEVEL': 'DEBUG',\n  'DOWNLOADER_MIDDLEWARES':\n {  \"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\": 500\n  },\n  'REDIRECT_ENABLED': True,\n  'REDIRECT_MAX_TIMES': 2 }) \n```", "```py\n<div id='start'>\n   <button>Start</button>\n</div>\n```", "```py\n<div id='finish'>\n   <h4>Hello World!\"</h4>\n</div>\n```", "```py\nclicked\nHello World!\n```", "```py\nfrom selenium import webdriver\nfrom selenium.webdriver.support import ui\n```", "```py\ndriver = webdriver.PhantomJS() driver.get(\"http://the-internet.herokuapp.com/dynamic_loading/2\")\n```", "```py\nbutton = driver.find_element_by_xpath(\"//*/div[@id='start']/button\")\n```", "```py\nbutton.click() print(\"clicked\")\n```", "```py\nwait = ui.WebDriverWait(driver, 10)\n```", "```py\nwait.until(lambda driver: driver.find_element_by_xpath(\"//*/div[@id='finish']\"))\n```", "```py\nfinish_element=driver.find_element_by_xpath(\"//*/div[@id='finish']/h4\") print(finish_element.text)\n```", "```py\nclass Spider(scrapy.spiders.SitemapSpider):\n  name = 'spider'\n  sitemap_urls = ['https://www.nasa.gov/sitemap.xml']\n  allowed_domains=['nasa.gov']    def parse(self, response):\n  print(\"Parsing: \", response) \n```", "```py\n{\n\"has_next\": true,\n\"page\": 2,\n\"quotes\": [{\n\"author\": {\n\"goodreads_link\": \"/author/show/82952.Marilyn_Monroe\",\n\"name\": \"Marilyn Monroe\",\n\"slug\": \"Marilyn-Monroe\"\n},\n\"tags\": [\"friends\", \"heartbreak\", \"inspirational\", \"life\", \"love\", \"sisters\"],\n\"text\": \"\\u201cThis life is what you make it....\"\n}, {\n\"author\": {\n\"goodreads_link\": \"/author/show/1077326.J_K_Rowling\",\n\"name\": \"J.K. Rowling\",\n\"slug\": \"J-K-Rowling\"\n},\n\"tags\": [\"courage\", \"friends\"],\n\"text\": \"\\u201cIt takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.\\u201d\"\n},\n```", "```py\n<200 http://spidyquotes.herokuapp.com/api/quotes?page=2>\n2017-10-29 16:17:37 [scrapy.core.scraper] DEBUG: Scraped from <200 http://spidyquotes.herokuapp.com/api/quotes?page=2>\n{'text': \"\u201cThis life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.\u201d\", 'author': 'Marilyn Monroe', 'tags': ['friends', 'heartbreak', 'inspirational', 'life', 'love', 'sisters']}\n2017-10-29 16:17:37 [scrapy.core.scraper] DEBUG: Scraped from <200 http://spidyquotes.herokuapp.com/api/quotes?page=2>\n{'text': '\u201cIt takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.\u201d', 'author': 'J.K. Rowling', 'tags': ['courage', 'friends']}\n2017-10-29 16:17:37 [scrapy.core.scraper] DEBUG: Scraped from <200 http://spidyquotes.herokuapp.com/api/quotes?page=2>\n{'text': \"\u201cIf you can't explain it to a six year old, you don't understand it yourself.\u201d\", 'author': 'Albert Einstein', 'tags': ['simplicity', 'understand']}\n```", "```py\nclass Spider(scrapy.Spider):\n  name = 'spidyquotes'\n  quotes_base_url = 'http://spidyquotes.herokuapp.com/api/quotes'\n  start_urls = [quotes_base_url]\n  download_delay = 1.5\n```", "```py\n  def parse(self, response):\n  print(response)\n  data = json.loads(response.body)\n```", "```py\n  for item in data.get('quotes', []):\n  yield {\n  'text': item.get('text'),\n  'author': item.get('author', {}).get('name'),\n  'tags': item.get('tags'),\n } \n```", "```py\nif data['has_next']:\n    next_page = data['page'] + 1\n  yield scrapy.Request(self.quotes_base_url + \"?page=%s\" % next_page)\n```", "```py\nfrom selenium import webdriver\nimport time\n\ndriver = webdriver.PhantomJS()   print(\"Starting\") driver.get(\"https://twitter.com\") scroll_pause_time = 1.5   # Get scroll height last_height = driver.execute_script(\"return document.body.scrollHeight\") while True:\n  print(last_height)\n  # Scroll down to bottom\n  driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")    # Wait to load page\n  time.sleep(scroll_pause_time)    # Calculate new scroll height and compare with last scroll height\n  new_height = driver.execute_script(\"return document.body.scrollHeight\")\n  print(new_height, last_height)    if new_height == last_height:\n  break\n  last_height = new_height\n```", "```py\nStarting\n4882\n8139 4882\n8139\n11630 8139\n11630\n15055 11630\n15055\n15055 15055\nProcess finished with exit code 0\n```", "```py\nprocess = CrawlerProcess({\n    'LOG_LEVEL': 'CRITICAL',\n    'DEPTH_LIMIT': 2,\n    'DEPT_STATS': True })\n```", "```py\nParsing: <200 http://localhost:8080/CrawlDepth0-1.html>\nRequesting crawl of: http://localhost:8080/CrawlDepth0-2.html\nRequesting crawl of: http://localhost:8080/Depth1/CrawlDepth1-1.html\nParsing: <200 http://localhost:8080/Depth1/CrawlDepth1-1.html>\nRequesting crawl of: http://localhost:8080/Depth1/CrawlDepth1-2.html\nRequesting crawl of: http://localhost:8080/Depth1/depth1/CrawlDepth1-2.html\nRequesting crawl of: http://localhost:8080/Depth1/depth2/CrawlDepth2-1.html\nParsing: <200 http://localhost:8080/CrawlDepth0-2.html>\nRequesting crawl of: http://localhost:8080/CrawlDepth0-3.html\n<scrapy.statscollectors.MemoryStatsCollector object at 0x109f754e0>\nCrawled: ['http://localhost:8080/CrawlDepth0-1.html', 'http://localhost:8080/Depth1/CrawlDepth1-1.html', 'http://localhost:8080/CrawlDepth0-2.html']\nRequested: ['http://localhost:8080/CrawlDepth0-2.html', 'http://localhost:8080/Depth1/CrawlDepth1-1.html', 'http://localhost:8080/Depth1/CrawlDepth1-2.html', 'http://localhost:8080/Depth1/depth1/CrawlDepth1-2.html', 'http://localhost:8080/Depth1/depth2/CrawlDepth2-1.html', 'http://localhost:8080/CrawlDepth0-3.html']\n```", "```py\nprocess = CrawlerProcess({\n  'LOG_LEVEL': 'CRITICAL',\n  'DEPTH_LIMIT': 2,\n  'DEPT_STATS': True })\n```", "```py\nParsing: <200 http://localhost:8080/CrawlDepth0-1.html>\nRequesting crawl of: http://localhost:8080/CrawlDepth0-2.html\nRequesting crawl of: http://localhost:8080/Depth1/CrawlDepth1-1.html\nParsing: <200 http://localhost:8080/Depth1/CrawlDepth1-1.html>\nRequesting crawl of: http://localhost:8080/Depth1/CrawlDepth1-2.html\nRequesting crawl of: http://localhost:8080/Depth1/depth1/CrawlDepth1-2.html\nRequesting crawl of: http://localhost:8080/Depth1/depth2/CrawlDepth2-1.html\nParsing: <200 http://localhost:8080/CrawlDepth0-2.html>\nRequesting crawl of: http://localhost:8080/CrawlDepth0-3.html\nParsing: <200 http://localhost:8080/Depth1/depth2/CrawlDepth2-1.html>\nParsing: <200 http://localhost:8080/CrawlDepth0-3.html>\nParsing: <200 http://localhost:8080/Depth1/CrawlDepth1-2.html>\nRequesting crawl of: http://localhost:8080/Depth1/CrawlDepth1-3.html\n<scrapy.statscollectors.MemoryStatsCollector object at 0x10d3d44e0>\nCrawled: ['http://localhost:8080/CrawlDepth0-1.html', 'http://localhost:8080/Depth1/CrawlDepth1-1.html', 'http://localhost:8080/CrawlDepth0-2.html', 'http://localhost:8080/Depth1/depth2/CrawlDepth2-1.html', 'http://localhost:8080/CrawlDepth0-3.html', 'http://localhost:8080/Depth1/CrawlDepth1-2.html']\nRequested: ['http://localhost:8080/CrawlDepth0-2.html', 'http://localhost:8080/Depth1/CrawlDepth1-1.html', 'http://localhost:8080/Depth1/CrawlDepth1-2.html', 'http://localhost:8080/Depth1/depth1/CrawlDepth1-2.html', 'http://localhost:8080/Depth1/depth2/CrawlDepth2-1.html', 'http://localhost:8080/CrawlDepth0-3.html', 'http://localhost:8080/Depth1/CrawlDepth1-3.html']\n```", "```py\nif __name__ == \"__main__\":\n  process = CrawlerProcess({\n  'LOG_LEVEL': 'INFO',\n  'CLOSESPIDER_PAGECOUNT': 5\n  })\n  process.crawl(Spider)\n  process.start()\n```", "```py\n<200 https://www.nasa.gov/exploration/systems/sls/multimedia/sls-hardware-being-moved-on-kamag-transporter.html>\n<200 https://www.nasa.gov/exploration/systems/sls/M17-057.html>\n<200 https://www.nasa.gov/press-release/nasa-awards-contract-for-center-protective-services-for-glenn-research-center/>\n<200 https://www.nasa.gov/centers/marshall/news/news/icymi1708025/>\n<200 https://www.nasa.gov/content/oracles-completed-suit-case-flight-series-to-ascension-island/>\n<200 https://www.nasa.gov/feature/goddard/2017/asteroid-sample-return-mission-successfully-adjusts-course/>\n<200 https://www.nasa.gov/image-feature/jpl/pia21754/juling-crater/>\n```", "```py\n//*/a[@class='next']\n```", "```py\nPage 1 Data\nPage 2 Data\nPage 3 Data\nPage 4 Data\nPage 5 Data\n```", "```py\nclass PaginatedSearchResultsSpider(CrawlSpider):\n    name = \"paginationscraper\"\n  start_urls = [\n\"http://localhost:5001/pagination/page1.html\"\n  ]\n```", "```py\nrules = (\n# Extract links for next pages\n  Rule(LinkExtractor(allow=(),\nrestrict_xpaths=(\"//*/a[@class='next']\")),\ncallback='parse_result_page', follow=True),\n)\n```", "```py\nall_items = []\n```", "```py\ndef parse_start_url(self, response):\n  return self.parse_result_page(response)\n```", "```py\ndef parse_result_page(self, response):\n    data_items = response.xpath(\"//*/div[@class='data']/h1/text()\")\nfor data_item in data_items:\n self.all_items.append(data_item.root)\n```", "```py\ndef closed(self, reason):\n  for i in self.all_items:\n  print(i) \n```", "```py\nif __name__ == \"__main__\":\n  process = CrawlerProcess({\n  'LOG_LEVEL': 'DEBUG',\n  'CLOSESPIDER_PAGECOUNT': 10   })\n  process.crawl(ImdbSearchResultsSpider)\n  process.start()\n```", "```py\n<form action=\"/Account/Login\" method=\"post\"><div>\n <label for=\"Username\">Username</label>\n <input type=\"text\" id=\"Username\" name=\"Username\" value=\"\" />\n <span class=\"field-validation-valid\" data-valmsg-for=\"Username\" data-valmsg-replace=\"true\"></span></div>\n<div>\n <label for=\"Password\">Password</label>\n <input type=\"password\" id=\"Password\" name=\"Password\" />\n <span class=\"field-validation-valid\" data-valmsg-for=\"Password\" data-valmsg-replace=\"true\"></span>\n </div> \n <input type=\"hidden\" name=\"returnUrl\" />\n<input name=\"submit\" type=\"submit\" value=\"Login\"/>\n <input name=\"__RequestVerificationToken\" type=\"hidden\" value=\"CfDJ8CqzjGWzUMJKkKCmxuBIgZf3UkeXZnVKBwRV_Wu4qUkprH8b_2jno5-1SGSNjFqlFgLie84xI2ZBkhHDzwgUXpz6bbBwER0v_-fP5iTITiZi2VfyXzLD_beXUp5cgjCS5AtkIayWThJSI36InzBqj2A\" /></form>\n```", "```py\nclass Spider(scrapy.Spider):\n  name = 'spider'\n  start_urls = ['http://localhost:5001/home/secured']\n  login_user = 'darkhelmet'\n  login_pass = 'vespa'\n```", "```py\ndef parse(self, response):\n  print(\"Parsing: \", response)    count_of_password_fields = int(float(response.xpath(\"count(//*/input[@type='password' and @id='Password'])\").extract()[0]))\n  if count_of_password_fields > 0:\n  print(\"Got a password page\") \n```", "```py\nreturn scrapy.FormRequest.from_response(\n response,\n  formdata={'Username': self.login_user, 'Password': self.login_pass},\n  callback=self.after_login)\n```", "```py\ndef after_login(self, response):\n  if \"This page is secured\" in str(response.body):\n  print(\"You have logged in ok!\")\n```", "```py\nParsing: <200 http://localhost:5001/account/login?ReturnUrl=%2Fhome%2Fsecured>\nGot a password page\nYou have logged in ok!\n```", "```py\nAuthorization: Basic ZGFya2hlbG1ldDp2ZXNwYQ==, with ZGFya2hlbG1ldDp2ZXNwYQ== being darkhelmet:vespa base 64 encoded.\n```", "```py\nclass SomeIntranetSiteSpider(CrawlSpider):\n    http_user = 'someuser'\n    http_pass = 'somepass'\n    name = 'intranet.example.com'\n    # .. rest of the spider code omitted ...\n```", "```py\n# Retry many times since proxies often fail\nRETRY_TIMES = 10\n# Retry on most error codes since proxies fail for different reasons\nRETRY_HTTP_CODES = [500, 503, 504, 400, 403, 404, 408]\n\nDOWNLOADER_MIDDLEWARES = {\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,\n 'scrapy_proxies.RandomProxy': 100,\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,\n}\n```", "```py\nPROXY_LIST = '/path/to/proxy/list.txt'\n```", "```py\n# Proxy mode\n# 0 = Every requests have different proxy\n# 1 = Take only one proxy from the list and assign it to every requests\n# 2 = Put a custom proxy to use in the settings\nPROXY_MODE = 0\n```", "```py\nCUSTOM_PROXY = \"http://host1:port\"\n```", "```py\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,\n    'random_useragent.RandomUserAgentMiddleware': 400\n}\n```", "```py\nUSER_AGENT_LIST = \"/path/to/useragents.txt\"\n```", "```py\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n}\n```", "```py\nif __name__ == \"__main__\":\n  process = CrawlerProcess({\n  'LOG_LEVEL': 'CRITICAL',\n  'CLOSESPIDER_PAGECOUNT': 50,\n  'HTTPCACHE_ENABLED': True,\n  'HTTPCACHE_DIR': \".\"\n  })\n  process.crawl(Spider)\n  process.start()\n```"]