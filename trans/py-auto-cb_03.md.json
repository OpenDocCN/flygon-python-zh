["```py\n$ echo \"requests==2.18.3\" >> requirements.txt\n$ source .venv/bin/activate\n(.venv) $ pip install -r requirements.txt \n```", "```py\n>>> import requests\n```", "```py\n>>> url = 'http://www.columbia.edu/~fdc/sample.html'\n>>> response = requests.get(url)\n```", "```py\n>>> response.status_code\n200\n```", "```py\n>>> response.text\n'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\\n<html>\\n<head>\\n\n...\nFULL BODY\n...\n<!-- close the <html> begun above -->\\n'\n```", "```py\n>>> response.request.headers\n{'User-Agent': 'python-requests/2.18.4', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n>>> response.headers\n{'Date': 'Fri, 25 May 2018 21:51:47 GMT', 'Server': 'Apache', 'Last-Modified': 'Thu, 22 Apr 2004 15:52:25 GMT', 'Accept-Ranges': 'bytes', 'Vary': 'Accept-Encoding,User-Agent', 'Content-Encoding': 'gzip', 'Content-Length': '8664', 'Keep-Alive': 'timeout=15, max=85', 'Connection': 'Keep-Alive', 'Content-Type': 'text/html', 'Set-Cookie': 'BIGipServer~CUIT~www.columbia.edu-80-pool=1764244352.20480.0000; expires=Sat, 26-May-2018 03:51:47 GMT; path=/; Httponly'}\n```", "```py\n>>> response.request\n<PreparedRequest [GET]>\n>>> response.request.url\n'http://www.columbia.edu/~fdc/sample.html'\n```", "```py\n$ echo \"beautifulsoup4==4.6.0\" >> requirements.txt\n$ pip install -r requirements.txt\n```", "```py\n>>> import requests >>> from bs4 import BeautifulSoup\n```", "```py\n>>> URL = 'http://www.columbia.edu/~fdc/sample.html'\n>>> response = requests.get(URL)\n>>> response\n<Response [200]>\n```", "```py\n>>> page = BeautifulSoup(response.text, 'html.parser')\n```", "```py\n>>> page.title\n<title>Sample Web Page</title>\n>>> page.title.string\n'Sample Web Page'\n```", "```py\n>>> page.find_all('h3')\n[<h3><a name=\"contents\">CONTENTS</a></h3>, <h3><a name=\"basics\">1\\. Creating a Web Page</a></h3>, <h3><a name=\"syntax\">2\\. HTML Syntax</a></h3>, <h3><a name=\"chars\">3\\. Special Characters</a></h3>, <h3><a name=\"convert\">4\\. Converting Plain Text to HTML</a></h3>, <h3><a name=\"effects\">5\\. Effects</a></h3>, <h3><a name=\"lists\">6\\. Lists</a></h3>, <h3><a name=\"links\">7\\. Links</a></h3>, <h3><a name=\"tables\">8\\. Tables</a></h3>, <h3><a name=\"install\">9\\. Installing Your Web Page on the Internet</a></h3>, <h3><a name=\"more\">10\\. Where to go from here</a></h3>]\n```", "```py\n>>> link_section = page.find('a', attrs={'name': 'links'})\n>>> section = []\n>>> for element in link_section.next_elements:\n...     if element.name == 'h3':\n...         break\n...     section.append(element.string or '')\n...\n>>> result = ''.join(section)\n>>> result\n'7\\. Links\\n\\nLinks can be internal within a Web page (like to\\nthe Table of ContentsTable of Contents at the top), or they\\ncan be to external web pages or pictures on the same website, or they\\ncan be to websites, pages, or pictures anywhere else in the world.\\n\\n\\n\\nHere is a link to the Kermit\\nProject home pageKermit\\nProject home page.\\n\\n\\n\\nHere is a link to Section 5Section 5 of this document.\\n\\n\\n\\nHere is a link to\\nSection 4.0Section 4.0\\nof the C-Kermit\\nfor Unix Installation InstructionsC-Kermit\\nfor Unix Installation Instructions.\\n\\n\\n\\nHere is a link to a picture:\\nCLICK HERECLICK HERE to see it.\\n\\n\\n'\n```", "```py\n>>> page.find_all(re.compile('^h(2|3)'))\n[<h2>Sample Web Page</h2>, <h3><a name=\"contents\">CONTENTS</a></h3>, <h3><a name=\"basics\">1\\. Creating a Web Page</a></h3>, <h3><a name=\"syntax\">2\\. HTML Syntax</a></h3>, <h3><a name=\"chars\">3\\. Special Characters</a></h3>, <h3><a name=\"convert\">4\\. Converting Plain Text to HTML</a></h3>, <h3><a name=\"effects\">5\\. Effects</a></h3>, <h3><a name=\"lists\">6\\. Lists</a></h3>, <h3><a name=\"links\">7\\. Links</a></h3>,\n```", "```py\n<h3><a name=\"tables\">8\\. Tables</a></h3>, <h3><a name=\"install\">9\\. Installing Your Web Page on the Internet</a></h3>, <h3><a name=\"more\">10\\. Where to go from here</a></h3>]\n```", "```py\n$ python simple_delay_server.py\n```", "```py\n...\n\ndef process_link(source_link, text):\n    logging.info(f'Extracting links from {source_link}')\n    parsed_source = urlparse(source_link)\n    result = requests.get(source_link)\n    # Error handling. See GitHub for details\n    ...\n    page = BeautifulSoup(result.text, 'html.parser')\n    search_text(source_link, page, text)\n    return get_links(parsed_source, page)\n\ndef get_links(parsed_source, page):\n    '''Retrieve the links on the page'''\n    links = []\n    for element in page.find_all('a'):\n        link = element.get('href')\n        # Validate is a valid link. See GitHub for details\n        ...\n        links.append(link)\n    return links\n```", "```py\n$ python crawling_web_step1.py https://localhost:8000/ -p python\nLink http://localhost:8000/: --> A smaller article , that contains a reference to Python\nLink http://localhost:8000/files/5eabef23f63024c20389c34b94dee593-1.html: --> A smaller article , that contains a reference to Python\nLink http://localhost:8000/files/33714fc865e02aeda2dabb9a42a787b2-0.html: --> This is the actual bit with a python reference that we are interested in.\nLink http://localhost:8000/files/archive-september-2018.html: --> A smaller article , that contains a reference to Python\nLink http://localhost:8000/index.html: --> A smaller article , that contains a reference to Python\n```", "```py\n$ python crawling_web_step1.py http://localhost:8000/ -p crocodile\n```", "```py\n>>> from urllib.parse import urlparse\n>>> >>> urlparse('http://localhost:8000/files/b93bec5d9681df87e6e8d5703ed7cd81-2.html')\nParseResult(scheme='http', netloc='localhost:8000', path='/files/b93bec5d9681df87e6e8d5703ed7cd81-2.html', params='', query='', fragment='')\n```", "```py\nfor element in page.find_all(text=re.compile(text)):\n    print(f'Link {source_link}: --> {element}')\n```", "```py\n# In get_links\nif link.endswith('pdf'):\n  continue\n```", "```py\n$ echo \"feedparser==5.2.1\" >> requirements.txt\n$ pip install -r requirements.txt\n```", "```py\nimport feedparser\nimport datetime\nimport delorean\nimport requests\n```", "```py\n>>> rss = feedparser.parse('http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml')\n```", "```py\n>>> rss.updated\n'Sat, 02 Jun 2018 19:50:35 GMT'\n```", "```py\n>>> time_limit = delorean.parse(rss.updated) - datetime.timedelta(hours=6)\n>>> entries = [entry for entry in rss.entries if delorean.parse(entry.published) > time_limit]\n```", "```py\n>>> len(entries)\n10\n>>> len(rss.entries)\n44\n```", "```py\n>>> entries[5]['title']\n'Loose Ends: How to Live to 108'\n>>> entries[5]['link']\n'https://www.nytimes.com/2018/06/02/opinion/sunday/how-to-live-to-108.html?partner=rss&emc=rss'\n>>> requests.get(entries[5].link)\n<Response [200]>\n```", "```py\n>>> rss.feed.title\n'NYT > Home Page'\n```", "```py\n>>> entries[5].keys()\ndict_keys(['title', 'title_detail', 'links', 'link', 'id', 'guidislink', 'media_content', 'summary', 'summary_detail', 'media_credit', 'credit', 'content', 'authors', 'author', 'author_detail', 'published', 'published_parsed', 'tags'])\n```", "```py\n# The collection of all posts\n/posts\n# A single post. X is the ID of the post\n/posts/X\n# The comments of post X\n/posts/X/comments\n```", "```py\n>>> import requests\n```", "```py\n>>> result = requests.get('https://jsonplaceholder.typicode.com/posts')\n>>> result\n<Response [200]>\n>>> result.json()\n# List of 100 posts NOT DISPLAYED HERE\n>>> result.json()[-1]\n{'userId': 10, 'id': 100, 'title': 'at nam consequatur ea labore ea harum', 'body': 'cupiditate quo est a modi nesciunt soluta\\nipsa voluptas error itaque dicta in\\nautem qui minus magnam et distinctio eum\\naccusamus ratione error aut'}\n```", "```py\n>>> new_post = {'userId': 10, 'title': 'a title', 'body': 'something something'}\n>>> result = requests.post('https://jsonplaceholder.typicode.com/posts',\n              json=new_post)\n>>> result\n<Response [201]>\n>>> result.json()\n{'userId': 10, 'title': 'a title', 'body': 'something something', 'id': 101}\n>>> result.headers['Location']\n'http://jsonplaceholder.typicode.com/posts/101'\n```", "```py\n>>> result = requests.get('https://jsonplaceholder.typicode.com/posts/2')\n>>> result\n<Response [200]>\n>>> result.json()\n{'userId': 1, 'id': 2, 'title': 'qui est esse', 'body': 'est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi nulla'}\n```", "```py\n>>> update = {'body': 'new body'}\n>>> result = requests.patch('https://jsonplaceholder.typicode.com/posts/2', json=update)\n>>> result\n<Response [200]>\n>>> result.json()\n{'userId': 1, 'id': 2, 'title': 'qui est esse', 'body': 'new body'}\n```", "```py\n>>> import requests\n>>> from bs4 import BeautifulSoup\n>>> import re\n```", "```py\n>>> response = requests.get('https://httpbin.org/forms/post')\n>>> page = BeautifulSoup(response.text)\n>>> form = soup.find('form')\n>>> {field.get('name') for field in form.find_all(re.compile('input|textarea'))}\n{'delivery', 'topping', 'size', 'custemail', 'comments', 'custtel', 'custname'}\n```", "```py\n>>> data = {'custname': \"Sean O'Connell\", 'custtel': '123-456-789', 'custemail': 'sean@oconnell.ie', 'size': 'small', 'topping': ['bacon', 'onion'], 'delivery': '20:30', 'comments': ''}\n```", "```py\n>>> response = requests.post('https://httpbin.org/post', data)\n>>> response\n<Response [200]>\n>>> response.json()\n{'args': {}, 'data': '', 'files': {}, 'form': {'comments': '', 'custemail': 'sean@oconnell.ie', 'custname': \"Sean O'Connell\", 'custtel': '123-456-789', 'delivery': '20:30', 'size': 'small', 'topping': ['bacon', 'onion']}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Connection': 'close', 'Content-Length':\n```", "```py\n'140', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.18.3'}, 'json': None, 'origin': '89.100.17.159', 'url': 'https://httpbin.org/post'}\n```", "```py\n>>> form.find(attrs={'name': 'token'}).get('value')\n'ABCEDF12345'\n```", "```py\n$ echo \"selenium==3.12.0\" >> requirements.txt\n$ pip install -r requirements.txt\n```", "```py\n>>> from selenium import webdriver\n>>> browser = webdriver.Chrome()\n>>> browser.get('https://httpbin.org/forms/post')\n```", "```py\n>>> custname = browser.find_element_by_name(\"custname\")\n>>> custname.clear()\n>>> custname.send_keys(\"Sean O'Connell\")\n```", "```py\n>>> for size_element in browser.find_elements_by_name(\"size\"):\n...     if size_element.get_attribute('value') == 'medium':\n...         size_element.click()\n...\n>>>\n```", "```py\n>>> for topping in browser.find_elements_by_name('topping'):\n...     if topping.get_attribute('value') in ['bacon', 'cheese']:\n...         topping.click()\n...\n>>>\n```", "```py\n>>> browser.find_element_by_tag_name('form').submit()\n```", "```py\n>>> browser.quit()\n```", "```py\n>>> from selenium.webdriver.chrome.options import Options\n>>> chrome_options = Options()\n>>> chrome_options.add_argument(\"--headless\")\n>>> browser = webdriver.Chrome(chrome_options=chrome_options)\n>>> browser.get('https://httpbin.org/forms/post')\n```", "```py\n>>> browser.save_screenshot('screenshot.png')\n```", "```py\n>>> import requests\n```", "```py\n>>> requests.get('https://httpbin.org/basic-auth/user/psswd', \n                 auth=('user', 'psswd'))\n<Response [200]>\n```", "```py\n>>> requests.get('https://httpbin.org/basic-auth/user/psswd', \n                 auth=('user', 'wrong'))\n<Response [401]>\n```", "```py\n>>> requests.get('https://user:psswd@httpbin.org/basic-auth/user/psswd')\n<Response [200]>\n>>> requests.get('https://user:wrong@httpbin.org/basic-auth/user/psswd')\n<Response [401]>\n```", "```py\n>>> s = requests.Session()\n>>> s.auth = ('user', 'psswd')\n>>> s.get('https://httpbin.org/basic-auth/user/psswd')\n<Response [200]>\n```", "```py\n$ python simple_delay_server.py -d 2\n```", "```py\n...\ndef process_link(source_link, text):\n    ...\n    return source_link, get_links(parsed_source, page)\n...\n\ndef main(base_url, to_search, workers):\n    checked_links = set()\n    to_check = [base_url]\n    max_checks = 10\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        while to_check:\n            futures = [executor.submit(process_link, url, to_search)\n                       for url in to_check]\n            to_check = []\n            for data in concurrent.futures.as_completed(futures):\n                link, new_links = data.result()\n\n                checked_links.add(link)\n                for link in new_links:\n                    if link not in checked_links and link not in to_check:\n                        to_check.append(link)\n\n                max_checks -= 1\n                if not max_checks:\n                    return\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    ...\n    parser.add_argument('-w', type=int, help='Number of workers',\n                        default=4)\n    args = parser.parse_args()\n\n    main(args.u, args.p, args.w)\n```", "```py\n$ time python crawling_web_step1.py http://localhost:8000/\n... REMOVED OUTPUT\nreal 0m12.221s\nuser 0m0.160s\nsys 0m0.034s\n```", "```py\n$ time python speed_up_step1.py -w 1\n... REMOVED OUTPUT\nreal 0m16.403s\nuser 0m0.181s\nsys 0m0.068s\n```", "```py\n$ time python speed_up_step1.py -w 2\n... REMOVED OUTPUT\nreal 0m10.353s\nuser 0m0.199s\nsys 0m0.068s\n```", "```py\n$ time python speed_up_step1.py -w 5\n... REMOVED OUTPUT\nreal 0m6.234s\nuser 0m0.171s\nsys 0m0.040s\n```", "```py\nwith concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n    while to_check:\n        futures = [executor.submit(process_link, url, to_search)\n                   for url in to_check]\n        to_check = []\n        for data in concurrent.futures.as_completed(futures):\n            link, new_links = data.result()\n\n            checked_links.add(link)\n            for link in new_links:\n                if link not in checked_links and link not in to_check:\n                    to_check.append(link)\n\n             max_checks -= 1\n             if not max_checks:\n                return\n```"]