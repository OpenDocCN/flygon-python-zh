- en: Using the CUDA Libraries with Scikit-CUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-CUDA与CUDA库
- en: In this chapter, we will be taking a tour of three of the standard CUDA libraries
    intended for streamlined numerical and scientific computation. The first that
    we will look at is **cuBLAS**, which is NVIDIA's implementation of the **Basic
    Linear Algebra Subprograms** (**BLAS**) specification for CUDA. (cuBLAS is NVIDIA's
    answer to various optimized, CPU-based implementations of BLAS, such as the free/open
    source OpenBLAS or Intel's proprietary Math Kernel Library.) The next library
    that we will look at is **cuFFT**, which can perform virtually every variation
    of the **fast Fourier transform** (**FFT**) on the GPU. We'll look at how we can
    use cuFFT for filtering in image processing in particular. We will then look at **cuSolver**,
    which can perform more involved linear algebra operations than those featured
    in cuBLAS, such as **singular value decomposition** (**SVD**) or Cholesky factorization.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍三个用于简化数值和科学计算的标准CUDA库。我们将首先看一下**cuBLAS**，这是NVIDIA针对CUDA的**基本线性代数子程序**（**BLAS**）规范的实现。（cuBLAS是NVIDIA对BLAS的各种优化的CPU实现的回应，例如免费/开源的OpenBLAS或英特尔的专有数学核心库。）接下来我们将看一下**cuFFT**，它可以在GPU上执行几乎每种**快速傅里叶变换**（**FFT**）的变体。我们将看看如何在图像处理中使用cuFFT进行滤波。然后我们将看一下**cuSolver**，它可以执行比cuBLAS中更复杂的线性代数运算，例如**奇异值分解**（**SVD**）或乔列斯基分解。
- en: So far, we have been primarily dealing with one single Python module that acted
    as our gateway to CUDA—PyCUDA. While PyCUDA is a very powerful and versatile Python
    library, its main purpose is to provide a gateway to program, compile, and launch
    CUDA kernels, rather than provide an interface to the CUDA libraries. To this
    end, fortunately, there is a free Python module available that provides a user-friendly
    wrapper interface to these libraries. This is called Scikit-CUDA.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要处理了一个作为我们与CUDA网关的单个Python模块——PyCUDA。虽然PyCUDA是一个非常强大和多功能的Python库，但它的主要目的是提供一个网关来编写、编译和启动CUDA内核，而不是提供一个接口给CUDA库。幸运的是，有一个免费的Python模块可用，它提供了一个用户友好的包装器接口给这些库。这就是Scikit-CUDA。
- en: While you don't have to know PyCUDA or even understand GPU programming to appreciate
    Scikit-CUDA, it is conveniently compatible with PyCUDA; Scikit-CUDA, for instance,
    can operate easily with PyCUDA's `gpuarray` class, and this allows you to easily
    pass data between our own CUDA kernel routines and Scikit-CUDA. Additionally,
    most routines will also work with PyCUDA's stream class, which will allow us to
    properly synchronize our own custom CUDA kernels with Scikit-CUDA's wrappers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您不必了解PyCUDA甚至理解GPU编程就能欣赏Scikit-CUDA，但它与PyCUDA兼容，例如，Scikit-CUDA可以轻松地与PyCUDA的`gpuarray`类一起使用，这使您可以轻松地在我们自己的CUDA内核例程和Scikit-CUDA之间传递数据。此外，大多数例程也可以与PyCUDA的stream类一起使用，这将允许我们正确地同步我们自己的自定义CUDA内核和Scikit-CUDA的包装器。
- en: Please note that, besides these three listed libraries, Scikit-CUDA also provides
    wrappers for the proprietary CULA library, as well as for the open source MAGMA
    library. Both have a lot of overlap with the functionality provided by the official
    NVIDIA libraries. Since these libraries are not installed by default with a standard
    CUDA installation, we will opt to not cover them in this chapter. Interested readers
    can learn more about CULA and MAGMA at [http://www.culatools.com](http://www.culatools.com) and [http://icl.utk.edu/magma/](http://icl.utk.edu/magma/),
    respectively. It is suggested that readers take a look at the official documentation
    for Scikit-CUDA, which is available here: [https://media.readthedocs.org/pdf/scikit-cuda/latest/scikit-cuda.pdf](https://media.readthedocs.org/pdf/scikit-cuda/latest/scikit-cuda.pdf).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了这三个列出的库之外，Scikit-CUDA还为专有的CULA库提供了包装器，以及开源的MAGMA库。这两者在功能上与官方的NVIDIA库有很多重叠。由于这些库在标准CUDA安装中默认未安装，我们将选择不在本章中涵盖它们。感兴趣的读者可以分别在[http://www.culatools.com](http://www.culatools.com)和[http://icl.utk.edu/magma/](http://icl.utk.edu/magma/)了解更多关于CULA和MAGMA的信息。建议读者查看Scikit-CUDA的官方文档，网址为：[https://media.readthedocs.org/pdf/scikit-cuda/latest/scikit-cuda.pdf](https://media.readthedocs.org/pdf/scikit-cuda/latest/scikit-cuda.pdf)。
- en: 'The learning outcomes for this chapter are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习成果如下：
- en: To learn how to install Scikit-CUDA
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何安装Scikit-CUDA
- en: To understand the basic purposes and differences between the standard CUDA libraries
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解标准CUDA库的基本目的和区别
- en: To learn how to use low-level cuBLAS functions for basic linear algebra
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何使用低级cuBLAS函数进行基本线性代数
- en: To learn how to use the SGEMM and DGEMM operations to measure the performance
    of a GPU in FLOPS
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何使用SGEMM和DGEMM操作来测量GPU在FLOPS中的性能
- en: To learn how to use cuFFT to perform 1D or 2D FFT operations on the GPU
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何使用cuFFT在GPU上执行1D或2D FFT操作
- en: To learn how to create a 2D convolutional filter using the FFT, and apply it
    to simple image processing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何使用FFT创建2D卷积滤波器，并将其应用于简单的图像处理
- en: To understand how to perform a Singular Value Decomposition (SVD) with cuSolver
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何使用cuSolver执行奇异值分解（SVD）
- en: To learn how to use cuSolver's SVD algorithm to perform basic principal component
    analysis
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何使用cuSolver的SVD算法执行基本主成分分析
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all of the necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) that
    includes the PyCUDA module is also required.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一台安装了现代NVIDIA GPU（2016年及以后）的Linux或Windows 10 PC，并安装了所有必要的GPU驱动程序和CUDA Toolkit（9.0及以后）。还需要一个合适的Python
    2.7安装（如Anaconda Python 2.7），其中包括PyCUDA模块。
- en: This chapter's code is also available on GitHub, and can be found at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可以在GitHub上找到，网址为：[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)。
- en: For more information about the prerequisites, check out the preface of this
    book. For more information about the software and hardware requirements, check
    out the README file at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有关先决条件的更多信息，请查看本书的前言。有关软件和硬件要求的更多信息，请查看[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)上的README文件。
- en: Installing Scikit-CUDA
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Scikit-CUDA
- en: It is suggested that you install the latest stable version of Scikit-CUDA directly
    from GitHub: [https://github.com/lebedov/scikit-cuda](https://github.com/lebedov/scikit-cuda).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 建议您直接从GitHub安装最新稳定版本的Scikit-CUDA：[https://github.com/lebedov/scikit-cuda](https://github.com/lebedov/scikit-cuda)。
- en: Unzip the package into a directory, and then open up the command line here and
    install the module by typing `python setup.py install` into the command line.
    You may then run the unit tests to ensure that a correct installation has been
    performed with `python setup.py test`. (This method is suggested for both Windows
    and Linux users.) Alternatively, Scikit-CUDA can be installed directly from the
    PyPI repository with `pip install scikit-cuda`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将软件包解压缩到一个目录中，然后在此处打开命令行，并键入`python setup.py install`来安装模块。然后，您可以运行单元测试以确保已使用`python
    setup.py test`执行了正确的安装。（此方法建议Windows和Linux用户均使用。）或者，可以直接使用`pip install scikit-cuda`从PyPI存储库安装Scikit-CUDA。
- en: Basic linear algebra with cuBLAS
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuBLAS进行基本线性代数
- en: We will start this chapter by learning how to use Scikit-CUDA's cuBLAS wrappers.
    Let's spend a moment discussing BLAS. BLAS (Basic Linear Algebra Subroutines)
    is a specification for a basic linear algebra library that was first standardized
    in the 1970s. BLAS functions are broken down into several categories, which are
    referred to as *levels*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从学习如何使用Scikit-CUDA的cuBLAS包装器开始这一章。让我们花一点时间讨论BLAS。BLAS（基本线性代数子程序）是一个基本线性代数库的规范，最早是在1970年代标准化的。BLAS函数被分为几个类别，被称为*级别*。
- en: Level 1 BLAS functions consist of operations purely on vectors—vector-vector
    addition and scaling (also known as *ax+y* operations, or AXPY), dot products,
    and norms. Level 2 BLAS functions consist of general matrix-vector operations
    (GEMV), such as matrix multiplication of a vector, while level 3 BLAS functions
    consist of "general matrix-matrix" (GEMM) operations, such as matrix-matrix multiplication. Originally,
    these libraries were written entirely in FORTRAN in the 1970s, so you should take
    into account that there are some seemingly archaic holdovers in usage and naming
    that may seem cumbersome to new users today.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Level 1 BLAS函数包括纯粹在向量上的操作——向量-向量加法和缩放（也称为*ax+y*操作，或AXPY），点积和范数。Level 2 BLAS函数包括一般矩阵-向量操作（GEMV），例如矩阵与向量的乘法，而Level
    3 BLAS函数包括“一般矩阵-矩阵”（GEMM）操作，例如矩阵-矩阵乘法。最初，这些库是在1970年代完全用FORTRAN编写的，因此您应该考虑到在使用和命名上可能存在一些看似过时的遗留问题，这可能对今天的新用户来说显得繁琐。
- en: cuBLAS is NVIDIA's own implementation of the BLAS specification, which is of
    course optimized to make full use of the GPU's parallelism. Scikit-CUDA provides
    wrappers for cuBLAS that are compatible with PyCUDA `gpuarray` objects, as well
    as with PyCUDA streams. This means that we can couple and interface these functions
    with our own custom CUDA-C kernels by way of PyCUDA, as well as synchronize these
    operations over multiple streams.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: cuBLAS是NVIDIA自己对BLAS规范的实现，当然是经过优化以充分利用GPU的并行性。Scikit-CUDA提供了与PyCUDA `gpuarray`对象兼容的cuBLAS包装器，以及与PyCUDA流兼容的包装器。这意味着我们可以通过PyCUDA将这些函数与我们自己的自定义CUDA-C内核耦合和接口，以及在多个流上同步这些操作。
- en: Level-1 AXPY with cuBLAS
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuBLAS进行Level-1 AXPY
- en: Let's start with a basic level-1 *ax + y* (or AXPY) operation with cuBLAS. Let's
    stop for a moment and review a bit of linear algebra and think about what this
    means. Here, *a* is considered to be a scalar; that is, a real number, such as
    -10, 0, 1.345, or 100. *x* and *y* are considered to be vectors in some vector
    space, ![](assets/47a6873c-3e1b-4b3c-95e8-d1a3a4f796eb.png). This means that *x*
    and *y* are n-tuples of real numbers, so in the case of ![](assets/d0e81dc7-0fa8-4bce-a264-941fee2e3ad7.png),
    these could be values such as `[1,2,3]` or `[-0.345, 8.15, -15.867]`. *ax* means
    the scaling of *x* by *a*, so if a is 10 and *x* is the first prior value, then
    *ax* is each individual value of *x* multiplied by *a;* that is, `[10, 20, 30]`.
    Finally, the sum *ax + y* means that we add each individual value in each slot
    of both vectors to produce a new vector, which would be as follows (assuming that *y*
    is the second vector given)—`[9.655, 28.15, 14.133]`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从cuBLAS开始进行基本的Level-1 *ax + y*（或AXPY）操作。让我们停下来，回顾一下线性代数的一点，并思考这意味着什么。在这里，*a*被认为是一个标量；也就是说，一个实数，比如-10、0、1.345或100。*x*和*y*被认为是某个向量空间中的向量，![](assets/47a6873c-3e1b-4b3c-95e8-d1a3a4f796eb.png)。这意味着*x*和*y*是实数的n元组，因此在![](assets/d0e81dc7-0fa8-4bce-a264-941fee2e3ad7.png)的情况下，这些值可以是`[1,2,3]`或`[-0.345,
    8.15, -15.867]`。*ax*表示*x*的缩放乘以*a*，因此如果*a*是10且*x*是先前的第一个值，则*ax*是*x*的每个单独值乘以*a*；也就是`[10,
    20, 30]`。最后，和*ax + y*表示我们将两个向量中每个槽的每个单独值相加以产生一个新的向量，假设*y*是给定的第二个向量，结果将如下所示-`[9.655,
    28.15, 14.133]`。
- en: 'Let''s do this in cuBLAS now. First, let''s import the appropriate modules:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在cuBLAS中做这个。首先，让我们导入适当的模块：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now let''s import cuBLAS:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们导入cuBLAS：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now set up our vector arrays and copy them to the GPU. Note that we
    are using 32-bit (single precision) floating point numbers:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以设置我们的向量数组并将它们复制到GPU。请注意，我们使用32位（单精度）浮点数：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now have to create a **cuBLAS context**. This is similar in nature to CUDA
    contexts, which we discussed in [Chapter 5](ea648e20-8c72-44a9-880d-11469d0e291f.xhtml),
    *Streams, Events, Contexts, and Concurrency*, only this time it is used explicitly
    for managing cuBLAS sessions. The `cublasCreate` function creates a cuBLAS context
    and gives a handle to it as its output. We will need to hold onto this handle
    for as long as we intend to use cuBLAS in this session:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须创建一个**cuBLAS上下文**。这在本质上类似于CUDA上下文，在[第5章](ea648e20-8c72-44a9-880d-11469d0e291f.xhtml)中我们讨论过，只是这一次它被显式用于管理cuBLAS会话。`cublasCreate`函数创建一个cuBLAS上下文，并将其句柄作为输出。我们需要保存这个句柄，只要我们打算在此会话中使用cuBLAS：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now use the `cublasSaxpy` function. The `S` stands for single precision,
    which is what we will need since we are working with 32-bit floating point arrays:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`cublasSaxpy`函数。`S`代表单精度，这是我们需要的，因为我们正在处理32位浮点数组：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let's discuss what we just did. Also, let's keep in mind that this is a direct
    wrapper to a low-level C function, so the input may seem more like a C function
    than a true Python function. In short, this performed an "AXPY" operation, ultimately
    putting the output data into the `y_gpu` array. Let's go through each input parameter
    one by one.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下我们刚刚做的事情。还要记住，这是一个直接包装到低级C函数的函数，因此输入可能更像C函数而不是真正的Python函数。简而言之，这执行了一个“AXPY”操作，最终将输出数据放入`y_gpu`数组中。让我们逐个讨论每个输入参数。
- en: 'The first input is always the CUDA context handle. We then have to specify
    the size of the vectors, since this function will be ultimately operating on C
    pointers; we can do this by using the `size` parameter of a gpuarray. Having typecasted
    our scalar already to a NumPy `float32` variable, we can pass the `a` variable
    right over as the scalar parameter. We then hand the underlying C pointer of the
    `x_gpu` array to this function using the `gpudata` parameter. Then we specify
    the **stride** of the first array as 1: the stride specifies how many steps we
    should take between each input value. (In contrast, if you were using a vector
    from a column in a row-wise matrix, you would set the stride to the width of the
    matrix.) We then put in the pointer to the `y_gpu` array, and set its stride to
    1 as well.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输入始终是CUDA上下文句柄。然后我们必须指定向量的大小，因为这个函数最终将在C指针上操作；我们可以使用gpuarray的`size`参数来做到这一点。已经将我们的标量类型转换为NumPy的`float32`变量，我们可以将`a`变量直接作为标量参数传递。然后我们使用`gpudata`参数将`x_gpu`数组的底层C指针传递给这个函数。然后我们将第一个数组的**步长**设置为1：步长指定每个输入值之间应该走多少步。
    （相反，如果您正在使用来自行向矩阵的列的向量，则将步长设置为矩阵的宽度。）然后我们放入`y_gpu`数组的指针，并将其步长也设置为1。
- en: 'We are done with our computation; now we have to explicitly destroy our cuBLAS
    context:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了计算；现在我们必须明确销毁我们的cuBLAS上下文：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now verify whether this is close with NumPy''s `allclose` function,
    like so:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用NumPy的`allclose`函数来验证这是否接近，就像这样：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Again, notice that the final output was put into the `y_gpu` array, which was
    also an input.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，最终输出被放入了`y_gpu`数组中，这也是一个输入。
- en: Always remember that BLAS and CuBLAS functions act in-place to save time and
    memory from a new allocation call. This means that an input array will also be
    used as an output!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 始终记住，BLAS和CuBLAS函数是就地操作，以节省时间和内存，而不是进行新的分配调用。这意味着输入数组也将用作输出！
- en: We just saw how to perform an `AXPY` operation using the `cublasSaxpy` function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到如何使用`cublasSaxpy`函数执行`AXPY`操作。
- en: Let's discuss the prominent upper case S. Like we mentioned previously, this
    stands for single precision that is, 32-bit real floating point values (`float32`).
    If we want to operate on arrays of 64-bit real floating point values, (`float64`
    in NumPy and PyCUDA), then we would use the `cublasDaxpy` function; for 64-bit
    single precision complex values (`complex64`), we would use `cublasCaxpy`, while
    for 128-bit double precision complex values (`complex128`), we would use `cublasZaxpy`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论突出的大写字母S。正如我们之前提到的，这代表单精度，即32位实数浮点值（`float32`）。如果我们想要操作64位实数浮点值数组（NumPy和PyCUDA中的`float64`），那么我们将使用`cublasDaxpy`函数；对于64位单精度复数值（`complex64`），我们将使用`cublasCaxpy`，而对于128位双精度复数值（`complex128`），我们将使用`cublasZaxpy`。
- en: We can tell what type of data a BLAS or CuBLAS function operates on by checking
    the letter preceding the rest of the function name. Functions that use single
    precision reals are always preceded with S, double precision reals with D, single
    precision complex with C, and double precision complex with Z.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查函数名称其余部分之前的字母来确定BLAS或CuBLAS函数操作的数据类型。使用单精度实数的函数总是以S开头，双精度实数以D开头，单精度复数以C开头，双精度复数以Z开头。
- en: Other level-1 cuBLAS functions
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他一级cuBLAS函数
- en: 'Let''s look at a few other level-1 functions. We won''t go over their operation
    in depth, but the steps are similar to the ones we just covered: create a cuBLAS
    context, call the function with the appropriate array pointers (which is accessed
    with the `gpudata` parameter from a PyCUDA `gpuarray`), and set the strides accordingly.
    Another thing to keep in mind is that if the output of a function is a single
    value as opposed to an array (for example, a dot product function), the function
    will directly output this value to the host rather than within an array of memory
    that has to be pulled off the GPU. (We will only cover the single precision real
    versions here, but the corresponding versions for other datatypes can be used
    by replacing the S with the appropriate letter.)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其他一些一级函数。我们不会深入介绍它们的操作，但步骤与我们刚刚介绍的类似：创建一个cuBLAS上下文，使用适当的数组指针调用函数（可以通过PyCUDA的`gpuarray`的`gpudata`参数访问），并相应地设置步长。另一件需要记住的事情是，如果函数的输出是单个值而不是数组（例如，点积函数），则函数将直接将该值输出到主机，而不是在必须从GPU中取出的内存数组中。（我们只会在这里介绍单精度实数版本，但其他数据类型的相应版本可以通过用适当的字母替换S来使用。）
- en: 'We can perform a dot product between two single precision real `gpuarray`s,
    `v_gpu`, and `w_gpu`. Again, the 1s are there to ensure that we are using stride-1
    in this calculation! Again, recall that a dot product is the sum of the point-wise
    multiple of two vectors:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对两个单精度实数`gpuarray`，`v_gpu`和`w_gpu`进行点积。再次，1是为了确保我们在这个计算中使用步长1！再次回想一下，点积是两个向量的逐点乘积的和：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can also perform the L2-norm of a vector like so (recall that for a vector, *x*, this
    is its L2-norm, or length, which is calculated with the ![](assets/839337d6-db29-481e-8467-bcd415a2ad7c.png) formula):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以执行向量的L2范数，就像这样（回想一下，对于一个向量*x*，这是它的L2范数，或长度，可以用![](assets/839337d6-db29-481e-8467-bcd415a2ad7c.png)公式来计算）：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Level-2 GEMV in cuBLAS
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuBLAS中的二级GEMV
- en: 'Let''s look at how to do a `GEMV` matrix-vector multiplication. This is defined
    as the following operation for an *m* x *n* matrix *A*, an n-dimensional vector
    *x*, a *m*-dimensional vector *y*, and for the scalars *alpha* and *beta*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何进行`GEMV`矩阵-向量乘法。对于一个*m* x *n*矩阵*A*，一个n维向量*x*，一个*m*维向量*y*，以及标量*alpha*和*beta*，这被定义为以下操作：
- en: '![](assets/0b6277ff-e027-45fe-ad2e-d312ea1a38f5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/0b6277ff-e027-45fe-ad2e-d312ea1a38f5.png)'
- en: 'Now let''s look at how the function is laid out before we continue:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在继续之前看一下函数的布局：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s go through these inputs one-by-one:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个检查这些输入：
- en: '`handle` refers to the cuBLAS context handle.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`handle`指的是cuBLAS上下文句柄。'
- en: '`trans` refers to the structure of the matrix—we can specify whether we want
    to use the original matrix, a direct transpose, or a conjugate transpose (for
    complex matrices). This is important to keep in mind because this function will
    expect that the matrix `A` is stored in **column-major** format.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trans`指的是矩阵的结构——我们可以指定是否要使用原始矩阵、直接转置或共轭转置（对于复数矩阵）。这很重要要记住，因为这个函数将期望矩阵`A`以**列主**格式存储。'
- en: '`m` and `n` are the number of rows and columns of the matrix `A` that we want
    to use.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m`和`n`是我们想要使用的矩阵`A`的行数和列数。'
- en: '`alpha` is the floating-point value for *α.*'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`是浮点值*α*。'
- en: '`A` is the *m x n* matrix *A.*'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`A`是*m x n*矩阵*A*。'
- en: '`lda` indicates the leading dimension of the matrix, where the total size of
    the matrix is actually `lda` x `n`. This is important in the column-major format
    because if `lda` is larger than `m`, this can cause problems for cuBLAS when it
    tries to access the values of `A` since its underlying structure of this matrix
    is a one-dimensional array.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lda`指示矩阵的主维度，矩阵的总大小实际上是`lda` x `n`。这在列主格式中很重要，因为如果`lda`大于`m`，这可能会导致cuBLAS在尝试访问`A`的值时出现问题，因为该矩阵的基础结构是一个一维数组。'
- en: We then have `x` and its stride, `incx`; `x` is the underlying C pointer of
    the vector being multiplied by `A`. Remember, `x` will have to be of size `n`; that
    is, the number of columns of `A`.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们有`x`及其步长`incx`；`x`是被`A`相乘的向量的基础C指针。记住，`x`的大小必须是`n`；也就是说，`A`的列数。
- en: '`beta`, which is the floating-point value for *β*.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta`是浮点值*β*。'
- en: Finally, we have `y` and its stride `incy` as the last parameters. We should
    remember that `y` should be of size `m`, or the number of rows of `A`.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们有`y`及其步长`incy`作为最后的参数。我们应该记住，`y`的大小应该是`m`，或者`A`的行数。
- en: 'Let''s test this by generating a 10 x 100 matrix of random values `A`, and
    a vector `x` of 100 random values. We''ll initialize `y` as a matrix of 10 zeros.
    We will set alpha to 1 and beta to 0, just to get a direct matrix multiplication
    with no scaling:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过生成一个10 x 100的随机值矩阵`A`，和一个包含100个随机值的向量`x`来测试这个。我们将初始化`y`为一个包含10个零的矩阵。我们将alpha设置为1，beta设置为0，以便直接进行矩阵乘法而不进行缩放：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will now have to get `A` into **column-major** (or column-wise) format.
    NumPy stores matrices as **row-major** (or row-wise) by default, meaning that
    the underlying one-dimensional array that is used to store a matrix iterates through
    all of the values of the first row, then all of the values of the second row,
    and so on. You should remember that a transpose operation swaps the columns of
    a matrix with its rows. However, the result will be that the new one-dimensional
    array underlying the transposed matrix will represent the original matrix in a
    column-major format. We can make a copy of the transposed matrix of `A` with `A.T.copy()` like
    so, and copy this as well as `x` and `y` to the GPU:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须将`A`转换为**列主**（或按列）格式。NumPy默认将矩阵存储为**行主**（或按行）格式，这意味着用于存储矩阵的基础一维数组会遍历所有第一行的值，然后遍历所有第二行的值，依此类推。您应该记住，转置操作会将矩阵的列与其行交换。然而，结果将是转置矩阵的新一维数组将以列主格式表示原始矩阵。我们可以通过`A.T.copy()`这样做，将`A`的转置矩阵的副本以及`x`和`y`一起复制到GPU上：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Since we now have the column-wise matrix stored properly on the GPU, we can
    set the `trans` variable to not take the transpose by using the `_CUBLAS_OP` dictionary:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在已经正确地在GPU上存储了按列的矩阵，我们可以通过使用`_CUBLAS_OP`字典将`trans`变量设置为不进行转置：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since the size of the matrix is exactly the same as the number of rows that
    we want to use, we now set `lda` as `m`. The strides for the *x* and *y* vectors
    are, again, 1\. We now have all of the values we need set up, and can now create
    our CuBLAS context and store its handle, like so:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵的大小与我们想要使用的行数完全相同，我们现在将`lda`设置为`m`。*x*和*y*向量的步长再次为1。我们现在已经设置好了所有需要的值，现在可以创建我们的CuBLAS上下文并存储它的句柄，就像这样：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can now launch our function. Remember that `A`, `x`, and `y` are actually
    PyCUDA `gpuarray` objects, so we have to use the `gpudata` parameter to input
    into this function. Other than doing this, this is pretty straightforward:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以启动我们的函数。记住，`A`，`x`和`y`实际上是PyCUDA `gpuarray`对象，所以我们必须使用`gpudata`参数输入到这个函数中。除了这个，这很简单：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now destroy our cuBLAS context and check the return value to ensure
    that it is correct:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以销毁我们的cuBLAS上下文并检查返回值以确保它是正确的：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Level-3 GEMM in cuBLAS for measuring GPU performance
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuBLAS中的三级GEMM用于测量GPU性能
- en: 'We will now look at how to perform a **general matrix-matrix multiplication**
    (**GEMM**) with CuBLAS. We will actually try to make something a little more utilitarian
    than the last few examples we saw in cuBLAS—we will use this as a performance
    metric for our GPU to determine the number of **Floating Point Operations Per
    Second** (**FLOPS**) it can perform, which will be two separate values: the case
    of single precision, and that of double precision. Using GEMM is a standard technique
    for evaluating the performance of computing hardware in FLOPS, as it gives a much
    better understanding of sheer computational power than using pure clock speed
    in MHz or GHz.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看如何使用CuBLAS执行**通用矩阵-矩阵乘法**（**GEMM**）。实际上，我们将尝试制作一些比我们在cuBLAS中看到的最后几个示例更实用的东西-我们将使用这个作为我们的GPU性能指标，以确定它可以执行的**每秒浮点运算次数**（**FLOPS**）的数量，这将是两个单独的值：单精度和双精度的情况。使用GEMM是评估FLOPS中计算硬件性能的标准技术，因为它比使用纯时钟速度（MHz或GHz）更好地理解了纯计算能力。
- en: If you need a brief review, recall that we covered matrix-matrix multiplication
    in depth in the last chapter. If you forgot how this works, it's strongly suggested
    that you review this chapter before you move on to this section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要简要回顾，请回想一下我们在上一章中深入讨论了矩阵-矩阵乘法。如果您忘记了这是如何工作的，强烈建议您在继续本节之前复习一下这一章。
- en: 'First, let''s see how a GEMM operation is defined:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看GEMM操作是如何定义的：
- en: '![](assets/6732ed55-6eea-497a-adcb-95731cc211b9.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6732ed55-6eea-497a-adcb-95731cc211b9.png)'
- en: This means that we perform a matrix multiplication of *A* and *B*, scale the
    result by *alpha*, and then add this to the *C* matrix that we have scaled by
    *beta, *placing the final result in *C*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们执行*A*和*B*的矩阵乘法，将结果缩放为*alpha*，然后加到我们已经通过*beta*缩放的*C*矩阵中，将最终结果放在*C*中。
- en: 'Let''s think about how many floating point operations are executed to get the
    final result of a real-valued GEMM operation, assuming that *A* is an *m* x *k*
    (where *m* is rows and* k* is columns)matrix, *B* is a *k* x *n* matrix, and C
    is an *m* x *n* matrix. First, let''s figure out how many operations are required
    for computing *AB*. Let''s take a single column of *A* and multiply it by *B*:
    this will amount to *k* multiplies and *k - 1* adds for each of the *m* rows in
    *A*, which means that this is *km + (k-1)m* total operations over *m* rows. There
    are *n* columns in *B*, so computing *AB* will total to *kmn + (k-1)mn = 2kmn
    - mn* operations. Now, we use *alpha* to scale *AB*, which will be *m**n* operations,
    since that is the size of the matrix *AB*; similarly, scaling *C* by *beta* is
    another *m**n* operation. Finally, we add these two resulting matrices, which
    is yet another *mn* operation. This means that we will have a total of *2kmn -
    mn + 3mn = 2kmn + 2mn = 2mn(k+1)* floating point operations in a given GEMM operation.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下执行实值GEMM操作的最终结果需要执行多少浮点运算，假设*A*是一个*m* x *k*（其中*m*是行，*k*是列）矩阵，*B*是一个*k*
    x *n*矩阵，C是一个*m* x *n*矩阵。首先，让我们计算*AB*需要多少操作。让我们取*A*的一列并将其乘以*B*：这将导致*m*行中的每一行需要*k*次乘法和*k-1*次加法，这意味着这是*m*行上的*km
    + (k-1)m*总操作。*B*中有*n*列，因此计算*AB*将总共需要*kmn + (k-1)mn = 2kmn - mn*次操作。现在，我们使用*alpha*来缩放*AB*，这将是*m**n*次操作，因为这是矩阵*AB*的大小；类似地，通过*beta*缩放*C*是另外*m**n*次操作。最后，我们将这两个结果矩阵相加，这又是*mn*次操作。这意味着在给定的GEMM操作中，我们将有*2kmn
    - mn + 3mn = 2kmn + 2mn = 2mn(k+1)*次浮点运算。
- en: Now the only thing we have to do is run a timed GEMM operation, taking note
    of the different sizes of the matrices, and divide *2kmn + 2mn* by the total time
    duration to calculate the FLOPS of our GPU. The resulting number will be very
    large, so we will represent this in terms of GFLOPS – that is, how many billions
    (10⁹) of operations that can be computed per second. We can compute this by multiplying
    the FLOPS value by 10^(-9).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们唯一需要做的就是运行一个计时的GEMM操作，注意矩阵的不同大小，并将*2kmn + 2mn*除以总时间持续时间来计算我们GPU的FLOPS。结果数字将非常大，因此我们将以GFLOPS的形式表示这一点-也就是说，每秒可以计算多少十亿（10⁹）次操作。我们可以通过将FLOPS值乘以10^(-9)来计算这一点。
- en: 'Now we are ready to start coding this up. Let''s start with our import statements,
    as well as the `time` function:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始编写代码。让我们从我们的导入语句开始，以及`time`函数：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we will set the `m`, `n`, and `k` variables for our matrix sizes. We want
    our matrices to be relatively big so that the time duration is sufficiently large
    so as to avoid divide by 0 errors. The following values should be sufficient for
    any GPU released up to mid-2018 or earlier; users with newer cards may consider
    increasing these values:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将为我们的矩阵大小设置`m`，`n`和`k`变量。我们希望我们的矩阵相对较大，以便时间持续足够长，以避免除以0的错误。以下值对于截至2018年中旬或更早发布的任何GPU来说应该足够了；拥有更新卡的用户可能考虑增加这些值：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will now write a function that computes the GFLOPS for both single and double
    precision. We will set the input value to `''D''` if we wish to use double precision,
    or `''S''` otherwise:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将编写一个计算单精度和双精度GFLOPS的函数。如果我们希望使用双精度，则将输入值设置为'D'，否则设置为'S'：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now let's generate some random matrices that are of the appropriate precision
    that we will use for timing. The GEMM operations act similarly to the GEMV operation
    we saw before, so we will have to transpose these before we copy them to the GPU.
    (Since we are just doing timing, this step isn't necessary, but it's good practice
    to remember this.)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们生成一些随机矩阵，这些矩阵具有我们将用于计时的适当精度。GEMM操作与我们之前看到的GEMV操作类似，因此我们必须在将它们复制到GPU之前对其进行转置。（由于我们只是在计时，这一步并不是必要的，但记住这一点是个好习惯。）
- en: 'We will set up some other necessary variables for GEMM, whose purpose should
    be self-explanatory at this point (`transa`, `lda`, `ldb`, and so on):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为GEMM设置一些其他必要的变量，这些变量在这一点上应该是不言自明的（`transa`，`lda`，`ldb`等）：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can now start the timer! First, we will create a cuBLAS context:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始计时了！首先，我们将创建一个cuBLAS上下文：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will now launch GEMM. Keep in mind that there are two versions for the real
    case: `cublasSgemm` for single precision and `cublasDgemm` for double precision.
    We can execute the appropriate function using a little Python trick: we will write
    a string with `cublas%sgemm` with the appropriate parameters, and then replace
    the `%s` with D or S by appending `% precision` to the string. We will then execute
    this string as Python code with the `exec` function, like so:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将启动GEMM。请记住，对于实数情况有两个版本：`cublasSgemm`用于单精度，`cublasDgemm`用于双精度。我们可以使用一个小小的Python技巧执行适当的函数：我们将用`cublas%sgemm`和适当的参数写一个字符串，然后通过附加`%
    precision`将`%s`替换为D或S。然后我们将使用`exec`函数将这个字符串作为Python代码执行，就像这样：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can now destroy the cuBLAS context and get the final time for our computation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以销毁cuBLAS上下文，并得到我们计算的最终时间：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then we need to compute the GFLOPS using the equation we derived and return
    it as the output of this function:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要使用我们推导出的方程计算GFLOPS，并将其作为这个函数的输出返回：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we can set up our main function. We will output the GFLOPS in both the
    single and double precision cases:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以设置我们的主函数。我们将输出单精度和双精度情况下的GFLOPS：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now let''s do a little homework before we run this program—go to [https://www.techpowerup.com](https://www.techpowerup.com)
    and search for your GPU, and then take note of two things—the single precision
    floating point performance and the double precision floating point performance.
    I am using a GTX 1050 right now, and it''s listing claims that it has 1,862 GFLOPS
    performance in single precision, and 58.20 GFLOPS performance in double precision.
    Let''s run this program right now and see if this aligns with the truth:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在运行这个程序之前，让我们做一点功课——去[https://www.techpowerup.com](https://www.techpowerup.com)搜索你的GPU，然后注意两件事——单精度浮点性能和双精度浮点性能。我现在使用的是GTX
    1050，它的列表声称在单精度上有1,862 GFLOPS性能，在双精度上有58.20 GFLOPS性能。让我们现在运行这个程序，看看这是否符合事实：
- en: '![](assets/0d014970-b902-4cd8-804a-433bf0b83d77.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/0d014970-b902-4cd8-804a-433bf0b83d77.png)'
- en: Lo and behold, it does!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 看哪，它成功了！
- en: This program is also available as the `cublas_gemm_flops.py` file under the
    directory in this book's repository.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序也可以在这本书的存储库目录下的`cublas_gemm_flops.py`文件中找到。
- en: Fast Fourier transforms with cuFFT
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuFFT进行快速傅里叶变换
- en: 'Now let''s look at how we can do some basic **fast Fourier transforms** (**FFT**)
    with cuFFT.  First, let''s briefly review what exactly a Fourier transform is.
    If you have taken an advanced Calculus or Analysis class, you might have seen
    the Fourier transform defined as an integral formula, like so:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用cuFFT进行一些基本的**快速傅里叶变换**（**FFT**）。首先，让我们简要回顾一下傅里叶变换到底是什么。如果你上过高级微积分或分析课程，你可能已经见过傅里叶变换被定义为一个积分公式，就像这样：
- en: '![](assets/d1c79c32-6eab-4a52-a2ef-4af0bc192f5c.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d1c79c32-6eab-4a52-a2ef-4af0bc192f5c.png)'
- en: What this does is take *f* as a time domain function over *x*. This gives us
    a corresponding frequency domain function over "ξ".  This turns out to be an incredibly
    useful tool that touches virtually all branches of science and engineering.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序将*f*作为*x*的时间域函数。这给了我们一个相应的频域函数，对应于"ξ"。这事实上是一个极其有用的工具，几乎触及到所有科学和工程的分支。
- en: 'Let''s remember that the integral can be thought of as a sum; likewise, there
    is a corresponding discrete, finite version of the Fourier Transform called the
    **discrete Fourier transform** (**DFT**). This operates on vectors of a finite
    length and allows them to be analyzed or modified in the frequency domain. The
    DFT of an *n*-dimensional vector *x* is defined as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住积分可以被看作是一个求和；同样地，有一个对应的离散、有限版本的傅里叶变换，称为**离散傅里叶变换**（**DFT**）。这对长度为*n*的向量进行操作，并允许它们在频域中进行分析或修改。*x*的DFT被定义如下：
- en: '![](assets/8b60bac2-8488-4c5f-9d90-3ac7eb73bd62.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8b60bac2-8488-4c5f-9d90-3ac7eb73bd62.png)'
- en: In other words, we can multiply a vector, *x*, by the complex *N* x *N* matrix ![](assets/96b3a1fb-9202-44fa-8b1d-381398412504.png)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以将一个向量*x*乘以复杂的*N* x *N*矩阵 ![](assets/96b3a1fb-9202-44fa-8b1d-381398412504.png)
- en: '(here, *k* corresponds to row number, while *n* corresponds to column number)
    to find its DFT. We should also note the inverse formula that lets us retrieve *x*
    from its DFT (replace *y* with the DFT of *x* here, and the output will be the
    original *x*):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: （这里，*k*对应行号，*n*对应列号）来找到它的DFT。我们还应该注意到逆公式，它让我们从它的DFT中检索*x*（在这里用*x*的DFT替换*y*，输出将是原始的*x*）：
- en: '![](assets/25229fe6-66c3-4ca5-b8d7-96ba5e639917.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/25229fe6-66c3-4ca5-b8d7-96ba5e639917.png)'
- en: Normally, computing a matrix-vector operation is of computational complexity
    O(*N²*) for a vector of length *N*. However, due to symmetries in the DFT matrix,
    this can always be reduced to O(*N log N*) by using an FFT. Let's look at how
    we can use an FFT with CuBLAS, and then we will move on to a more interesting
    example.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，计算矩阵-向量操作的计算复杂度对于长度为*N*的向量是O(*N²*)。然而，由于DFT矩阵中的对称性，这总是可以通过使用FFT减少到O(*N log
    N*)。让我们看看如何使用FFT与CuBLAS，然后我们将继续一个更有趣的例子。
- en: A simple 1D FFT
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的一维FFT
- en: Let's start by looking at how we can use cuBLAS to compute a simple 1D FFT.
    First, we will briefly discuss the cuFFT interface in Scikit-CUDA.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看如何使用cuBLAS计算简单的一维FFT。首先，我们将简要讨论Scikit-CUDA中的cuFFT接口。
- en: There are two submodules here that we can access the cuFFT library with, `cufft`
    and `fft`. `cufft` consists of a collection of low-level wrappers for the cuFFT
    library, while `fft` provides a more user-friendly interface; we will be working
    solely with `fft` in this chapter.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个子模块，我们可以使用`cufft`和`fft`访问cuFFT库。`cufft`包括了一系列cuFFT库的低级封装，而`fft`提供了一个更加用户友好的接口；在本章中我们将只使用`fft`。
- en: 'Let''s start with the appropriate imports, remembering to include the Scikit-CUDA
    `fft` submodule:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从适当的导入开始，记得包括Scikit-CUDA的`fft`子模块：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We now will set up some random array and copy it to the GPU. We will also set
    up an empty GPU array that will be used to store the FFT (notice that we are using
    a real float32 array as an input, but the output will be a complex64 array, since
    the Fourier transform is always complex-valued):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将设置一些随机数组并将其复制到GPU。我们还将设置一个空的GPU数组，用于存储FFT（请注意，我们使用实数float32数组作为输入，但输出将是complex64数组，因为傅立叶变换总是复值）：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will now set up a cuFFT plan for the forward FFT transform. This is an object
    that cuFFT uses to determine the shape, as well as the input and output data types
    of the transform:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为正向FFT变换设置一个cuFFT计划。这是cuFFT用来确定变换的形状，以及输入和输出数据类型的对象：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will also set up a plan for the inverse FFT plan object. Notice that this
    time we go from `complex64` to real `float32`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将为逆FFT计划对象设置一个计划。请注意，这一次我们从`complex64`到实数`float32`：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we must take the forward FFT from `x_gpu` into `x_hat`, and the inverse
    FFT from `x_hat` back into `x_gpu`. Notice that we set `scale=True` in the inverse
    FFT; we do this to indicate to cuFFT to scale the inverse FFT by 1/N:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须将`x_gpu`的正向FFT转换为`x_hat`，并将`x_hat`的逆FFT转换回`x_gpu`。请注意，在逆FFT中设置了`scale=True`；我们这样做是为了告诉cuFFT将逆FFT按1/N进行缩放：
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We now will check `x_hat` against a NumPy FFT of `x`, and `x_gpu` against `x`
    itself:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将检查`x_hat`与`x`的NumPy FFT，以及`x_gpu`与`x`本身：
- en: '[PRE30]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you run this, you will see that `x_hat` does not match `y`, yet, inexplicably,
    `x_gpu` matches `x`. How is this possible? Well, let's remember that `x` is real;
    if you look at how the Discrete Fourier Transform is computed, you can prove mathematically
    that the outputs of a real vector will repeat as their complex conjugates after
    N/2\. While the NumPy FFT fully computes these values anyway, cuFFT saves time
    by only computing the first half of the outputs when it sees that the input is
    real, and it sets the remaining outputs to `0`. You should verify that this is
    the case by checking the preceding variables.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个程序，你会发现`x_hat`与`y`不匹配，然而，莫名其妙的是，`x_gpu`与`x`匹配。这是怎么可能的？好吧，让我们记住`x`是实数；如果你看一下离散傅立叶变换是如何计算的，你可以数学上证明实向量的输出在N/2之后会重复为它们的复共轭。虽然NumPy
    FFT会完全计算这些值，但cuFFT通过只计算输入为实数时的输出的前半部分来节省时间，并将其余输出设置为`0`。你应该通过检查前面的变量来验证这一点。
- en: 'Thus, if we change the first print statement in the preceding code to only
    compare the first N/2 outputs between CuFFT and NumPy, then this will return true:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们将前面代码中的第一个打印语句更改为仅比较CuFFT和NumPy之间的前N/2个输出，那么这将返回true：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using an FFT for convolution
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FFT进行卷积
- en: 'We will now look at how we can use an FFT to perform **convolution**. Let''s
    review what exactly convolution is, first: given two one-dimensional vectors,
    *x* and *y*, their convolution is defined as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看看如何使用FFT执行**卷积**。首先让我们回顾一下卷积的确切定义：给定两个一维向量*x*和*y*，它们的卷积定义如下：
- en: '![](assets/34574397-830b-446f-8e5f-34b468e76b3e.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/34574397-830b-446f-8e5f-34b468e76b3e.png)'
- en: This is of interest to us because if *x* is some long, continuous signal, and
    *y* only has a small amount of localized non-zero values, then *y* will act as
    a filter on *x*; this has many applications in itself. First, we can use a filter
    to smooth the signal *x* (as is common in digital signal processing and image
    processing). We can also use it to collect samples of the signal *x* so as to
    represent the signal or compress it (as is common in the field of data compression
    or compressive sensing), or use filters to collect features for signal or image
    recognition in machine learning. This idea forms the basis for convolutional neural
    networks).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们很有意义，因为如果*x*是一些长的连续信号，而*y*只有一小部分局部非零值，那么*y*将作为*x*的滤波器；这本身有许多应用。首先，我们可以使用滤波器平滑信号*x*（在数字信号处理和图像处理中很常见）。我们还可以使用它来收集信号*x*的样本，以表示信号或压缩它（在数据压缩或压缩感知领域很常见），或者使用滤波器为机器学习中的信号或图像识别收集特征。这个想法构成了卷积神经网络的基础。
- en: 'Of course, computers cannot handle infinitely long vectors (at least, not yet),
    so we will be considering **circular convolution**. In circular convolution, we
    are dealing with two length *n*-vectors whose indices below 0 or above n-1 will
    wrap around to the other end; that is to say, *x*[-1] = *x*[n-1], *x*[-2] = *x*[n-2],
    *x*[n] = *x*[0], *x*[n+1] = *x*[1], and so on. We define circular convolution
    of *x* and *y* like so:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，计算机无法处理无限长的向量（至少目前还不能），所以我们将考虑**循环卷积**。在循环卷积中，我们处理两个长度为*n*的向量，其索引小于0或大于n-1的部分将会环绕到另一端；也就是说，*x*[-1]
    = *x*[n-1]，*x*[-2] = *x*[n-2]，*x*[n] = *x*[0]，*x*[n+1] = *x*[1]，依此类推。我们定义*x*和*y*的循环卷积如下：
- en: '![](assets/69ee6cc9-5c17-40f2-a4f5-675f5a0a9ee2.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/69ee6cc9-5c17-40f2-a4f5-675f5a0a9ee2.png)'
- en: 'It turns out that we can perform a circular convolution using an FFT quite
    easily; we can do this by performing an FFT on *x* and *y*, point-wise-multiplying
    the outputs, and then performing an inverse FFT on the final results. This result
    is known as the **convolution theorem**, which can also be expressed as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，我们可以很容易地使用FFT执行循环卷积；我们可以对*x*和*y*执行FFT，对输出进行逐点乘法，然后对最终结果执行逆FFT。这个结果被称为**卷积定理**，也可以表示如下：
- en: '![](assets/d4a9eaac-c5fb-4ec2-8544-35b8e7388209.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d4a9eaac-c5fb-4ec2-8544-35b8e7388209.png)'
- en: We will be doing this over two dimensions, since we wish to apply the result
    to signal processing. While we have only seen the math for FFTs and convolution
    along one dimension, two-dimensional convolution and FFTs work very similarly
    to their one-dimensional counterparts, only with some more complex indexing. We
    will opt to skip over this, however, so that we can get directly into the application.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在两个维度上进行这个操作，因为我们希望将结果应用到信号处理上。虽然我们只看到了一维卷积和FFT的数学运算，但二维卷积和FFT的工作方式与一维类似，只是索引更复杂一些。然而，我们选择跳过这一点，以便我们可以直接进入应用。
- en: Using cuFFT for 2D convolution
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuFFT进行2D卷积
- en: 'Now we are going to make a small program that performs **Gaussian filtering**
    on an image using cuFFT-based two-dimensional convolution. Gaussian filtering
    is an operation that smooths a rough image using what is known as a Gaussian filter.
    This is named as such because it is based on the Gaussian (normal) distribution
    in statistics. This is how the Gaussian filter is defined over two dimensions
    with a standard deviation of σ:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将制作一个小程序，使用基于cuFFT的二维卷积对图像进行**高斯滤波**。高斯滤波是一种使用所谓的高斯滤波器平滑粗糙图像的操作。之所以这样命名，是因为它基于统计学中的高斯（正态）分布。这是高斯滤波器在两个维度上以标准偏差σ定义的方式：
- en: '![](assets/06f381b9-2e9d-48aa-95e7-265b988f144d.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/06f381b9-2e9d-48aa-95e7-265b988f144d.png)'
- en: When we convolve a discrete image with a filter, we sometimes refer to the filter
    as a **convolution kernel**. Oftentimes, image processing engineers will just
    call this a plain kernel, but since we don't want to confuse these with CUDA kernels,
    we will always use the full term, convolution kernel. We will be using a discrete
    version of the Gaussian filter as our convolution kernel here.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们用滤波器对离散图像进行卷积时，有时我们会将滤波器称为**卷积核**。通常，图像处理工程师会简单地称之为核，但由于我们不想将其与CUDA核混淆，我们将始终使用完整术语卷积核。在这里，我们将使用离散版本的高斯滤波器作为我们的卷积核。
- en: 'Let''s start with the appropriate imports; notice that we will use the Scikit-CUDA
    submodule `linalg` here. This will provide a higher-level interface for us than
    cuBLAS. Since we''re working with images here, we will also import Matplotlib''s
    `pyplot` submodule. Also note that we will use Python 3-style division here, from
    the first line; this means that if we divide two integers with the `/` operator,
    then the return value will be a float without typecasting (we perform integer
    division with the `//` operator):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从适当的导入开始；请注意，我们将在这里使用Scikit-CUDA子模块`linalg`。这将为我们提供比cuBLAS更高级的接口。由于我们在这里处理图像，我们还将导入Matplotlib的`pyplot`子模块。还要注意，我们将在此处使用Python
    3风格的除法，从第一行开始；这意味着如果我们使用`/`运算符除两个整数，那么返回值将是浮点数，无需类型转换（我们使用`//`运算符进行整数除法）：
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s jump right in and start writing the convolution function. This will
    take in two NumPy arrays of the same size, `x` and `y`. We will typecast these
    to complex64 arrays, and then return `-1` if they are not of the same size:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们立即开始编写卷积函数。这将接受两个相同大小的NumPy数组`x`和`y`。我们将把它们转换为complex64数组，然后如果它们的大小不相同，就返回`-1`：
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will now set up our FFT plan and inverse FFT plan objects:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将设置我们的FFT计划和逆FFT计划对象：
- en: '[PRE34]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now we can copy our arrays to the GPU. We will also set up some empty arrays
    of the appropriate sizes to hold the FFTs of these arrays, plus one additional
    array that will hold the output of the final convolution, `out_gpu`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将我们的数组复制到GPU。我们还将设置一些空数组，用于保存这些数组的FFT的适当大小，另外一个数组将保存最终卷积的输出，`out_gpu`：
- en: '[PRE35]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We now can perform our FFTs:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进行我们的FFT：
- en: '[PRE36]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We will now perform pointwise (Hadamard) multiplication between `x_fft` and
    `y_fft` with the `linalg.multiply` function. We will set `overwrite=True` so as
    to write the final value into `y_fft`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用`linalg.multiply`函数在`x_fft`和`y_fft`之间执行逐点（Hadamard）乘法。我们将设置`overwrite=True`，以便将最终值写入`y_fft`：
- en: '[PRE37]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we will call the inverse FFT, outputting the final result into `out_gpu`.
    We transfer this value to the host and return it:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将调用逆FFT，将最终结果输出到`out_gpu`。我们将这个值传输到主机并返回它：
- en: '[PRE38]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We are not done yet. Our convolution kernel will be much smaller than our input
    image, so we will have to adjust the sizes of our two 2D arrays (both the convolution
    kernel and the image) so that they are equal and perform the pointwise multiplication
    between them. Not only should we ensure that they are equal, but we also need
    to ensure that we perform **zero padding** on the arrays and that we appropriately
    center the convolution kernel. Zero padding means that we add a buffer of zeros
    on the sides of the images so as to prevent a wrap-around error. If we are using
    an FFT to perform our convolution, remember that it is a circular convolution,
    so the edges will literally always wrap-around. When we are done with our convolution,
    we can remove the buffer from the outside of the image to get the final output
    image.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。我们的卷积核将比输入图像小得多，因此我们将不得不调整我们的两个2D数组的大小（卷积核和图像），使它们相等，并在它们之间执行逐点乘法。我们不仅应该确保它们相等，还需要确保我们在数组上执行**零填充**，并适当地将卷积核居中。零填充意味着我们在图像的两侧添加零缓冲区，以防止环绕错误。如果我们使用FFT来执行卷积，记住它是循环卷积，因此边缘将始终环绕。当我们完成卷积后，我们可以去除图像外部的缓冲区，得到最终的输出图像。
- en: 'Let''s create a new function called `conv_2d` that takes in a convolution kernel, `ker`,
    and an image, `img`. The padded image size will be (`2*ker.shape[0] + img.shape[0]`,
    `2*ker.shape[1] + img.shape[1]`). Let''s set up the padded convolution kernel
    first. We will create a 2D array of zeros of this size, and then set the upper-left
    submatrix as our convolution kernel, like so:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为`conv_2d`的新函数，它接受卷积核`ker`和图像`img`。填充后的图像大小将是（`2*ker.shape[0] + img.shape[0]`，`2*ker.shape[1]
    + img.shape[1]`）。让我们首先设置填充的卷积核。我们将创建一个这个大小的零矩阵，然后将左上角子矩阵设置为我们的卷积核，如下所示：
- en: '[PRE39]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We will now have to shift our convolution kernel so that its center is precisely
    at the coordinate (0,0). We can do this with the NumPy `roll` command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要移动卷积核，使其中心精确地位于坐标（0,0）处。我们可以使用NumPy的`roll`命令来实现这一点：
- en: '[PRE40]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we need to pad the input image:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要对输入图像进行填充：
- en: '[PRE41]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we have two arrays of the same size that are appropriately formatted. We
    can now use our `cufft_conv` function that we just wrote here:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个大小相同且格式正确的数组。我们现在可以使用我们刚刚在这里编写的`cufft_conv`函数：
- en: '[PRE42]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We now can remove the zero buffer outside of our image. We then return the
    result:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以去除图像外部的零缓冲区。然后返回结果：
- en: '[PRE43]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We are not yet done. Let''s write some small functions to set up our Gaussian
    filter, and then we can move on to applying this to an image. We can write the
    basic filter itself with a single line using a lambda function:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。让我们编写一些小函数来设置我们的高斯滤波器，然后我们可以继续将其应用于图像。我们可以使用lambda函数一行代码来编写基本滤波器本身：
- en: '[PRE44]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can now write a function that uses this filter to output a discrete convolution
    kernel. The convolution kernel will be of height and length `2*sigma + 1`, which
    is fairly standard:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写一个使用此滤波器输出离散卷积核的函数。卷积核的高度和长度将为`2*sigma + 1`，这是相当标准的：
- en: Notice that we normalize the values of our Gaussian kernel by summing its values
    into `total_` and dividing it.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们通过将其值求和为`total_`并将其除以来规范化高斯核的值。
- en: '[PRE45]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We are now ready to test this on an image! As our test case, we will use Gaussian
    filtering to blur a color JPEG image of this book's editor, *Akshada Iyer*. (This
    image is available under the `Chapter07` directory in the GitHub repository with
    the file name `akshada.jpg`.) We will use Matplotlib's `imread` function to read
    the image; this is stored as an array of unsigned 8-bit integers ranging from
    0 to 255 by default. We will typecast this to an array of floats and normalize
    it so that all of the values will range from 0 to 1.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备在图像上测试这个！作为我们的测试案例，我们将使用高斯滤波来模糊这本书编辑*Akshada Iyer*的彩色JPEG图像。（此图像在GitHub存储库的`Chapter07`目录中，文件名为`akshada.jpg`。）我们将使用Matplotlib的`imread`函数来读取图像；默认情况下，这将存储为0到255范围内的无符号8位整数数组。我们将将其强制转换为浮点数组并对其进行规范化，以便所有值的范围为0到1。
- en: 'Note to the readers of the print edition of this text: although the print edition
    of this text is in greyscale, this a color image.We will then set up an empty
    array of zeros that will store the blurred image:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书印刷版的读者注意：尽管本书的印刷版是灰度图像，但这是一幅彩色图像。然后，我们将设置一个空的零数组，用于存储模糊的图像：
- en: '[PRE46]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s set up our convolution kernel. Here, a standard deviation of 15 should
    be enough:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置我们的卷积核。在这里，标准差为15应该足够：
- en: '[PRE47]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can now blur the image. Since this is a color image, we will have to apply
    Gaussian filtering to each color layer (red, green, and blue) individually; this
    is indexed by the third dimension in the image arrays:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以模糊图像。由于这是一幅彩色图像，我们将不得不分别对每个颜色层（红色、绿色和蓝色）应用高斯滤波；这在图像数组的第三维中进行索引：
- en: '[PRE48]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now let''s look at the Before and After images side-by-side by using some Matplotlib
    tricks:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过使用一些Matplotlib技巧并排查看Before和After图像：
- en: '[PRE49]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can now run the program and observe the effects of Gaussian filtering:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行程序并观察高斯滤波的效果：
- en: '![](assets/ae723dc0-ccbd-4163-9c08-bb7caad3aa74.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ae723dc0-ccbd-4163-9c08-bb7caad3aa74.png)'
- en: This program is available in the `Chapter07` directory in a file called `conv_2d.py`
    in the repository for this book.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序在此书的存储库中的`Chapter07`目录中的名为`conv_2d.py`的文件中可用。
- en: Using cuSolver from Scikit-CUDA
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-CUDA中的cuSolver
- en: We will now look at how we can use cuSolver from Scikit-CUDA's `linalg` submodule.
    Again, this provides a high-level interface for both cuBLAS and cuSolver, so we
    don't have to get caught up in the small details.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看看如何使用Scikit-CUDA的`linalg`子模块中的cuSolver。同样，这为cuBLAS和cuSolver提供了一个高级接口，因此我们不必陷入细节。
- en: As we noted in the introduction, cuSolver is a library that's used for performing
    more advanced linear algebra operations than cuBLAS, such as the Singular Value
    Decomposition, LU/QR/Cholesky factorization, and eigenvalue computations. Since
    cuSolver, like cuBLAS and cuFFT, is another vast library, we will only take the
    time to look at one of the most fundamental operations in data science and machine
    learning—SVD.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在介绍中指出的，cuSolver是一个用于执行比cuBLAS更高级的线性代数运算的库，例如奇异值分解、LU/QR/Cholesky分解和特征值计算。由于cuSolver、cuBLAS和cuFFT一样，是另一个庞大的库，我们只会花时间来看数据科学和机器学习中最基本的操作之一——SVD。
- en: Please refer to NVIDIA's official documentation on cuSOLVER if you would like
    further information on this library: [https://docs.NVIDIA.com/cuda/cusolver/index.html](https://docs.nvidia.com/cuda/cusolver/index.html).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想进一步了解此库，请参考NVIDIA关于cuSOLVER的官方文档：[https://docs.NVIDIA.com/cuda/cusolver/index.html](https://docs.nvidia.com/cuda/cusolver/index.html)。
- en: Singular value decomposition (SVD)
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）
- en: SVD takes any *m* x *n* matrix *A*, and then returns three matrices in return—*U*, *Σ*,
    and *V*. Here, *U* is an *m* x *m* unitary matrix, *Σ* is an *m* x *n* diagonal
    matrix, and *V* is an *n* x *n* unitary matrix. By *unitary*, we mean that a matrix's
    columns form an orthonormal basis; by *diagonal*, we mean that all values in the
    matrix are zero, except for possibly the values along its diagonal.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: SVD接受任何*m* x *n*矩阵*A*，然后返回三个矩阵—*U*、*Σ*和*V*。在这里，*U*是一个*m* x *m*酉矩阵，*Σ*是一个*m*
    x *n*对角矩阵，*V*是一个*n* x *n*酉矩阵。这里，*酉*表示矩阵的列形成一个正交归一基；*对角*表示矩阵中的所有值都为零，除了可能沿着对角线的值。
- en: The significance of the SVD is that this decomposes *A* into these matrices
    so that we have *A = UΣV^T* ; moreover, the values along the diagonal of *Σ* will
    all be positive or zero, and are known as the singular values. We will see some
    applications of this soon, but you should keep in mind that the computational
    complexity of SVD is of the order O(*mn²*)—for large matrices, it is definitely
    a good idea to use a GPU, since this algorithm is parallelizable.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: SVD的重要性在于它将*A*分解为这些矩阵，使得我们有*A = UΣV^T*；此外，*Σ*对角线上的值将全部为正值或零，并且被称为奇异值。我们很快将看到一些应用，但您应该记住，SVD的计算复杂度为O(*mn²*)——对于大矩阵来说，使用GPU绝对是一个好主意，因为这个算法是可并行化的。
- en: 'We''ll now look at how we can compute the SVD of a matrix. Let''s make the
    appropriate import statements:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何计算矩阵的SVD。让我们进行适当的导入语句：
- en: '[PRE50]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We will now generate a relatively large random matrix and transfer it to the
    GPU:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将生成一个相对较大的随机矩阵并将其传输到GPU：
- en: '[PRE51]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can now execute the SVD. This will have three outputs corresponding to the
    matrices that we just described. The first parameter will be the matrix array
    we just copied to the GPU. Then we need to specify that we want to use cuSolver
    as our backend for this operation:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以执行SVD。这将有三个输出，对应于我们刚刚描述的矩阵。第一个参数将是我们刚刚复制到GPU的矩阵数组。然后我们需要指定我们要使用cuSolver作为此操作的后端：
- en: '[PRE52]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now let''s copy these arrays from the GPU to the host:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这些数组从GPU复制到主机：
- en: '[PRE53]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '`s` is actually stored as a one-dimensional array; we will have to create a
    zero matrix of size 1000 x 5000 and copy these values along the diagonal. We can
    do this with the NumPy `diag` function, coupled with some array slicing:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`s`实际上存储为一维数组；我们将不得不创建一个大小为1000 x 5000的零矩阵，并将这些值沿对角线复制。我们可以使用NumPy的`diag`函数来做到这一点，再加上一些数组切片：'
- en: '[PRE54]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can now matrix-multiply these values on the host with the NumPy `dot` function
    to verify that they match up to our original array:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在主机上使用NumPy的`dot`函数对这些值进行矩阵相乘，以验证它们是否与我们的原始数组匹配：
- en: '[PRE55]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Since we are using only float32s and our matrix is relatively large, a bit of
    numerical error was introduced; we had to set the "tolerance" level (`atol`) a
    little higher than usual here, but it's still small enough to verify that the
    two arrays are sufficiently close.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只使用float32，并且我们的矩阵相对较大，引入了一些数值误差；我们不得不将“容差”级别（`atol`）设置得比平常高一点，但仍然足够小，以验证这两个数组是足够接近的。
- en: Using SVD for Principal Component Analysis (PCA)
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SVD进行主成分分析（PCA）
- en: '**Principal Component Analysis** (**PCA**) is a tool that''s used primarily
    for dimensionality reduction. We can use this to look at a dataset and find which
    dimensions and linear subspaces are the most salient. While there are several
    ways to implement this, we will show you how to perform PCA using SVD.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是主要用于降维的工具。我们可以使用它来查看数据集，并找出哪些维度和线性子空间最显著。虽然有几种实现方法，但我们将向您展示如何使用SVD执行PCA。'
- en: 'We''ll do this as follows—we will work with a dataset that exists in 10 dimensions.
    We will start by creating two vectors that are heavily weighted in the front,
    and 0 otherwise:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这样做——我们将使用一个存在于10个维度中的数据集。我们将首先创建两个在前面有很大权重的向量，其他位置为0：
- en: '[PRE56]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We will then add 9,000 additional vectors: 6,000 of these will be the same
    as the first two vectors, only with a little added random white noise, and the
    remaining 3,000 will just be random white noise:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将添加9000个额外的向量：其中6000个将与前两个向量相同，只是加了一点随机白噪声，剩下的3000个将只是随机白噪声：
- en: '[PRE57]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We will now typecast the `vals` list to a `float32` NumPy array. We take the
    mean over the rows and subtract this value from each row. (This is a necessary
    step for PCA.) We then transpose this matrix, since cuSolver requires that input
    matrices have fewer or equal rows compared to the columns:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将`vals`列表转换为`float32`的NumPy数组。我们对行进行平均，并从每行中减去这个值。（这是PCA的必要步骤。）然后我们转置这个矩阵，因为cuSolver要求输入矩阵的行数少于或等于列数：
- en: '[PRE58]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We will now run cuSolver, just like we did previously, and copy the output
    values off of the GPU:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将运行cuSolver，就像我们之前做的那样，并将输出值从GPU复制出来：
- en: '[PRE59]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now we are ready to begin our investigative work. Let''s open up IPython and
    take a closer look at `u` and `s`. First, let''s look at s; its values are actually
    the square roots of the **principal values**, so we will square them and then
    take a look:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始我们的调查工作。让我们打开IPython，仔细查看`u`和`s`。首先，让我们看看s；它的值实际上是**主要值**的平方根，所以我们将它们平方然后看一看：
- en: '![](assets/28321a31-a6fb-49e8-974f-1b2caecfe01b.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/28321a31-a6fb-49e8-974f-1b2caecfe01b.png)'
- en: 'You will notice that the first two principal values are of the order 10⁵, while
    the remaining components are of the order 10^(-3). This tells us there is only
    really a two-dimensional subspace that is even relevant to this data at all, which
    shouldn''t be surprising. These are the first and second values, which will correspond
    to the first and second principal components that is, the corresponding vectors.
    Let''s take a look at these vectors, which will be stored in `U`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，前两个主要值的数量级为10⁵，而其余的分量的数量级为10^(-3)。这告诉我们，实际上只有一个二维子空间与这些数据相关，这并不令人意外。这些是第一个和第二个值，它们将对应于第一个和第二个主成分，即相应的向量。让我们来看看这些向量，它们将存储在`U`中：
- en: '![](assets/9d685877-5a4c-4449-8da4-68b1b21d1e66.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: ！[](assets/9d685877-5a4c-4449-8da4-68b1b21d1e66.png)
- en: You will notice that these two vectors are very heavily weighted in the first
    two entries, which are of the order 10^(-1); the remaining entries are all of
    the order 10^(-6) or lower, and are comparably irrelevant. This is what we should
    have expected, considering how biased we made our data in the first two entries.
    That, in a nutshell, is the idea behind PCA.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到这两个向量在前两个条目中有很大的权重，数量级为10^(-1)；其余的条目都是10^(-6)或更低，相对不相关。考虑到我们在前两个条目中使数据偏向，这正是我们应该期望的。这就是PCA背后的思想。
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started this chapter by looking at how to use the wrappers for the cuBLAS
    library from Scikit-CUDA; we have to keep many details in mind here, such as when
    to use column-major storage, or if an input array will be overwritten in-place.
    We then look at how to perform one- and two-dimensional FFTs with cuFFT from Scikit-CUDA,
    and how to create a simple convolutional filter. We then showed you how to apply
    this for a simple Gaussian blurring effect on an image. Finally, we looked at
    how to perform a singular value decomposition (SVD) on the GPU with cuSolver,
    which is normally a very computationally onerous operation, but which parallelizes
    fairly well onto the GPU. We ended this chapter by looking at how to use the SVD
    for basic PCA.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从查看如何使用Scikit-CUDA库的cuBLAS包装器开始了本章；在这里，我们必须记住许多细节，比如何使用列主存储，或者输入数组是否会被就地覆盖。然后，我们看了如何使用Scikit-CUDA的cuFFT执行一维和二维FFT，以及如何创建一个简单的卷积滤波器。然后，我们向您展示了如何将其应用于图像的简单高斯模糊效果。最后，我们看了如何使用cuSolver在GPU上执行奇异值分解（SVD），这通常是一个非常计算密集的操作，但在GPU上可以很好地并行化。我们通过查看如何使用SVD进行基本的PCA来结束本章。
- en: Questions
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Suppose you get a job translating some old legacy FORTRAN BLAS code to CUDA.
    You open a file and see a function called SBLAH, and another called ZBLEH. Can
    you tell what datatypes these two functions use without looking them up?
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你得到了一个工作，需要将一些旧的遗留FORTRAN BLAS代码转换成CUDA。你打开一个文件，看到一个名为SBLAH的函数，另一个名为ZBLEH。你能在不查找的情况下告诉这两个函数使用的数据类型吗？
- en: Can you alter the cuBLAS level-2 GEMV example to work by directly copying the
    matrix `A` to the GPU, without taking the transpose on the host to set it column-wise?
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能修改cuBLAS level-2 GEMV示例，直接将矩阵`A`复制到GPU，而不是在主机上进行转置以设置为列优先吗？
- en: Use cuBLAS 32-bit real dot-product (`cublasSdot`) to implement matrix-vector
    multiplication using one row-wise matrix and one stride-1 vector.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用cuBLAS 32位实数点积（`cublasSdot`）来实现使用一个按行矩阵和一个步幅为1的向量进行矩阵-向量乘法。
- en: Implement matrix-matrix multiplication using `cublasSdot`.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cublasSdot`实现矩阵-矩阵乘法。
- en: Can you implement a method to precisely measure the GEMM operations in the performance
    measurement example?
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能实现一种精确测量性能测量示例中的GEMM操作的方法吗？
- en: In the example of the 1D FFT, try typecasting `x` as a `complex64` array, and
    then switching the FFT and inverse FFT plans to be `complex64` valued in both
    directions. Then confirm whether `np.allclose(x, x_gpu.get())` is true without
    checking the first half of the array. Why do you think this works now?
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一维FFT的示例中，尝试将`x`强制转换为`complex64`数组，然后将FFT和逆FFT计划都设置为`complex64`值。然后确认`np.allclose(x,
    x_gpu.get())`是否为真，而不检查数组的前一半。你认为为什么现在这样做会有效？
- en: Notice that there is a dark edge around the blurred image in the convolution
    example. Why is this in the blurred image but not in the original? Can you think
    of a method that you can use to mitigate this?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在卷积示例中，模糊图像周围有一个暗边。为什么模糊图像中有这个暗边，而原始图像中没有呢？你能想到一种方法来减轻这个问题吗？
