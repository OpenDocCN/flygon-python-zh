- en: Implementation of a Deep Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现深度神经网络
- en: We will now use our accumulated knowledge of GPU programming to implement our
    very own deep neural network (DNN) with PyCUDA. DNNs have attracted a lot of interest
    in the last decade, as they provide a robust and elegant model for machine learning
    (ML). DNNs was also one of the first applications (outside of rendering graphics)
    that were able to show the true power of GPUs by leveraging their massive parallel
    throughput, which ultimately helped NVIDIA rise to become a major player in the
    field of artificial intelligence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将利用我们对GPU编程的积累知识，使用PyCUDA实现我们自己的深度神经网络（DNN）。在过去的十年里，DNN吸引了很多关注，因为它们为机器学习（ML）提供了一个强大而优雅的模型。DNN也是第一个能够展示GPU真正强大之处的应用之一（除了渲染图形），通过利用它们的大规模并行吞吐量，最终帮助NVIDIA成为人工智能领域的主要参与者。
- en: In the course of this book, we have mostly been covering individual topics in
    a *bubble* on a chapter-by-chapter basis—here, we will build on many of the subjects
    we have learned about thus far for our very own implementation of a DNN. While
    there are several open source frameworks for GPU-based DNNs currently available
    to the general public—for example, Google's TensorFlow and Keras, Microsoft's
    CNTK, Facebook's Caffe2, and PyTorch—it is very instructive to go through an implementation
    of one from scratch, which will give us a greater insight and appreciation of
    the underlying technologies required for DNNs. We have a lot of material to cover
    here, so we'll cut right to the chase after a brief introduction to some of the
    basic concepts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的过程中，我们大多是按章节覆盖单个主题的——在这里，我们将在我们学到的许多主题的基础上构建我们自己的DNN实现。虽然目前有几个面向普通公众的基于GPU的DNN的开源框架，例如Google的TensorFlow和Keras，微软的CNTK，Facebook的Caffe2和PyTorch，但从头开始实现一个DNN非常有教育意义，这将使我们更深入地了解和欣赏DNN所需的基础技术。我们有很多材料要涵盖，所以在简要介绍一些基本概念后，我们将直奔主题。
- en: 'In this chapter, we will be looking at the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究以下内容：
- en: Understanding what an **artificial neuron** (**AN**) is
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解**人工神经元**（AN）是什么
- en: Understanding how many ANs can be combined together in a **deep neural network**
    (**DNN**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何将多个AN组合在一起形成**深度神经网络**（DNN）
- en: Implementing a DNN from scratch in CUDA and Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CUDA和Python中从头开始实现DNN
- en: Understanding how cross-entropy loss can be used to evaluate the output of a
    neural network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何使用交叉熵损失来评估神经网络的输出
- en: Implementing gradient descent to train an NN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现梯度下降来训练NN
- en: Learning how to train and test an NN on a small dataset
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何在小数据集上训练和测试NN
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all of the necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一台装有现代NVIDIA GPU（2016年以后）的Linux或Windows 10 PC，并安装了所有必要的GPU驱动程序和CUDA Toolkit（9.0及以上）。还需要一个合适的Python
    2.7安装（如Anaconda Python 2.7），并安装了PyCUDA模块。
- en: This chapter's code is also available on GitHub at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可以在GitHub上找到：[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)。
- en: For more information about the prerequisites for this chapter, check out the
    preface of this book. For the software and hardware requirements, check out the
    README file in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本章的先决条件的更多信息，请查看本书的前言。有关软件和硬件要求，请查看[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)中的README文件。
- en: Artificial neurons and neural networks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元和神经网络
- en: Let's briefly go over some of the basics of **machine learning (ML)** and **neural networks
    (NNs)**. In Machine Learning, our goal is to take a collection of data with a
    particular set of labeled classes or characteristics and use these examples to
    train our system to predict the values of future data. We call a program or function
    that predicts classes or labels of future data based on prior training data a
    **classifier**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一些**机器学习（ML）**和**神经网络（NNs）**的基础知识。在机器学习中，我们的目标是使用具有特定标记类别或特征的数据集，并利用这些示例来训练我们的系统以预测未来数据的值。我们称根据先前训练数据预测未来数据的类别或标签的程序或函数为**分类器**。
- en: 'There are many types of classifiers, but here we will be focusing on NNs. The
    idea behind NNs is that they (allegedly) work in a way that is similar to the
    human brain, in that they learn and classify data using a collection of **artificial
    neurons (ANs)**, all connected together to form a particular structure. Let''s
    step back for a moment, though, and look at what an individual AN is. In mathematics,
    this is just an *affine* function from the linear space **R^n** to **R**, like
    so:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的分类器，但在这里我们将专注于NNs。NNs的理念是它们（据说）以类似于人脑的方式工作，通过使用一组**人工神经元（ANs）**来学习和分类数据，所有这些神经元连接在一起形成特定的结构。不过，让我们暂停一下，看看一个单独的AN是什么。在数学上，这只是从线性空间**R^n**到**R**的*仿射*函数，如下所示：
- en: '![](assets/25478608-c882-49f1-85a9-9e3f6b0d472d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/25478608-c882-49f1-85a9-9e3f6b0d472d.png)'
- en: We can see that this can be characterized as a dot product between a constant
    weight vector ***w*** and an input vector ***x***, with an additional bias constant *b*
    added to the end. (Again, the only *input* into this function here is *x*; the
    other values are constants!)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这可以被描述为一个常量权重向量***w***和输入向量***x***之间的点积，最后加上一个额外的偏置常量*b*。（再次强调，这个函数的唯一*输入*是*x*；其他值都是常数！）
- en: 'Now, individuall*y* a single AN is fairly useless (and stupid), as their *intelligence* only
    emerges when acting in cooperation with a large number of other ANs. Our first
    step is to stack a collection of *m* similar ANs on top of each other so as to
    form what we will call a **dense layer (DL)**. This is dense because each neuron
    will process every single input value from *x – *each AN will take in an array
    or vector value from **R^n** and output a single value in **R.** Since there are
    *m* neurons, this means that we can say their output collectively is in the space
    **R^m**. We will notice that if we stack the weights for each neuron in our layer,
    so as to form an *m x n* matrix of weights, we can then just calculate the output
    of each neuron with a matrix multiplication followed by the addition of the appropriate
    biases:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，单个AN本身是相当无用（而且愚蠢）的，只有当它们与大量其他AN合作时，它们的*智能*才会显现出来。我们的第一步是将一系列相似的AN堆叠在一起，以形成我们将称之为**密集层（DL）**的东西。这是密集的，因为每个神经元将处理来自*x*的每个输入值
    - 每个AN将接收来自**R^n**的数组或向量值，并在**R**中输出一个值。由于有*m*个神经元，这意味着它们的输出集体位于**R^m**空间中。我们将注意到，如果我们堆叠我们层中每个神经元的权重，以形成一个*m
    x n*的权重矩阵，然后我们可以通过矩阵乘法计算每个神经元的输出，然后加上适当的偏差：
- en: '![](assets/03bf9c48-e6dc-4f86-9b18-9a0eac05c7cb.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/03bf9c48-e6dc-4f86-9b18-9a0eac05c7cb.png)'
- en: Now, let's suppose that we want to build an NN classifier that can classify
    *k* different classes; we can create a new additional dense layer that takes in
    the *m* values from the prior dense layer, and outputs *k* values. Supposing that
    we have the appropriate weight and bias values for each layer (which are certainly
    not trivial to find), and that we also have the appropriate **activation function**
    set up after each layer (which we will define later), this will act as a classifier
    between our *k* distinct classes, giving us the probability of *x* falling into
    each respective class based on the outputs of the final layer. Of course, we're
    getting way ahead of ourselves here, but that is, in a nutshell, how an NN works.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想要构建一个能够对*k*个不同类别进行分类的NN分类器；我们可以创建一个新的附加密集层，该层接收来自先前密集层的*m*个值，并输出*k*个值。假设我们对每一层都有适当的权重和偏差值（这显然不容易找到），并且在每一层之后也有适当的**激活函数**设置（我们稍后会定义），这将作为我们*k*个不同类别之间的分类器，根据最终层的输出给出*x*落入每个类别的概率。当然，我们在这里走得太远了，但这就是NN的工作原理。
- en: Now, it seems like we can just keep connecting dense layers to each other into
    long chains to achieve classifications. This is what is known as a DNN. When we
    have a layer that is not directly connected to the inputs or outputs, that is
    known as a hidden layer. The strength of a DNN is that the additional layers allow
    the NN to capture abstractions and subtleties of the data that a shallow NN could
    not pick up on.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，似乎我们可以将密集层连接到长链中以实现分类。这就是所谓的DNN。当我们有一层不直接连接到输入或输出时，这就是一个隐藏层。DNN的优势在于额外的层允许NN捕捉浅层NN无法捕捉到的数据的抽象和细微差别。
- en: Implementing a dense layer of artificial neurons
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现人工神经元的密集层
- en: 'Now, let''s implement the most important building block of an NN, the **dense
    layer**. Let''s start by declaring a CUDA kernel, like so:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现NN最重要的构建模块，**密集层**。让我们从声明CUDA内核开始，就像这样：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's go over the inputs, one by one. `num_outputs`, of course, indicates the
    total number of outputs this layer has; this is exactly the number of neurons
    in the layer. `num_inputs` tells us the size of the input data. Setting a positive
    value for `relu` and `sigmoid` will indicate that we should use the corresponding
    activation function on the output of this layer, which we will define later. `w`
    and `b` are arrays containing the weights and biases of this layer, while `x`
    and `y` will act as our inputs and outputs. Oftentimes, we wish to classify more
    than one piece of data at a time. We can indicate this by setting `batch_size`
    to be the number of points we wish to predict. Finally, `w_t`, `b_t`, and `delta`
    will be used in the training process to determine the appropriate weights and
    biases for this layer by means of **gradient descent**. (We will see more on gradient
    descent in a later section.)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个检查输入。`num_outputs`当然表示这一层的总输出数量；这正好是该层的神经元数量。`num_inputs`告诉我们输入数据的大小。为`relu`和`sigmoid`设置正值将表示我们应该在该层的输出上使用相应的激活函数，我们稍后会定义。`w`和`b`是包含该层权重和偏差的数组，而`x`和`y`将作为我们的输入和输出。通常，我们希望一次对多个数据进行分类。我们可以通过将`batch_size`设置为我们希望预测的点数来指示这一点。最后，`w_t`、`b_t`和`delta`将在训练过程中使用，通过**梯度下降**来确定该层的适当权重和偏差。（我们将在后面的部分中更多地了解梯度下降。）
- en: 'Now, let''s start writing our kernel. We will parallelize the computations
    over each output, so we will set an integer `i` to be the global thread ID to
    this end, and have any unnecessary extra threads which happen to be running this
    kernel to just not do anything with the appropriate `if` statement:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始编写我们的内核。我们将在每个输出上并行计算，因此我们将设置一个整数`i`作为全局线程ID，然后使用适当的`if`语句让任何不必要的额外线程不执行任何操作：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s iterate over each data point in the batch with the appropriate
    `for` loop:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用适当的`for`循环迭代批处理中的每个数据点：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will multiply and accumulate the 32-bit floats from the weights and inputs
    into a 64-bit double `temp` and then add the appropriate bias point. We will then
    typecast this back to a 32-bit float and put the value in the output array, and
    then close off the loop over `k`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从权重和输入中的32位浮点数中进行乘法和累加，得到64位双精度`temp`，然后加上适当的偏差点。然后我们将这个值强制转换回32位浮点数并将值放入输出数组中，然后关闭对`k`的循环：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Multiply and accumulate* types of operations are generally subject to a great
    loss of numerical precision. This can be mitigated by using a temporary variable
    of higher precision to store values in the course of the operation, and then typecasting
    this variable back to the original precision after the operation is completed.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*乘法和累加*类型的操作通常会导致严重的数值精度损失。这可以通过使用更高精度的临时变量来存储操作过程中的值，然后在操作完成后将此变量强制转换回原始精度来减轻。 '
- en: 'To train an NN, we will ultimately have to calculate the derivative (from calculus)
    of our NN with respect to each weight and bias within each individual layer, which
    is with respect to a particular batch of inputs. Remember that the derivative
    of a mathematical function *f* at the value *x* can be estimated as *f**(x + δ)
    - f(x) / δ*, where delta (δ) is some sufficiently small positive value. We will
    use the input values `w_t` and `b_t` to indicate to the kernel whether we want
    to calculate the derivative with respect to a particular weight or bias; otherwise,
    we will set these input values to a negative value to evaluate only for this layer.
    We will also set delta to be an appropriately small value for the calculation
    of the derivative, and use this to increment the value of the appropriate bias
    or weight:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个NN，我们最终将不得不计算我们的NN对每个权重和偏差的导数（来自微积分），这是针对特定输入批次的。请记住，数学函数*f*在值*x*处的导数可以估计为*f**(x
    + δ) - f(x) / δ*，其中delta（δ）是某个足够小的正值。我们将使用输入值`w_t`和`b_t`来指示内核是否要计算相对于特定权重或偏差的导数；否则，我们将将这些输入值设置为负值，仅对此层进行评估。我们还将设置delta为适当小的值，用于计算导数，并使用它来增加适当偏差或权重的值：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we will add some code for what is known as the **rectified linear unit**
    (or **ReLU**) and **sigmoid activation functions**. These are used for processing
    the immediate output of a dense neural layer. ReLU just sets all negative values
    to 0, while acting as an identity for positive inputs, while sigmoid just computes
    the value of the `sigmoid` function on each value ( *1 / (1 + e^(-x))* ). ReLU
    (or any other activation function) is used between hidden layers in an NN as a
    means to make the entire NN act as a nonlinear function; otherwise, the entire
    NN would constitute a trivial (and inefficiently computed) matrix operation. (While
    there are many other nonlinear activation functions that can be used between layers,
    ReLU has been found to be a particularly effective function for training.) Sigmoid
    is used as a final layer in an NN intended for **labeling**, that is, one that
    may assign multiple labels for a given input, as opposed to assigning an input
    to a single class.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将添加一些代码，用于所谓的**修正线性单元**（或**ReLU**）和**Sigmoid激活函数**。这些用于处理密集神经层的直接输出。ReLU只将所有负值设为0，同时作为正输入的恒等式，而sigmoid只计算每个值上的`sigmoid`函数的值（*1
    / (1 + e^(-x))*）。ReLU（或任何其他激活函数）用于NN中隐藏层之间，作为使整个NN成为非线性函数的手段；否则，整个NN将构成一个微不足道（且计算效率低下）的矩阵操作。（虽然可以在层之间使用许多其他非线性激活函数，但发现ReLU对训练来说是一个特别有效的函数。）Sigmoid用作NN中用于**标签**的最终层，即可能为给定输入分配多个标签的层，而不是将输入分配给单个类别。
- en: 'Let''s go up a little bit in the file, before we even begin to define this
    CUDA kernel, and define these operations as C macros. We will also remember to
    put in the CUDA-C code we''ve just written while we are at it:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在文件中稍微上移一点，甚至在我们开始定义这个CUDA内核之前，将这些操作定义为C宏。我们还要记得在我们写完CUDA-C代码后将其放入其中：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we will use the kernel inputs `relu` and `sigmoid` to indicate whether
    we should use these additional layers; we will take a positive input from these
    to indicate that they should be used, respectively. We can add this, close off
    our kernel, and compile it into a usable Python function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用内核输入`relu`和`sigmoid`来指示我们是否应该使用这些额外的层；我们将从这些中获取积极的输入，以指示它们应该分别使用。我们可以添加这个，关闭我们的内核，并将其编译成可用的Python函数：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s go to the beginning of the file and set up the appropriate import
    statements. Notice that we will include the `csv` module, which will be used for
    processing data inputs for testing and training:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到文件的开头，并设置适当的导入语句。请注意，我们将包括`csv`模块，该模块将用于处理测试和训练的数据输入：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s continue setting up our dense layer; we will want to wrap this
    within a Python class for ease of use, which will make our lives much easier when
    we start connecting these dense layers together into a full-blown NN. We''ll call `class
    DenseLayer` and start by writing a constructor. Most of the inputs and setup here
    should be self-explanatory: we should definitely add an option to load weights
    and biases from a pre-trained network, and we''ll also include the option to specify
    a default *delta *value as well as a default stream. (If no weights or biases
    are given, weights are initialized to random values, while all biases are set
    to 0.) We will also specify whether to use ReLU or sigmoid layers here, as well.
    Toward the end, notice how we set up the block and grid sizes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续设置我们的密集层；我们将希望将其包装在一个Python类中以便使用，这将使我们在开始将这些密集层连接成一个完整的NN时更加轻松。我们将称之为`class
    DenseLayer`并开始编写构造函数。这里的大部分输入和设置应该是不言自明的：我们绝对应该添加一个选项，从预训练的网络中加载权重和偏差，并且我们还将包括指定默认*delta*值以及默认流的选项。（如果没有给出权重或偏差，权重将被初始化为随机值，而所有偏差将设置为0。）我们还将在这里指定是否使用ReLU或sigmoid层。最后，请注意我们如何设置块和网格大小：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we will set up a function in this class to evaluate inputs from this layer;
    we will meticulously check the input (x) to determine if it is already on the
    GPU (transferring it over to a `gpuarray` if not), and we will let the user specify
    a preallocated `gpuarray` for output (y), manually allocating an output array
    if one is not specified. We will also check the delta and `w_t`/`b_t` values for
    the case of training, as well as `batch_size`. We will then run the kernel on
    the `x` input with outputs going into `y`, and finally return `y` as the output
    value:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在这个类中设置一个函数来评估来自这一层的输入；我们将仔细检查输入（x）以确定它是否已在GPU上（如果没有，则将其转移到`gpuarray`），并让用户指定预分配的`gpuarray`用于输出（y），如果没有指定，则手动分配一个输出数组。我们还将检查训练时的delta和`w_t`/`b_t`值，以及`batch_size`。然后我们将在`x`输入上运行核函数，输出到`y`，最后将`y`作为输出值返回：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There we go. We have fully implemented a dense layer!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完全实现了一个密集层！
- en: Implementation of the softmax layer
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现softmax层
- en: We will now look at how we can implement a **softmax layer**. As we have already
    discussed, a sigmoid layer is used for assigning labels to a class—that is, if
    you want to have multiple nonexclusive characteristics that you want to infer
    from an input, you should use a sigmoid layer. A **softmax layer** is used when
    you only want to assign a single class to a sample by inference—this is done by
    computing a probability for each possible class (with probabilities over all classes,
    of course, summing to 100%). We can then select the class with the highest probability
    to give the final classification.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何实现**softmax层**。正如我们已经讨论过的，sigmoid层用于为类分配标签——也就是说，如果您想要从输入中推断出多个非排他性特征，您应该使用sigmoid层。**softmax层**用于仅通过推断为样本分配单个类别——这是通过计算每个可能类别的概率来实现的（当然，所有类别的概率总和为100%）。然后我们可以选择具有最高概率的类别来给出最终分类。
- en: 'Now, let''s see exactly what the softmax layer does—given a set of a collection
    of *N* real numbers (*c[0], ..., c[N-1]*) , we first compute the sum of the exponential
    function on each number (![](assets/90e15296-aedc-4f2b-a9bf-8099d5a28a07.png)),
    and then calculate the exponential of each number divided by this sum to yield
    the softmax:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看softmax层到底做了什么——给定一组*N*个实数(*c[0], ..., c[N-1]*)，我们首先计算每个数字的指数函数的总和，然后计算每个数字除以这个总和的指数，得到softmax：
- en: '![](assets/ce20fc54-36ee-46a9-86a9-40387fb73545.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ce20fc54-36ee-46a9-86a9-40387fb73545.png)'
- en: 'Let''s start with our implementation. We will start by writing two very short
    CUDA kernels: one that takes the exponential of each input, and another that takes
    the mean over all of the points:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的实现开始。我们将首先编写两个非常简短的CUDA内核：一个用于计算每个输入的指数，另一个用于计算所有点的平均值：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s write a Python wrapper class, like we did previously. First, we
    will start with the constructor, and we will indicate the number of both inputs
    and outputs with `num`. We can also specify a default stream, if we wish:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个Python包装类，就像我们之前做的那样。首先，我们将从构造函数开始，并使用`num`指示输入和输出的数量。如果需要，我们还可以指定默认流：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s write `eval_ function` in a way that is similar to the dense layer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个类似于密集层的`eval_`函数：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Implementation of Cross-Entropy loss
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现交叉熵损失
- en: 'Now, let''s implement what is known as the **cross-entropy loss** function.
    This is used to measure how accurate an NN is on a small subset of data points
    during the training process; the bigger the value that is output by our loss function,
    the more inaccurate our NN is at properly classifying the given data. We do this
    by calculating a standard mean log-entropy difference between the expected output
    and the actual output of the NN. For numerical stability, we will limit the value
    of the output to `1`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现所谓的**交叉熵损失**函数。这用于在训练过程中测量神经网络在数据子集上的准确性；损失函数输出的值越大，我们的神经网络在正确分类给定数据方面就越不准确。我们通过计算期望输出和实际输出之间的标准平均对数熵差来实现这一点。为了数值稳定性，我们将限制输出值为`1`：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Implementation of a sequential network
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个顺序网络
- en: Now, let's implement one final class that will combine multiple dense layer
    and softmax layer objects into a single coherent feed-forward sequential neural
    network. This will be implemented as another class, which will subsume the other
    classes. Let's first start by writing the constructor—we will be able to set the
    max batch size here, which will affect how much memory is allocated for the use
    of this network – we'll store some allocated memory used for weights and input/output
    for each layer in the list variable, `network_mem`. We will also store the `DenseLayer`
    and `SoftmaxLayer` objects in the list network, and information about each layer
    in the NN in `network_summary`. Notice how we can also set up some training parameters
    here, including the delta, how many streams to use for gradient descent (we'll
    see this later), as well as the number of training epochs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现最后一个类，将多个密集层和softmax层对象组合成一个连贯的前馈顺序神经网络。这将作为另一个类来实现，它将包含其他类。让我们首先从编写构造函数开始——我们将能够在这里设置最大批处理大小，这将影响为此网络分配多少内存——我们将在列表变量`network_mem`中存储一些用于权重和每个层的输入/输出的分配内存。我们还将在列表network中存储DenseLayer和SoftmaxLayer对象，并在network_summary中存储有关NN中每个层的信息。请注意，我们还可以在这里设置一些训练参数，包括delta，用于梯度下降的流的数量（稍后我们将看到），以及训练时期的数量。
- en: 'We can also see one other input at the beginning called layers. Here, we can
    indicate the construction of the NN by describing each layer, which the constructor
    will create by iterating through each element of layers and calling the `add_layer`
    method, which we will implement next:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到开始时的另一个输入称为layers。在这里，我们可以通过描述每个层来指示NN的构造，构造函数将通过迭代layers的每个元素并调用`add_layer`方法来创建它，接下来我们将实现这个方法：
- en: '[PRE14]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let''s implement the `add_layer` method. We will use a dictionary data
    type to pass all of the relevant information about the layer to the sequential
    network—including the type of layer (dense, softmax, and so on), the number of
    inputs/outputs, weights, and biases. This will append the appropriate object and
    information to the object''s network and `network_summary` list variables, as
    well as appropriately allocate `gpuarray` objects to the `network_mem` list:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现`add_layer`方法。我们将使用字典数据类型将有关该层的所有相关信息传递给顺序网络，包括层的类型（密集、softmax等）、输入/输出的数量、权重和偏差。这将向对象的网络和`network_summary`列表变量追加适当的对象和信息，并适当地为`network_mem`列表分配`gpuarray`对象：
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Implementation of inference methods
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推断方法的实现
- en: We will now add two methods for inference to our `SequentialNetwork` class—that
    is, for predicting an output given for a particular input. The first method we
    will just call `predict`, which will be used by the end user. In the course of
    the training process, we will have to make predictions based on a partial result
    from only some of the layers, and we will make another method to this end called
    `partial_predict`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在我们的`SequentialNetwork`类中添加两种推断方法——即，根据特定输入预测输出的方法。我们将首先称之为`predict`，这将由最终用户使用。在训练过程中，我们将不得不基于仅部分层的部分结果进行预测，我们将为此制作另一种方法，称为`partial_predict`。
- en: 'Let''s start by implementing *predict*.  This will take two inputs—a collection
    of samples in the form of a one- or two-dimensional NumPy array, and possibly
    a user-defined CUDA stream. We will start by doing some type-checks and formatting
    on the samples (here, called `x`), remembering that the samples will be stored
    row-wise:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从实现*predict*开始。这将接受两个输入——以一维或二维NumPy数组形式的样本集，可能还有用户定义的CUDA流。我们将从样本（这里称为`x`）进行一些类型检查和格式化，记住样本将以行方式存储：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let''s perform the actual inference step. We just have to iterate through
    our entire neural network, performing an `eval_` on each layer:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行实际的推断步骤。我们只需遍历整个神经网络，在每一层上执行`eval_`：
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will now pull the final output of the NN, the GPU, and return it to the
    user. If the number of samples in `x` is actually smaller than the maximum batch
    size, we will slice the output array appropriately before it is returned:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将提取NN的最终输出，GPU，并将其返回给用户。如果`x`中的样本数量实际上小于最大批处理大小，则在返回之前我们将适当地切片输出数组：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, with that done, let''s implement `partial_predict`. Let''s briefly discuss
    the idea behind this. When we are in the training process, we will evaluate a
    collection of samples, and then look at how a subtle change of adding *delta* to
    each weight and bias individually will affect the outputs. To save time, we can
    calculate the outputs of each layer and store them for a given collection of samples,
    and then only recompute the output for the layer where we change the weight, as
    well as for all subsequent layers. We''ll see the idea behind this in a little
    more depth soon, but for now, we can implement this like so:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，完成了这一步，让我们实现`partial_predict`。让我们简要讨论一下这个想法。当我们处于训练过程中时，我们将评估一组样本，然后看看如何通过逐个添加*delta*到每个权重和偏差上会影响输出。为了节省时间，我们可以计算每一层的输出并将它们存储给定的样本集，然后只重新计算我们改变权重的层的输出，以及所有后续层的输出。我们很快就会更深入地了解这个想法，但现在，我们可以这样实现：
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Gradient descent
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: We will now make a full implementation of the training method for our NN in
    the form of **batch-stochastic gradient descent (BSGD)**. Let's think about what
    this means, word by word. **Batch** means that this training algorithm will operate
    on a collection of training samples at once, rather than all of the samples simultaneously,
    while **stochastic** indicates that each batch is chosen randomly. **Gradient** means
    that we will be using a gradient from calculus—which, here, is the collection
    of derivatives for each weight and bias on the loss function. Finally, **descent** means
    that we are trying to reduce the loss function—we do this by iteratively making
    subtle changes on the weights and biases by *subtracting* the Gradient.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将以**批量随机梯度下降（BSGD）**的形式对我们的NN进行训练方法的完整实现。让我们逐字逐句地思考这意味着什么。**批量**意味着这个训练算法将一次操作一组训练样本，而不是同时处理所有样本，而**随机**表示每个批次是随机选择的。**梯度**意味着我们将使用微积分中的梯度，这里是每个权重和偏差对损失函数的导数集合。最后，**下降**意味着我们试图减少损失函数——我们通过迭代地对权重和偏差进行微小的更改来实现这一点，通过*减去*梯度。
- en: Remember from calculus that the gradient of a point always points in the direction
    of the greatest *increase*, with its opposite direction being that of the greatest
    *decrease*. Since we want a *decrease*, we subtract the gradient.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从微积分中我们知道，一个点的梯度总是指向最大*增加*的方向，其相反方向是最大*减少*的方向。因为我们想要*减少*，所以我们减去梯度。
- en: 'We will now implement BSGD as the `bsgd` method in our `SequentialNetwork`
    class. Let''s go over the input parameters of `bsgd`, one by one:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在我们的`SequentialNetwork`类中实现BSGD作为`bsgd`方法。让我们逐一讨论`bsgd`的输入参数：
- en: '`training` will be a two-dimensional NumPy array of training samples'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`将是一个二维NumPy数组的训练样本'
- en: '`labels` will be the desired output of the final layer of the NN corresponding
    to each training sample'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`将是NN最终层的期望输出，对应于每个训练样本'
- en: '`delta` will indicate how much we should increase a weight for the calculation
    of derivatives by'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delta`将指示我们在计算导数时应该增加权重多少'
- en: '`max_streams` will indicate the maximum number of concurrent CUDA streams that
    BSGD will perform calculations over'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_streams`将指示BSGD将在其上执行计算的最大并发CUDA流的数量'
- en: '`batch_size` will indicate how large we want the batches that we will calculate
    the loss function on for each update of the weights'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`将指示我们希望对每次更新权重计算损失函数的批次有多大'
- en: '`epochs` will indicate how many times we shuffle the order of the current set
    of samples, break into a collection of batches, and then perform BSGD on'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`将指示我们对当前样本集的顺序进行多少次洗牌，分成一系列批次，然后进行BSGD'
- en: '`training_rate` will indicate the rate at which we will update our weights
    and biases with our gradient calculations'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training_rate`将指示我们使用梯度计算更新权重和偏差的速率'
- en: 'We''ll start out this method as usual and perform some checks and typecasting,
    set up the collection of CUDA stream objects into a Python list, and allocate
    some additional needed GPU memory in another list:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像往常一样开始这个方法，并进行一些检查和类型转换，将CUDA流对象的集合设置为Python列表，并在另一个列表中分配一些额外需要的GPU内存：
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we can begin training. We will start by doing an iteration of the entire
    BSGD for each `epoch`, performing a random shuffle of the entire dataset for each
    epoch. We''ll print some information to the terminal as well so that the user
    will have some status updates in the training process:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始训练。我们将从每个`epoch`开始执行整个BSGD的迭代，对每个epoch对整个数据集进行随机洗牌。我们还将在终端打印一些信息，以便用户在训练过程中获得一些状态更新：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we will make a loop that iterates over each batch in the shuffled dataset.
    We start by calculating the entropy from the current batch, and we will print
    this as well. If the user sees decreases in entropy, then they will know that
    gradient descent is working here:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将循环遍历洗牌数据集中的每个批次。我们首先计算当前批次的熵，然后将其打印出来。如果用户看到熵的减少，那么他们将知道梯度下降在这里起作用：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will now iterate through each dense layer of our NN, calculating the gradient
    for the entire set of weights and biases. We will store these derivatives for
    the weights and biases in *flattened* (one-dimensional) arrays, which will correspond
    to the `w_t` and `b_t` indices in our CUDA kernels, which are also flattened.
    Since we will have multiple streams process different outputs for different weights,
    we will use a Python Queue container to store the set of weights and biases that
    are yet to be processed for this batch: we can then just pop values off the top
    of this container to the next available stream (we''ll store these as tuples,
    with the first element indicating whether this is a weight or bias, in particular):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将迭代我们的NN的每个密集层，计算整套权重和偏差的梯度。我们将把这些导数存储在*扁平化*（一维）数组中，这将对应于我们的CUDA内核中的`w_t`和`b_t`索引，它们也是扁平化的。由于我们将有多个流处理不同权重的不同输出，我们将使用Python队列容器来存储尚未处理的这一批权重和偏差：然后我们可以从该容器的顶部弹出值到下一个可用流（我们将这些存储为元组，第一个元素指示这是权重还是偏差）：
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we need to iterate over each and every weight and bias, which we can do
    with a `while` loop that checks if the `queue` object we just set up is empty.
    We will set up another queue, `stream_weights`, that will help us organize which
    weights and biases each stream has processed. After setting up the weight and
    bias inputs appropriately, we can now use `partial_predict` by using the current
    stream and corresponding GPU memory arrays:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要迭代每一个权重和偏差，我们可以使用`while`循环来检查我们刚刚设置的`queue`对象是否为空。我们将设置另一个队列`stream_weights`，这将帮助我们组织每个流处理的权重和偏差。适当设置权重和偏差输入后，我们现在可以使用`partial_predict`，使用当前流和相应的GPU内存数组：
- en: Notice that we already performed a `predict` for this batch of samples to calculate
    the entropy, so we are now able to perform `partial_predict` on this batch, provided
    we are careful about which memory and layers we use.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经对这批样本执行了`predict`来计算熵，所以我们现在可以对这批样本执行`partial_predict`，只要我们小心使用内存和层。
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We have only computed the prediction of the output for alterations of a small
    set of weights and biases. We will have to compute the entropy for each, and then
    store the value of the derivative in the flattened arrays:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只计算了一小部分权重和偏差的输出预测。我们将不得不为每个计算熵，然后将导数值存储在扁平化的数组中：
- en: '[PRE25]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We have now finished the `while` loop. Once we reach the outside of this, we
    will know that we''ve calculated the derivatives for all weights and biases for
    this particular layer. Before we iterate to the next layer, we will append the
    calculated values for the gradient of the current set of weights and biases into
    the `all_grad` list. We will also reshape the flattened list of weights back into
    the original shape while we''re at it:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了`while`循环。一旦我们到达外部，我们将知道我们已经计算了这个特定层的所有权重和偏差的导数。在迭代到下一层之前，我们将把当前权重和偏差的梯度计算值附加到`all_grad`列表中。我们还将把扁平化的权重列表重新整形成原始形状：
- en: '[PRE26]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After we are done iterating over every layer, we can perform the optimization
    of the weights and biases of our NN on this batch. Notice how if the `training_rate` variable
    is far less than `1`, this will reduce how fast the weights are updated:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在迭代每一层之后，我们可以对这一批的NN的权重和偏差进行优化。请注意，如果`training_rate`变量远小于`1`，这将减少权重更新的速度：
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We have fully implemented a (very simple) GPU-based DNN!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完全实现了（非常简单的）基于GPU的DNN！
- en: Conditioning and normalizing data
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的调整和归一化
- en: 'Before we move on to training and testing our brand-new NN, we need to step
    back for a moment and talk about **conditioning** and **normalizing** data. NNs
    are highly susceptible to numerical error, especially when inputs have a large
    variance in scale. This can be mitigated by properly **conditioning** our training
    data; this means that for each point in an input sample, we will calculate the
    mean and variance of each point over all samples, and then subtract the mean and
    divide by the standard deviation for each point in each sample before it is input
    into the NN for either training or inference (prediction). This method is known
    as n**ormalization**. Let''s put together a small Python function that can do
    this for us:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续训练和测试全新的NN之前，我们需要退后一步，谈谈**数据调整**和**数据归一化**。NN对数值误差非常敏感，特别是当输入的规模差异很大时。这可以通过正确**调整**我们的训练数据来减轻，这意味着对于输入样本中的每个点，我们将计算所有样本中每个点的平均值和方差，然后在输入到NN进行训练或推断（预测）之前，对每个样本中的每个点减去平均值并除以标准差。这种方法称为**归一化**。让我们组合一个小的Python函数来为我们做这个：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The Iris dataset
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鸢尾花数据集
- en: 'We will now construct our very own DNN for a real-life problem: classification
    of flower types based on the measurements of petals. We will be working with the
    well-known *Iris dataset* for this. This dataset is stored as a comma-separated
    value (CSV) text file, with each line containing four different numerical values
    (petal measurements), followed by the flower type (here, there are three classes—*Irissetosa*, *Irisversicolor*,
    and *Irisvirginica*). We will now design a small DNN that will classify the type
    of iris, based on this set.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为一个真实问题构建我们自己的DNN：根据花瓣的测量来对花的类型进行分类。我们将使用众所周知的*鸢尾花数据集*。该数据集存储为逗号分隔值（CSV）文本文件，每行包含四个不同的数值（花瓣测量），然后是花的类型（这里有三个类别——*山鸢尾*、*变色鸢尾*和*维吉尼亚鸢尾*）。我们现在将设计一个小型DNN，根据这个数据集对鸢尾花的类型进行分类。
- en: Before we continue, please download the Iris dataset and put it into your working
    directory.  This is available from the UC Irvine Machine Learning repository,
    which can be found here: [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，请下载鸢尾花数据集并将其放入您的工作目录。这可以从UC Irvine机器学习存储库中获取，网址为：[https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)。
- en: 'We will start by processing this file into appropriate data arrays that we
    can use for training and validating our DNN. Let''s start by opening up our main
    function; we will need to translate the names of the flowers into actual classes
    that a DNN can output, so let''s make a small dictionary that will give us a corresponding
    label for each class. We will also set up some empty lists to store our training
    data and labels:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将此文件处理成适当的数据数组，以便用于训练和验证我们的DNN。让我们从打开我们的主函数开始；我们需要将花的名称转换为DNN可以输出的实际类别，因此让我们创建一个小字典，为每个类别提供相应的标签。我们还将设置一些空列表来存储我们的训练数据和标签：
- en: '[PRE29]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, let''s read from the CSV file. We will use the `reader` function from
    Python''s `csv` module, which we imported earlier:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从CSV文件中读取。我们将使用Python的`csv`模块中的`reader`函数，这是我们之前导入的：
- en: '[PRE30]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will now randomly shuffle the data and use two-third of these samples as
    training data. The remaining one-third will be used for test (validation) data:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将随机洗牌数据，并使用三分之二的样本作为训练数据。剩下的三分之一将用于测试（验证）数据：
- en: '[PRE31]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, finally, we can begin building our DNN! First, let''s create a `SequentialNetwork`
    object. We''ll set the `max_batch_size` to `32`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最后，我们可以开始构建我们的DNN！首先，让我们创建一个`SequentialNetwork`对象。我们将`max_batch_size`设置为`32`：
- en: '[PRE32]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let''s create our NN. This will consist of four dense layers (two hidden)
    and a softmax layer. We will increment the number of neurons in each layer until
    the final layer, which will only have three outputs (one for each class). This
    increasing amount of neurons per layer allows us to capture some of the subtleties
    of the data:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的NN。这将包括四个密集层（两个隐藏层）和一个softmax层。我们将逐层增加神经元的数量，直到最后一层，最后一层只有三个输出（每个类别一个）。每层神经元数量的增加允许我们捕捉数据的一些细微差别：
- en: '[PRE33]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will now condition our training data and begin the training with our BSGD
    method that we just implemented. We will train with `batch_size` set to `16`, `max_streams` set
    to `10`, the number of `epochs` set to 100, the `delta` set to 0.0001, and the
    `training_rate` set to 1—these will be admissible parameters for virtually any
    modern GPU. We will also time the training procedure while we''re at it, which
    can be rather time-consuming:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将调整我们的训练数据，并使用我们刚刚实现的BSGD方法进行训练。我们将使用`batch_size`设置为`16`，`max_streams`设置为`10`，`epochs`的数量设置为100，`delta`设置为0.0001，`training_rate`设置为1——这些将是几乎任何现代GPU可接受的参数。我们还将计时训练过程，这可能会非常耗时。
- en: '[PRE34]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, our DNN is fully trained. We are ready to begin the validation process!
    Let''s set up a Python variable called `hits` to count the total number of correct
    classifications. We will also need to condition the validation/testing data too.
    One more thing—we determine the class by the index corresponding to the largest
    value of the softmax layer of our DNN. We can check whether this gives us the
    correct classification by using NumPy''s `argmax` function, like so:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的DNN已经完全训练好了。我们准备开始验证过程！让我们设置一个名为`hits`的Python变量来计算正确分类的总数。我们还需要对验证/测试数据进行条件设置。还有一件事——我们通过DNN的softmax层的最大值对应的索引来确定类别。我们可以使用NumPy的`argmax`函数来检查这是否给出了正确的分类，就像这样：
- en: '[PRE35]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, we are ready to check how well our DNN actually works. Let''s output the
    accuracy as well as the total training time:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备检查我们的DNN实际工作得有多好。让我们输出准确率以及总训练时间：
- en: '[PRE36]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, we are done. We can now fully implement a DNN with Python and CUDA! Generally
    speaking, you can expect an accuracy ranging from 80%-97% for this particular
    problem, with a training time of 10-20 minutes on any Pascal-level GPU.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们完成了。我们现在可以完全用Python和CUDA实现一个DNN！一般来说，您可以期望在这个特定问题上的准确率在80%-97%之间，使用任何Pascal级别的GPU的训练时间为10-20分钟。
- en: The code for this chapter is available in the `deep_neural_network.py` file,
    under the appropriate directory in this book's GitHub repository.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在本书的GitHub存储库的适当目录下的`deep_neural_network.py`文件中找到。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by giving the definition of an artificial neural
    network, and showed you how individual ANs can be combined into dense layers,
    which combine together into a full-on deep neural network. We then implemented
    a dense layer in CUDA-C and made an appropriate corresponding Python wrapper class.
    We also included functionality to add ReLU and sigmoid layers on the outputs of
    a dense layer. We saw the definition and motivation of using a softmax layer,
    which is used for classification problems, and then implemented this in CUDA-C
    and Python. Finally, we implemented a Python class so that we could build a sequential
    feed-forward DNN from the prior classes;  we implemented a cross-entropy loss
    function, and then used this in our loss function in our implementation of gradient
    descent to train the weights and biases in our DNN. Finally, we used our implementation
    to construct, train, and test a DNN on a real-life dataset.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先给出了人工神经网络的定义，并向您展示了如何将单个AN组合成密集层，然后再将其组合成完整的深度神经网络。然后，我们在CUDA-C中实现了一个密集层，并制作了一个相应的Python包装类。我们还包括了在密集层的输出上添加ReLU和sigmoid层的功能。我们看到了使用softmax层的定义和动机，这用于分类问题，然后在CUDA-C和Python中实现了这一功能。最后，我们实现了一个Python类，以便我们可以从先前的类构建一个顺序前馈DNN；我们实现了一个交叉熵损失函数，然后在我们的梯度下降实现中使用这个损失函数来训练我们DNN中的权重和偏差。最后，我们使用我们的实现在真实数据集上构建、训练和测试了一个DNN。
- en: We now have a great deal of self-confidence in our CUDA programming abilities,
    since we can write our own GPU-based DNN! We will now move on to some very advanced
    material in the next two chapters, where we will look at how we can write our
    own interfaces to compiled CUDA code, as well as some of the very technical ins
    and outs of NVIDIA GPUs.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对我们的CUDA编程能力有了很大的自信，因为我们可以编写自己基于GPU的DNN！我们现在将在接下来的两章中学习一些非常高级的内容，我们将看看如何编写我们自己的接口到编译后的CUDA代码，以及一些关于NVIDIA
    GPU非常技术性的细节。
- en: Questions
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Suppose you construct a DNN and after training it, it yields only garbage. After
    inspection, you find that all of the weights and biases are either huge numbers
    or NaNs. What might the problem be?
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设您构建了一个DNN，并在训练后，它只产生垃圾。经过检查，您发现所有的权重和偏差要么是巨大的数字，要么是NaN。问题可能是什么？
- en: Name one possible problem with a small `training_rate` value.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小`training_rate`值可能存在的一个问题是什么？
- en: Name one possible problem with a large `training_rate` value.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大`training_rate`值可能存在的一个问题是什么？
- en: Suppose we want to train a DNN that will assign multiple labels to an image
    of an animal ("slimey", "furry", "red", "brown", and so on). Should we use a sigmoid
    or softmax layer at the end of the DNN?
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们想要训练一个DNN，将多个标签分配给动物的图像（“黏滑的”，“毛茸茸的”，“红色的”，“棕色的”等等）。在DNN的末尾应该使用sigmoid还是softmax层？
- en: Suppose we want to classify an image of a single animal as either a cat or dog.
    Do we use sigmoid or softmax?
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们想要将单个动物的图像分类为猫或狗。我们应该使用sigmoid还是softmax？
- en: If we decrease the batch size, will there be more or less updates to the weights
    and biases during gradient descent training?
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们减小批量大小，梯度下降训练过程中权重和偏差的更新会更多还是更少？
