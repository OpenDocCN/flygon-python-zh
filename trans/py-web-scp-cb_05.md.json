["```py\nUser-agent: *\nDisallow: /exec/obidos/account-access-login\nDisallow: /exec/obidos/change-style\nDisallow: /exec/obidos/flex-sign-in\nDisallow: /exec/obidos/handle-buy-box\nDisallow: /exec/obidos/tg/cm/member/\nDisallow: /gp/aw/help/id=sss\nDisallow: /gp/cart\nDisallow: /gp/flex\n\n...\n\nAllow: /wishlist/universal*\nAllow: /wishlist/vendor-button*\nAllow: /wishlist/get-button*\n\n...\n\nUser-agent: Googlebot\nDisallow: /rss/people/*/reviews\nDisallow: /gp/pdp/rss/*/reviews\nDisallow: /gp/cdp/member-reviews/\nDisallow: /gp/aw/cr/\n\n...\nAllow: /wishlist/universal*\nAllow: /wishlist/vendor-button*\nAllow: /wishlist/get-button*\n\n```", "```py\npip install reppy\n```", "```py\nCFLAGS=-stdlib=libc++ pip install reppy\n```", "```py\nTrue: http://www.amazon.com/\nFalse: http://www.amazon.com/gp/dmusic/\nTrue: http://www.amazon.com/gp/dmusic/promotions/PrimeMusic/\nFalse: http://www.amazon.com/gp/registry/wishlist/\n```", "```py\nfrom reppy.robots import Robots\n```", "```py\nurl = \"http://www.amazon.com\" robots = Robots.fetch(url + \"/robots.txt\")\n```", "```py\npaths = [\n  '/',\n  '/gp/dmusic/', '/gp/dmusic/promotions/PrimeMusic/',\n '/gp/registry/wishlist/'  ]   for path in paths:\n  print(\"{0}: {1}\".format(robots.allowed(path, '*'), url + path))\n```", "```py\nTrue: http://www.amazon.com/\nFalse: http://www.amazon.com/gp/dmusic/\nTrue: http://www.amazon.com/gp/dmusic/promotions/PrimeMusic/\nFalse: http://www.amazon.com/gp/registry/wishlist/\n```", "```py\nDisallow: /gp/dmusic/\nAllow: /gp/dmusic/promotions/PrimeMusic\n```", "```py\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\" \n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n   xsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\">\n    <url>\n        <loc>http://example.com/</loc>\n        <lastmod>2006-11-18</lastmod>\n        <changefreq>daily</changefreq>\n        <priority>0.8</priority>\n    </url>\n</urlset>\n```", "```py\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n   <sitemap>\n      <loc>http://www.example.com/sitemap1.xml.gz</loc>\n      <lastmod>2014-10-01T18:23:17+00:00</lastmod>\n   </sitemap>\n</sitemapindex>\n```", "```py\nSitemap: https://www.microsoft.com/en-us/explore/msft_sitemap_index.xml\nSitemap: https://www.microsoft.com/learning/sitemap.xml\nSitemap: https://www.microsoft.com/en-us/licensing/sitemap.xml\nSitemap: https://www.microsoft.com/en-us/legal/sitemap.xml\nSitemap: https://www.microsoft.com/filedata/sitemaps/RW5xN8\nSitemap: https://www.microsoft.com/store/collections.xml\nSitemap: https://www.microsoft.com/store/productdetailpages.index.xml\n```", "```py\nFound 35511 urls\n{'lastmod': '2017-10-11T18:23Z', 'loc': 'http://www.nasa.gov/centers/marshall/history/this-week-in-nasa-history-apollo-7-launches-oct-11-1968.html', 'tag': 'url'}\n{'lastmod': '2017-10-11T18:22Z', 'loc': 'http://www.nasa.gov/feature/researchers-develop-new-tool-to-evaluate-icephobic-materials', 'tag': 'url'}\n{'lastmod': '2017-10-11T17:38Z', 'loc': 'http://www.nasa.gov/centers/ames/entry-systems-vehicle-development/roster.html', 'tag': 'url'}\n{'lastmod': '2017-10-11T17:38Z', 'loc': 'http://www.nasa.gov/centers/ames/entry-systems-vehicle-development/about.html', 'tag': 'url'}\n{'lastmod': '2017-10-11T17:22Z', 'loc': 'http://www.nasa.gov/centers/ames/earthscience/programs/MMS/instruments', 'tag': 'url'}\n{'lastmod': '2017-10-11T18:15Z', 'loc': 'http://www.nasa.gov/centers/ames/earthscience/programs/MMS/onepager', 'tag': 'url'}\n{'lastmod': '2017-10-11T17:10Z', 'loc': 'http://www.nasa.gov/centers/ames/earthscience/programs/MMS', 'tag': 'url'}\n{'lastmod': '2017-10-11T17:53Z', 'loc': 'http://www.nasa.gov/feature/goddard/2017/nasa-s-james-webb-space-telescope-and-the-big-bang-a-short-qa-with-nobel-laureate-dr-john', 'tag': 'url'}\n{'lastmod': '2017-10-11T17:38Z', 'loc': 'http://www.nasa.gov/centers/ames/entry-systems-vehicle-development/index.html', 'tag': 'url'}\n{'lastmod': '2017-10-11T15:21Z', 'loc': 'http://www.nasa.gov/feature/mark-s-geyer-acting-deputy-associate-administrator-for-technical-human-explorations-and-operations', 'tag': 'url'}\n```", "```py\nmap = sitemap.get_sitemap(\"https://www.nasa.gov/sitemap.xml\")\n```", "```py\ndef get_sitemap(url):\n  get_url = requests.get(url)    if get_url.status_code == 200:\n  return get_url.text\n    else:\n  print ('Unable to fetch sitemap: %s.' % url) \n```", "```py\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"//www.nasa.gov/sitemap.xsl\"?>\n<sitemapindex >\n<sitemap><loc>http://www.nasa.gov/sitemap-1.xml</loc><lastmod>2017-10-11T19:30Z</lastmod></sitemap>\n<sitemap><loc>http://www.nasa.gov/sitemap-2.xml</loc><lastmod>2017-10-11T19:30Z</lastmod></sitemap>\n<sitemap><loc>http://www.nasa.gov/sitemap-3.xml</loc><lastmod>2017-10-11T19:30Z</lastmod></sitemap>\n<sitemap><loc>http://www.nasa.gov/sitemap-4.xml</loc><lastmod>2017-10-11T19:30Z</lastmod></sitemap>\n</sitemapindex>\n```", "```py\ndef parse_sitemap(s):\n  sitemap = process_sitemap(s)\n```", "```py\ndef process_sitemap(s):\n  soup = BeautifulSoup(s, \"lxml\")\n  result = []    for loc in soup.findAll('loc'):\n  item = {}\n  item['loc'] = loc.text\n        item['tag'] = loc.parent.name\n        if loc.parent.lastmod is not None:\n  item['lastmod'] = loc.parent.lastmod.text\n        if loc.parent.changeFreq is not None:\n  item['changeFreq'] = loc.parent.changeFreq.text\n        if loc.parent.priority is not None:\n  item['priority'] = loc.parent.priority.text\n        result.append(item)    return result\n```", "```py\nwhile sitemap:\n  candidate = sitemap.pop()    if is_sub_sitemap(candidate):\n  sub_sitemap = get_sitemap(candidate['loc'])\n  for i in process_sitemap(sub_sitemap):\n  sitemap.append(i)\n  else:\n  result.append(candidate)\n```", "```py\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"//www.nasa.gov/sitemap.xsl\"?>\n<urlset >\n<url><loc>http://www.nasa.gov/</loc><changefreq>daily</changefreq><priority>1.0</priority></url>\n<url><loc>http://www.nasa.gov/connect/apps.html</loc><lastmod>2017-08-14T22:15Z</lastmod><changefreq>yearly</changefreq></url>\n<url><loc>http://www.nasa.gov/socialmedia</loc><lastmod>2017-09-29T21:47Z</lastmod><changefreq>monthly</changefreq></url>\n<url><loc>http://www.nasa.gov/multimedia/imagegallery/iotd.html</loc><lastmod>2017-08-21T22:00Z</lastmod><changefreq>yearly</changefreq></url>\n<url><loc>http://www.nasa.gov/archive/archive/about/career/index.html</loc><lastmod>2017-08-04T02:31Z</lastmod><changefreq>yearly</changefreq></url>\n```", "```py\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass Spider(scrapy.spiders.SitemapSpider):\n  name = 'spider'\n  sitemap_urls = ['https://www.nasa.gov/sitemap.xml']    def parse(self, response):\n  print(\"Parsing: \", response)   if __name__ == \"__main__\":\n  process = CrawlerProcess({\n  'DOWNLOAD_DELAY': 0,\n  'LOG_LEVEL': 'DEBUG'\n  })\n  process.crawl(Spider)\n  process.start()\n```", "```py\n2017-10-11 20:34:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nasa.gov/sitemap.xml> (referer: None)\n2017-10-11 20:34:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nasa.gov/sitemap-4.xml> from <GET http://www.nasa.gov/sitemap-4.xml>\n2017-10-11 20:34:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nasa.gov/sitemap-2.xml> from <GET http://www.nasa.gov/sitemap-2.xml>\n2017-10-11 20:34:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nasa.gov/sitemap-3.xml> from <GET http://www.nasa.gov/sitemap-3.xml>\n2017-10-11 20:34:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nasa.gov/sitemap-1.xml> from <GET http://www.nasa.gov/sitemap-1.xml>\n2017-10-11 20:34:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nasa.gov/sitemap-4.xml> (referer: None)\n```", "```py\n2017-10-11 20:34:30 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.nasa.gov/image-feature/jpl/pia21629/neptune-from-saturn/> from <GET https://www.nasa.gov/image-feature/jpl/pia21629/neptune-from-saturn>\n2017-10-11 20:34:30 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.nasa.gov/centers/ames/earthscience/members/nasaearthexchange/Ramakrishna_Nemani/> from <GET https://www.nasa.gov/centers/ames/earthscience/members/nasaearthexchang\n```", "```py\ne/Ramakrishna_Nemani>\nParsing: <200 https://www.nasa.gov/exploration/systems/sls/multimedia/sls-hardware-being-moved-on-kamag-transporter.html>\nParsing: <200 https://www.nasa.gov/exploration/systems/sls/M17-057.html>\n```", "```py\n05 $ scrapy runspider 04_scrape_with_delay.py -s LOG_LEVEL=WARNING\nParsing: <200 https://blog.scrapinghub.com>\nParsing: <200 https://blog.scrapinghub.com/page/2/>\nParsing: <200 https://blog.scrapinghub.com/page/3/>\nParsing: <200 https://blog.scrapinghub.com/page/4/>\nParsing: &lt;200 https://blog.scrapinghub.com/page/5/>\nParsing: <200 https://blog.scrapinghub.com/page/6/>\nParsing: <200 https://blog.scrapinghub.com/page/7/>\nParsing: <200 https://blog.scrapinghub.com/page/8/>\nParsing: <200 https://blog.scrapinghub.com/page/9/>\nParsing: <200 https://blog.scrapinghub.com/page/10/>\nParsing: <200 https://blog.scrapinghub.com/page/11/>\nTotal run time: 0:00:07.006148\nMichaels-iMac-2:05 michaelheydt$ \n```", "```py\n05 $ scrapy runspider 04_scrape_with_delay.py -s DOWNLOAD_DELAY=5 -s LOG_LEVEL=WARNING\nParsing: <200 https://blog.scrapinghub.com>\nParsing: <200 https://blog.scrapinghub.com/page/2/>\nParsing: <200 https://blog.scrapinghub.com/page/3/>\nParsing: <200 https://blog.scrapinghub.com/page/4/>\nParsing: <200 https://blog.scrapinghub.com/page/5/>\nParsing: <200 https://blog.scrapinghub.com/page/6/>\nParsing: <200 https://blog.scrapinghub.com/page/7/>\nParsing: <200 https://blog.scrapinghub.com/page/8/>\nParsing: <200 https://blog.scrapinghub.com/page/9/>\nParsing: <200 https://blog.scrapinghub.com/page/10/>\nParsing: <200 https://blog.scrapinghub.com/page/11/>\nTotal run time: 0:01:01.099267\n```", "```py\nclass Spider(scrapy.Spider):\n  name = 'spider'\n  start_urls = ['https://blog.scrapinghub.com']\n```", "```py\ndef close(spider, reason):\n  start_time = spider.crawler.stats.get_value('start_time')\n  finish_time = spider.crawler.stats.get_value('finish_time')\n  print(\"Total run time: \", finish_time-start_time)\n```", "```py\nif __name__ == \"__main__\":\n  process = CrawlerProcess({\n  'DOWNLOAD_DELAY': 5,\n  'RANDOMIZED_DOWNLOAD_DELAY': False,\n  'LOG_LEVEL': 'DEBUG'\n  })\n  process.crawl(Spider)\n  process.start()\n```", "```py\nurl = 'https://api.github.com/some/endpoint'\nheaders = {'user-agent': 'MyCompany-MyCrawler (mybot@mycompany.com)'}\nr = requests.get(url, headers=headers) \n```", "```py\nprocess = CrawlerProcess({\n 'USER_AGENT': 'MyCompany-MyCrawler (mybot@mycompany.com)'  }) process.crawl(Spider) process.start()\n```", "```py\nprocess = CrawlerProcess({\n 'CONCURRENT_REQUESTS_PER_DOMAIN': 1  }) process.crawl(Spider) process.start()\n```", "```py\nprocess = CrawlerProcess({\n 'AUTOTHROTTLE_TARGET_CONCURRENCY': 3  }) process.crawl(Spider) process.start()\n```", "```py\nprocess = CrawlerProcess({\n 'AUTOTHROTTLE_TARGET_CONCURRENCY': 3  }) process.crawl(Spider) process.start()\n```"]