["```py\nfrom __future__ import print_function\nimport argparse\nimport csv\nimport os\nimport sqlite3\nimport sys\n```", "```py\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"IEF_DATABASE\", help=\"Input IEF database\")\n    parser.add_argument(\"OUTPUT_DIR\", help=\"Output DIR\")\n    args = parser.parse_args()\n```", "```py\n    if not os.path.exists(args.OUTPUT_DIR):\n        os.makedirs(args.OUTPUT_DIR)\n\n    if os.path.exists(args.IEF_DATABASE) and \\\n            os.path.isfile(args.IEF_DATABASE):\n        main(args.IEF_DATABASE, args.OUTPUT_DIR)\n    else:\n        print(\"[-] Supplied input file {} does not exist or is not a \"\n              \"file\".format(args.IEF_DATABASE))\n        sys.exit(1)\n```", "```py\ndef main(database, out_directory):\n    print(\"[+] Connecting to SQLite database\")\n    conn = sqlite3.connect(database)\n    c = conn.cursor()\n```", "```py\n    print(\"[+] Querying IEF database for list of all tables to extract\")\n    c.execute(\"select * from sqlite_master where type='table'\")\n    # Remove tables that start with \"_\" or end with \"_DATA\"\n    tables = [x[2] for x in c.fetchall() if not x[2].startswith('_') and\n              not x[2].endswith('_DATA')]\n```", "```py\n    print(\"[+] Dumping {} tables to CSV files in {}\".format(\n        len(tables), out_directory))\n    for table in tables:\n        c.execute(\"pragma table_info('{}')\".format(table))\n        table_columns = [x[1] for x in c.fetchall()]\n        c.execute(\"select * from '{}'\".format(table))\n        table_data = c.fetchall()\n```", "```py\n        csv_name = table + '.csv'\n        csv_path = os.path.join(out_directory, csv_name)\n        print('[+] Writing {} table to {} CSV file'.format(table,\n                                                           csv_name))\n        with open(csv_path, \"w\", newline=\"\") as csvfile:\n            csv_writer = csv.writer(csvfile)\n            csv_writer.writerow(table_columns)\n            csv_writer.writerows(table_data)\n```", "```py\nfrom __future__ import print_function\nimport argparse\nimport csv\nimport json\nimport os\nimport sqlite3\nimport sys\n```", "```py\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"IEF_DATABASE\", help=\"Input IEF database\")\n    parser.add_argument(\"OUTPUT_CSV\", help=\"Output CSV\")\n    args = parser.parse_args()\n```", "```py\n    directory = os.path.dirname(args.OUTPUT_CSV)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    if os.path.exists(args.IEF_DATABASE) and \\\n            os.path.isfile(args.IEF_DATABASE):\n        main(args.IEF_DATABASE, args.OUTPUT_CSV)\n    else:\n        print(\n            \"[-] Supplied input file {} does not exist or is not a \"\n            \"file\".format(args.IEF_DATABASE))\n        sys.exit(1)\n```", "```py\ndef main(database, out_csv):\n    print(\"[+] Connecting to SQLite database\")\n    conn = sqlite3.connect(database)\n    c = conn.cursor()\n```", "```py\n    print(\"[+] Querying IEF database for Yahoo Contact Fragments from \"\n          \"the Chrome Cache Records Table\")\n    try:\n        c.execute(\n            \"select * from 'Chrome Cache Records' where URL like \"\n            \"'https://data.mail.yahoo.com\"\n            \"/classicab/v2/contacts/?format=json%'\")\n    except sqlite3.OperationalError:\n        print(\"Received an error querying the database -- database may be\"\n              \"corrupt or not have a Chrome Cache Records table\")\n        sys.exit(2)\n```", "```py\n    contact_cache = c.fetchall()\n    contact_data = process_contacts(contact_cache)\n    write_csv(contact_data, out_csv)\n```", "```py\ndef process_contacts(contact_cache):\n    print(\"[+] Processing {} cache files matching Yahoo contact cache \"\n          \" data\".format(len(contact_cache)))\n    results = []\n    for contact in contact_cache:\n        url = contact[0]\n        first_visit = contact[1]\n        last_visit = contact[2]\n        last_sync = contact[3]\n        loc = contact[8]\n        contact_json = json.loads(contact[7].decode())\n        total_contacts = contact_json[\"total\"]\n        total_count = contact_json[\"count\"]\n```", "```py\n        if \"contacts\" not in contact_json:\n            continue\n\n        for c in contact_json[\"contacts\"]:\n            name, anni, bday, emails, phones, links = (\n                \"\", \"\", \"\", \"\", \"\", \"\")\n```", "```py\n            if \"name\" in c:\n                name = c[\"name\"][\"givenName\"] + \" \" + \\\n                    c[\"name\"][\"middleName\"] + \" \" + c[\"name\"][\"familyName\"]\n            if \"anniversary\" in c:\n                anni = c[\"anniversary\"][\"month\"] + \\\n                    \"/\" + c[\"anniversary\"][\"day\"] + \"/\" + \\\n                    c[\"anniversary\"][\"year\"]\n            if \"birthday\" in c:\n                bday = c[\"birthday\"][\"month\"] + \"/\" + \\\n                    c[\"birthday\"][\"day\"] + \"/\" + c[\"birthday\"][\"year\"]\n            if \"emails\" in c:\n                emails = ', '.join([x[\"ep\"] for x in c[\"emails\"]])\n            if \"phones\" in c:\n                phones = ', '.join([x[\"ep\"] for x in c[\"phones\"]])\n            if \"links\" in c:\n                links = ', '.join([x[\"ep\"] for x in c[\"links\"]])\n```", "```py\n            company = c.get(\"company\", \"\")\n            title = c.get(\"jobTitle\", \"\")\n            notes = c.get(\"notes\", \"\")\n```", "```py\n            results.append([\n                url, first_visit, last_visit, last_sync, loc, name, bday,\n                anni, emails, phones, links, company, title, notes,\n                total_contacts, total_count])\n    return results\n```", "```py\ndef write_csv(data, output):\n    print(\"[+] Writing {} contacts to {}\".format(len(data), output))\n    with open(output, \"w\", newline=\"\") as csvfile:\n        csv_writer = csv.writer(csvfile)\n        csv_writer.writerow([\n            \"URL\", \"First Visit (UTC)\", \"Last Visit (UTC)\",\n            \"Last Sync (UTC)\", \"Location\", \"Contact Name\", \"Bday\",\n            \"Anniversary\", \"Emails\", \"Phones\", \"Links\", \"Company\", \"Title\",\n            \"Notes\", \"Total Contacts\", \"Count of Contacts in Cache\"])\n        csv_writer.writerows(data)\n```", "```py\npip install bs4==0.0.1\n```", "```py\nfrom __future__ import print_function\nimport argparse\nfrom bs4 import BeautifulSoup, SoupStrainer\nfrom datetime import datetime\nimport hashlib\nimport logging\nimport os\nimport ssl\nimport sys\nfrom urllib.request import urlopen\nimport urllib.error\n\nlogger = logging.getLogger(__name__)\n```", "```py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"DOMAIN\", help=\"Website Domain\")\n    parser.add_argument(\"OUTPUT_DIR\", help=\"Preservation Output Directory\")\n    parser.add_argument(\"-l\", help=\"Log file path\",\n                        default=__file__[:-3] + \".log\")\n    args = parser.parse_args()\n```", "```py\n    logger.setLevel(logging.DEBUG)\n    msg_fmt = logging.Formatter(\"%(asctime)-15s %(funcName)-10s\"\n                                \"%(levelname)-8s %(message)s\")\n    strhndl = logging.StreamHandler(sys.stderr)\n    strhndl.setFormatter(fmt=msg_fmt)\n    fhndl = logging.FileHandler(args.l, mode='a')\n    fhndl.setFormatter(fmt=msg_fmt)\n\n    logger.addHandler(strhndl)\n    logger.addHandler(fhndl)\n```", "```py\n    logger.info(\"Starting BS Preservation\")\n    logger.debug(\"Supplied arguments: {}\".format(sys.argv[1:]))\n    logger.debug(\"System \" + sys.platform)\n    logger.debug(\"Version \" + sys.version)\n```", "```py\n    if not os.path.exists(args.OUTPUT_DIR):\n        os.makedirs(args.OUTPUT_DIR)\n\n    main(args.DOMAIN, args.OUTPUT_DIR)\n```", "```py\ndef main(website, output_dir):\n    base_name = website.replace(\n        \"https://\", \"\").replace(\"http://\", \"\").replace(\"www.\", \"\")\n    link_queue = set()\n    if \"http://\" not in website and \"https://\" not in website:\n        logger.error(\n            \"Exiting preservation - invalid user input: {}\".format(\n                website))\n        sys.exit(1)\n    logger.info(\"Accessing {} webpage\".format(website))\n    context = ssl._create_unverified_context()\n```", "```py\n    try:\n        index = urlopen(website, context=context).read().decode(\"utf-8\")\n    except urllib.error.HTTPError as e:\n        logger.error(\n            \"Exiting preservation - unable to access page: {}\".format(\n                website))\n        sys.exit(2)\n    logger.debug(\"Successfully accessed {}\".format(website))\n```", "```py\n    write_output(website, index, output_dir)\n    link_queue = find_links(base_name, index, link_queue)\n    logger.info(\"Found {} initial links on webpage\".format(\n        len(link_queue)))\n    recurse_pages(website, link_queue, context, output_dir)\n    logger.info(\"Completed preservation of {}\".format(website))\n```", "```py\ndef write_output(name, data, output_dir, counter=0):\n    name = name.replace(\"http://\", \"\").replace(\"https://\", \"\").rstrip(\"//\")\n    directory = os.path.join(output_dir, os.path.dirname(name))\n    if not os.path.exists(directory) and os.path.dirname(name) != \"\":\n        os.makedirs(directory)\n```", "```py\n    logger.debug(\"Writing {} to {}\".format(name, output_dir))\n    logger.debug(\"Data Hash: {}\".format(hash_data(data)))\n    path = os.path.join(output_dir, name)\n    path = path + \"_\" + str(counter)\n    with open(path, \"w\") as outfile:\n        outfile.write(data)\n    logger.debug(\"Output File Hash: {}\".format(hash_file(path)))\n```", "```py\ndef hash_data(data):\n    sha256 = hashlib.sha256()\n    sha256.update(data.encode(\"utf-8\"))\n    return sha256.hexdigest()\n```", "```py\ndef hash_file(file):\n    sha256 = hashlib.sha256()\n    with open(file, \"rb\") as in_file:\n        sha256.update(in_file.read())\n    return sha256.hexdigest()\n```", "```py\ndef find_links(website, page, queue):\n    for link in BeautifulSoup(page, \"html.parser\",\n                              parse_only=SoupStrainer(\"a\", href=True)):\n        if website in link.get(\"href\"):\n            if not os.path.basename(link.get(\"href\")).startswith(\"#\"):\n                queue.add(link.get(\"href\"))\n    return queue\n```", "```py\ndef recurse_pages(website, queue, context, output_dir):\n    processed = []\n    counter = 0\n    while True:\n        counter += 1\n        if len(processed) == len(queue):\n            break\n```", "```py\n        for link in queue.copy():\n            if link in processed:\n                continue\n            processed.append(link)\n```", "```py\n            try:\n                page = urlopen(link, context=context).read().decode(\n                    \"utf-8\")\n            except urllib.error.HTTPError as e:\n                msg = \"Error accessing webpage: {}\".format(link)\n                logger.error(msg)\n                continue\n```", "```py\n            write_output(link, page, output_dir, counter)\n            queue = find_links(website, page, queue)\n    logger.info(\"Identified {} links throughout website\".format(\n        len(queue)))\n```", "```py\npip install tqdm==4.11.2\n```", "```py\nfrom __future__ import print_function\nimport argparse\nimport os\nimport ssl\nimport sys\nimport tqdm\nfrom urllib.request import urlopen\nimport urllib.error\n```", "```py\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"OUTPUT_HASH\", help=\"Output Hashset\")\n    parser.add_argument(\"--start\", type=int,\n                        help=\"Optional starting location\")\n    args = parser.parse_args()\n```", "```py\n    directory = os.path.dirname(args.OUTPUT_HASH)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    if args.start:\n        main(args.OUTPUT_HASH, start=args.start)\n    else:\n        main(args.OUTPUT_HASH)\n```", "```py\ndef main(hashset, **kwargs):\n    url = \"https://virusshare.com/hashes.4n6\"\n    print(\"[+] Identifying hash set range from {}\".format(url))\n    context = ssl._create_unverified_context()\n```", "```py\n    try:\n        index = urlopen(url, context=context).read().decode(\"utf-8\")\n    except urllib.error.HTTPError as e:\n        print(\"[-] Error accessing webpage - exiting..\")\n        sys.exit(1)\n```", "```py\n    tag = index.rfind(r'<a href=\"hashes/VirusShare_')\n    stop = int(index[tag + 27: tag + 27 + 5].lstrip(\"0\"))\n\n    if \"start\" not in kwargs:\n        start = 0\n    else:\n        start = kwargs[\"start\"]\n```", "```py\n    if start < 0 or start > stop:\n        print(\"[-] Supplied start argument must be greater than or equal \"\n              \"to zero but less than the latest hash list, \"\n              \"currently: {}\".format(stop))\n        sys.exit(2)\n\n    print(\"[+] Creating a hashset from hash lists {} to {}\".format(\n        start, stop))\n    hashes_downloaded = 0\n```", "```py\n    for x in tqdm.trange(start, stop + 1, unit_scale=True,\n                         desc=\"Progress\"):\n        url_hash = \"https://virusshare.com/hashes/VirusShare_\"\\\n                   \"{}.md5\".format(str(x).zfill(5))\n        try:\n            hashes = urlopen(\n                url_hash, context=context).read().decode(\"utf-8\")\n            hashes_list = hashes.split(\"\\n\")\n        except urllib.error.HTTPError as e:\n            print(\"[-] Error accessing webpage for hash list {}\"\n                  \" - continuing..\".format(x))\n            continue\n```", "```py\n\n        with open(hashset, \"a+\") as hashfile:\n            for line in hashes_list:\n                if not line.startswith(\"#\") and line != \"\":\n                    hashes_downloaded += 1\n                    hashfile.write(line + '\\n')\n\n    print(\"[+] Finished downloading {} hashes into {}\".format(\n        hashes_downloaded, hashset))\n```", "```py\npip install requests==2.18.4\n```", "```py\nfrom __future__ import print_function\nimport argparse\nimport csv\nimport hashlib\nimport json\nimport os\nimport requests\nimport sys\nimport time\n```", "```py\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"INPUT_FILE\",\n                        help=\"Text File containing list of file paths/\"\n                             \"hashes or domains/IPs\")\n    parser.add_argument(\"OUTPUT_CSV\",\n                        help=\"Output CSV with lookup results\")\n    parser.add_argument(\"API_KEY\", help=\"Text File containing API key\")\n    parser.add_argument(\"-t\", \"--type\",\n                        help=\"Type of data: file or domain\",\n                        choices=(\"file\", \"domain\"), default=\"domain\")\n    parser.add_argument(\n        \"--limit\", action=\"store_true\",\n        help=\"Limit requests to comply with public API key restrictions\")\n    args = parser.parse_args()\n```", "```py\n\n    directory = os.path.dirname(args.OUTPUT_CSV)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    if os.path.exists(args.INPUT_FILE) and os.path.isfile(args.INPUT_FILE):\n        main(args.INPUT_FILE, args.OUTPUT_CSV,\n             args.API_KEY, args.limit, args.type)\n    else:\n        print(\"[-] Supplied input file {} does not exist or is not a \"\n              \"file\".format(args.INPUT_FILE))\n        sys.exit(1)\n\n```", "```py\ndef main(input_file, output, api, limit, type):\n    objects = set()\n    with open(input_file) as infile:\n        for line in infile:\n            if line.strip() != \"\":\n                objects.add(line.strip())\n```", "```py\n    if type == \"domain\":\n        data = query_domain(objects, api, limit)\n    else:\n        data = query_file(objects, api, limit)\n    write_csv(data, output)\n```", "```py\ndef query_domain(domains, api, limit):\n    if not os.path.exists(api) and os.path.isfile(api):\n        print(\"[-] API key file {} does not exist or is not a file\".format(\n            api))\n        sys.exit(2)\n\n    with open(api) as infile:\n        api = infile.read().strip()\n    json_data = []\n```", "```py\n    print(\"[+] Querying {} Domains / IPs using VirusTotal API\".format(\n        len(domains)))\n    count = 0\n    for domain in domains:\n        count += 1\n        params = {\"resource\": domain, \"apikey\": api, \"scan\": 1}\n        response = requests.post(\n            'https://www.virustotal.com/vtapi/v2/url/report',\n            params=params)\n        json_response = response.json()\n```", "```py\n        if \"Scan finished\" in json_response[\"verbose_msg\"]:\n            json_data.append(json_response)\n```", "```py\n        if limit and count == 3:\n            print(\"[+] Halting execution for a minute to comply with \"\n                  \"public API key restrictions\")\n            time.sleep(60)\n            print(\"[+] Continuing execution of remaining Domains / IPs\")\n            count = 0\n\n    return json_data\n```", "```py\ndef query_file(files, api, limit):\n    if not os.path.exists(api) and os.path.isfile(api):\n        print(\"[-] API key file {} does not exist or is not a file\".format(\n            api))\n        sys.exit(3)\n\n    with open(api) as infile:\n        api = infile.read().strip()\n    json_data = []\n```", "```py\n    print(\"[+] Hashing and Querying {} Files using VirusTotal API\".format(\n        len(files)))\n    count = 0\n    for file_entry in files:\n        if os.path.exists(file_entry):\n            file_hash = hash_file(file_entry)\n        elif len(file_entry) == 32:\n            file_hash = file_entry\n        else:\n            continue\n        count += 1\n```", "```py\ndef hash_file(file_path):\n    sha256 = hashlib.sha256()\n    with open(file_path, 'rb') as open_file:\n        buff_size = 1024\n        buff = open_file.read(buff_size)\n\n        while buff:\n            sha256.update(buff)\n            buff = open_file.read(buff_size)\n    return sha256.hexdigest()\n```", "```py\n        params = {\"resource\": file_hash, \"apikey\": api}\n        response = requests.post(\n            'https://www.virustotal.com/vtapi/v2/file/report',\n            params=params)\n        json_response = response.json()\n```", "```py\n        if \"Scan finished\" in json_response[\"verbose_msg\"]:\n            json_data.append(json_response)\n\n        if limit and count == 3:\n            print(\"[+] Halting execution for a minute to comply with \"\n                  \"public API key restrictions\")\n            time.sleep(60)\n            print(\"[+] Continuing execution of remaining files\")\n            count = 0\n\n    return json_data\n```", "```py\ndef write_csv(data, output):\n    if data == []:\n        print(\"[-] No output results to write\")\n        sys.exit(4)\n```", "```py\n    print(\"[+] Writing output for {} domains with results to {}\".format(\n        len(data), output))\n    flatten_data = []\n    field_list = [\"URL\", \"Scan Date\", \"Service\",\n                  \"Detected\", \"Result\", \"VirusTotal Link\"]\n    for result in data:\n        for service in result[\"scans\"]:\n            flatten_data.append(\n                {\"URL\": result.get(\"url\", \"\"),\n                 \"Scan Date\": result.get(\"scan_date\", \"\"),\n                 \"VirusTotal Link\": result.get(\"permalink\", \"\"),\n                 \"Service\": service,\n                 \"Detected\": result[\"scans\"][service][\"detected\"],\n                 \"Result\": result[\"scans\"][service][\"result\"]})\n```", "```py\n    with open(output, \"w\", newline=\"\") as csvfile:\n        csv_writer = csv.DictWriter(csvfile, fieldnames=field_list)\n        csv_writer.writeheader()\n        for result in flatten_data:\n            csv_writer.writerow(result)\n```", "```py\nfrom __future__ import print_function\nimport argparse\nimport csv\nimport json\nimport os\nimport subprocess\nimport sys\n```", "```py\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=__description__,\n        epilog=\"Developed by {} on {}\".format(\n            \", \".join(__authors__), __date__)\n    )\n    parser.add_argument(\"INPUT_DOMAINS\",\n                        help=\"Text File containing Domains and/or IPs\")\n    parser.add_argument(\"OUTPUT_CSV\",\n                        help=\"Output CSV with lookup results\")\n    args = parser.parse_args()\n```", "```py\n    directory = os.path.dirname(args.OUTPUT_CSV)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    if os.path.exists(args.INPUT_DOMAINS) and \\\n            os.path.isfile(args.INPUT_DOMAINS):\n        main(args.INPUT_DOMAINS, args.OUTPUT_CSV)\n    else:\n        print(\n            \"[-] Supplied input file {} does not exist or is not a \"\n            \"file\".format(args.INPUT_DOMAINS))\n        sys.exit(1)\n```", "```py\ndef main(domain_file, output):\n    domains = set()\n    with open(domain_file) as infile:\n        for line in infile:\n            domains.add(line.strip())\n    json_data = query_domains(domains)\n    write_csv(json_data, output)\n```", "```py\ndef query_domains(domains):\n    json_data = []\n    print(\"[+] Querying {} domains/IPs using PassiveTotal API\".format(\n        len(domains)))\n    for domain in domains:\n        if \"https://\" in domain:\n            domain = domain.replace(\"https://\", \"\")\n        elif \"http://\" in domain:\n            domain = domain.replace(\"http://\", \"\")\n```", "```py\n        proc = subprocess.Popen(\n            [\"pt-client\", \"pdns\", \"-q\", domain], stdout=subprocess.PIPE)\n        results, err = proc.communicate()\n        result_json = json.loads(results.decode())\n```", "```py\n        if \"message\" in result_json:\n            if \"quota_exceeded\" in result_json[\"message\"]:\n                print(\"[-] API Search Quota Exceeded\")\n                continue\n```", "```py\n        result_count = result_json[\"totalRecords\"]\n\n        print(\"[+] {} results for {}\".format(result_count, domain))\n        if result_count == 0:\n            pass\n        else:\n            json_data.append(result_json[\"results\"])\n\n    return json_data\n```", "```py\ndef write_csv(data, output):\n    if data == []:\n        print(\"[-] No output results to write\")\n        sys.exit(2)\n\n    print(\"[+] Writing output for {} domains/IPs with \"\n          \"results to {}\".format(len(data), output))\n    field_list = [\"value\", \"firstSeen\", \"lastSeen\", \"collected\",\n                  \"resolve\", \"resolveType\", \"source\", \"recordType\",\n                  \"recordHash\"]\n```", "```py\n    with open(output, \"w\", newline=\"\") as csvfile:\n        csv_writer = csv.DictWriter(csvfile, fieldnames=field_list)\n        csv_writer.writeheader()\n        for result in data:\n            for dictionary in result:\n                csv_writer.writerow(dictionary)\n```"]