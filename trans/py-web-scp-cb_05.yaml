- en: Scraping - Code of Conduct
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取 - 行为准则
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Scraping legality and scraping politely
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抓取的合法性和有礼貌的抓取
- en: Respecting robots.txt
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尊重robots.txt
- en: Crawling using the sitemap
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用站点地图进行爬行
- en: Crawling with delays
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带延迟的爬行
- en: Using identifiable user agents
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可识别的用户代理
- en: Setting the number of concurrent requests per domain
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置每个域的并发请求数量
- en: Using auto throttling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动节流
- en: Caching responses
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存响应
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: While you can technically scrape any website, it is important to know whether
    scraping is legal or not. We will discuss scraping legal concerns, explore general
    rules of thumb, and see best practices to scrape politely and minimize potential
    damage to the target websites.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您在技术上可以抓取任何网站，但重要的是要知道抓取是否合法。我们将讨论抓取的法律问题，探讨一般的法律原则，并了解有礼貌地抓取和最大程度地减少对目标网站的潜在损害的最佳做法。
- en: Scraping legality and scraping politely
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取的合法性和有礼貌的抓取
- en: There's no real code in this recipe.  It's simply an exposition of some of the
    concepts related to the legal issues involved in scraping.  I'm not a lawyer,
    so don't take anything I write here as legal advice.  I'll just point out a few
    things you need to be concerned with when using a scraper.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方中没有真正的代码。这只是对涉及抓取的法律问题的一些概念的阐述。我不是律师，所以不要把我在这里写的任何东西当作法律建议。我只是指出在使用抓取器时需要关注的一些事情。
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The legality of scraping breaks down into two issues:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 抓取的合法性分为两个问题：
- en: Ownership of content
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容所有权
- en: Denial of service
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拒绝服务
- en: Fundamentally, anything posted on the web is open for reading.  Every time you
    load a page, any page, your browser downloads that content from the web server
    and visually presents it to you. So in a sense, you and your browser are already
    scraping anything you look at on the web.  And by the nature of the web, because
    someone is posting content publicly on the web, they are inherently asking you
    to take that information, but often only for specific purposes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，网上发布的任何内容都是公开阅读的。每次加载页面时，您的浏览器都会从网络服务器下载内容并将其可视化呈现给您。因此，在某种意义上，您和您的浏览器已经在网上查看任何内容。由于网络的性质，因为有人在网上公开发布内容，他们本质上是在要求您获取这些信息，但通常只是为了特定目的。
- en: The big issue comes with creating automated tools that directly look for and
    make copies of *things* on the internet, with a *thing* being either data, images,
    videos, or music - essentially things that are created by others and represent
    something that has value to the creator, or owners. These items may create issues
    when explicitly making a copy of the item for your own personal use, and are much
    more likely to create issues when making a copy and using that copy for your or
    others' gain.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大问题在于创建直接寻找并复制互联网上的*事物*的自动化工具，*事物*可以是数据、图像、视频或音乐 - 基本上是由他人创建并代表对创建者或所有者有价值的东西。当明确复制项目供您个人使用时，这些项目可能会产生问题，并且在复制并将其用于您或他人的利益时，可能会更有可能产生问题。
- en: Videos, books, music, and images are some of the obvious items of concern over
    the legality of making copies either for personal or commercial use. In general,
    if you scrape content such as this from open sites, such as those that do not
    require authorized access or require payment for access to the content, then you
    are fine.  There are also *fair use* rules that allow the reuse of content in
    certain situations, such as small amounts of document sharing in a classroom scenario,
    where knowledge that is published for people to learn is shared and there is no
    real economic impact.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 视频、书籍、音乐和图像是一些明显引起关注的项目，涉及制作个人或商业用途的副本的合法性。一般来说，如果您从无需授权访问或需要付费访问内容的开放网站（如不需要授权访问或需要付费访问内容的网站）上抓取此类内容，那么您就没问题。还有*公平使用*规则允许在某些情况下重复使用内容，例如在课堂场景中共享少量文件，其中发布供人们学习的知识并没有真正的经济影响。
- en: Scraping of *data* from websites is often a much fuzzier problem.  By data I
    mean information that is provided as a service.  A good example, from my experience,
    is energy prices that are published to a provider's website.  These are often
    provided as a convenience to customers, but not for you to scrape freely and use
    the data for your own commercial analytics service. That data can often be used
    without concern if you are just collecting it for a non-public database or you
    are only using for your own use, then it is likely fine.  But if you use that
    database to drive your own website and share that content under your own name,
    then you might want to watch out.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从网站上抓取*数据*通常是一个更加模糊的问题。我的意思是作为服务提供的信息。从我的经验来看，一个很好的例子是能源价格，这些价格发布在供应商的网站上。这些通常是为了方便客户而提供的，而不是供您自由抓取并将数据用于自己的商业分析服务。如果您只是为了非公开数据库而收集数据，或者只是为了自己的使用而收集数据，那么可能没问题。但是，如果您使用该数据库来驱动自己的网站并以自己的名义分享该内容，那么您可能需要小心。
- en: The point is, check out the disclaimers / terms of service on the site for what
    you can do with that information. It should be documented, but if it is not, then
    that does not mean that you are in the clear to go crazy. Always be careful and
    use common sense, as you are taking other peoples content for you own purposes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 重点是，查看网站的免责声明/服务条款，了解您可以如何使用这些信息。这应该有记录，但如果没有，那并不意味着您可以肆意妄为。始终要小心并运用常识，因为您正在为自己的目的获取他人的内容。
- en: The other concern, which I lump into a concept known as denial of service, relates
    to the actual process of collecting information and how often you do it. The process
    of manually reading content on a site differs significantly to writing automated
    bots that relentlessly badger web servers for content. Taken to an extreme, this
    access frequency could be so significant that it denies other legitimate users
    access to the content, hence denying them service. It can also increase costs
    for the hosters of the content by increasing their cost for bandwidth, or even
    electrical costs for running the servers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关注点是我归为拒绝服务的概念，它涉及到收集信息的实际过程以及你收集信息的频率。在网站上手动阅读内容的过程与编写自动机器人不断骚扰网络服务器以获取内容的过程有很大的不同。如果访问频率过高，可能会拒绝其他合法用户访问内容，从而拒绝为他们提供服务。这也可能会增加内容的主机的成本，增加他们的带宽成本，甚至是运行服务器的电费。
- en: A well managed website will identify these types of repeated and frequent access
    and shut them down using tools such as web application firewalls with rules to
    block your access based on IP address, headers, and cookies.  In other cases,
    these may be identified and your ISP contacted to get you to stop doing these
    tasks. Remember, you are never truly anonymous, and smart hosters can figure out
    who you are, exactly what you accessed, and when you accessed it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好管理的网站将识别这些重复和频繁的访问，并使用诸如基于IP地址、标头和cookie的规则的Web应用程序防火墙关闭它们。在其他情况下，这些可能会被识别，并联系您的ISP要求您停止执行这些任务。请记住，您永远不是真正匿名的，聪明的主机可以找出您是谁，确切地知道您访问了什么内容以及何时访问。
- en: How to do it
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'So how do you go about being a good scraper?  There are several factors to
    this that we will cover in this chapter:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何成为一个好的爬虫呢？在本章中，我们将涵盖几个因素：
- en: You can start with respecting the `robots.txt` file
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以从尊重`robots.txt`文件开始
- en: Don't crawl every link you find on a site, just those given in a site map
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要爬取您在网站上找到的每个链接，只爬取站点地图中给出的链接。
- en: 'Throttle your requests, so as do as Han Solo said to Chewbacca: Fly Casual;
    or, don''t look like you are repeatedly taking content by Crawling Casual'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制您的请求，就像汉·索洛对丘巴卡说的那样：放轻松；或者，不要看起来像您在重复爬取内容
- en: Identify yourself so that you are known to the site
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让自己被网站识别
- en: Respecting robots.txt
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尊重robots.txt
- en: 'Many sites want to be crawled. It is inherent in the nature of the beast: Web
    hosters put content on their sites to be seen by humans. But it is also important
    that other computers see the content. A great example is search engine optimization
    (SEO). SEO is a process where you actually design your site to be crawled by spiders
    such as Google, so you are actually encouraging scraping. But at the same time,
    a publisher may only want specific parts of their site crawled, and to tell crawlers
    to keep their spiders off of certain portions of the site, either it is not for
    sharing, or not important enough to be crawled and wast the web server resources.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多网站希望被爬取。这是兽性的本质：Web主机将内容放在其网站上供人类查看。但同样重要的是其他计算机也能看到内容。一个很好的例子是搜索引擎优化（SEO）。SEO是一个过程，您实际上设计您的网站以便被Google等搜索引擎的爬虫爬取，因此您实际上是在鼓励爬取。但与此同时，发布者可能只希望网站的特定部分被爬取，并告诉爬虫不要爬取网站的某些部分，要么是因为不适合分享，要么是因为不重要而浪费了Web服务器资源。
- en: The rules of what you are and are not allowed to crawl are usually contained
    in a file that is on most sites known as `robots.txt`. The `robots.txt` is a human
    readable but parsable file, which can be used to identify the places you are allowed,
    and not allowed, to scrape.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您被允许和不被允许爬取的规则包含在大多数网站上的一个名为`robots.txt`的文件中。`robots.txt`是一个可读但可解析的文件，可用于识别您被允许和不被允许爬取的位置。
- en: 'The format of the `robots.txt` file is unfortunately not standard and anyone
    can make their own modifications, but there is very strong consensus on the format.
    A `robots.txt` file is normally found at the root URL of the site. To demonstrate
    a`robots.txt` file, the following code contains excerpts of the one provided by
    Amazon at [http://amazon.com/robots.txt](http://amazon.com/robots.txt).  I''ve
    edited it down to just show the important concepts:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`robots.txt`文件的格式不幸地不是标准的，任何人都可以进行自己的修改，但是对于格式有很强的共识。`robots.txt`文件通常位于站点的根URL。为了演示`robots.txt`文件，以下代码包含了亚马逊在[http://amazon.com/robots.txt](http://amazon.com/robots.txt)上提供的摘录。我编辑了它，只显示了重要的概念：'
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It can be seen that there are three main elements in the file:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到文件中有三个主要元素：
- en: A user agent declaration for which the following lines, until the end of file
    or next user agent statement, are to be applied
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户代理声明，以下行直到文件结束或下一个用户代理声明将被应用
- en: A set of URLs that are allowed to be crawled
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许爬取的一组URL
- en: A set of URLs are prohibited from being crawled
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁止爬取的一组URL
- en: The syntax is actually quite simple, and Python libraries exist to help us implement
    the rules contained within `robots.txt`. We will be using the `reppy` library
    to facilitate honoring `robots.txt`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 语法实际上非常简单，Python库存在以帮助我们实现`robots.txt`中包含的规则。我们将使用`reppy`库来尊重`robots.txt`。
- en: Getting ready
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Let's examine how to demonstrate using `robots.txt` with the reppy library. For
    more information on reppy, see its GitHub page at [https://github.com/seomoz/reppy](https://github.com/seomoz/reppy).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用`reppy`库来演示`robots.txt`。有关`reppy`的更多信息，请参阅其GitHub页面[https://github.com/seomoz/reppy](https://github.com/seomoz/reppy)。
- en: '`reppy` can be installed like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 可以这样安装`reppy`：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'However, I found that on my Mac I got an error during installation, and it
    required the following command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我发现在我的Mac上安装时出现了错误，需要以下命令：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: General information/searching on Google for a `robots.txt` Python parsing library
    will generally guide you toward using the robotparser library. This library is
    available for Python 2.x.  For Python 3, it has been moved into the `urllib` library. 
    However, I have found that this library reports incorrect values in specific scenarios.
    I'll point that out in our example.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google上搜索`robots.txt` Python解析库的一般信息通常会引导您使用robotparser库。此库适用于Python 2.x。对于Python
    3，它已移至`urllib`库。但是，我发现该库在特定情况下报告不正确的值。我将在我们的示例中指出这一点。
- en: How to do it
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'To run the recipe, execute the code in  `05/01_sitemap.py`.  The script will
    examine whether several URLs are allowed to be crawled on amazon.com.  When running
    it, you will see the following output:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行该示例，请执行`05/01_sitemap.py`中的代码。脚本将检查amazon.com上是否允许爬取多个URL。运行时，您将看到以下输出：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'The script begins by importing `reppy.robots`:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 脚本首先通过导入`reppy.robots`开始：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The code then uses `Robots` to fetch the `robots.txt` for amazon.com.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，代码使用`Robots`来获取amazon.com的`robots.txt`。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using the content that was fetched, the script checks several URLs for accessibility:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用获取的内容，脚本检查了几个URL的可访问性：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results of this code is the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的结果如下：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The call to `robots.allowed` is given the URL and the user agent. It returns
    `True` or `False` based upon whether the URL is allowed to be crawled. In this
    case, the results where True, False, True and False for the specified URLs. Let's
    examine how.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对`robots.allowed`的调用给出了URL和用户代理。它根据URL是否允许爬取返回`True`或`False`。在这种情况下，指定的URL的结果为True、False、True和False。让我们看看如何。
- en: 'The / URL has no entry in `robots.txt`, so it is allowed by default.  But in
    the file under the * user agent group are the following two lines:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: / URL在`robots.txt`中没有条目，因此默认情况下是允许的。但是，在*用户代理组下的文件中有以下两行：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '/gp/dmusic is not allowed, so False is returned. /gp/dmusic/promotions/PrimeMusic
    is explicitly allowed. If the Allowed: entry was not specified, then the Disallow:
    /gp/dmusic/ line would also disallow any further paths down from /gp/dmusic/. 
    This essentially says that any URLs starting with /gp/dmusic/ are disallowed,
    except that you are allowed to crawl /gp/dmusic/promotions/PrimeMusic.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不允许/gp/dmusic，因此返回False。/gp/dmusic/promotions/PrimeMusic是明确允许的。如果未指定Allowed:条目，则Disallow:/gp/dmusic/行也将禁止从/gp/dmusic/进一步的任何路径。这基本上表示以/gp/dmusic/开头的任何URL都是不允许的，除了允许爬取/gp/dmusic/promotions/PrimeMusic。
- en: Here is where there is a difference when using the `robotparser` library. `robotparser`
    reports that `/gp/dmusic/promotions/PrimeMusic` is disallowed. The library does
    not handle this type of scenario correctly, as it stops scanning `robots.txt`
    at the first match, and does not continue further into the file to look for any
    overrides of this kind.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`robotparser`库时存在差异。`robotparser`报告`/gp/dmusic/promotions/PrimeMusic`是不允许的。该库未正确处理此类情况，因为它在第一次匹配时停止扫描`robots.txt`，并且不会进一步查找文件以寻找此类覆盖。
- en: There's more...
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: First, for detailed information on `robots.txt`, see [https://developers.google.com/search/reference/robots_txt](https://developers.google.com/search/reference/robots_txt).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有关`robots.txt`的详细信息，请参阅[https://developers.google.com/search/reference/robots_txt](https://developers.google.com/search/reference/robots_txt)。
- en: Note that not all sites have a `robots.txt`, and its absence does not imply
    you have free rights to crawl all the content.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，并非所有站点都有`robots.txt`，其缺失并不意味着您有权自由爬取所有内容。
- en: Also, a `robots.txt` file may contain information on where to find the sitemap(s)
    for the website. We examine these sitemaps in the next recipe.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`robots.txt`文件可能包含有关在网站上查找站点地图的信息。我们将在下一个示例中检查这些站点地图。
- en: Scrapy can also read `robots.txt` and find sitemaps for you.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy还可以读取`robots.txt`并为您找到站点地图。
- en: Crawling using the sitemap
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用站点地图进行爬行
- en: A sitemap is a protocol that allows a webmaster to inform search engines about
    URLs on a website that are available for crawling.  A webmaster would want to
    use this as they actually want their information to be crawled by a search engine.
    The webmaster wants to make that content available for you to find, at least through
    search engines. But you can also use this information to your advantage.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 站点地图是一种允许网站管理员通知搜索引擎有关可用于爬取的网站上的URL的协议。网站管理员希望使用此功能，因为他们实际上希望他们的信息被搜索引擎爬取。网站管理员希望使该内容可通过搜索引擎找到，至少通过搜索引擎。但您也可以利用这些信息。
- en: 'A sitemap lists the URLs on a site, and allows a webmasters to specify additional
    information about each URL:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 站点地图列出了站点上的URL，并允许网站管理员指定有关每个URL的其他信息：
- en: When it was last updated
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上次更新时间
- en: How often the content changes
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容更改的频率
- en: How important the URL is in relation to others
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: URL在与其他URL的关系中有多重要
- en: 'Sitemaps are useful on websites where:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 站点地图在以下情况下很有用：
- en: Some areas of the website are not available through the browsable interface;
    that is, you cannot reach those pages
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网站的某些区域无法通过可浏览的界面访问；也就是说，您无法访问这些页面
- en: Ajax, Silverlight, or Flash content is used but not normally processed by search
    engines
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ajax、Silverlight或Flash内容通常不会被搜索引擎处理
- en: The site is very large and there is a chance for the web crawlers to overlook
    some of the new or recently updated content
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网站非常庞大，网络爬虫有可能忽略一些新的或最近更新的内容
- en: When websites have a huge number of pages that are isolated or not well linked
    together
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当网站具有大量孤立或链接不良的页面时
- en: When a website has few external links
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当网站具有较少的外部链接时
- en: 'A sitemap file has the following structure:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 站点地图文件具有以下结构：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Each URL in the site will be represented with a `<url></url>` tag, with all
    those tags wrapped in an outer `<urlset></urlset>` tag. There will always a `<loc></loc>`
    tag specifying the URL. The other three tags are optional.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 站点中的每个URL都将用`<url></url>`标签表示，所有这些标签都包裹在外部的`<urlset></urlset>`标签中。始终会有一个指定URL的`<loc></loc>`标签。其他三个标签是可选的。
- en: 'Sitemaps files can be incredibly large, so they are often broken into multiple
    files and then referenced by a single sitemap index file. This file has the following
    format:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 网站地图文件可能非常庞大，因此它们经常被分成多个文件，然后由单个网站地图索引文件引用。该文件的格式如下：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In most cases, the `sitemap.xml` file is found at the root of the domain.  As
    an example, for nasa.gov it is[ https://www.nasa.gov/sitemap.xml](https://www.nasa.gov/sitemap.xml). 
    But note that this is not a standard, and different sites may have the map, or
    maps, at different locations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，`sitemap.xml` 文件位于域的根目录下。例如，对于 nasa.gov，它是[https://www.nasa.gov/sitemap.xml](https://www.nasa.gov/sitemap.xml)。但请注意，这不是一个标准，不同的网站可能在不同的位置拥有地图或地图。
- en: 'A sitemap for a particular website may also be located within the site''s `robots.txt`
    file. As an example, the `robots.txt` file for microsoft.com ends with the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 特定网站的网站地图也可能位于该网站的 `robots.txt` 文件中。例如，microsoft.com 的 `robots.txt` 文件以以下内容结尾：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Therefore, to get microsoft.com's sitemaps, we would first need to read the
    `robots.txt` file and extract that information.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要获取 microsoft.com 的网站地图，我们首先需要读取 `robots.txt` 文件并提取该信息。
- en: Let's now look at parsing a sitemap.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看如何解析网站地图。
- en: Getting ready
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Everything you need is in the `05/02_sitemap.py` script, along with the `sitemap.py`
    file in then same folder. The `sitemap.py` file implements a basic sitemap parser
    that we will use in the main script. For the purposes of this example, we will
    get the sitemap data for nasa.gov.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你所需要的一切都在 `05/02_sitemap.py` 脚本中，以及与其在同一文件夹中的 `sitemap.py` 文件。`sitemap.py` 文件实现了一个基本的网站地图解析器，我们将在主脚本中使用它。在这个例子中，我们将获取
    nasa.gov 的网站地图数据。
- en: How to do it
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'First execute the `05/02_sitemap.py` file. Make sure that the associated `sitemap.py`
    file is in the same directory or your path. When running, after a few seconds
    you will get output similar to the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先执行 `05/02_sitemap.py` 文件。确保相关的 `sitemap.py` 文件与其在同一目录或路径下。运行后，几秒钟后，你将会得到类似以下的输出：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The program found 35,511 URLs throughout all of the nasa.gov sitemaps! The code
    only printed the first 10 as this would have been quite a bit of output. Using
    this info to initialize a crawl of all of these URLs will definitely take quite
    a long time!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 程序在所有 nasa.gov 的网站地图中找到了 35,511 个 URL！代码只打印了前 10 个，因为输出量会相当大。使用这些信息来初始化对所有这些
    URL 的爬取肯定需要相当长的时间！
- en: But this is also the beauty of the sitemap. Many, if not all, of these results
    have a `lastmod` tag that tells you when the content at the end of that associated
    URL was last modified. If you are implementing a polite crawler of nasa.gov, you
    would want to keep these URLs and their timestamp in a database, and then before
    crawling that URL check to see if the content has actually changed, and don't
    crawl if it hasn't.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 但这也是网站地图的美妙之处。许多，如果不是所有的结果都有一个 `lastmod` 标签，告诉你与该关联 URL 末端的内容上次修改的时间。如果你正在实现一个有礼貌的爬虫来爬取
    nasa.gov，你会想把这些 URL 及其时间戳保存在数据库中，然后在爬取该 URL 之前检查内容是否实际上已经改变，如果没有改变就不要爬取。
- en: Now let's see how this actually worked.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这实际是如何工作的。
- en: How it works
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: 'The recipe works as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的工作如下：
- en: 'The script starts by calling `get_sitemap()`:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 脚本开始调用 `get_sitemap()`：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This is given a URL to the sitemap.xml file (or any other file - non-gzipped).
    The implementation simply gets the content at the URL and returns it:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个指向 sitemap.xml 文件（或任何其他文件 - 非压缩）的 URL。该实现简单地获取 URL 处的内容并返回它：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The bulk of the work is done by passing that content to `parse_sitemap()`.
    In the case of nasa.gov, this sitemap contains the following content, a sitemap
    index file:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大部分工作是通过将该内容传递给 `parse_sitemap()` 来完成的。在 nasa.gov 的情况下，这个网站地图包含以下内容，即网站地图索引文件：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`process_sitemap()` starts with a call to `process_sitemap()`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`process_sitemap()` 从调用 `process_sitemap()` 开始：'
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This function starts by calling `process_sitemap()`, which returns a list of
    Python dictionary objects with `loc`, `lastmod`, `changeFreq`, and priority key
    value pairs:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个函数开始调用 `process_sitemap()`，它返回一个包含 `loc`、`lastmod`、`changeFreq` 和 priority
    键值对的 Python 字典对象列表：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is performed by parsing the sitemap using `BeautifulSoup` and `lxml`. The  `loc`
    property is always `set`, and `lastmod`, `changeFreq` and priority are set if
    there is an an associated XML tag. The .tag property itself just notes whether
    this content was retrieved from a `<sitemap>` tag or a `<url>` tag (`<loc>` tags
    can be on either).
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是通过使用 `BeautifulSoup` 和 `lxml` 解析网站地图来执行的。`loc` 属性始终被设置，如果有相关的 XML 标签，则会设置
    `lastmod`、`changeFreq` 和 priority。.tag 属性本身只是指出这个内容是从 `<sitemap>` 标签还是 `<url>`
    标签中检索出来的（`<loc>` 标签可以在任何一个标签上）。
- en: '`parse_sitemap()` then continues with processing those results one by one:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse_sitemap()` 然后继续逐一处理这些结果：'
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Each item is examined. If it is from a sitemap index file (the URL ends in .xml
    and the .tag is the sitemap), then we need to read that .xml file and parse its
    content, whose results are placed into our list of items to process. In this example,
    four sitemap files are identified, and each of these are read, processed, parsed,
    and their URLs added to the result.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查每个项目。如果它来自网站地图索引文件（URL 以 .xml 结尾且 .tag 是网站地图），那么我们需要读取该 .xml 文件并解析其内容，然后将结果放入我们要处理的项目列表中。在这个例子中，识别出了四个网站地图文件，每个文件都被读取、处理、解析，并且它们的
    URL 被添加到结果中。
- en: 'To demonstrate some of this content, the following are the first few lines
    of sitemap-1.xml:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示一些内容，以下是 sitemap-1.xml 的前几行：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Overall, this one sitemap has 11,006 lines, so roughly 11,000 URLs!  And in
    total, as was reported, there are 35,511 URLs  across all three sitemaps.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这一个网站地图有 11,006 行，所以大约有 11,000 个 URL！而且总共，正如报道的那样，所有三个网站地图中共有 35,511 个
    URL。
- en: There's more...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Sitemap files may also be zipped, and end in a .gz extension. This is because
    it likely contains many URLs and the compression will save a lot of space. While
    the code we used does not process gzip sitemap files, it is easy to add this using
    functions in the gzip library.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 网站地图文件也可能是经过压缩的，并以 .gz 扩展名结尾。这是因为它可能包含许多 URL，压缩将节省大量空间。虽然我们使用的代码不处理 gzip 网站地图文件，但可以使用
    gzip 库中的函数轻松添加这个功能。
- en: 'Scrapy also provides a facility for starting crawls using the sitemap. One
    of these is a specialization of the Spider class, SitemapSpider. This class has
    the smarts to parse the sitemap for you, and then start following the URLs. To
    demonstrate, the script `05/03_sitemap_scrapy.py` will start the crawl at the
    nasa.gov top-level sitemap index:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy还提供了使用网站地图开始爬取的功能。其中之一是Spider类的一个特化，SitemapSpider。这个类有智能来解析网站地图，然后开始跟踪URL。为了演示，脚本`05/03_sitemap_scrapy.py`将从nasa.gov的顶级网站地图索引开始爬取：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When running this, there will be a ton of output, as the spider is going to
    start crawling all 30000+ URLs.  Early in the output, you will see output such
    as the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时，会有大量输出，因为爬虫将开始爬取所有30000多个URL。在输出的早期，您将看到以下输出：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Scrapy has found all of the sitemaps and read in their content. Soon afterwards,
    you will start to see a number of redirections and notifications that certain
    pages are being parsed:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy已经找到了所有的网站地图并读取了它们的内容。不久之后，您将开始看到许多重定向和通知，指出正在解析某些页面：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Crawling with delays
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带延迟的爬取
- en: Fast scraping is considered a bad practice. Continuously pounding a website
    for pages can burn up CPU and bandwidth, and a robust site will identify you doing
    this and block your IP. And if you are unlucky, you might get a nasty letter for
    violating terms of service!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 快速抓取被认为是一种不良实践。持续不断地访问网站页面可能会消耗CPU和带宽，而且强大的网站会识别到您这样做并阻止您的IP。如果您运气不好，可能会因违反服务条款而收到一封恶意的信！
- en: The technique of delaying requests in your crawler depends upon how your crawler
    is implemented. If you are using Scrapy, then you can set a parameter that informs
    the crawler how long to wait between requests. In a simple crawler just sequentially
    processing URLs in a list, you can insert a thread.sleep statement.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬虫中延迟请求的技术取决于您的爬虫是如何实现的。如果您使用Scrapy，那么您可以设置一个参数，告诉爬虫在请求之间等待多长时间。在一个简单的爬虫中，只需按顺序处理URL的列表，您可以插入一个thread.sleep语句。
- en: Things can get more complicated if you have implemented a distributed cluster
    of crawlers that spread the load of page requests, such as using a message queue
    with competing consumers. That can have a number of different solutions, which
    are beyond the scope provided in this context.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您实施了一个分布式爬虫集群，以分散页面请求的负载，比如使用具有竞争消费者的消息队列，情况可能会变得更加复杂。这可能有许多不同的解决方案，这超出了本文档提供的范围。
- en: Getting ready
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will examine using Scrapy with delays. The sample is in `o5/04_scrape_with_delay.py`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用带延迟的Scrapy。示例在`o5/04_scrape_with_delay.py`中。
- en: How to do it
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: Scrapy by default imposes a delay of 0 seconds between page requests. That is,
    it does not wait between requests by default.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy默认在页面请求之间强加了0秒的延迟。也就是说，默认情况下不会在请求之间等待。
- en: 'This can be controlled using the `DOWNLOAD_DELAY` setting.  To demonstrate,
    let''s run the script from the command line:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这可以使用`DOWNLOAD_DELAY`设置来控制。为了演示，让我们从命令行运行脚本：
- en: '[PRE24]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This crawls all of the pages at blog.scrapinghub.com, and reports the total
    time to perform the crawl. `LOG_LEVEL=WARNING` removes most logging output and
    just gives out the output from print statements. This used the default wait between
    pages of 0 and resulted in a crawl roughly seven seconds in length.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这将爬取blog.scrapinghub.com上的所有页面，并报告执行爬取的总时间。`LOG_LEVEL=WARNING`会删除大部分日志输出，只会输出打印语句的输出。这使用了默认的页面等待时间为0，爬取大约需要七秒钟。
- en: 'The wait between pages can be set using the `DOWNLOAD_DELAY` setting. The following
    delays for five seconds between page requests:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 页面之间的等待时间可以使用`DOWNLOAD_DELAY`设置。以下在页面请求之间延迟五秒：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: By default, this does not actually wait 5 seconds. It will wait `DOWNLOAD_DELAY`
    seconds, but by a random factor between 0.5 and 1.5 times `DOWNLOAD_DELAY`. Why
    do this? This makes your crawler look "less robotic." You can turn this off by
    using the `RANDOMIZED_DOWNLOAD_DELAY=False` setting.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，这实际上并不会等待5秒。它将等待`DOWNLOAD_DELAY`秒，但是在`DOWNLOAD_DELAY`的0.5到1.5倍之间有一个随机因素。为什么这样做？这会让您的爬虫看起来“不那么机械化”。您可以通过使用`RANDOMIZED_DOWNLOAD_DELAY=False`设置来关闭这个功能。
- en: How it works
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'This crawler is implemented as a Scrapy spider. The class definition begins
    with declaring the spider name and the start URL:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个爬虫是作为一个Scrapy爬虫实现的。类定义从声明爬虫名称和起始URL开始：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The parse method looks for CSS 'div.prev-post > a', and follows those links.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 解析方法查找CSS 'div.prev-post > a'，并跟踪这些链接。
- en: 'The scraper also defines a close method, which is called by Scrapy when the
    crawl is complete:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫还定义了一个close方法，当爬取完成时，Scrapy会调用这个方法：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This accesses the spiders crawler stats object, retrieves the start and finish
    time for the spider, and reports the difference to the user.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这访问了爬虫的统计对象，检索了爬虫的开始和结束时间，并向用户报告了差异。
- en: There's more...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The script also defines code for when executing the script directly with Python:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本还定义了在直接使用Python执行脚本时的代码：
- en: '[PRE28]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This begins by creating a CrawlerProcess object. This object can be passed a
    dictionary representing the settings and values to configure the crawl with. This
    defaults to a five-second delay, without randomization, and an output level of
    DEBUG.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过创建一个CrawlerProcess对象开始的。这个对象可以传递一个表示设置和值的字典，以配置爬取。这默认为五秒的延迟，没有随机化，并且输出级别为DEBUG。
- en: Using identifiable user agents
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用可识别的用户代理
- en: What happens if you violate the terms of service and get flagged by the website
    owner? How can you help the site owners in contacting you, so that they can nicely
    ask you to back off to what they consider a reasonable level of scraping?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您违反了服务条款并被网站所有者标记了怎么办？您如何帮助网站所有者联系您，以便他们可以友好地要求您停止对他们认为合理的抓取级别？
- en: 'What you can do to facilitate this is add info about yourself in the User-Agent
    header of the requests. We have seen an example of this in `robots.txt` files,
    such as from amazon.com. In their `robots.txt` is an explicit statement of a user
    agent for Google: GoogleBot.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便这一点，您可以在请求的User-Agent标头中添加有关自己的信息。我们已经在`robots.txt`文件中看到了这样的例子，比如来自amazon.com。在他们的`robots.txt`中明确声明了一个用于Google的用户代理：GoogleBot。
- en: During scraping, you can embed your own information within the User-Agent header
    of the HTTP requests. To be polite, you can enter something such as 'MyCompany-MyCrawler
    (mybot@mycompany.com)'. The remote server, if tagging you in violation, will definitely
    be capturing this information, and if provided like this, it gives them a convenient
    means of contacting your instead of just shutting you down.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬取过程中，您可以在HTTP请求的User-Agent标头中嵌入自己的信息。为了礼貌起见，您可以输入诸如'MyCompany-MyCrawler（mybot@mycompany.com）'之类的内容。如果远程服务器标记您违规，肯定会捕获这些信息，如果提供了这样的信息，他们可以方便地与您联系，而不仅仅是关闭您的访问。
- en: How to do it
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'Setting the user agent differs depending upon what tools you use. Ultimately,
    it is just ensuring that the User-Agent header is set to a string that you specify.
    When using a browser, this is normally set by the browser to identity the browser
    and the operating system. But you can put anything you want into this header.
    When using requests, it is very straightforward:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您使用的工具，设置用户代理会有所不同。最终，它只是确保User-Agent标头设置为您指定的字符串。在使用浏览器时，这通常由浏览器设置为标识浏览器和操作系统。但您可以在此标头中放入任何您想要的内容。在使用请求时，这非常简单：
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'When using Scrapy, it is as simple as configuring a setting:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Scrapy时，只需配置一个设置即可：
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: Outgoing HTTP requests have a number of different headers. These ensure that
    the User-Agent header is set to this value for all requests made of the target
    web server.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 传出的HTTP请求有许多不同的标头。这些确保User-Agent标头对目标网站的所有请求都设置为此值。
- en: There's more...
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While it is possible to set any content you want in the User-Agent header, some
    web servers will inspect the User-Agent header and make decisions on how to respond
    based upon the content. A common example of this is using the header to identify
    mobile devices to provide a mobile presentation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能将任何内容设置为User-Agent标头，但有些Web服务器会检查User-Agent标头并根据内容做出响应。一个常见的例子是使用标头来识别移动设备以提供移动展示。
- en: But some sites also only allow access to content to specific User-Agent values.
    Setting your own value could have the effect of having the web server not respond
    or return other errors, such as unauthorized. So when you use this technique,
    make sure to check it will work.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 但有些网站只允许特定User-Agent值访问内容。设置自己的值可能导致Web服务器不响应或返回其他错误，比如未经授权。因此，在使用此技术时，请确保检查它是否有效。
- en: Setting the number of concurrent requests per domain
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置每个域的并发请求数量
- en: It is generally inefficient to crawl a site one URL at a time. Therefore, there
    is normally a number of simultaneous page requests made to the target site at
    any given time. Normally, the remote web server can quite effectively handle multiple
    simultaneous requests, and on your end you are just waiting for data to come back
    in for each, so concurrency generally works well for your scraper.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一次只爬取一个网址通常效率低下。因此，通常会同时向目标站点发出多个页面请求。通常情况下，远程Web服务器可以相当有效地处理多个同时的请求，而在您的端口，您只是在等待每个请求返回数据，因此并发通常对您的爬虫工作效果很好。
- en: But this is also a pattern that smart websites can identify and flag as suspicious
    activity. And there are practical limits on both your crawler's end and the website.
    The more concurrent requests that are made, the more memory, CPU, network connections,
    and network bandwidth is required on both sides. These have costs involved, and
    there are practical limits on these values too.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 但这也是聪明的网站可以识别并标记为可疑活动的模式。而且，您的爬虫端和网站端都有实际限制。发出的并发请求越多，双方都需要更多的内存、CPU、网络连接和网络带宽。这都涉及成本，并且这些值也有实际限制。
- en: So it is generally a good practice to set a limit on the number of requests
    that you will simultaneously make to any web server.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常最好的做法是设置您将同时向任何Web服务器发出的请求的数量限制。
- en: How it works
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: There are number of techniques that can be used to control concurrency levels,
    and the process can often be quite complicated with controlling multiple requests
    and threads of execution. We won't discuss here how this is done at the thread
    level and only mention the construct built into Scrapy.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多技术可以用来控制并发级别，这个过程通常会相当复杂，需要控制多个请求和执行线程。我们不会在这里讨论如何在线程级别进行操作，只提到了内置在Scrapy中的构造。
- en: 'Scrapy is inherently concurrent in its requests.  By default, Scrapy will dispatch
    at most eight simultaneous requests to any given domain. You can change this using
    the `CONCURRENT_REQUESTS_PER_DOMAIN` setting. The following sets the value to
    1 concurrent request:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy在其请求中天生是并发的。默认情况下，Scrapy将最多同时向任何给定域发送八个请求。您可以使用`CONCURRENT_REQUESTS_PER_DOMAIN`设置来更改这一点。以下将该值设置为1个并发请求：
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using auto throttling
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自动节流
- en: Fairly closely tied to controlling the maximum level of concurrency is the concept
    of throttling. Websites vary in their ability to handle requests, both across
    multiple websites and on a single website at different times. During periods of
    slower response times, it makes sense to lighten up of the number of requests
    during that time. This can be a tedious process to monitor and adjust by hand.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与控制最大并发级别紧密相关的是节流的概念。不同的网站在不同时间对请求的处理能力不同。在响应时间较慢的时期，减少请求的数量是有意义的。这可能是一个繁琐的过程，需要手动监控和调整。
- en: Fortunately for us, scrapy also provides an ability to do this via an extension
    named `AutoThrottle`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于我们来说，scrapy还提供了通过名为`AutoThrottle`的扩展来实现这一点的能力。
- en: How to do it
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'AutoThrottle can easily be configured using the `AUTOTHROTTLE_TARGET_CONCURRENCY`
    setting:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`AUTOTHROTTLE_TARGET_CONCURRENCY`设置轻松配置AutoThrottle。
- en: '[PRE32]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: How it works
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: scrapy tracks the latency on each request. Using that information, it can adjust
    the delay between requests to a specific domain so that there are no more than
    `AUTOTHROTTLE_TARGET_CONCURRENCY` requests simultaneously active for that domain,
    and that the requests are evenly distributed in any given time span.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: scrapy跟踪每个请求的延迟。利用这些信息，它可以调整请求之间的延迟，以便在特定域上同时活动的请求不超过`AUTOTHROTTLE_TARGET_CONCURRENCY`，并且请求在任何给定的时间跨度内均匀分布。
- en: There's more...
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There are lot of options for controlling throttling. You can get an overview
    of them at [https://doc.scrapy.org/en/latest/topics/autothrottle.html?&_ga=2.54316072.1404351387.1507758575-507079265.1505263737#settings.](https://doc.scrapy.org/en/latest/topics/autothrottle.html?&_ga=2.54316072.1404351387.1507758575-507079265.1505263737#settings)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多控制节流的选项。您可以在以下网址上获得它们的概述：[https://doc.scrapy.org/en/latest/topics/autothrottle.html?&_ga=2.54316072.1404351387.1507758575-507079265.1505263737#settings.](https://doc.scrapy.org/en/latest/topics/autothrottle.html?&_ga=2.54316072.1404351387.1507758575-507079265.1505263737#settings)
- en: Using an HTTP cache for development
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用HTTP缓存进行开发
- en: The development of a web crawler is a process of exploration, and one that will
    iterate through various refinements to retrieve the requested information. During
    the development process, you will often be hitting remote servers, and the same
    URLs on those servers, over and over. This is not polite. Fortunately, scrapy
    also comes to the rescue by providing caching middleware that is specifically designed
    to help in this situation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫的开发是一个探索过程，将通过各种细化来迭代检索所需的信息。在开发过程中，您经常会反复访问远程服务器和这些服务器上的相同URL。这是不礼貌的。幸运的是，scrapy也通过提供专门设计用于帮助解决这种情况的缓存中间件来拯救您。
- en: How to do it
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'Scrapy will cache requests using a middleware module named HttpCacheMiddleware.
    Enabling it is as simple as configuring the `HTTPCACHE_ENABLED` setting to `True`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy将使用名为HttpCacheMiddleware的中间件模块缓存请求。启用它就像将`HTTPCACHE_ENABLED`设置为`True`一样简单：
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: The implementation of HTTP caching is simple, yet complex at the same time.
    The `HttpCacheMiddleware` provided by Scrapy has a plethora of configuration options
    based upon your needs. Ultimately, it comes down to storing each URL and its content
    in a store along with an associated duration for cache expiration. If a second
    request is made for a URL within the expiration interval, then the local copy
    will be retrieved instead of making a remote request. If the time has expired,
    then the contents are fetched from the web server, stored in the cache, and a
    new expiration time set.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP缓存的实现既简单又复杂。Scrapy提供的`HttpCacheMiddleware`根据您的需求有大量的配置选项。最终，它归结为将每个URL及其内容存储在存储器中，并附带缓存过期的持续时间。如果在过期间隔内对URL进行第二次请求，则将检索本地副本，而不是进行远程请求。如果时间已经过期，则从Web服务器获取内容，存储在缓存中，并设置新的过期时间。
- en: There's more...
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There are many options for configuration scrapy caching, including means of
    storing content (file system, DBM, or LevelDB), cache policies, and how Http Cache-Control
    directives from the server are handled. To explore these options, check out the
    following URL: [https://doc.scrapy.org/en/latest/topics/downloader-middleware.html?_ga=2.50242598.1404351387.1507758575-507079265.1505263737#dummy-policy-default.](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html?_ga=2.50242598.1404351387.1507758575-507079265.1505263737#dummy-policy-default.)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多配置scrapy缓存的选项，包括存储内容的方式（文件系统、DBM或LevelDB）、缓存策略以及如何处理来自服务器的Http缓存控制指令。要探索这些选项，请查看以下网址：[https://doc.scrapy.org/en/latest/topics/downloader-middleware.html?_ga=2.50242598.1404351387.1507758575-507079265.1505263737#dummy-policy-default.](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html?_ga=2.50242598.1404351387.1507758575-507079265.1505263737#dummy-policy-default.)
