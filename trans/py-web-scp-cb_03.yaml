- en: Processing Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Working with CSV and JSON data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CSV和JSON数据
- en: Storing data using AWS S3
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS S3存储数据
- en: Storing data using MySQL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MySQL存储数据
- en: Storing data using PostgreSQL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PostgreSQL存储数据
- en: Storing store data using Elasticsearch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Elasticsearch存储数据
- en: How to build robust ETL pipelines with AWS SQS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用AWS SQS构建健壮的ETL管道
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we will introduce the use of data in JSON, CSV, and XML formats.
    This will include the means of parsing and converting this data to other formats,
    including storing that data in relational databases, search engines such as Elasticsearch,
    and cloud storage including AWS S3\. We will also discuss the creation of distributed
    and large-scale scraping tasks through the use of messaging systems including
    AWS Simple Queue Service (SQS).  The goal is to provide both an understanding
    of the various forms of data you may retrieve and need to parse, and an instruction
    the the various backends where you can store the data you have scraped.  Finally,
    we get a first introduction to one and Amazon Web Service (AWS) offerings.  By
    the end of the book we will be getting quite heavy into AWS and this gives a gentle
    introduction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍JSON、CSV和XML格式的数据使用。这将包括解析和将这些数据转换为其他格式的方法，包括将数据存储在关系数据库、Elasticsearch等搜索引擎以及包括AWS
    S3在内的云存储中。我们还将讨论通过使用AWS Simple Queue Service（SQS）等消息系统创建分布式和大规模的抓取任务。目标是既了解您可能检索和需要解析的各种数据形式，又了解可以存储您已抓取的数据的各种后端。最后，我们首次介绍了Amazon
    Web Service（AWS）的一项服务。在本书结束时，我们将深入研究AWS，并进行初步介绍。
- en: Working with CSV and JSON data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CSV和JSON数据
- en: Extracting data from HTML pages is done using the techniques in the previous
    chapter, primarily using XPath through various tools and also with Beautiful Soup.
    While we will focus primarily on HTML, HTML is a variant of XML (eXtensible Markup
    Language).  XML one was the most popular for  of expressing data on the web, but
    other have become popular, and even exceeded XML in popularity.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从HTML页面中提取数据是使用上一章节中的技术完成的，主要是使用XPath通过各种工具和Beautiful Soup。虽然我们主要关注HTML，但HTML是XML（可扩展标记语言）的一种变体。XML曾经是在Web上表达数据的最流行形式之一，但其他形式已经变得流行，甚至超过了XML。
- en: Two common formats that you will see are JSON (JavaScript Object Notation) and
    CSV (Comma Separated Values).  CSV is easy to create and a common form for many
    spreadsheet applications, so many web sites provide data in that for, or you will
    need to convert scraped data to that format for further storage or collaboration.
    JSON really has become the preferred format, due to its easy within programming
    languages such as JavaScript (and Python), and many database now support it as
    a native data format.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到的两种常见格式是JSON（JavaScript对象表示）和CSV（逗号分隔值）。CSV易于创建，是许多电子表格应用程序的常见形式，因此许多网站提供该格式的数据，或者您需要将抓取的数据转换为该格式以进行进一步存储或协作。由于JSON易于在JavaScript（和Python）等编程语言中使用，并且许多数据库现在支持它作为本机数据格式，因此JSON确实已成为首选格式。
- en: In this recipe let's examine converting scraped data to CSV and JSON, as well
    as writing the data to files and also reading those data files from remote servers.
    The tools we will examine are the Python CSV and JSON libraries. We will also
    examine using `pandas` for these techniques.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，让我们来看看将抓取的数据转换为CSV和JSON，以及将数据写入文件，以及从远程服务器读取这些数据文件。我们将研究Python CSV和JSON库。我们还将研究使用`pandas`进行这些技术。
- en: Also implicit in these examples is the conversion of XML data to CSV and JSON,
    so we won't have a dedicated section for those examples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例中还隐含了将XML数据转换为CSV和JSON的过程，因此我们不会为这些示例专门设置一个部分。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will be using the planets data page and converting that data into CSV and
    JSON files. Let''s start by loading the planets data from the page into a list
    of python dictionary objects. The following code (found in (`03/get_planet_data.py`)
    provides a function that performs this task, which will be reused throughout the
    chapter:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用行星数据页面，并将该数据转换为CSV和JSON文件。让我们从将行星数据从页面加载到Python字典对象列表中开始。以下代码（在（`03/get_planet_data.py`）中找到）提供了执行此任务的函数，该函数将在整个章节中重复使用：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Running the script gives the following output (briefly truncated):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本会产生以下输出（简要截断）：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It may be required to install csv, json and pandas.  You can do that with the
    following three commands:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要安装csv、json和pandas。您可以使用以下三个命令来完成：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How to do it
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: We will start by converting the planets data into a CSV file.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将行星数据转换为CSV文件。
- en: 'This will be performed using `csv`.  The following code writes the planets
    data to a CSV file (the code is in`03/create_csv.py`):'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将使用`csv`执行。以下代码将行星数据写入CSV文件（代码在`03/create_csv.py`中）：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output file is put into the www folder of our project.  Examining it we
    see the following content::'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出文件放入我们项目的www文件夹中。检查它，我们看到以下内容：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We wrote this file into the www directory so that we can download it with our
    web server.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个文件写入www目录，以便我们可以通过我们的Web服务器下载它。
- en: 'This data can now be used in applications that support CSV content, such as
    Excel:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在可以在支持CSV内容的应用程序中使用这些数据，例如Excel：
- en: '![](assets/a00f3815-56b8-4bfb-bcd7-e9dbd035caa9.png)The File Opened in Excel'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/a00f3815-56b8-4bfb-bcd7-e9dbd035caa9.png)在Excel中打开的文件'
- en: 'CSV data can also be read from a web server using the `csv` library and by
    first retrieving the content with `requests` .  The following code is in the `03/read_csv_from_web.py`):'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还可以使用`csv`库从Web服务器读取CSV数据，并首先使用`requests`检索内容。以下代码在`03/read_csv_from_web.py`中：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The following is a portion of the output
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是部分输出
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'One thing to point our is that the CSV writer left a trailing blank like would
    add an empty list item if not handled. This was handled by slicing the rows: This
    following statement returned all lines except the last one:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有一点要指出的是，CSV写入器留下了一个尾随空白，如果不处理，就会添加一个空列表项。这是通过切片行来处理的：以下语句返回除最后一行之外的所有行：
- en: '`lines = [line for line in reader][:-1]`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`lines = [line for line in reader][:-1]`'
- en: 'This can also be done quite easily using pandas. The following constructs a
    DataFrame from the scraped data. The code is in `03/create_df_planets.py`:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这也可以很容易地使用pandas完成。以下从抓取的数据构造一个DataFrame。代码在`03/create_df_planets.py`中：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Running this gives the following output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令将产生以下输出：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And the `DataFrame` can be saved to a CSV file with a simple call to `.to_csv()`
    (code is in `03/save_csv_pandas.py`):'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame`也可以通过简单调用`.to_csv()`保存到CSV文件中（代码在`03/save_csv_pandas.py`中）：'
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A CSV file can be read in from a `URL` very easily with `pd.read_csv()` - no
    need for other libraries.  You can use the code in`03/read_csv_via_pandas.py`):'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`pd.read_csv()`非常轻松地从`URL`中读取CSV文件，无需其他库。您可以使用`03/read_csv_via_pandas.py`中的代码：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Converting data to JSON is also quite easy. Manipulation of JSON with Python
    can be done with the Python `json` library.  This library can be used to convert
    Python objects to and from JSON. The following converts the list of planets into
    JSON and prints it to the console:prints the planets data as JSON (code in `03/convert_to_json.py`):'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为JSON也非常容易。使用Python可以使用Python的`json`库对JSON进行操作。该库可用于将Python对象转换为JSON，也可以从JSON转换为Python对象。以下将行星列表转换为JSON并将其打印到控制台：将行星数据打印为JSON（代码在`03/convert_to_json.py`中）：
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Executing this script produces the following output (some of the output is
    omitted):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此脚本将产生以下输出（省略了部分输出）：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And this can also be used to easily save JSON to a file (`03/save_as_json.py`):'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这也可以用于轻松地将JSON保存到文件（`03/save_as_json.py`）：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Checking the output using `!head -n 13 ../../www/planets.json` shows:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`!head -n 13 ../../www/planets.json`检查输出，显示：
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'JSON can be read from a web server with `requests` and converted to a Python
    object (`03/read_http_json_requests.py`):'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`requests`从Web服务器读取JSON并将其转换为Python对象（`03/read_http_json_requests.py`）：
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'pandas also provides JSON capabilities to save to CSV (`03/save_json_pandas.py`):'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pandas还提供了将JSON保存为CSV的功能（`03/save_json_pandas.py`）：
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Unfortunately, there is not currently a way to pretty-print the JSON that is
    output from `.to_json()`. Also note the use of `orient='records'` and the use
    of `rest_index()`. This is necessary for reproducing an identical JSON structure
    to the JSON written using the JSON library example.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，目前还没有一种方法可以漂亮地打印从`.to_json()`输出的JSON。还要注意使用`orient='records'`和使用`rest_index()`。这对于复制与使用JSON库示例写入的相同JSON结构是必要的。
- en: 'JSON can be read into a DataFrame using `.read_json()`, as well as from HTTP
    and files (`03/read_json_http_pandas.py`):'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`.read_json()`将JSON读入DataFrame，也可以从HTTP和文件中读取（`03/read_json_http_pandas.py`）：
- en: '[PRE17]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How it works
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: The `csv` and `json` libraries are a standard part of Python, and provide a
    straightforward means of reading and writing data in both formats.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`csv`和`json`库是Python的标准部分，提供了一种简单的方法来读取和写入这两种格式的数据。'
- en: pandas does not come as standard in some Python distributions and you will likely
    need to install it. The pandas functions for both CSV and JSON are also a much
    higher level in operation, with many powerful data operations available, and also
    with support for accessing data from remote servers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些Python发行版中，pandas并不是标准配置，您可能需要安装它。pandas对CSV和JSON的功能也更高级，提供了许多强大的数据操作，还支持从远程服务器访问数据。
- en: There's more...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The choice of csv, json, or pandas libraries is yours to make but I tend to
    like pandas and we will examine its use in scraping more throughout the book,
    although we won't get too deep into its usage.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 选择csv、json或pandas库由您决定，但我倾向于喜欢pandas，并且我们将在整本书中更多地研究其在抓取中的使用，尽管我们不会深入研究其用法。
- en: For an in-depth understanding of pandas, check out `pandas.pydata.org`, or pick
    up my other book From Packt,  Learning pandas, 2ed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解pandas，请查看`pandas.pydata.org`，或者阅读我在Packt出版的另一本书《Learning pandas, 2ed》。
- en: For more info on the csv library, see [https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有关csv库的更多信息，请参阅[https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html)
- en: For more on the json library, see [https://docs.python.org/3/library/json.html](https://docs.python.org/3/library/json.html)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有关json库的更多信息，请参阅[https://docs.python.org/3/library/json.html](https://docs.python.org/3/library/json.html)
- en: Storing data using AWS S3
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS S3存储数据
- en: There are many cases where we just want to save content that we scrape into
    a local copy for archive purposes, backup, or later bulk analysis. We also might
    want to save media from those sites for later use. I've built scrapers for advertisement
    compliance companies, where we would track and download advertisement based media
    on web sites to ensure proper usage, and also to store for later analysis, compliance
    and transcoding.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多情况下，我们只想将我们抓取的内容保存到本地副本以进行存档、备份或以后进行批量分析。我们还可能希望保存这些网站的媒体以供以后使用。我为广告合规公司构建了爬虫，我们会跟踪并下载网站上基于广告的媒体，以确保正确使用，并且以供以后分析、合规和转码。
- en: The storage required for these types of systems can be immense, but with the
    advent of cloud storage services such as AWS S3 (Simple Storage Service), this
    becomes much easier and more cost effective than managing a large SAN (Storage
    Area Network) in your own IT department. Plus, S3 can also automatically move
    data from hot to cold storage, and then to long-term storage, such as a glacier,
    which can save you much more money.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型系统所需的存储空间可能是巨大的，但随着云存储服务（如AWS S3（简单存储服务））的出现，这比在您自己的IT部门中管理大型SAN（存储区域网络）要容易得多，成本也更低。此外，S3还可以自动将数据从热存储移动到冷存储，然后再移动到长期存储，例如冰川，这可以为您节省更多的钱。
- en: We won't get into all of those details, but simply look at storing our `planets.html`
    file into an S3 bucket. Once you can do this, you can save any content you want
    to year hearts desire.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入研究所有这些细节，而只是看看如何将我们的`planets.html`文件存储到S3存储桶中。一旦您能做到这一点，您就可以保存任何您想要的内容。
- en: Getting ready
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To perform the following example, you will need an AWS account and have access
    to secret keys for use in your Python code. They will be unique to your account.
     We will use the `boto3` library for S3 access. You can install this using `pip
    install boto3`.  Also, you will need to have environment variables set to authenticate. 
    These will look like the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行以下示例，您需要一个AWS账户，并且可以访问用于Python代码的密钥。它们将是您账户的唯一密钥。我们将使用`boto3`库来访问S3。您可以使用`pip
    install boto3`来安装它。此外，您需要设置环境变量进行身份验证。它们看起来像下面这样：
- en: '`AWS_ACCESS_KEY_ID=AKIAIDCQ5PH3UMWKZEWA`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`AWS_ACCESS_KEY_ID=AKIAIDCQ5PH3UMWKZEWA`'
- en: '`AWS_SECRET_ACCESS_KEY=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`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`AWS_SECRET_ACCESS_KEY=ZLGS/a5TGIv+ggNPGSPhGt+lwLwUip7u53vXfgWo`'
- en: These are available in the AWS portal under IAM (Identity Access Management)
    portion of the portal.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以在AWS门户的IAM（身份访问管理）部分找到。
- en: It's a good practice to put these keys in environment variables.  Having them
    in code can lead to their theft.  During the writing of this book, I had this
    hard coded and accidentally checked them in to GitHub.  The next morning I woke
    up to critical messages from AWS that I had thousands of servers running!  There
    are GitHub scrapers looking for these keys and they will get found and use for
    nefarious purposes.  By the time I had them all turned off, my bill was up to
    $6000, all accrued overnight. Thankfully, AWS waived these fees!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些密钥放在环境变量中是一个好习惯。在代码中使用它们可能会导致它们被盗。在编写本书时，我将它们硬编码并意外地将它们检入GitHub。第二天早上，我醒来收到了来自AWS的关键消息，说我有成千上万台服务器在运行！GitHub有爬虫在寻找这些密钥，它们会被找到并用于不正当目的。等我把它们全部关闭的时候，我的账单已经涨到了6000美元，全部是在一夜之间产生的。幸运的是，AWS免除了这些费用！
- en: How to do it
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'We won''t parse the data in the `planets.html` file, but simply retrieve it
    from the local web server using requests:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会解析`planets.html`文件中的数据，而只是使用requests从本地web服务器检索它：
- en: 'The following code, (found in `03/S3.py`), reads the planets web page and stores
    it in S3:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码（在`03/S3.py`中找到）读取行星网页并将其存储在S3中：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This app will give you output similar to the following, which is S3 info telling
    you various facts about the new item.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个应用程序将给出类似以下的输出，这是S3信息，告诉您关于新项目的各种事实。
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This output shows us that the object was successfully created in the bucket.
    At this point, you can navigate to the S3 console and see your bucket:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个输出告诉我们对象已成功创建在存储桶中。此时，您可以转到S3控制台并查看您的存储桶：
- en: '![](assets/29fbd119-7ee5-43eb-8b2f-9bc34998ff53.png)The Bucket in S3'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/29fbd119-7ee5-43eb-8b2f-9bc34998ff53.png)S3中的存储桶'
- en: 'Inside the bucket you will see the `planet.html` file:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在存储桶中，您将看到`planet.html`文件：
- en: '![](assets/49cc32c4-5ac3-4177-a397-35385afbcf4e.png)The File in the Bucket'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/49cc32c4-5ac3-4177-a397-35385afbcf4e.png)存储桶中的文件'
- en: 'By clicking on the file you can see the property and URL to the file within
    S3:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击文件，您可以看到S3中文件的属性和URL：
- en: '![](assets/6c5b035d-009f-4878-9806-034b4db8e500.png)The Properties of the File
    in S3'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/6c5b035d-009f-4878-9806-034b4db8e500.png)S3中文件的属性'
- en: How it works
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: The boto3 library wraps the AWS S3 API in a Pythonic syntax. The .`client()`
    call authenticates with AWS and gives us an object to use to communicate with
    S3\. Make sure you have your keys in environment variables, as otherwise this
    will not work.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: boto3库以Pythonic语法封装了AWS S3 API。`.client()`调用与AWS进行身份验证，并为我们提供了一个用于与S3通信的对象。确保您的密钥在环境变量中，否则这将无法工作。
- en: The bucket name must be globally unique. At the time of writing, this bucket
    is available, but you will likely need to change the name. The `.create_bucket()`
    call creates the bucket and sets its ACL. `put_object()` uses the `boto3` upload
    manager to upload the scraped data into the object in the bucket.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 存储桶名称必须是全局唯一的。在撰写本文时，这个存储桶是可用的，但您可能需要更改名称。`.create_bucket()`调用创建存储桶并设置其ACL。`put_object()`使用`boto3`上传管理器将抓取的数据上传到存储桶中的对象。
- en: There's more...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'There a lot of details to learn for working with S3\. You can find API documentation
    at: [http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html](http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html).
    Boto3 documents can be found at: [https://boto3.readthedocs.io/en/latest/](https://boto3.readthedocs.io/en/latest/).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多细节需要学习来使用S3。您可以在以下网址找到API文档：[http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html](http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html)。Boto3文档可以在以下网址找到：[https://boto3.readthedocs.io/en/latest/](https://boto3.readthedocs.io/en/latest/)。
- en: While we only saved a web page, this model can be used to store any type of
    file based data in S3.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们只保存了一个网页，但这个模型可以用来在S3中存储任何类型的基于文件的数据。
- en: Storing data using MySQL
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MySQL存储数据
- en: MySQL is a freely available, open source Relational Database Management System
    (RDBMS).  In this example, we will read the planets data from the website and
    store it into a MySQL database.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL是一个免费的、开源的关系数据库管理系统（RDBMS）。在这个例子中，我们将从网站读取行星数据并将其存储到MySQL数据库中。
- en: Getting ready
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to have access to a MySQL database. You can install one locally
    installed, in the cloud, within a container.  I am using a locally installed MySQL
    server and have the `root` password set to `mypassword`. You will also need to
    install the MySQL python library.  You can do this with `pip install mysql-connector-python`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要访问一个MySQL数据库。您可以在本地安装一个，也可以在云中安装，也可以在容器中安装。我正在使用本地安装的MySQL服务器，并且将`root`密码设置为`mypassword`。您还需要安装MySQL
    python库。您可以使用`pip install mysql-connector-python`来安装它。
- en: 'The first thing to do is to connect to the database using the `mysql` command
    at the terminal:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先要做的是使用终端上的`mysql`命令连接到数据库：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we can create a database that will be used to store our scraped information:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个数据库，用来存储我们抓取的信息：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now use the new database:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用新的数据库：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And create a Planets table in the database to store our data:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并在数据库中创建一个行星表来存储我们的数据：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now we are ready to scrape data and put it into the MySQL database.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好抓取数据并将其放入MySQL数据库中。
- en: How to do it
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'The following code (found in `03/store_in_mysql.py`) will read the planets
    data and write it to MySQL:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码（在`03/store_in_mysql.py`中找到）将读取行星数据并将其写入MySQL：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This results in the following output:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using MySQL Workbench we can see the the records were written to the database
    (you could use the mysql command line also):'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MySQL Workbench，我们可以看到记录已写入数据库（您也可以使用mysql命令行）：
- en: '![](assets/c8a2c090-dce7-40f2-b0b3-0c6d72ff3885.png)Records displayed using
    MySQL Workbench'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/c8a2c090-dce7-40f2-b0b3-0c6d72ff3885.png)使用MySQL Workbench显示的记录'
- en: 'The following code can be used to retrieve the data (`03/read_from_mysql.py`):'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码可用于检索数据（`03/read_from_mysql.py`）：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This results in the following output:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: 'Accessing a MySQL database using the `mysql.connector` involves the use of
    two classes from the library: `connect` and `cursor`. The `connect` class opens
    and manages a connection with the database server.  From that connection object,
    we can create a cursor object.  This cursor is used for reading and writing data
    using SQL statements.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`mysql.connector`访问MySQL数据库涉及使用库中的两个类：`connect`和`cursor`。`connect`类打开并管理与数据库服务器的连接。从该连接对象，我们可以创建一个光标对象。该光标用于使用SQL语句读取和写入数据。
- en: In the first example, we used the cursor to insert nine records into the database.
    Those records are not written to the database until the `commit()` method of the
    connection is called.  This executes the writes of all the rows to the database.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，我们使用光标将九条记录插入数据库。直到调用连接的`commit()`方法，这些记录才会被写入数据库。这将执行将所有行写入数据库的操作。
- en: Reading data uses a similar model except that we execute an SQL query (`SELECT`)
    using the cursor and iterate across the rows that were retrieved. Since we are
    reading and not writing, there is no need to call `commit()` on the connection.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据使用类似的模型，只是我们使用光标执行SQL查询（`SELECT`），并遍历检索到的行。由于我们是在读取而不是写入，因此无需在连接上调用`commit()`。
- en: There's more...
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You can learn more about MySQL and install it from:  `https://dev.mysql.com/doc/refman/5.7/en/installing.html`.
    Information on MySQL Workbench is available at:  `https://dev.mysql.com/doc/workbench/en/`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下网址了解更多关于MySQL并安装它：`https://dev.mysql.com/doc/refman/5.7/en/installing.html`。有关MySQL
    Workbench的信息，请访问：`https://dev.mysql.com/doc/workbench/en/`。
- en: Storing data using PostgreSQL
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PostgreSQL存储数据
- en: In this recipe we store our planet data in PostgreSQL. PostgreSQL is an open
    source relational database management system (RDBMS). It is developed by a worldwide
    team of volunteers, is not controlled by any corporation or other private entity,
    and the source code is available free of charge.  It has a lot of unique features
    such as hierarchical data models.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将我们的行星数据存储在PostgreSQL中。PostgreSQL是一个开源的关系数据库管理系统（RDBMS）。它由一个全球志愿者团队开发，不受任何公司或其他私人实体控制，源代码可以免费获得。它具有许多独特的功能，如分层数据模型。
- en: Getting ready
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: First make sure you have access to a PostgreSQL data instance.  Again, you can
    install one locally, run one in a container, or get an instance in the cloud.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先确保您可以访问PostgreSQL数据实例。同样，您可以在本地安装一个，运行一个容器，或者在云中获取一个实例。
- en: As with MySQL, we need to first create a database. The process is almost identical
    to that of MySQL but with slightly different commands and parameters.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与MySQL一样，我们需要首先创建一个数据库。该过程与MySQL几乎相同，但命令和参数略有不同。
- en: 'From the terminal execute the psql command at the terminal.  This takes you
    into the psql command processor:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从终端执行终端上的psql命令。这将带您进入psql命令处理器：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now create the scraping database:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在创建抓取数据库：
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then switch to the new database:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后切换到新数据库：
- en: '[PRE30]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we can create the Planets table. We first need to create a sequence table:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以创建Planets表。我们首先需要创建一个序列表：
- en: '[PRE31]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And now we can create the table:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以创建表：
- en: '[PRE32]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: To access PostgreSQL from Python we will use the `psycopg2` library, so make
    sure it is installed in your Python environment using `pip install psycopg2`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Python访问PostgreSQL，我们将使用`psycopg2`库，因此请确保在Python环境中安装了它，使用`pip install psycopg2`。
- en: We are now ready to write Python to store the planets data in PostgreSQL.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好编写Python将行星数据存储在PostgreSQL中。
- en: How to do it
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'We proceed with the recipe as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下步骤进行：
- en: 'The following code will read the planets data and write it to the database
    (code in `03/save_in_postgres.py`):'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码将读取行星数据并将其写入数据库（代码在`03/save_in_postgres.py`中）：
- en: '[PRE33]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If successful you will see the following:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果成功，您将看到以下内容：
- en: '[PRE34]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Using GUI tools such as pgAdmin you can examine the data within the database:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用诸如pgAdmin之类的GUI工具，您可以检查数据库中的数据：
- en: '![](assets/e1060188-c3d3-4a2d-aaf4-4f9124294d9e.png)Records Displayed in pgAdmin'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/e1060188-c3d3-4a2d-aaf4-4f9124294d9e.png)在pgAdmin中显示的记录'
- en: 'The data can be queried with the following Python code (found in `03/read_from_postgresql.py`):'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下Python代码查询数据（在`03/read_from_postgresql.py`中找到）：
- en: '[PRE35]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And results in the following output (truncated a little bit:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并导致以下输出（略有截断：
- en: '[PRE36]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: 'Accessing a PostgreSQL database using the `psycopg2` library as we did involves
    the use of two classes from the library: `connect` and `cursor`. The `connect`
    class opens and manages a connection with the database server. From that connection
    object, we can create a `cursor` object. This cursor is used for reading and writing
    data using SQL statements.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`psycopg2`库访问PostgreSQL数据库涉及使用库中的两个类：`connect`和`cursor`。`connect`类打开并管理与数据库服务器的连接。从该连接对象，我们可以创建一个`cursor`对象。该光标用于使用SQL语句读取和写入数据。
- en: In the first example, we used the cursor to insert nine records into the database.
     Those records are not written to the database until the `commit()` method of
    the connection is called.  This executes the writes of all the rows to the database.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，我们使用光标将九条记录插入数据库。直到调用连接的`commit()`方法，这些记录才会被写入数据库。这将执行将所有行写入数据库的操作。
- en: Reading data uses a similar model, except that we execute an SQL query (`SELECT`)
    using the cursor and iterate across the rows that were retrieved.  Since we are
    reading and not writing, there is no need to call `commit()` on the connection.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据使用类似的模型，只是我们使用游标执行SQL查询（`SELECT`），并遍历检索到的行。由于我们是在读取而不是写入，所以不需要在连接上调用`commit()`。
- en: There's more...
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Information on PostgreSQL is available at `https://www.postgresql.org/`.  pgAdmin
    can be obtained at: `https://www.pgadmin.org/`  Reference materials for `psycopg`
    are at: `http://initd.org/psycopg/docs/usage.html`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 有关PostgreSQL的信息可在`https://www.postgresql.org/`找到。pgAdmin可以在`https://www.pgadmin.org/`获得。`psycopg`的参考资料位于`http://initd.org/psycopg/docs/usage.html`
- en: Storing data in Elasticsearch
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Elasticsearch中存储数据
- en: 'Elasticsearch is a search engine based on Lucene. It provides a distributed,
    multitenant-capable, full-text search engine with an HTTP web interface and schema-free
    JSON documents. It is a non-relational database (often stated as NoSQL), focusing
    on the storage of documents instead of records. These documents can be many formats,
    one of which is useful to us: JSON.  This makes using Elasticsearch very simple
    as we do not need to convert our data to/from JSON.  We will use Elasticsearch
    much more later in the book'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是基于Lucene的搜索引擎。它提供了一个分布式、多租户能力的全文搜索引擎，具有HTTP Web界面和无模式的JSON文档。它是一个非关系型数据库（通常称为NoSQL），专注于存储文档而不是记录。这些文档可以是许多格式之一，其中之一对我们有用：JSON。这使得使用Elasticsearch非常简单，因为我们不需要将我们的数据转换为/从JSON。我们将在本书的后面更多地使用Elasticsearch
- en: For now, let's go and store our planets data in Elasticsearch.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们去将我们的行星数据存储在Elasticsearch中。
- en: Getting ready
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We will access a locally installed Elasticsearch server. To do this from Python,
    we will use the `Elasticsearch-py` library. It is most likely that you will need
    to install this using pip: `pip install elasticsearch`.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将访问一个本地安装的Elasticsearch服务器。为此，我们将使用`Elasticsearch-py`库从Python中进行操作。您很可能需要使用pip来安装它：`pip
    install elasticsearch`。
- en: Unlike PostgreSQL and MySQL, we do not need to create tables in Elasticsearch
    ahead of time. Elasticsearch does not care about structured data schemas (although
    it does have indexes), so we don't have to go through this procedure.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与PostgreSQL和MySQL不同，我们不需要提前在Elasticsearch中创建表。Elasticsearch不关心结构化数据模式（尽管它确实有索引），因此我们不必经历这个过程。
- en: How to do it
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到
- en: 'Writing data to Elasticsearch is really simple. The following Python code performs
    this task with our planets data (`03/write_to_elasticsearch.py`):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据写入Elasticsearch非常简单。以下Python代码使用我们的行星数据执行此任务（`03/write_to_elasticsearch.py`）：
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Executing this results in the following output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作将产生以下输出：
- en: '[PRE38]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The output shows the result of each insertion, giving us information such as
    the `_id` assigned to the document by elasticsearch.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了每次插入的结果，为我们提供了elasticsearch分配给文档的`_id`等信息。
- en: 'If you have logstash and kibana installed too, you can see the data inside
    of Kibana:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您也安装了logstash和kibana，您可以在Kibana内部看到数据：
- en: '![](assets/d9054e7d-acb5-4324-8bcd-33c00accd7c8.png)Kibana Showing and Index'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d9054e7d-acb5-4324-8bcd-33c00accd7c8.png)Kibana显示和索引'
- en: 'And we can query the data with the following Python code. This code retrieves
    all of the documents in the ''planets'' index and prints the name, mass, and radius
    of each planet (`03/read_from_elasticsearch.py`):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下Python代码查询数据。此代码检索“planets”索引中的所有文档，并打印每个行星的名称、质量和半径（`03/read_from_elasticsearch.py`）：
- en: '[PRE39]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This results in the following output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE41]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: How it works
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: Elasticsearch is both a NoSQL database and a search engine. You give documents
    to Elasticsearch and it parses the data in the documents and creates search indexes
    for that data automatically.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch既是NoSQL数据库又是搜索引擎。您将文档提供给Elasticsearch，它会解析文档中的数据并自动为该数据创建搜索索引。
- en: During the insertion process, we used the `elasticsearch` libraries' `.index()`
    method and specified an index, named "planets", a document type, `planets_info`,
    and the finally the body of the document, which is our planet Python object. The
    `elasticsearch` library that object to JSON and sends it off to Elasticsearch
    for storage and indexing.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在插入过程中，我们使用了`elasticsearch`库的`.index()`方法，并指定了一个名为“planets”的索引，一个文档类型`planets_info`，最后是文档的主体，即我们的行星Python对象。`elasticsearch`库将该对象转换为JSON并将其发送到Elasticsearch进行存储和索引。
- en: The index parameter is used to inform Elasticsearch how to create an index,
    which it will use for indexing and which we can use to specify a set of documents
    to search for when we query. When we performed the query, we specified the same
    index "planets" and executed a query to match all of the documents.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 索引参数用于通知Elasticsearch如何创建索引，它将用于索引和我们在查询时可以用来指定要搜索的一组文档。当我们执行查询时，我们指定了相同的索引“planets”并执行了一个匹配所有文档的查询。
- en: There's more...
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can find out much more about elasticsearch at: `https://www.elastic.co/products/elasticsearch`.
     Information on the python API can be found at: `http://pyelasticsearch.readthedocs.io/en/latest/api/`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在`https://www.elastic.co/products/elasticsearch`找到有关elasticsearch的更多信息。有关python
    API的信息可以在`http://pyelasticsearch.readthedocs.io/en/latest/api/`找到
- en: We will also come back to Elasticsearch in later chapters of the book.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将在本书的后面章节回到Elasticsearch。
- en: How to build robust ETL pipelines with AWS SQS
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用AWS SQS构建强大的ETL管道
- en: Scraping a large quantity of sites and data can be a complicated and slow process. 
    But it is one that can take great advantage of parallel processing, either locally
    with multiple processor threads, or distributing scraping requests to report scrapers
    using a message queue system. There may also be the need for multiple steps in
    a process similar to an Extract, Transform, and Load pipeline (ETL). These pipelines
    can also be easily built using a message queuing architecture in conjunction with
    the scraping.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 爬取大量站点和数据可能是一个复杂和缓慢的过程。但它可以充分利用并行处理，无论是在本地使用多个处理器线程，还是使用消息队列系统将爬取请求分发给报告爬虫。在类似于提取、转换和加载流水线（ETL）的过程中，可能还需要多个步骤。这些流水线也可以很容易地使用消息队列架构与爬取相结合来构建。
- en: 'Using a message queuing architecture gives our pipeline two advantages:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用消息队列架构给我们的流水线带来了两个优势：
- en: Robustness
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健壮性
- en: Scalability
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可伸缩性
- en: The processing becomes robust, as if processing of an individual message fails,
    then the message can be re-queued for processing again. So if the scraper fails,
    we can restart it and not lose the request for scraping the page, or the message
    queue system will deliver the request to another scraper.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 处理变得健壮，因为如果处理单个消息失败，那么消息可以重新排队进行处理。因此，如果爬虫失败，我们可以重新启动它，而不会丢失对页面进行爬取的请求，或者消息队列系统将把请求传递给另一个爬虫。
- en: It provides scalability, as multiple scrapers on the same, or different, systems
    can listen on the queue. Multiple messages can then be processed at the same time
    on different cores or, more importantly, different systems. In a cloud-based scraper,
    you can scale up the number of scraper instances on demand to handle greater load.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了可伸缩性，因为在同一系统或不同系统上可以监听队列上的多个爬虫。然后，可以在不同的核心或更重要的是不同的系统上同时处理多个消息。在基于云的爬虫中，您可以根据需要扩展爬虫实例的数量以处理更大的负载。
- en: 'Common message queueing systems that can be used include: Kafka, RabbitMQ,
    and Amazon SQS. Our example will utilize Amazon SQS, although both Kafka and RabbitMQ
    are quite excellent to use (we will see RabbitMQ in use later in the book). We
    use SQS to stay with a model of using AWS cloud-based services as we did earlier
    in the chapter with S3.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用的常见消息队列系统包括：Kafka、RabbitMQ和Amazon SQS。我们的示例将利用Amazon SQS，尽管Kafka和RabbitMQ都非常适合使用（我们将在本书的后面看到RabbitMQ的使用）。我们使用SQS来保持使用AWS基于云的服务的模式，就像我们在本章早些时候使用S3一样。
- en: Getting ready
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: As an example, we will build a vary simple ETL process that will read the main
    planets page and store the planets data in MySQL. It will also pass a single message
    for each *more info* link in the page to a queue, where 0 or more processes can
    receive those requests and perform further processing on those links.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们将构建一个非常简单的ETL过程，该过程将读取主行星页面并将行星数据存储在MySQL中。它还将针对页面中的每个*更多信息*链接传递单个消息到队列中，其中0个或多个进程可以接收这些请求，并对这些链接执行进一步处理。
- en: To access SQS from Python, we will revisit using the `boto3` library.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Python访问SQS，我们将重新使用`boto3`库。
- en: How to do it - posting messages to an AWS queue
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作-将消息发布到AWS队列
- en: 'The `03/create_messages.py` file contains code to read the planets data and
    to post the URL in the MoreInfo property to an SQS queue:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`03/create_messages.py`文件包含了读取行星数据并将URL发布到SQS队列的代码：'
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Run the code in a terminal and you will see output similar to the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中运行代码，您将看到类似以下的输出：
- en: '[PRE43]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now go into the AWS SQS console. You should see the queue has been created
    and that it holds 9 messages:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入AWS SQS控制台。您应该看到队列已经被创建，并且它包含9条消息：
- en: '![](assets/2ad3b7c1-9f39-4d02-ac61-d23619a9c409.png)The Queue in SQS'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2ad3b7c1-9f39-4d02-ac61-d23619a9c409.png)SQS中的队列'
- en: How it works
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: The code connects to the given account and the us-west-2 region of AWS. A queue
    is then created if one does not exist. Then, for each planet in the source content,
    the program sends a message which consists of the *more info* URL for the planet.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码连接到给定帐户和AWS的us-west-2地区。然后，如果队列不存在，则创建队列。然后，对于源内容中的每个行星，程序发送一个消息，该消息包含该行星的*更多信息*
    URL。
- en: At this point, there is no one listening to the queue, so the messages will
    sit there until eventually read or they expire. The default life for each message
    is 4 days.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，没有人在监听队列，因此消息将一直保留在那里，直到最终被读取或它们过期。每条消息的默认生存期为4天。
- en: How to do it - reading and processing messages
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作-读取和处理消息
- en: 'To process the messages, run the `03/process_messages.py` program:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理消息，请运行`03/process_messages.py`程序：
- en: '[PRE44]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Run the script using `python process_messages.py`.  You will see output similar
    to the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`python process_messages.py`运行脚本。您将看到类似以下的输出：
- en: '[PRE45]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: How it works
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: The program connects to SQS and opens the queue. Opening the queue for reading
    is also done using `sqs.create_queue`, which will simply return the queue if it
    already exists.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 程序连接到SQS并打开队列。打开队列以进行读取也是使用`sqs.create_queue`完成的，如果队列已经存在，它将简单地返回队列。
- en: Then, it enters a loop calling `sqs.receive_message`, specifying the URL of
    the queue, the number of messages to receive in each read, and the maximum amount
    of time to wait in seconds if there are no messages available.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它进入一个循环调用`sqs.receive_message`，指定队列的URL，每次读取消息的数量，以及如果没有消息可用时等待的最长时间（以秒为单位）。
- en: If a message is read, the URL in the message is retrieved and scraping techniques
    are used to read the page at the URL and extract the planet's name and information
    about its albedo.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果读取了一条消息，将检索消息中的URL，并使用爬取技术读取URL的页面并提取行星的名称和有关其反照率的信息。
- en: Note that we retrieve the receipt handle of the message. This is needed to delete
    the message from the queue. If we do not delete the message, it will be made available
    in the queue after a period of time.   So if our scraper crashed and didn't perform
    this acknowledgement, the messages will be made available again by SQS for another
    scraper to process (or the same one when it is back up).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们会检索消息的接收处理。这是删除队列中的消息所必需的。如果我们不删除消息，它将在一段时间后重新出现在队列中。因此，如果我们的爬虫崩溃并且没有执行此确认，消息将由SQS再次提供给另一个爬虫进行处理（或者在其恢复正常时由相同的爬虫处理）。
- en: There's more...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can find more information about S3 at: `https://aws.amazon.com/s3/`.  Specifics
    on the details of the API are available at: `https://aws.amazon.com/documentation/s3/`.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下网址找到有关S3的更多信息：`https://aws.amazon.com/s3/`。有关API详细信息的具体内容，请访问：`https://aws.amazon.com/documentation/s3/`。
