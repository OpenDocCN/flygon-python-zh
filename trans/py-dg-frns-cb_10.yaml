- en: Exploring Windows Forensic Artifacts Recipes - Part II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Windows取证工件食谱-第二部分
- en: 'In this chapter, the following recipes will be covered:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将涵盖以下内容：
- en: Parsing prefetch files
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析预取文件
- en: A series of fortunate events
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一系列幸运事件
- en: Indexing internet history
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引互联网历史记录
- en: Shadow of a former self
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 昔日的阴影
- en: Dissecting the SRUM database
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解剖SRUM数据库
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Microsoft Windows is one of the most common operating systems found on machines
    during forensic analysis. This has led to a large effort in the community over
    the past two decades to develop, share, and document artifacts deposited by this
    operating system for use in forensic casework.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Windows是在取证分析中发现的机器上最常见的操作系统之一。这导致社区在过去的二十年中付出了大量努力，以开发、共享和记录这个操作系统产生的证据，用于取证工作。
- en: 'In this chapter, we continue to look at various Windows artifacts and how to
    process them using Python. We will leverage the framework we developed in [Chapter
    8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe), *Working with Forensic
    Evidence Container Recipes* to process these artifacts directly from forensic
    acquisitions. We''ll use various `libyal` libraries to handle the underlying processing
    of various files, including `pyevt`, `pyevtx`, `pymsiecf`, `pyvshadow`, and `pyesedb`.
    We''ll also explore how to process prefetch files using `struct` and a file format
    table of offsets and data types of interest. Here''s what we''ll learn to do in
    this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续研究各种Windows取证工件以及如何使用Python处理它们。我们将利用我们在[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)中开发的框架，直接从取证获取中处理这些工件。我们将使用各种`libyal`库来处理各种文件的底层处理，包括`pyevt`、`pyevtx`、`pymsiecf`、`pyvshadow`和`pyesedb`。我们还将探讨如何使用`struct`和偏移量和感兴趣的数据类型的文件格式表来处理预取文件。在本章中，我们将学习以下内容：
- en: Parsing prefetch files for application execution information
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析预取文件以获取应用程序执行信息
- en: Searching for event logs and extract events to a spreadsheet
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索事件日志并将事件提取到电子表格中
- en: Extracting internet history from `index.dat` files
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`index.dat`文件中提取互联网历史记录
- en: Enumerating and creating file listings of volume shadow copies
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 枚举和创建卷影复制的文件列表
- en: Dissecting the Windows 10 SRUM database
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解剖Windows 10 SRUM数据库
- en: For a full listing of `libyal` repositories, visit [https://github.com/libyal](https://github.com/libyal).
    Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`libyal`存储库的完整列表，请访问[https://github.com/libyal](https://github.com/libyal)。访问[www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)下载本章的代码包。'
- en: Parsing prefetch files
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析预取文件
- en: 'Recipe difficulty: Medium'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：中等
- en: 'Python version: 2.7'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating system: Linux'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: Prefetch files are a common artifact to rely on for information about application
    execution. While they may not always be present, they are undoubtedly worth reviewing
    in scenarios where they exist. Recall that prefetching can be enabled to various
    degrees or disabled based upon the value of the `PrefetchParameters` subkey in
    the `SYSTEM` hive. This recipe searches for files with the prefetch extension
    (`.pf`) and processes them for valuable application information. We will only
    demonstrate this process for Windows XP prefetch files; however, be aware that
    the underlying process we use is similar to other iterations of Windows.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 预取文件是一个常见的证据，用于获取有关应用程序执行的信息。虽然它们可能并不总是存在，但在存在的情况下，无疑值得审查。请记住，根据`SYSTEM`注册表中`PrefetchParameters`子键的值，可以启用或禁用预取。此示例搜索具有预取扩展名（`.pf`）的文件，并处理它们以获取有价值的应用程序信息。我们将仅演示这个过程用于Windows
    XP的预取文件；但请注意，我们使用的基本过程类似于Windows的其他版本。
- en: Getting started
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'Because we have decided to build out the Sleuth Kit and its dependencies on
    an Ubuntu environment, we continue development on that operating system for ease
    of use. This script will require the installation, if they are not already present,
    of three additional libraries: `pytsk3`, `pyewf`, and `unicodecsv`. All other
    libraries used in this script are present in Python''s standard library.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们决定在Ubuntu环境中构建Sleuth Kit及其依赖项，所以我们将继续在该操作系统上进行开发，以便使用。如果尚未安装，此脚本将需要安装三个额外的库：`pytsk3`、`pyewf`和`unicodecsv`。此脚本中使用的所有其他库都包含在Python的标准库中。
- en: 'Refer to *[Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    Working with Forensic Evidence Container Recipes* for a detailed explanation of
    installing the `pytsk3` and `pyewf` modules*.* Because we are developing these
    recipes in Python 2.x, we are likely to encounter Unicode encode and decode errors.
    To account for that, we use the `unicodecsv` library to write all CSV output in
    this chapter. This third-party module takes care of Unicode support, unlike Python
    2.x''s standard `csv` module, and will be put to great use here. As usual, we
    can use `pip` to install `unicodecsv`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅*[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，与取证证据容器食谱一起工作*。因为我们在Python
    2.x中开发这些食谱，所以可能会遇到Unicode编码和解码错误。为了解决这个问题，我们使用`unicodecsv`库在本章中编写所有CSV输出。这个第三方模块处理Unicode支持，不像Python
    2.x的标准`csv`模块，并且在这里将得到很好的应用。像往常一样，我们可以使用`pip`来安装`unicodecsv`：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In addition to these, we'll continue to use the `pytskutil` module developed
    from [Chapter 8](https://cdp.packtpub.com/python_digital_forensics_cookbook/wp-admin/post.php?post=260&action=edit#post_218),
    *Working with Forensic Evidence Container Recipes**,* to allow interaction with
    forensic acquisitions. This module is largely similar to what we previously wrote,
    with some minor changes to better suit our purposes. You can review the code by
    navigating to the utility directory within the code package.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们将继续使用从[第8章](https://cdp.packtpub.com/python_digital_forensics_cookbook/wp-admin/post.php?post=260&action=edit#post_218)开发的`pytskutil`模块，以允许与取证获取进行交互。这个模块与我们之前编写的大致相似，只是对一些细微的更改，以更好地适应我们的目的。您可以通过导航到代码包中的实用程序目录来查看代码。
- en: How to do it...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We process prefetch files following these basic principles:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循以下基本原则处理预取文件：
- en: Scan for files ending with the `.pf` extension.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描以`.pf`扩展名结尾的文件。
- en: Eliminate false positives through signature verification.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过签名验证消除误报。
- en: Parse the Windows XP prefetch file format.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析Windows XP预取文件格式。
- en: Create a spreadsheet of parsed results to the current working directory.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在当前工作目录中创建解析结果的电子表格。
- en: How it works...
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We import a number of libraries to assist with argument parsing, parsing dates,
    interpreting binary data, writing CSVs, and the custom `pytskutil` module.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了许多库来帮助解析参数、解析日期、解释二进制数据、编写CSV文件以及自定义的`pytskutil`模块。
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This recipe''s command-line handler takes two positional arguments, `EVIDENCE_FILE`
    and `TYPE`, which represent the path to the evidence file and the type of evidence
    file (that is, `raw` or `ewf`). Most of the recipes featured in this chapter will
    only feature the two positional inputs. The output from these recipes will be
    spreadsheets created in the current working directory. This recipe has an optional
    argument, `d`, which specifies the path to scan for prefetch files. By default,
    this is set to the `/Windows/Prefetch` directory, although users can elect to
    scan the entire image or a separate directory if desired. After performing some
    input validation on the evidence file, we supply the `main()` function with the
    three inputs and begin executing the script:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序接受两个位置参数，`EVIDENCE_FILE`和`TYPE`，它们代表证据文件的路径和证据文件的类型（即`raw`或`ewf`）。本章中大多数配方只包括这两个位置输入。这些配方的输出将是在当前工作目录中创建的电子表格。这个配方有一个可选参数`d`，它指定要扫描预取文件的路径。默认情况下，这被设置为`/Windows/Prefetch`目录，尽管用户可以选择扫描整个镜像或其他目录。在对证据文件进行一些输入验证后，我们向`main()`函数提供了三个输入，并开始执行脚本：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the `main()` function, we first create the `TSKUtil` object, `tsk_util`,
    which represents the `pytsk3` image object. With the `TSKUtil` object, we can
    call a number of helper functions to directly interact with the evidence file.
    We use the `TSKUtil.query_directory()` function to confirm that the specified
    directory exists. If it does, we use the `TSKUtil.recurse_files()` method to recurse
    through the specified directory and identify any file that ends with the `.pf`
    extension. This method returns a list of tuples, where each tuple contains a number
    of potentially useful objects, including the `filename`, path, and object itself.
    If no such files are found, `None` is returned instead.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main()`函数中，我们首先创建`TSKUtil`对象`tsk_util`，它代表`pytsk3`图像对象。有了`TSKUtil`对象，我们可以调用许多辅助函数直接与证据文件进行交互。我们使用`TSKUtil.query_directory()`函数确认指定的目录是否存在。如果存在，我们使用`TSKUtil.recurse_files()`方法来递归遍历指定目录，并识别以`.pf`扩展名结尾的任何文件。该方法返回一个元组列表，其中每个元组包含许多潜在有用的对象，包括`filename`、路径和对象本身。如果找不到这样的文件，则返回`None`。
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If we do find files matching the search criteria, we print a status message
    to the console with the number of files found. Next, we set up the `prefetch_data`
    list, which will be used to store the parsed prefetch data from each valid file.
    As we iterate through each hit in the search, we extract the file object, the
    second index of the tuple, for further processing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找到与搜索条件匹配的文件，我们会在控制台上打印状态消息，显示找到的文件数量。接下来，我们设置`prefetch_data`列表，用于存储从每个有效文件中解析的预取数据。当我们遍历搜索中的每个命中时，我们提取文件对象（元组的第二个索引）以进行进一步处理。
- en: Before we do anything with the file object, we validate the file signature of
    the potential prefetch file with the `check_signature()` method. If the file does
    not match the known prefetch file signature, `None` is returned as the `pf_version`
    variable, preventing further processing from occurring for this particular file.
    Before we delve any further into the actual processing of the file, let's look
    at how this `check_signature()` method functions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对文件对象执行任何操作之前，我们使用`check_signature()`方法验证潜在预取文件的文件签名。如果文件与已知的预取文件签名不匹配，则将`None`作为`pf_version`变量返回，阻止对该特定文件进行进一步处理。在我们进一步深入实际处理文件之前，让我们看看这个`check_signature()`方法是如何工作的。
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `check_signature()` method takes the file object as its input and returns
    either the prefetch version or, if the file is not a valid prefetch file, returns
    `None`. We use `struct` to extract two little-endian `32-bit` integers from the
    first `8` bytes of the potential prefetch file. The first integer represents the
    file version, while the second integer is the file's signature. The file signature
    should be `0x53434341`, whose decimal representation is `1,094,927,187`. We compare
    the value we extracted from the file to that number to determine whether the file
    signatures match. If they do match, we return the prefetch version to the `main()`
    function. The prefetch version tells us what type of prefetch file we are working
    with (Windows XP, 7, 10, and so on). We return this value back to dictate how
    to process the file as prefetch files have changed slightly in different versions
    of Windows. Now, back to the `main()` function!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`check_signature()`方法以文件对象作为输入，返回预取版本，如果文件不是有效的预取文件，则返回`None`。我们使用`struct`从潜在的预取文件的前8个字节中提取两个小端`32位`整数。第一个整数代表文件版本，而第二个整数是文件的签名。文件签名应为`0x53434341`，其十进制表示为`1,094,927,187`。我们将从文件中提取的值与该数字进行比较，以确定文件签名是否匹配。如果它们匹配，我们将预取版本返回给`main()`函数。预取版本告诉我们我们正在处理哪种类型的预取文件（Windows
    XP、7、10等）。我们将此值返回以指示如何处理文件，因为不同版本的Windows中预取文件略有不同。现在，回到`main()`函数！'
- en: To learn more about prefetch versions and file formats, visit [http://www.forensicswiki.org/wiki/Windows_Prefetch_File_Format](http://www.forensicswiki.org/wiki/Windows_Prefetch_File_Format).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于预取版本和文件格式的信息，请访问[http://www.forensicswiki.org/wiki/Windows_Prefetch_File_Format](http://www.forensicswiki.org/wiki/Windows_Prefetch_File_Format)。
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Back in the `main()` function, we check that the `pf_version` variable is not
    `None`, indicating it was successfully validated. Following that, we extract the
    name of the file to the `pf_name` variable, which is stored at the zero index
    of the tuple. Next, we check which version of prefetch file we are working with.
    A breakdown of prefetch versions and their related operating systems can be viewed
    here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main()`函数中，我们检查`pf_version`变量是否不是`None`，这表明它已成功验证。随后，我们将文件名提取到`pf_name`变量中，该变量存储在元组的零索引处。接下来，我们检查我们正在处理哪个版本的预取文件。预取版本及其相关操作系统的详细信息可以在这里查看：
- en: '| **Prefetch version** | **Windows desktop operating system** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **预取版本** | **Windows桌面操作系统** |'
- en: '| 17 | Windows XP |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 17 | Windows XP |'
- en: '| 23 | Windows Vista, Windows 7 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 23 | Windows Vista，Windows 7 |'
- en: '| 26 | Windows 8.1 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 26 | Windows 8.1 |'
- en: '| 30 | Windows 10 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 30 | Windows 10 |'
- en: 'This recipe has only been developed to process Windows XP prefetch files using
    the file format as recorded on the previously referenced forensics wiki page.
    However, there are placeholders to add in the logic to support the other prefetch
    formats. They are largely similar, with the exception of Windows 10, and can be
    parsed by following the same basic methodology used for Windows XP. Windows 10
    prefetch files are MAM compressed and must be decompressed first before they can
    be processed--other than that, they can be handled in a similar manner. For version
    17 (Windows XP format), we call the parsing function, providing the TSK file object
    and name of the prefetch file:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程只开发了处理Windows XP预取文件的方法，使用的是之前引用的取证wiki页面上记录的文件格式。然而，有占位符可以添加逻辑来支持其他预取格式。它们在很大程度上是相似的，除了Windows
    10，可以通过遵循用于Windows XP的相同基本方法来解析。Windows 10预取文件是MAM压缩的，必须先解压缩才能处理--除此之外，它们可以以类似的方式处理。对于版本17（Windows
    XP格式），我们调用解析函数，提供TSK文件对象和预取文件的名称：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We begin processing the Windows XP prefetch file by storing the `create` and
    `modify` timestamps of the file itself into local variables. These `Unix` timestamps
    are converted using the `convertUnix()` method, which we have worked with before.
    Besides `Unix` timestamps, we also encounter `FILETIME` timestamps embedded within
    the prefetch file themselves. Let''s look at these functions briefly to get them
    out of the way before continuing our discussion of the `main()` method:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始处理Windows XP预取文件，将文件本身的`create`和`modify`时间戳存储到本地变量中。这些`Unix`时间戳使用我们之前使用过的`convertUnix()`方法进行转换。除了`Unix`时间戳，我们还遇到了嵌入在预取文件中的`FILETIME`时间戳。在继续讨论`main()`方法之前，让我们简要看一下这些函数：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Both functions rely on the `datetime` module to appropriately convert the timestamps
    into a human-readable format. Both functions check whether the supplied timestamp
    string is equal to `"0"` and return an empty string if that is the case. Otherwise,
    for the `convert_unix()` method, we use the `utcfromtimestamp()` method to convert
    the `Unix` timestamp to a `datetime` object and return that. For the `FILETIME`
    timestamp, we add the number of 100 nanoseconds elapsed since January 1, 1601,
    and return the resulting `datetime` object. With our brief dalliance with time
    complete, let's get back to the `main()` function.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数都依赖于`datetime`模块，以适当地将时间戳转换为人类可读的格式。这两个函数都检查提供的时间戳字符串是否等于`"0"`，如果是，则返回空字符串。否则，对于`convert_unix()`方法，我们使用`utcfromtimestamp()`方法将`Unix`时间戳转换为`datetime`对象并返回。对于`FILETIME`时间戳，我们添加自1601年1月1日以来经过的100纳秒数量，并返回结果的`datetime`对象。完成了我们与时间的短暂交往，让我们回到`main()`函数。
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have extracted the file metadata, we start using `struct` to extract
    the data embedded within the prefetch file itself. We read in `136` bytes from
    the file using the `pytsk3.read_random()` method and `struct` and unpack that
    data into Python variables. Specifically, in those `136` bytes, we extract five
    `32-bit` integers (`i`), one `64-bit` integer (`q`), and a 60-character string
    (`s`). In parentheses in the preceding sentence are the `struct` format characters
    related to those data types. This can also be seen in the `struct` format string
    `"<i60s32x3iq16xi"`, where the number preceding the `struct` format character
    instructs `struct` how many there are (for example, `60s` tells `struct` to interpret
    the next `60` bytes as a string). Likewise, the `"x"` `struct` format character
    is a null value. If `struct` receives `136` bytes to read, it must also receive
    format characters accounting for each of those `136` bytes. Therefore, we must
    supply these null values to ensure we appropriately account for the data we are
    reading in and ensure we are interpreting the values at the appropriate offsets.
    The `"**<**"` character at the beginning of the string ensures all values are
    interpreted as little-endian.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经提取了文件元数据，我们开始使用`struct`来提取预取文件中嵌入的数据。我们使用`pytsk3.read_random()`方法和`struct`从文件中读取`136`字节，并将这些数据解包到Python变量中。具体来说，在这`136`字节中，我们提取了五个`32位`整数（`i`），一个`64位`整数（`q`），和一个60字符的字符串（`s`）。在上述句子中的括号中是与这些数据类型相关的`struct`格式字符。这也可以在`struct`格式字符串`"<i60s32x3iq16xi"`中看到，其中在`struct`格式字符之前的数字告诉`struct`有多少个（例如，`60s`告诉`struct`将下一个`60`字节解释为字符串）。同样，`"x"`
    `struct`格式字符是一个空值。如果`struct`接收到`136`字节要读取，它也必须接收到格式字符来解释每个这`136`字节。因此，我们必须提供这些空值，以确保我们适当地解释我们正在读取的数据，并确保我们正在适当的偏移量上解释值。字符串开头的`"<"`字符确保所有值都被解释为小端。
- en: 'Right, that was maybe a bit much, but we probably all have a better understanding
    of `struct` now. After `struct` interprets the data, it returns a tuple of unpacked
    data types in the order in which they were unpacked. We assign these to a series
    of local variables including the prefetch file size, application name, last executed
    `FILETIME,` and the execution count. The application''s `name` variable, the 60-character
    string we extracted, needs to be UTF-16 decoded, and we need to remove all `x00`
    values padding the string. Notice that one of the values we extracted, `vol_info`,
    is the pointer to where volume information is stored within the prefetch file.
    We extract this information next:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，可能有点多，但我们现在可能都对`struct`有了更好的理解。在`struct`解释数据后，它以解包的数据类型元组的顺序返回。我们将这些分配给一系列本地变量，包括预取文件大小，应用程序名称，最后执行的`FILETIME`和执行计数。我们提取的应用程序的`name`变量，即我们提取的60个字符的字符串，需要进行UTF-16解码，并且我们需要删除填充字符串的所有`x00`值。请注意，我们提取的值之一，`vol_info`，是存储在预取文件中卷信息的指针。我们接下来提取这些信息：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let's look at a simpler example with `struct`. We read `20` bytes, starting
    from the `vol_info` pointer, and extract three `32-bit` integers and one `64-bit`
    integer. These are the volume name offset and length, the volume serial number,
    and the volume creation date. Most forensic programs display the volume serial
    number as two four-character hex values separated by a dash. We do the same by
    converting the integer to hex and removing the prepended `"0x"` value to isolate
    just the eight-character hex value. Next, we append a dash halfway between the
    volume serial number using string slicing and concatenation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个更简单的例子，使用`struct`。我们从`vol_info`指针开始读取`20`字节，并提取三个`32位`整数和一个`64位`整数。这些是卷名偏移和长度，卷序列号和卷创建日期。大多数取证程序将卷序列号显示为由破折号分隔的两个四字符十六进制值。我们通过将整数转换为十六进制并删除前置的`"0x"`值来做到这一点，以隔离出八字符十六进制值。接下来，我们使用字符串切片和连接在卷序列号的中间添加一个破折号。
- en: 'Finally, we use the volume name offset and length we extracted to pull out
    the volume name. We use string formatting to insert the volume name length in
    the `struct` format string. We must multiply the length by two to extract the
    full string. Similar to the application name, we must decode the string as UTF-16
    and remove any `"/x00"` values present. We append the extracted elements from
    the prefetch file to the list. Notice how we perform a few last-minute operations
    while doing so, including converting two `FILETIME` timestamps and joining the
    prefetch path with the name of the file. Note that if we do not remove the prepended
    `"**/**"` character from the `filename`, the `os.path.join()` method will not
    combine these two strings correctly. Therefore, we use `lstrip()` to remove it
    from the beginning of the string:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用提取的卷名偏移和长度来提取卷名。我们使用字符串格式化将卷名长度插入`struct`格式字符串中。我们必须将长度乘以二来提取完整的字符串。与应用程序名称类似，我们必须将字符串解码为UTF-16并删除任何存在的`"/x00"`值。我们将从预取文件中提取的元素附加到列表中。请注意，我们在这样做时执行了一些最后一刻的操作，包括转换两个`FILETIME`时间戳并将预取路径与文件名结合在一起。请注意，如果我们不从`filename`中删除前置的`"**/**"`字符，则`os.path.join()`方法将无法正确组合这两个字符串。因此，我们使用`lstrip()`将其从字符串的开头删除：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we discussed at the beginning of this recipe, we currently only support
    Windows XP-format prefetch files. We have left placeholders to support the other
    format types. Currently, however, if these formats are encountered, an unsupported
    message is printed to the console and we continue onto the next prefetch file:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本教程开始时讨论的那样，我们目前仅支持Windows XP格式的预取文件。我们已留下占位符以支持其他格式类型。但是，当前，如果遇到这些格式，将在控制台上打印不支持的消息，然后我们继续到下一个预取文件：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Recall back to the beginning of this recipe how we checked if the `pf_version`
    variable was `None`. If that is the case, the prefetch file does not pass signature
    verification, and so we print a message to that effect and continue onto the next
    file. Once we have finished processing all prefetch files, we send the list containing
    the parsed data to the `write_output()` method:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下本教程开始时我们如何检查`pf_version`变量是否为`None`。如果是这种情况，预取文件将无法通过签名验证，因此我们会打印一条相应的消息，然后继续到下一个文件。一旦我们完成处理所有预取文件，我们将包含解析数据的列表发送到`write_output()`方法：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `write_output()` method takes the data list we created and writes that data
    out to a CSV file. We use the `os.getcwd()` method to identify the current working
    directory, where we write the CSV file. After printing a status message to the
    console, we create our CSV file, write the names of our columns, and then use
    the `writerows()` method to write all of the lists of parsed prefetch data within
    the data list.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_output()` 方法接受我们创建的数据列表，并将该数据写入CSV文件。我们使用`os.getcwd()`方法来识别当前工作目录，在那里我们写入CSV文件。在向控制台打印状态消息后，我们创建我们的CSV文件，写入我们列的名称，然后使用`writerows()`方法在数据列表中写入所有解析的预取数据列表。'
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When we run this script, we generate a CSV document with the following columns:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个脚本时，我们会生成一个包含以下列的CSV文档：
- en: '![](../images/00107.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00107.jpeg)'
- en: Scrolling left, we can see the following columns for the same entries (the file
    path column is not shown due to its size).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 向左滚动，我们可以看到相同条目的以下列（由于其大小，文件路径列未显示）。
- en: '![](../images/00108.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00108.jpeg)'
- en: There's more...
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们在这里提供了一个或多个建议：
- en: Add support for other Windows prefetch file formats. Starting with Windows 10,
    prefetch files now have MAM compression and must first be decompressed prior to
    parsing the data with `struct`
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加对其他Windows预取文件格式的支持。从Windows 10开始，预取文件现在具有MAM压缩，必须在使用`struct`解析数据之前首先进行解压缩
- en: Check out the `libscca` ([https://github.com/libyal/libscca](https://github.com/libyal/libscca))
    library and its Python bindings, `pyscca`, which was developed to process prefetch
    files
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看`libscca` ([https://github.com/libyal/libscca](https://github.com/libyal/libscca))库及其Python绑定`pyscca`，该库是用于处理预取文件的
- en: A series of fortunate events
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一系列幸运的事件
- en: 'Recipe Difficulty: Hard'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 示例难度：困难
- en: 'Python Version: 2.7'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating System: Linux'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: Event logs, if configured appropriately, contain a wealth of information useful
    in any cyber investigation. These logs retain historical user activity information,
    such as logons, RDP access, Microsoft Office file access, system changes, and
    application-specific events. In this recipe, we use the `pyevt` and `pyevtx` libraries
    to process both legacy and current Windows event log formats.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 事件日志，如果配置适当，包含了在任何网络调查中都有用的大量信息。这些日志保留了历史用户活动信息，如登录、RDP访问、Microsoft Office文件访问、系统更改和特定应用程序事件。在这个示例中，我们使用`pyevt`和`pyevtx`库来处理传统和当前的Windows事件日志格式。
- en: Getting started
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'This recipe requires the installation of five third-party modules to function:
    `pytsk3`, `pyewf`, `pyevt`, `pyevtx`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Parsing prefetch files* recipe, for details on installing
    `unicodecsv`. All other libraries used in this script are present in Python''s
    standard library. When it comes to installing the Python bindings of most `libyal`
    libraries, they follow a very similar path.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例需要安装五个第三方模块才能运行：`pytsk3`，`pyewf`，`pyevt`，`pyevtx`和`unicodecsv`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，*使用取证证据容器*
    *示例*。同样，有关安装`unicodecsv`的详细信息，请参阅*开始*部分中的*解析预取文件*示例。此脚本中使用的所有其他库都包含在Python的标准库中。在安装大多数`libyal`库的Python绑定时，它们遵循非常相似的路径。
- en: 'Navigate to the GitHub repository and download the desired release for each
    library. This recipe was developed using the `libevt-alpha-20170120` and `libevtx-alpha-20170122`
    releases of the `pyevt` and `pyevtx` libraries, respectively. Next, once the contents
    of the release are extracted, open a terminal and navigate to the extracted directory
    and execute the following commands for each release:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 转到GitHub存储库，并下载每个库的所需版本。这个示例是使用`pyevt`和`pyevtx`库的`libevt-alpha-20170120`和`libevtx-alpha-20170122`版本开发的。接下来，一旦提取了发布的内容，打开终端并导航到提取的目录，然后对每个发布执行以下命令：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: To learn more about the `pyevt` library, visit [https://github.com/libyal/libevt](https://github.com/libyal/libevt).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`pyevt`库，请访问[https://github.com/libyal/libevt](https://github.com/libyal/libevt)。
- en: To learn more about the `pyevtx` library, visit [https://github.com/libyal/libevtx](https://github.com/libyal/libevtx).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`pyevtx`库，请访问[https://github.com/libyal/libevtx](https://github.com/libyal/libevtx)。
- en: Lastly, we can check the libraries installation by opening a Python interpreter,
    importing `pyevt` and `pyevtx`, and running their respective `get_version()` methods
    to ensure we have the correct release versions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过打开Python解释器，导入`pyevt`和`pyevtx`，并运行它们各自的`get_version()`方法来检查库的安装情况，以确保我们有正确的发布版本。
- en: How to do it...
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We extract event logs with these basic steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下基本步骤提取事件日志：
- en: Search for all event logs matching the input argument.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索与输入参数匹配的所有事件日志。
- en: Eliminate false positives with file signature verification.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文件签名验证消除误报。
- en: Process each event log found with the appropriate library.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用适当的库处理找到的每个事件日志。
- en: Output a spreadsheet of all discovered events to the current working directory.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有发现的事件输出到当前工作目录的电子表格中。
- en: How it works...
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We import a number of libraries to assist with argument parsing, writing CSVs,
    processing event logs, and the custom `pytskutil` module.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了许多库来帮助解析参数、编写CSV、处理事件日志和自定义的`pytskutil`模块。
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This recipe''s command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `TYPE`, and `LOG_NAME`, which represents the path to the evidence file, the type
    of evidence file, and the name of the event log to process. Additionally, the
    user may specify the directory within the image to scan with the `"d"` switch
    and enable fuzzy searching with the `"f"` switch. If the user does not supply
    a directory to scan, the script defaults to the `"/Windows/System32/winevt"` directory.
    The fuzzy search, when comparing file names, will check whether the suppled `LOG_NAME`
    is a substring of the `filename` rather than equal to the filename. This capability
    allows a user to search for a very specific event log or any file with an `.evt`
    or `.evtx` extension, and anything in between. After performing input validation
    checks, we pass the five arguments to the `main()` function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的命令行处理程序接受三个位置参数，`EVIDENCE_FILE`，`TYPE`和`LOG_NAME`，分别表示证据文件的路径，证据文件的类型和要处理的事件日志的名称。此外，用户可以使用`"d"`开关指定要扫描的镜像内目录，并使用`"f"`开关启用模糊搜索。如果用户没有提供要扫描的目录，脚本将默认为`"/Windows/System32/winevt"`目录。在比较文件名时，模糊搜索将检查提供的`LOG_NAME`是否是`filename`的子字符串，而不是等于文件名。这种能力允许用户搜索非常特定的事件日志或任何带有`.evt`或`.evtx`扩展名的文件，以及两者之间的任何内容。在执行输入验证检查后，我们将这五个参数传递给`main()`函数：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the `main()` function, we create our `TSKUtil` object, which we will be interacting
    with to query the existence of the user-supplied path. If the path exists and
    is not `None`, we then check whether fuzzy searching has been enabled. Regardless,
    we call the same function, `recurse_files()`, and pass it the log to search for
    and the directory to scan. If fuzzy searching was enabled, we supply the `recurse_files()`
    method an additional optional argument by setting logic to `"equal"`. Without
    specifying this optional argument, the function will check whether the log is
    a substring of a given file rather than an exact match. We store any resulting
    hits in the `event_log` variable.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main()`函数中，我们创建了我们的`TSKUtil`对象，我们将与其交互以查询用户提供的路径是否存在。如果路径存在且不为`None`，我们然后检查是否启用了模糊搜索。无论如何，我们都调用相同的`recurse_files()`函数，并将其传递要搜索的日志和要扫描的目录。如果启用了模糊搜索，我们通过将逻辑设置为`"equal"`来向`recurse_files()`方法提供一个额外的可选参数。如果不指定此可选参数，函数将检查日志是否是给定文件的子字符串，而不是精确匹配。我们将任何结果命中存储在`event_log`变量中。
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If we do have hits for the log, we set up the `event_data` list, which will
    hold the parsed event log data. Next, we begin iterating through each discovered
    event log. For each hit, we extract its file object, which is the second index
    of the tuple returned by the `recurse_files()` method, and send that to be temporarily
    written to the host filesystem with the `write_file()` method. This will be a
    common practice in further recipes so that these third-party libraries can more
    easily interact with the file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们确实有日志的命中，我们设置`event_data`列表，它将保存解析后的事件日志数据。接下来，我们开始迭代每个发现的事件日志。对于每个命中，我们提取其文件对象，这是`recurse_files()`方法返回的元组的第二个索引，并将其发送到`write_file()`方法中，暂时写入主机文件系统。这将是以后的常见做法，以便这些第三方库可以更轻松地与文件交互。
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `write_file()` method is rather simplistic. All it does is open a Python
    `File` object in `"w"` mode with the same name and write the entire contents of
    the input file to the current working directory. We return the name of this output
    file back to the `main()` method.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_file()`方法相当简单。它所做的就是以`"w"`模式打开一个Python`File`对象，并使用相同的名称将输入文件的整个内容写入当前工作目录。我们将此输出文件的名称返回给`main()`方法。'
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Back in the `main()` method, we use the `pyevt.check_file_signature()` method
    to check whether the file we just cached is a valid `evt` file. If it is, we use
    the `pyevt.open()` method to create our `evt` object. After printing a status
    message to the console, we iterate through all of the records within the event
    log. The record can have a number of strings, and so we iterate through those
    and ensure they are added to the `strings` variable. We then append a number of
    event log attributes to the `event_data` list, including the computer name, the
    SID, the creation and written time, the category, source name, event ID, event
    type, the strings, and the file path.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main()`方法中，我们使用`pyevt.check_file_signature()`方法来检查我们刚刚缓存的文件是否是有效的`evt`文件。如果是，我们使用`pyevt.open()`方法来创建我们的`evt`对象。在控制台打印状态消息后，我们迭代事件日志中的所有记录。记录可能有许多字符串，因此我们遍历这些字符串，并确保它们被添加到`strings`变量中。然后，我们将一些事件日志属性附加到`event_data`列表中，包括计算机名称、SID、创建和写入时间、类别、来源名称、事件ID、事件类型、字符串和文件路径。
- en: You may notice the empty string added as the second-to-last item in the list.
    This empty string is there due to a lack of an equivalent counterpart found in
    `.evtx` files and is necessary to maintain proper spacing as the output spreadsheet
    is designed to accommodate both `.evt` and `.evtx` results. That's all we need
    to do to process the legacy event log format. Let's now move on to the scenario
    where the log file is an `.evtx` file.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到空字符串添加为列表中倒数第二个项目。由于在`.evtx`文件中找不到等效的对应项，因此需要这个空字符串，以保持输出电子表格的正确间距，因为它设计用于容纳`.evt`和`.evtx`结果。这就是我们处理传统事件日志格式所需做的全部。现在让我们转向日志文件是`.evtx`文件的情况。
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Thankfully, both `pyevt` and `pyevtx` libraries handle similarly. We start
    by validating the file signature of the log search hit using the `pyevtx.check_file_signature()`
    method. As with its `pyevt` counterpart, this method returns a Boolean `True`
    or `False` depending on the results of the file signature check. If the file''s
    signature checks out, we use the `pyevtx.open()` method to create an `evtx` object,
    write a status message to the console, and begin iterating through the records
    present in the event log:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 值得庆幸的是，`pyevt`和`pyevtx`库的处理方式相似。我们首先使用`pyevtx.check_file_signature()`方法验证日志搜索命中的文件签名。与其`pyevt`对应项一样，该方法根据文件签名检查的结果返回布尔值`True`或`False`。如果文件的签名检查通过，我们使用`pyevtx.open()`方法创建一个`evtx`对象，在控制台写入状态消息，并开始迭代事件日志中的记录。
- en: After storing all strings into the `strings` variable, we append a number of
    event log record attributes to the event log list. These include the computer
    name, SID, written time, event level, source, event ID, strings, any XML strings,
    and the event log path. Note there are a number of empty strings, which are present
    to maintain spacing and fill gaps where an `.evt` equivalent is not fount. For
    example, there is no `creation_time` timestamp as seen in the legacy `.evt` logs,
    and therefore, an empty string replaced it instead.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在将所有字符串存储到`strings`变量后，我们将一些事件日志记录属性附加到事件日志列表中。这些属性包括计算机名称、SID、写入时间、事件级别、来源、事件ID、字符串、任何XML字符串和事件日志路径。请注意，有许多空字符串，这些空字符串用于保持间距，并填补`.evt`等效项不存在的空白。例如，在传统的`.evt`日志中看不到`creation_time`时间戳，因此用空字符串替换它。
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If the given log hit from the search cannot be validated as either a `.evt`
    or `.evtx` log, we print a status message to the console, remove the cached file
    with the `os.remove()` method, and continue onto the next hit. Note that we only
    remove cached event logs if they could not be validated. Otherwise, we leave them
    in the current working directory so as to allow the user the opportunity to process
    them further with other tools if desired. After we have finished processing all
    of the event logs, we write the parsed list of lists to a CSV with the `write_output()`
    method. The two remaining `else` statements handle situations where there are
    either no event log hits from our search or the directory we scanned for does
    not exist in the evidence file:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果从搜索中获得的日志命中无法验证为`.evt`或`.evtx`日志，则我们会向控制台打印状态消息，使用`os.remove()`方法删除缓存文件，并继续处理下一个命中。请注意，我们只会在无法验证时删除缓存的事件日志。否则，我们会将它们留在当前工作目录中，以便用户可以使用其他工具进一步处理。在处理完所有事件日志后，我们使用`write_output()`方法将解析的列表写入CSV。剩下的两个`else`语句处理了两种情况：要么搜索中没有事件日志命中，要么我们扫描的目录在证据文件中不存在。
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `write_output()` method behaves similarly to that discussed in the previous
    recipe. We create a CSV in the current working directory and write all of the
    parsed results to it using the `writerows()` method.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_output()`方法的行为与前一个示例中讨论的类似。我们在当前工作目录中创建一个CSV，并使用`writerows()`方法将所有解析的结果写入其中。'
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following screenshot shows basic information about events in the specified
    log files:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了指定日志文件中事件的基本信息：
- en: '![](../images/00109.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00109.jpeg)'
- en: 'The second screenshot shows additional columns for these rows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个截图显示了这些行的额外列：
- en: '![](../images/00110.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00110.jpeg)'
- en: There's more...
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们在这里提供了一个或多个建议：
- en: Enable loose file support
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用松散文件支持
- en: Add an event ID argument to selectively extract events only matching the given
    event ID
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加事件ID参数以选择性地提取与给定事件ID匹配的事件
- en: Indexing internet history
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 索引互联网历史
- en: 'Recipe Difficulty: Medium'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 示例难度：中等
- en: 'Python Version: 2.7'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating System: Linux'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: Internet history can be invaluable during an investigation. These records can
    give insight into a user's thought process and provide context around other user
    activity occurring on the system. Microsoft has been persistent in getting users
    to use Internet Explorer as their browser of choice. As a result, it is not uncommon
    to see internet history information present in `index.dat` files used by Internet
    Explorer. In this recipe, we scour the evidence file for these `index.dat` files
    and attempt to process them using `pymsiecf`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在调查过程中，互联网历史记录可能非常有价值。这些记录可以揭示用户的思维过程，并为系统上发生的其他用户活动提供背景。微软一直在努力让用户将Internet
    Explorer作为他们的首选浏览器。因此，在Internet Explorer使用的`index.dat`文件中经常可以看到互联网历史信息。在这个示例中，我们在证据文件中搜索这些`index.dat`文件，并尝试使用`pymsiecf`处理它们。
- en: Getting started
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'This recipe requires the installation of four third-party modules to function:
    `pytsk3`, `pyewf`, `pymsiecf`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes,* for a detailed explanation
    on installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Parsing prefetch files* recipe for details on installing
    `unicodecsv`. All other libraries used in this script are present in Python''s
    standard library'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例需要安装四个第三方模块才能运行：`pytsk3`、`pyewf`、`pymsiecf`和`unicodecsv`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，*使用取证证据容器*
    *示例*。同样，有关安装`unicodecsv`的详细信息，请参阅*解析预取文件*示例中的*入门*部分。此脚本中使用的所有其他库都包含在Python的标准库中。
- en: 'Navigate to the GitHub repository and download the desired release of the `pymsiecf`
    library. This recipe was developed using the `libmsiecf-alpha-20170116` release.
    Once the contents of the release are extracted, open a terminal and navigate to
    the extracted directory and execute the following commands:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 转到GitHub存储库并下载所需版本的`pymsiecf`库。这个示例是使用`libmsiecf-alpha-20170116`版本开发的。提取版本的内容后，打开终端并转到提取的目录，执行以下命令：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: To learn more about the `pymsiecf` library, visit [https://github.com/libyal/libmsiecf](https://github.com/libyal/libmsiecf).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`pymsiecf`库的信息，请访问[https://github.com/libyal/libmsiecf](https://github.com/libyal/libmsiecf)。
- en: Lastly, we can check our library's installation by opening a Python interpreter,
    importing `pymsiecf`, and running the `gpymsiecf.get_version()` method to ensure
    we have the correct release version.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过打开Python解释器，导入`pymsiecf`，并运行`gpymsiecf.get_version()`方法来检查我们的库是否安装了正确的版本。
- en: How to do it...
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We follow these steps to extract Internet Explorer history:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下步骤提取Internet Explorer历史记录：
- en: Find and verify all `index.dat` files within the image.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找并验证图像中的所有`index.dat`文件。
- en: Process the files for internet history.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理互联网历史文件。
- en: Output a spreadsheet of the results to the current working directory.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果输出到当前工作目录的电子表格中。
- en: How it works...
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'We import a number of libraries to assist with argument parsing, writing CSVs,
    processing `index.dat` files, and the custom `pytskutil` module:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了许多库来帮助解析参数、编写CSV、处理`index.dat`文件和自定义的`pytskutil`模块：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This recipe's command-line handler takes two positional arguments, `EVIDENCE_FILE`
    and `TYPE`, which represent the path to the evidence file and the type of evidence
    file, respectively. Similar to the previous recipe, the optional `d` switch can
    be supplied to specify a directory to scan. Otherwise, the recipe starts scanning
    at the `"/Users"` directory. After performing input validation checks, we pass
    the three arguments to the `main()` function.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序接受两个位置参数，`EVIDENCE_FILE`和`TYPE`，分别代表证据文件的路径和证据文件的类型。与之前的配方类似，可以提供可选的`d`开关来指定要扫描的目录。否则，配方将从`"/Users"`目录开始扫描。在执行输入验证检查后，我们将这三个参数传递给`main()`函数。
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `main()` function starts by creating the now-familiar `TSKUtil` object and
    scans the specified directory to confirm it exists within the evidence file. If
    it does exist, we recursively scan from the specified directory for any file that
    is equal to the string `"index.dat"`. These files are returned from the `recurse_files()`
    method as a list of tuples, where each tuple represents a particular file matching
    the search criteria.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数首先创建了一个现在熟悉的`TSKUtil`对象，并扫描指定的目录以确认它是否存在于证据文件中。如果存在，我们会从指定的目录递归扫描任何文件，这些文件等于字符串`"index.dat"`。这些文件以元组的形式从`recurse_files()`方法返回，其中每个元组代表符合搜索条件的特定文件。'
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If we do find potential `index.dat` files to process, we print a status message
    to the console and set up a list to retain the parsed results of the said files.
    We begin to iterate through hits; extract the `index.dat` file object, which is
    the second index of the tuple; and write it out to the host filesystem using the
    `write_file()` method:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找到了潜在的`index.dat`文件要处理，我们会在控制台打印状态消息，并设置一个列表来保留这些文件解析结果。我们开始遍历命中的文件；提取元组的第二个索引，即`index.dat`文件对象；并使用`write_file()`方法将其写入主机文件系统：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `write_file()` method was discussed in more detail in the previous recipe.
    It is identical to what we previously discussed. In essence, this function copies
    out the `index.dat` file in the evidence container to the current working directory
    to allow processing by the third-party module. Once that output is created, we
    return the name of the output file, which in this case is going to always be `index.dat`,
    back to the `main()` function:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_file()`方法在之前的配方中有更详细的讨论。它与我们之前讨论的内容相同。本质上，这个函数将证据容器中的`index.dat`文件复制到当前工作目录，以便第三方模块进行处理。一旦创建了这个输出，我们将输出文件的名称，这种情况下总是`index.dat`，返回给`main()`函数：'
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Similar to the other `libyal` libraries before, the `pymsiecf` module has a
    built-in method, `check_file_signature()`, which we use to determine if the search
    hit is a valid `index.dat` file. If it is, we use the `pymsiecf.open()` method
    to create an object we can manipulate with the library. We print a status message
    to the console and begin iterating through the items present in the `.dat` file.
    The very first thing we attempt is to access the `data` attribute. This contains
    the bulk of information we will be interested in but is not necessarily always
    available. If the attribute is present, however, and is not `None`, we remove
    an appended `"\x00"` value:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的`libyal`库类似，`pymsiecf`模块有一个内置方法`check_file_signature()`，我们用它来确定搜索命中是否是有效的`index.dat`文件。如果是，我们使用`pymsiecf.open()`方法创建一个可以用库操作的对象。我们在控制台打印状态消息，并开始遍历`.dat`文件中的项目。我们首先尝试访问`data`属性。这包含了我们感兴趣的大部分信息，但并不总是可用。然而，如果属性存在且不是`None`，我们会移除追加的`"\x00"`值：
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As alluded to before, there are scenarios where there will be no `data` attribute.
    Two examples are the `pymsiecf.redirected` and `pymsiecf.leak` objects. These
    objects, however, still have data that may potentially be relevant. Therefore,
    in the exception, we check if the record is an instance of one of those two objects
    and append what data is available to our list of parsed `index.dat` data. We continue
    on to the next `record` after we have appended this data to our list or if the
    record is not an instance of either of those types, except `AttributeError`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，有些情况下可能没有`data`属性。`pymsiecf.redirected`和`pymsiecf.leak`对象就是两个例子。然而，这些对象仍然可能包含相关的数据。因此，在异常情况下，我们检查记录是否是这两个对象中的一个实例，并将可用的数据追加到我们解析的`index.dat`数据列表中。在我们将这些数据追加到列表中或者记录不是这两种类型的实例时，我们继续处理下一个`record`，除非出现`AttributeError`：
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In most scenarios, the `data` attribute is present and we can extract a number
    of potentially relevant information points from the record. This includes the
    filename, the type, a number of timestamps, the location, number of hits, and
    the data itself. To be clear, the `data` attribute is often a URL of some sort
    recorded as a result of browsing activity on the system:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，`data`属性是存在的，我们可以从记录中提取许多潜在相关的信息点。这包括文件名、类型、若干时间戳、位置、命中次数和数据本身。需要明确的是，`data`属性通常是系统上浏览活动的记录的某种URL：
- en: '[PRE32]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If the `index.dat` file cannot be validated, we remove the offending cached
    file and continue iterating through all other search results. Likewise, this time
    we elect to remove the `index.dat` cached file regardless of whether it was valid
    or not after we finish processing the final one. Because all of these files will
    have the same name, they will overwrite each other as they are being processed.
    Therefore, it did mot make sense to keep only one in the current working directory
    for further processing. If desired, however, one could do something a bit more
    elaborate and cache each file to the host filesystem while preserving its path.
    The remaining two `else` statements are reserved for situations where no `index.dat`
    files are found and the directory to scan for does not exist in the evidence file,
    respectively:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法验证`index.dat`文件，我们将删除有问题的缓存文件，并继续迭代所有其他搜索结果。同样，这一次我们选择删除`index.dat`缓存文件，无论它是否有效，因为我们完成处理最后一个后。因为所有这些文件都将具有相同的名称，它们在处理过程中将相互覆盖。因此，在当前工作目录中仅保留一个文件是没有意义的。但是，如果需要，可以做一些更复杂的事情，并将每个文件缓存到主机文件系统，同时保留其路径。剩下的两个`else`语句是用于在取证文件中找不到`index.dat`文件和要扫描的目录不存在的情况：
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `write_output()` method behaves like the other methods of the same name
    in the previous recipes. We create a mildly descriptive output name, create the
    output CSV in the current working directory, and then write the headers and data
    to the file. With that, we have completed this recipe and can now add processed
    `index.dat` files to our toolbox:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_output()`方法的行为类似于前几个食谱中同名方法的行为。我们创建一个略微描述性的输出名称，在当前工作目录中创建输出CSV，然后将标题和数据写入文件。通过这样，我们已经完成了这个食谱，现在可以将处理过的`index.dat`文件添加到我们的工具箱中：'
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'When we execute the script, we can review a spreadsheet containing data such
    as the one shown here:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行脚本时，可以查看包含数据的电子表格，如下所示：
- en: '![](../images/00111.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00111.jpeg)'
- en: 'While this report has many columns, the following screenshot shows a snippet
    of a few additional columns for the same rows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这份报告有很多列，但以下截图显示了同一行的一些额外列的片段：
- en: '![](../images/00112.jpeg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00112.jpeg)'
- en: There's more...
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们在这里提供了一个或多个建议：
- en: Create summary metrics of available data (most and least popular domain visited,
    average time-frame of internet usage, and so on)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建可用数据的摘要指标（访问最受欢迎和最不受欢迎的域，互联网使用的平均时间范围等）
- en: Shadow of a former self
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前任的影子
- en: 'Recipe Difficulty: Hard'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：困难
- en: 'Python Version: 2.7'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating System: Linux'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: Volume shadow copies can contain data from files that are no longer present
    on the active system. This can give an examiner some historical information about
    how the system changed over time and what files used to exist on the computer.
    In this recipe, we will use the `pvyshadow` library to enumerate and access any
    volume shadow copies present in the forensic image.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 卷影副本可以包含来自活动系统上不再存在的文件的数据。这可以为检查人员提供一些关于系统随时间如何变化以及计算机上曾经存在哪些文件的历史信息。在这个食谱中，我们将使用`pvyshadow`库来枚举和访问取证图像中存在的任何卷影副本。
- en: Getting started
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'This recipe requires the installation of five third-party modules to function:
    `pytsk3`, `pyewf`, `pyvshadow`, `unicodecsv`, and `vss`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    on installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Parsing prefetch files* recipe for details on installing
    `unicodecsv`. All other libraries used in this script are present in Python''s
    standard library.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱需要安装五个第三方模块才能运行：`pytsk3`、`pyewf`、`pyvshadow`、`unicodecsv`和`vss`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，*使用取证证据容器*
    *食谱*。同样，有关安装`unicodecsv`的详细信息，请参阅*解析预取文件*食谱中的*入门*部分。在这个脚本中使用的所有其他库都包含在Python的标准库中。
- en: 'Navigate to the GitHub repository and download the desired release for the
    `pyvshadow` library. This recipe was developed using the `libvshadow-alpha-20170715`
    release. Once the contents of the release are extracted, open a terminal, navigate
    to the extracted directory, and execute the following commands:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到GitHub存储库并下载所需的`pyvshadow`库的发布版本。这个食谱是使用`libvshadow-alpha-20170715`版本开发的。一旦释放的内容被提取出来，打开一个终端，导航到提取的目录，并执行以下命令：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Learn more about the `pyvshadow` library at [https://github.com/libyal/libvshadow](https://github.com/libyal/libvshadow).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在[https://github.com/libyal/libvshadow](https://github.com/libyal/libvshadow)了解更多关于`pyvshadow`库的信息。
- en: The `pyvshadow` module is designed to work only with raw images and does not
    support other forensic image types out of the box. As noted in a blog post by
    *David Cowen* at [http://www.hecfblog.com/2015/05/automating-dfir-how-to-series-on_25.html](http://www.hecfblog.com/2015/05/automating-dfir-how-to-series-on_25.html),
    the plaso project has created a helper library, `vss`, that can be integrated
    with `pyvshadow`, which we will use here. The `vss` code can be found in the same
    blog post.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`pyvshadow`模块仅设计用于处理原始图像，并不支持其他取证图像类型。正如*David Cowen*在[http://www.hecfblog.com/2015/05/automating-dfir-how-to-series-on_25.html](http://www.hecfblog.com/2015/05/automating-dfir-how-to-series-on_25.html)的博客文章中所指出的，plaso项目已经创建了一个辅助库`vss`，可以与`pyvshadow`集成，我们将在这里使用。`vss`代码可以在同一篇博客文章中找到。'
- en: Lastly, we can check our library's installation by opening a Python interpreter,
    importing `pyvshadow`, and running the `pyvshadow.get_version()` method to ensure
    we have the correct release version.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过打开Python解释器，导入`pyvshadow`，并运行`pyvshadow.get_version()`方法来检查我们是否有正确的发布版本。
- en: How to do it...
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We access volume shadow copies using the following steps:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下步骤访问卷影副本：
- en: Access the volume of the raw image and identify all NTFS partitions.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问原始图像的卷并识别所有NTFS分区。
- en: Enumerate over each volume shadow copy found on valid NTFS partitions.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 枚举在有效的NTFS分区上找到的每个卷影副本。
- en: Create a file listing of data within the snapshots.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建快照内数据的文件列表。
- en: How it works...
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We import a number of libraries to assist with argument parsing, date parsing,
    writing CSVs, processing volume shadow copies, and the custom `pytskutil` module.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了许多库来帮助解析参数、日期解析、编写CSV、处理卷影副本以及自定义的`pytskutil`模块。
- en: '[PRE36]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This recipe''s command-line handler takes two positional arguments: `EVIDENCE_FILE`
    and `OUTPUT_CSV`. These represent the path to the evidence file and the file path
    for the output spreadsheet, respectively. Notice the conspicuous absence of the
    evidence type argument. This script only supports raw image files and does not
    work with `E01s`. To prepare an EWF image for use with the script you may either
    convert it to a raw image or mount it with `ewfmount`, a tool associated with
    `libewf`, and provide the mount point as the input.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本的命令行处理程序接受两个位置参数：`EVIDENCE_FILE`和`OUTPUT_CSV`。它们分别代表证据文件的路径和输出电子表格的文件路径。请注意，这里没有证据类型参数。这个脚本只支持原始镜像文件，不支持`E01s`。要准备一个EWF镜像以便与脚本一起使用，您可以将其转换为原始镜像，或者使用与`libewf`相关的`ewfmount`工具进行挂载，并将挂载点作为输入。
- en: '[PRE37]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: After parsing the input arguments, we separate the directory from the `OUTPUT_CSV`
    input and confirm that it exists or create it if it is not present. We also validate
    the input file path's existence before passing the two positional arguments to
    the `main()` function.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 解析输入参数后，我们将`OUTPUT_CSV`输入中的目录与文件分开，并确认它存在或者如果不存在则创建它。我们还在将两个位置参数传递给`main()`函数之前，验证输入文件路径的存在。
- en: '[PRE38]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `main()` function calls a few new functions within the `TSKUtil` object
    that we have not explored yet. After we create our `TSKUtil` object, we extract
    its volume using the `return_vol()` method. Interacting with an evidence file's
    volume, as we have seen in previous recipes, is one of the requisite steps before
    we can interact with the filesystem. However, this process has been previously
    performed in the background when necessary. This time, however, we need access
    to the `pytsk3` volume object to iterate through each partition to identify NTFS
    filesystems. The `detect_ntfs()` method returns a Boolean value if the specific
    partition has an NTFS filesystem.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数调用了`TSKUtil`对象中的一些新函数，我们还没有探索过。创建了`TSKUtil`对象后，我们使用`return_vol()`方法提取它的卷。与证据文件的卷交互，正如我们在之前的示例中看到的那样，是在我们可以与文件系统交互之前必不可少的步骤之一。然而，这个过程以前在必要时已经在后台执行过。然而，这一次，我们需要访问`pytsk3`卷对象，以便遍历每个分区以识别NTFS文件系统。`detect_ntfs()`方法返回一个布尔值，指示特定分区是否有NTFS文件系统。'
- en: For each NTFS filesystem we encounter, we pass the evidence file, the offset
    of the discovered NTFS partition, and the output CSV file to the `explore_vss()`
    function. If the volume object is `None`, we print a status message to the console
    to remind users that the evidence file must be a physical device image as opposed
    to only a logical image of a specific partition.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们遇到的每个NTFS文件系统，我们将证据文件、发现的NTFS分区的偏移量和输出CSV文件传递给`explore_vss()`函数。如果卷对象是`None`，我们会在控制台打印状态消息，提醒用户证据文件必须是物理设备镜像，而不仅仅是特定分区的逻辑镜像。
- en: '[PRE39]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `explore_vss()` method starts by creating a `pyvshadow.volume()` object.
    We use this volume to open the `vss_handle` object created from the `vss.VShadowVolume()`
    method. The `vss.VShadowVolume()` method takes the evidence file and the partition
    offset value and exposes a volume-like object that is compatible with the `pyvshadow`
    library, which does not natively support physical disk images. The `GetVssStoreCount()`
    function returns the number of volume shadow copies found in the evidence.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`explore_vss()`方法首先创建一个`pyvshadow.volume()`对象。我们使用这个卷来打开从`vss.VShadowVolume()`方法创建的`vss_handle`对象。`vss.VShadowVolume()`方法接受证据文件和分区偏移值，并公开一个类似卷的对象，与`pyvshadow`库兼容，该库不原生支持物理磁盘镜像。`GetVssStoreCount()`函数返回在证据中找到的卷影副本的数量。'
- en: If there are volume shadows, we open our `vss_handle` object with the `pyvshadow
    vss_volume` and instantiate a list to hold our data. We create a `for` loop to
    iterate through each volume shadow copy present and perform the same series of
    steps. First, we use the `pyvshadow get_store()` method to access the particular
    volume shadow copy of interest. Then, we use the `vss` helper library `VShadowImgInfo`
    to create a `pytsk3` image handle. Lastly, we pass the image handle to the `openVSSFS()`
    method and append the returned data to our list. The `openVSSFS()` method uses
    similar methods as discussed before to create a `pytsk3` filesystem object and
    then recurse through the directories present to return an active file listing.
    After we have performed these steps on all of the volume shadow copies, we pass
    the data and the output CSV file path to our `csvWriter()` method.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有卷影副本，我们使用`pyvshadow vss_volume`打开我们的`vss_handle`对象，并实例化一个列表来保存我们的数据。我们创建一个`for`循环来遍历每个存在的卷影副本，并执行相同的一系列步骤。首先，我们使用`pyvshadow
    get_store()`方法访问感兴趣的特定卷影副本。然后，我们使用`vss`辅助库`VShadowImgInfo`来创建一个`pytsk3`图像句柄。最后，我们将图像句柄传递给`openVSSFS()`方法，并将返回的数据追加到我们的列表中。`openVSSFS()`方法使用与之前讨论过的类似方法来创建一个`pytsk3`文件系统对象，然后递归遍历当前目录以返回一个活动文件列表。在我们对所有卷影副本执行了这些步骤之后，我们将数据和输出CSV文件路径传递给我们的`csvWriter()`方法。
- en: '[PRE40]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `write_csv()` method functions as you would expect it to. It first checks
    if there is any data to write. If there isn't, it prints a status message to the
    console before exiting the script. Alternatively, it creates a CSV file using
    the user-provided input, writes the spreadsheet headers, and iterates through
    each list, calling `writerows()` for each volume shadow copy. To prevent the headers
    from ending up several times in the CSV output, we will check to see if the CSV
    already exists and add new data in for review. This allows us to dump information
    after each volume is processed for volume shadow copies.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_csv()`方法的功能与您期望的一样。它首先检查是否有要写入的数据。如果没有，它会在退出脚本之前在控制台上打印状态消息。或者，它使用用户提供的输入创建一个CSV文件，写入电子表格标题，并遍历每个列表，为每个卷影复制调用`writerows()`。为了防止标题多次出现在CSV输出中，我们将检查CSV是否已经存在，并添加新数据进行审查。这使我们能够在处理每个卷影副本后转储信息。'
- en: '[PRE41]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'After running this script, we can review the files found within each volume
    shadow copy and learn about the metadata of each item:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此脚本后，我们可以查看每个卷影副本中找到的文件，并了解每个项目的元数据：
- en: '![](../images/00113.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00113.jpeg)'
- en: There's more...
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们在这里提供了一个或多个建议：
- en: Add support for logical acquisitions and additional forensic acquisition types
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加对逻辑获取和其他取证获取类型的支持
- en: Add support to process artifacts found within snapshots using previously written
    recipes
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加支持以处理先前编写的配方中发现的快照中的工件
- en: Dissecting the SRUM database
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解剖SRUM数据库
- en: 'Recipe Difficulty: Hard'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 配方难度：困难
- en: 'Python Version: 2.7'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating System: Linux'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: With the major release of popular operating systems, everyone in the cyber community
    gets excited (or worried) about the potential new artifacts and changes to existing
    artifacts. With the advent of Windows 10, we saw a few changes (such as the MAM
    compression of prefetch files) and new artifacts as well. One of these artifacts
    is the **System Resource Usage Monitor** (**SRUM**), which can retain execution
    and network activity for applications. This includes information such as when
    a connection was established by a given application and how many bytes were sent
    and received by this application. Obviously, this can be very useful in a number
    of different scenarios. Imagine having this information on hand with a disgruntled
    employee who uploads many gigabytes of data on their last day using the Dropbox
    desktop application.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 随着流行操作系统的主要发布，网络社区中的每个人都对潜在的新工件和现有工件的变化感到兴奋（或担忧）。随着Windows 10的出现，我们看到了一些变化（例如对预取文件的MAM压缩）以及新的工件。其中一个工件是**系统资源使用监视器**（**SRUM**），它可以保留应用程序的执行和网络活动。这包括诸如特定应用程序建立连接的时间以及此应用程序发送和接收的字节数等信息。显然，在许多不同的情况下，这可能非常有用。想象一下，在最后一天使用Dropbox桌面应用程序上传了许多千兆字节数据的不满员工手头有这些信息。
- en: In this recipe, we leverage the `pyesedb` library to extract data from the database.
    We will also implement logic to interpret this data as the appropriate type. With
    this accomplished, we will be able to view historical application information
    stored within the `SRUM.dat` file found on Windows 10 machines.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们利用`pyesedb`库从数据库中提取数据。我们还将实现逻辑来解释这些数据为适当的类型。完成这些后，我们将能够查看存储在Windows
    10机器上的`SRUM.dat`文件中的历史应用程序信息。
- en: To learn more about the SRUM database, visit [https://www.sans.org/summit-archives/file/summit-archive-1492184583.pdf](https://www.sans.org/summit-archives/file/summit-archive-1492184583.pdf).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关SRUM数据库的更多信息，请访问[https://www.sans.org/summit-archives/file/summit-archive-1492184583.pdf](https://www.sans.org/summit-archives/file/summit-archive-1492184583.pdf)。
- en: Getting started
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'This recipe requires the installation of four third-party modules to function:
    `pytsk3`, `pyewf`, `pyesedb`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes,* for a detailed explanation
    on installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Parsing prefetch files* recipe for details on installing
    `unicodecsv`. All other libraries used in this script are present in Python''s
    standard library.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方需要安装四个第三方模块才能运行：`pytsk3`，`pyewf`，`pyesedb`和`unicodecsv`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，*使用取证证据容器*
    *配方*。同样，有关安装`unicodecsv`的详细信息，请参阅*解析预取文件*配方中的*入门*部分。此脚本中使用的所有其他库都包含在Python的标准库中。
- en: 'Navigate to the GitHub repository and download the desired release for each
    library. This recipe was developed using the `libesedb-experimental-20170121`
    release. Once the contents of the release are extracted, open a terminal, navigate
    to the extracted directory, and execute the following commands:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到GitHub存储库，并下载每个库的所需版本。此配方是使用`libesedb-experimental-20170121`版本开发的。提取发布的内容后，打开终端，导航到提取的目录，并执行以下命令：
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: To learn more about the `pyesedb` library, visit [**https://github.com/libyal/libesedb**](https://github.com/libyal/libesedb)**.**Lastly,
    we can check our library's installation by opening a Python interpreter, importing
    `pyesedb`, and running the `gpyesedb.get_version()` method to ensure we have the
    correct release version.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关`pyesedb`库的更多信息，请访问[**https://github.com/libyal/libesedb**](https://github.com/libyal/libesedb)**。**最后，我们可以通过打开Python解释器，导入`pyesedb`，并运行`gpyesedb.get_version()`方法来检查我们的库安装，以确保我们有正确的发布版本。
- en: How to do it...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We use the following methodology to accomplish our objective:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下方法来实现我们的目标：
- en: Determine if the `SRUDB.dat` file exists and perform a file signature verification.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定`SRUDB.dat`文件是否存在并执行文件签名验证。
- en: Extract tables and table data using `pyesedb.`
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pyesedb`提取表和表数据。
- en: Interpret extracted table data as appropriate data types.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据适当的数据类型解释提取的表数据。
- en: Create multiple spreadsheets for each table present within the database.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为数据库中的每个表创建多个电子表格。
- en: How it works...
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'We import a number of libraries to assist with argument parsing, date parsing,
    writing CSVs, processing the ESE database, and the custom `pytskutil` module:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了许多库来帮助解析参数、日期解析、编写 CSV、处理 ESE 数据库和自定义的 `pytskutil` 模块：
- en: '[PRE43]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This script uses two global variables during its execution. The `TABLE_LOOKUP`
    variable is a lookup table matching various SRUM table names to a more human-friendly
    description. These descriptions were pulled from *Yogesh Khatri's* presentation,
    referenced at the beginning of the recipe. The `APP_ID_LOOKUP` dictionary will
    store data from the SRUM `SruDbIdMapTable` table, which assigns applications to
    an integer value referenced in other tables.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本在执行过程中使用了两个全局变量。`TABLE_LOOKUP` 变量是一个查找表，将各种 SRUM 表名与更人性化的描述匹配。这些描述是从 *Yogesh
    Khatri* 的演示文稿中提取的，该演示文稿在配方开头引用。`APP_ID_LOOKUP` 字典将存储来自 SRUM `SruDbIdMapTable`
    表的数据，该表将应用程序分配给其他表中引用的整数值。
- en: '[PRE44]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This recipe's command-line handler takes two positional arguments, `EVIDENCE_FILE`
    and `TYPE`, which represent the evidence file and the type of evidence file, respectively.
    After validating the provided arguments, we pass these two inputs to the `main()`
    method, where the action kicks off.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序接受两个位置参数，`EVIDENCE_FILE` 和 `TYPE`，分别表示证据文件和证据文件的类型。在验证提供的参数后，我们将这两个输入传递给
    `main()` 方法，动作就此开始。
- en: '[PRE45]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The `main()` method starts by creating a `TSKUtil` object and creating a variable
    to reference the folder which contains the SRUM database on Windows 10 systems.
    Then, we use the `query_directory()` method to determine if the directory exist.
    If it does, we use the `recurse_files()` method to return the SRUM database from
    the evidence (if present):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()` 方法首先创建一个 `TSKUtil` 对象，并创建一个变量来引用包含 Windows 10 系统上 SRUM 数据库的文件夹。然后，我们使用
    `query_directory()` 方法来确定目录是否存在。如果存在，我们使用 `recurse_files()` 方法从证据中返回 SRUM 数据库（如果存在）：'
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'If we do find the SRUM database, we print a status message to the console and
    iterate through each hit. For each hit, we extract the file object stored in the
    second index of the tuple returned by the `recurse_files()` method and use the
    `write_file()` method to cache the file to the host filesystem for further processing:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找到了 SRUM 数据库，我们会在控制台打印状态消息，并遍历每个命中。对于每个命中，我们提取存储在 `recurse_files()` 方法返回的元组的第二个索引中的文件对象，并使用
    `write_file()` 方法将文件缓存到主机文件系统以进行进一步处理：
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The `write_file()` method, as seen before, simply creates a file of the same
    name on the host filesystem. This method reads the entire contents of the file
    in the evidence container and writes it to the temporary file. After this has
    completed, it returns the name of the file to the parent function.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_file()` 方法，如前所述，只是在主机文件系统上创建一个同名文件。该方法读取证据容器中文件的全部内容，并将其写入临时文件。完成后，它将文件的名称返回给父函数。'
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Back in the `main()` method, we use the `pyesedb.check_file_signature()` method
    to validate the file hit before proceeding with any further processing. After
    the file is validated, we use the `pyesedb.open()` method to create the `pyesedb`
    object and print a status message to the console with the number of tables contained
    within the file. Next, we create a `for` loop to iterate through all of the tables
    within the database. Specifically, we look for the `SruDbIdMapTable` as we first
    need to populate the `APP_ID_LOOKUP` dictionary with the integer-to-application
    name pairings before processing any other table.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 `main()` 方法，我们使用 `pyesedb.check_file_signature()` 方法验证文件命中，然后再进行任何进一步处理。验证文件后，我们使用
    `pyesedb.open()` 方法创建 `pyesedb` 对象，并在控制台上打印包含在文件中的表的数量的状态消息。接下来，我们创建一个 `for` 循环来遍历数据库中的所有表。具体来说，我们首先寻找
    `SruDbIdMapTable`，因为我们首先需要使用整数到应用程序名称的配对来填充 `APP_ID_LOOKUP` 字典，然后再处理任何其他表。
- en: Once that table is found, we read each record within the table. The integer
    value of interest is stored in the first index while the application name is stored
    in the second index. We use the `get_value_data_as_integer()` method to extract
    and interpret the integer appropriately. Using the `get_value_data()` method instead,
    we can extract the application name from the record and attempt to replace any
    padding bytes from the string. Finally, we store both of these values in the global
    `APP_ID_LOOKUP` dictionary, using the integer as a key and the application name
    as the value.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到该表，我们就会读取表中的每条记录。感兴趣的整数值存储在第一个索引中，而应用程序名称存储在第二个索引中。我们使用 `get_value_data_as_integer()`
    方法来提取和适当解释整数。而使用 `get_value_data()` 方法，我们可以从记录中提取应用程序名称，并尝试替换字符串中的任何填充字节。最后，我们将这两个值存储在全局的
    `APP_ID_LOOKUP` 字典中，使用整数作为键，应用程序名称作为值。
- en: '[PRE49]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: After creating the `app lookup` dictionary, we are ready to iterate over each
    table (again) and this time actually extract the data. For each table, we assign
    its name to a local variable and print a status message to the console regarding
    execution progress. Then, within the dictionary that will hold our processed data,
    we create a key using the table's name and a dictionary containing column and
    data lists. The column list represents the actual column names from the table
    itself. These are extracted using list comprehension and then assigned to the
    column's key within our dictionary structure.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `app lookup` 字典后，我们准备再次遍历每个表，并实际提取数据。对于每个表，我们将其名称分配给一个本地变量，并在控制台上打印有关执行进度的状态消息。然后，在将保存我们处理过的数据的字典中，我们使用表的名称创建一个键，以及包含列和数据列表的字典。列列表表示表本身的实际列名。这些是使用列表推导提取的，然后分配给我们字典结构中列的键。
- en: '[PRE50]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: With the columns handle, we turn our attention to the data itself. As we iterate
    through each row in the table, we use the `number_of_values()` method to create
    a loop to iterate through each value in the row. As we do this, we append the
    interpreted value to a list, which itself is later assigned to the data key within
    the dictionary. The SRUM database stores a number of different types of data (`32-bit`
    integers, `64-bit` integers, strings, and so on). The `pyesedb` library does not
    necessarily support each data type present using the various `get_value_as` methods.
    We must interpret the data for ourselves and have created a new function, `convert_data()`,
    to do just that. This function needs the value's raw data, the column name, and
    the column's type (which correlates to the type of data present). Let's focus
    on this method now.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完列后，我们将注意力转向数据本身。当我们迭代表中的每一行时，我们使用`number_of_values()`方法创建一个循环来迭代行中的每个值。在这样做时，我们将解释后的值附加到列表中，然后将列表本身分配给字典中的数据键。SRUM数据库存储多种不同类型的数据（`32位`整数、`64位`整数、字符串等）。`pyesedb`库并不一定支持每种数据类型，使用各种`get_value_as`方法。我们必须自己解释数据，并创建了一个新函数`convert_data()`来做到这一点。现在让我们专注于这个方法。
- en: If the search hit fails the file signature verification, we print a status message
    to the console, delete the temporary file, and continue onto the next hit. The
    remaining `else` statements handle scenarios where there are no SRUM databases
    found and where the SRUM database directory does not exist, respectively.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果搜索失败，文件签名验证，我们将在控制台打印状态消息，删除临时文件，并继续下一个搜索。其余的`else`语句处理了未找到SRUM数据库和SRUM数据库目录不存在的情况。
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The `convert_data()` method relies on the column type to dictate how the data
    should be interpreted. For the most part, we use `struct` to unpack the data as
    the appropriate data types. This function is one large `if-elif-else` statement.
    In the first scenario, we check if the data is `None`, and if it is, return an
    empty string. In the first `elif` statement, we check if the column name is `"AppId"`;
    if it is, we unpack the `32-bit` integer representing the value from the `SruDbIdMapTable`,
    which correlates to an application name. We return the proper application name
    using the global `APP_ID_LOOKUP` dictionary created previously. Next, we create
    cases for various column values to return the appropriate data types, such as
    `8-bit` unsigned integers, `16-` and `32-bit` signed integers, `32-bit` floats,
    and `64-bit` doubles.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`convert_data()`方法依赖于列类型来决定如何解释数据。在大多数情况下，我们使用`struct`来解压数据为适当的数据类型。这个函数是一个大的`if-elif-else`语句。在第一种情况下，我们检查数据是否为`None`，如果是，返回一个空字符串。在第一个`elif`语句中，我们检查列名是否为`"AppId"`；如果是，我们解压代表值的`32位`整数，该值来自`SruDbIdMapTable`，对应一个应用程序名称。我们使用之前创建的全局`APP_ID_LOOKUP`字典返回正确的应用程序名称。接下来，我们为各种列值创建情况，返回适当的数据类型，如`8位`无符号整数、`16位`和`32位`有符号整数、`32位`浮点数和`64位`双精度浮点数。'
- en: '[PRE52]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Continuing where the other paragraph left off, we have an `OLE` timestamp when
    the column type is equal to `8`. We must unpack the value as a `64-bit` integer
    and then use the `convert_ole()` method to convert this to a `datetime` object.
    Column types `5`, `9`, `10`, `12`, `13`, and `16` are returned as is without any
    additional processing. Most of the other `elif` statements use different `struct`
    format characters to interpret the data appropriately. Column type `15` can also
    be a timestamp or a `64-bit` integer. Therefore, specific to the SRUM database,
    we check if the column name is either `"EventTimestamp"` or `"ConnectStartTime"`,
    in which case the value is a `FILETIME` timestamp and must be converted. Regardless
    of the column type, suffice to say that it is handled here and returned to the
    `main()` method as the appropriate type.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接着上一段，当列类型等于`8`时，我们有一个`OLE`时间戳。我们必须将该值解压为`64位`整数，然后使用`convert_ole()`方法将其转换为`datetime`对象。列类型`5`、`9`、`10`、`12`、`13`和`16`返回为原始值，无需额外处理。大多数其他`elif`语句使用不同的`struct`格式字符来适当解释数据。列类型`15`也可以是时间戳或`64位`整数。因此，针对SRUM数据库，我们检查列名是否为`"EventTimestamp"`或`"ConnectStartTime"`，在这种情况下，该值是`FILETIME`时间戳，必须进行转换。无论列类型如何，可以肯定的是在这里处理并将其作为适当的类型返回到`main()`方法中。
- en: 'Enough of this; let''s go look at these timestamp conversion methods:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 够了，让我们去看看这些时间戳转换方法：
- en: '[PRE53]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: To learn more about the ESE database column types, visit [https://github.com/libyal/libesedb/blob/b5abe2d05d5342ae02929c26475774dbb3c3aa5d/include/libesedb/definitions.h.in](https://github.com/libyal/libesedb/blob/b5abe2d05d5342ae02929c26475774dbb3c3aa5d/include/libesedb/definitions.h.in).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关ESE数据库列类型的更多信息，请访问[https://github.com/libyal/libesedb/blob/b5abe2d05d5342ae02929c26475774dbb3c3aa5d/include/libesedb/definitions.h.in](https://github.com/libyal/libesedb/blob/b5abe2d05d5342ae02929c26475774dbb3c3aa5d/include/libesedb/definitions.h.in)。
- en: The `convert_filetime()` method takes an integer and attempts to convert it
    using the tried-and-true method shown before. We have observed scenarios where
    the input integer can be too large for the `datetime` method and have added some
    error handling for that scenario. Otherwise, this method is similar to what has
    been previously discussed.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`convert_filetime()`方法接受一个整数，并尝试使用之前展示的经过验证的方法进行转换。我们观察到输入整数可能太大，超出`datetime`方法的范围，并为这种情况添加了一些错误处理。否则，该方法与之前讨论的类似。'
- en: '[PRE54]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: New to any of our recipes is the `convert_ole()` method. The `OLE` timestamp
    format is a floating point number representing the number of days since midnight
    of December 30, 1899\. We take the `64-bit` integer supplied to the function and
    pack and unpack it into the appropriate format required for the date conversion.
    Then, we use the familiar process, with `datetime` specifying our epoch and `timedelta`
    to provide the appropriate offset. If we find this value to be too large, we catch
    the `OverflowError` and return the `64-bit` integer as is.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的任何食谱中都是`convert_ole()`方法。`OLE`时间戳格式是一个浮点数，表示自1899年12月30日午夜以来的天数。我们将传递给函数的`64位`整数打包和解包为日期转换所需的适当格式。然后，我们使用熟悉的过程，使用`datetime`指定我们的时代和`timedelta`来提供适当的偏移量。如果我们发现这个值太大，我们捕获`OverflowError`并将`64位`整数原样返回。
- en: '[PRE55]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: To learn more about common timestamp formats (including `ole`), visit [https://blogs.msdn.microsoft.com/oldnewthing/20030905-02/?p=42653](https://blogs.msdn.microsoft.com/oldnewthing/20030905-02/?p=42653).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多常见的时间戳格式（包括`ole`），请访问[https://blogs.msdn.microsoft.com/oldnewthing/20030905-02/?p=42653](https://blogs.msdn.microsoft.com/oldnewthing/20030905-02/?p=42653)。
- en: The `write_output()` method is called for every table in the database. We check
    the dictionary and return the function if there are no results for the given table.
    As long as we do have results, we create an output name to distinguish the SRUM
    table and create it in the current working directory. We then open the spreadsheet,
    create the CSV writer, and then write the columns and data to the spreadsheet
    using the `writerow()` and `writerows()` methods, respectively.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据库中的每个表，都会调用`write_output()`方法。我们检查字典，如果给定表没有结果，则返回该函数。只要我们有结果，我们就会创建一个输出名称来区分SRUM表，并将其创建在当前工作目录中。然后，我们打开电子表格，创建CSV写入器，然后使用`writerow()`和`writerows()`方法将列和数据写入电子表格。
- en: '[PRE56]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'After running the code, we can review the extracted values in the spreadsheets.
    The following two screenshots display the first few values found in our application
    resource usage report:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们可以在电子表格中查看提取出的数值。以下两个屏幕截图显示了我们应用程序资源使用报告中找到的前几个数值：
- en: '![](../images/00114.jpeg)![](../images/00115.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00114.jpeg)![](../images/00115.jpeg)'
- en: There's more...
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们在这里提供了一个或多个建议：
- en: Further research the file format and extend support for other information of
    interest via this recipe
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过这个方法进一步研究文件格式，并扩展对其他感兴趣信息的支持
- en: Check out `srum-dump` by Mark Baggett ([https://github.com/MarkBaggett/srum-dump](https://github.com/MarkBaggett/srum-dump))
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Mark Baggett的`srum-dump`（[https://github.com/MarkBaggett/srum-dump](https://github.com/MarkBaggett/srum-dump)）
- en: Conclusion
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Whether this was your first time using Python, or you have employed it numerous
    times before, you can see how the correct code can make all the difference in
    your investigative process. Python gives you the ability to effectively sift through
    large datasets and more effectively find that proverbial smoking gun in an investigation.
    As you continue developing, you'll find automation becomes second nature and you're
    many times more productive because of it.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 无论这是你第一次使用Python，还是之前多次使用过，你都可以看到正确的代码如何在调查过程中起到重要作用。Python让你能够有效地筛选大型数据集，并更有效地找到调查中的关键信息。随着你的发展，你会发现自动化变得自然而然，因此你的工作效率会提高很多倍。
- en: The quote "While we teach, we learn", attributed to Roman philosopher Seneca,
    is fitting here, even if a computer was not originally thought of as the subject
    being taught at the quote's conception. But it is apropos, writing code helps
    refine your knowledge of a given artifact by requiring you to understand its structure
    and content at a deeper level.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 引用“当我们教学时，我们在学习”归因于罗马哲学家塞内卡，即使在引用的概念中最初并没有将计算机作为教学的主题。但写代码有助于通过要求你更深入地理解其结构和内容来完善你对给定工件的知识。
- en: 'We hope you have learned a lot and continue to do so. There are a plethora
    of freely available resources worth checking out and open source projects to work
    on to better hone your skills. If there''s one thing you should have learned from
    this book: how to write an amazing CSV writer. But, really, we hope through these
    examples you''ve developed a better feel for when and how to use Python to your
    advantage. Good luck cookin''.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望你已经学到了很多，并且会继续学习。有大量免费资源值得查看和开源项目可以帮助你更好地磨练技能。如果有一件事你应该从这本书中学到：如何编写一个了不起的CSV写入器。但是，真的，我们希望通过这些例子，你已经更好地掌握了何时以及如何利用Python来发挥你的优势。祝你好运。
