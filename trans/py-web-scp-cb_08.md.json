["```py\nimport json\nimport requests\n\nraw_json = requests.get(\"http://www.freegeoip.net/json/63.153.113.92\").text\nparsed = json.loads(raw_json) print(json.dumps(parsed, indent=4, sort_keys=True)) \n```", "```py\n{\n    \"city\": \"Deer Lodge\",\n    \"country_code\": \"US\",\n    \"country_name\": \"United States\",\n    \"ip\": \"63.153.113.92\",\n    \"latitude\": 46.3797,\n    \"longitude\": -112.7202,\n    \"metro_code\": 754,\n    \"region_code\": \"MT\",\n    \"region_name\": \"Montana\",\n    \"time_zone\": \"America/Denver\",\n    \"zip_code\": \"59722\"\n}\n```", "```py\nReading page: https://en.wikipedia.org/w/index.php?title=Web_scraping&offset=&limit=500&action=history\nGot 106 ip addresses\n{'ip': '2601:647:4a04:86d0:1cdf:8f8a:5ca5:76a0', 'country_code': 'US', 'country_name': 'United States', 'region_code': 'CA', 'region_name': 'California', 'city': 'Sunnyvale', 'zip_code': '94085', 'time_zone': 'America/Los_Angeles', 'latitude': 37.3887, 'longitude': -122.0188, 'metro_code': 807}\n{'ip': '194.171.56.13', 'country_code': 'NL', 'country_name': 'Netherlands', 'region_code': '', 'region_name': '', 'city': '', 'zip_code': '', 'time_zone': 'Europe/Amsterdam', 'latitude': 52.3824, 'longitude': 4.8995, 'metro_code': 0}\n{'ip': '109.70.55.226', 'country_code': 'DK', 'country_name': 'Denmark', 'region_code': '85', 'region_name': 'Zealand', 'city': 'Roskilde', 'zip_code': '4000', 'time_zone': 'Europe/Copenhagen', 'latitude': 55.6415, 'longitude': 12.0803, 'metro_code': 0}\n{'ip': '101.177.247.131', 'country_code': 'AU', 'country_name': 'Australia', 'region_code': 'TAS', 'region_name': 'Tasmania', 'city': 'Lenah Valley', 'zip_code': '7008', 'time_zone': 'Australia/Hobart', 'latitude': -42.8715, 'longitude': 147.2751, 'metro_code': 0}\n\n```", "```py\nif __name__ == \"__main__\":\n  geo_ips = collect_geo_ips('Web_scraping', 500)\n  for geo_ip in geo_ips:\n  print(geo_ip)\n  with open('geo_ips.json', 'w') as outfile:\n  json.dump(geo_ips, outfile)\n```", "```py\ndef collect_geo_ips(article_title, limit):\n  ip_addresses = get_history_ips(article_title, limit)\n  print(\"Got %s ip addresses\" % len(ip_addresses))\n  geo_ips = get_geo_ips(ip_addresses)\n  return geo_ips\n```", "```py\ndef get_history_ips(article_title, limit):\n  history_page_url = \"https://en.wikipedia.org/w/index.php?title=%s&offset=&limit=%s&action=history\" % (article_title, limit)\n  print(\"Reading page: \" + history_page_url)\n  html = requests.get(history_page_url).text\n    soup = BeautifulSoup(html, \"lxml\")    anon_ip_anchors = soup.findAll(\"a\", {\"class\": \"mw-anonuserlink\"})\n  addresses = set()\n  for ip in anon_ip_anchors:\n  addresses.add(ip.get_text())\n  return addresses\n```", "```py\ndef get_geo_ips(ip_addresses):\n  geo_ips = []\n  for ip in ip_addresses:\n  raw_json = requests.get(\"http://www.freegeoip.net/json/%s\" % ip).text\n        parsed = json.loads(raw_json)\n  geo_ips.append(parsed)\n  return geo_ips\n```", "```py\n>>> import pandas as pd\n>>> import matplotlib.pyplot as plt\n```", "```py\n>>> df = pd.read_json(\"geo_ips.json\") >>> df[:5]) city country_code country_name ip latitude \\\n0 Hanoi VN Vietnam 118.70.248.17 21.0333 \n1 Roskilde DK Denmark 109.70.55.226 55.6415 \n2 Hyderabad IN India 203.217.144.211 17.3753 \n3 Prague CZ Czechia 84.42.187.252 50.0833 \n4 US United States 99.124.83.153 37.7510\n\nlongitude metro_code region_code region_name time_zone \\\n0 105.8500 0 HN Thanh Pho Ha Noi Asia/Ho_Chi_Minh \n1 12.0803 0 85 Zealand Europe/Copenhagen \n2 78.4744 0 TG Telangana Asia/Kolkata \n3 14.4667 0 10 Hlavni mesto Praha Europe/Prague \n4 -97.8220 0\nzip_code \n0 \n1 4000 \n2 \n3 130 00 \n4\n```", "```py\n>>> countries_only = df.country_code\n>>> countries_only[:5]\n\n0 VN\n1 DK\n2 IN\n3 CZ\n4 US\nName: country_code, dtype:object\n```", "```py\n>>> counts = df.groupby('country_code').country_code.count().sort_values(ascending=False) >>> counts[:5]\n\ncountry_code\nUS 28\nIN 12\nBR 7\nNL 7\nRO 6\nName: country_code, dtype: int64 \n```", "```py\ncounts.plot(kind='bar') plt.show()\n```", "```py\nfrom wordcloud import WordCloud\nfrom nltk.probability import FreqDist\n```", "```py\nfreq_dist = FreqDist(cleaned) wordcloud = WordCloud(width=1200, height=800).generate_from_frequencies(freq_dist) \n```", "```py\nimport matplotlib.pyplot as plt\nplt.imshow(wordcloud, interpolation='bilinear') plt.axis(\"off\") plt.show()\n```", "```py\n/Users/michaelheydt/anaconda/bin/python3.6 /Users/michaelheydt/Dropbox/Packt/Books/PyWebScrCookbook/code/py/08/05_wikipedia_scrapy.py\nparsing: https://en.wikipedia.org/wiki/Python_(programming_language)\nparsing: https://en.wikipedia.org/wiki/C_(programming_language)\nparsing: https://en.wikipedia.org/wiki/Object-oriented_programming\nparsing: https://en.wikipedia.org/wiki/Ruby_(programming_language)\nparsing: https://en.wikipedia.org/wiki/Go_(programming_language)\nparsing: https://en.wikipedia.org/wiki/Java_(programming_language)\n------------------------------------------------------------\n0 Python_(programming_language) C_(programming_language)\n0 Python_(programming_language) Java_(programming_language)\n0 Python_(programming_language) Go_(programming_language)\n0 Python_(programming_language) Ruby_(programming_language)\n0 Python_(programming_language) Object-oriented_programming\n```", "```py\nprocess = CrawlerProcess({\n    'LOG_LEVEL': 'ERROR',\n    'DEPTH_LIMIT': 1 })\n\nprocess.crawl(WikipediaSpider)\nspider = next(iter(process.crawlers)).spider\nprocess.start()\n```", "```py\nprint(\"-\"*60)\n\nfor pm in spider.linked_pages:\n    print(pm.depth, pm.title, pm.child_title)\n```", "```py\nclass WikipediaSpider(Spider):\n    name = \"wikipedia\"\n  start_urls = [ \"https://en.wikipedia.org/wiki/Python_(programming_language)\" ]\n```", "```py\npage_map = {}\nlinked_pages = []\nmax_items_per_page = 5 max_crawl_depth = 1\n```", "```py\ndef parse(self, response):\n    print(\"parsing: \" + response.url)\n\n    links = response.xpath(\"//*/a[starts-with(@href, '/wiki/')]/@href\")\n\n    link_counter = {}\n```", "```py\nclass LinkReferenceCount:\n    def __init__(self, link):\n        self.link = link\n  self.count = 0\n```", "```py\nfor l in links:\n    link = l.root\n    if \":\" not in link and \"International\" not in link and link != self.start_urls[0]:\n        if link not in link_counter:\n            link_counter[link] = LinkReferenceCount(link)\n        link_counter[link].count += 1\n```", "```py\nreferences = list(link_counter.values())\ns = sorted(references, key=lambda x: x.count, reverse=True)\ntop = s[:self.max_items_per_page]\n```", "```py\nclass PageToPageMap:\n    def __init__(self, link, child_link, depth): #, parent):\n  self.link = link\n  self.child_link = child_link\n  self.title = self.get_page_title(self.link)\n        self.child_title = self.get_page_title(self.child_link)\n        self.depth = depth    def get_page_title(self, link):\n        parts = link.split(\"/\")\n        last = parts[len(parts)-1]\n        label = urllib.parse.unquote(last)\n        return label\n```", "```py\nfor item in top:\n    new_request = Request(\"https://en.wikipedia.org\" + item.link,\n                          callback=self.parse, meta={ \"parent\": pm })\n    yield new_request\n```", "```py\ndepth = 0 if \"parent\" in response.meta:\n    parent = response.meta[\"parent\"]\n    depth = parent.depth + 1\n```", "```py\nmeta={ \"parent\": pm }\n```", "```py\ncrawl_depth = 1 process = CrawlerProcess({\n    'LOG_LEVEL': 'ERROR',\n    'DEPTH_LIMIT': crawl_depth\n})\nprocess.crawl(WikipediaSpider)\nspider = next(iter(process.crawlers)).spider\nspider.max_items_per_page = 5 spider.max_crawl_depth = crawl_depth\nprocess.start()\n\nfor pm in spider.linked_pages:\n    print(pm.depth, pm.link, pm.child_link)\nprint(\"-\"*80)\n```", "```py\ng = nx.Graph()\n```", "```py\nnodes = {}\nedges = {}\n\nfor pm in spider.linked_pages:\n    if pm.title not in nodes:\n        nodes[pm.title] = pm\n        g.add_node(pm.title)\n\n    if pm.child_title not in nodes:\n        g.add_node(pm.child_title)\n\n    link_key = pm.title + \" ==> \" + pm.child_title\n    if link_key not in edges:\n        edges[link_key] = link_key\n        g.add_edge(pm.title, pm.child_title)\n```", "```py\nplt.figure(figsize=(10,8))\n\nnode_positions = nx.spring_layout(g)\n\nnx.draw_networkx_nodes(g, node_positions, g.nodes, node_color='green', node_size=50)\nnx.draw_networkx_edges(g, node_positions)\n\nlabels = { node: node for node in g.nodes() }\nnx.draw_networkx_labels(g, node_positions, labels, font_size=9.5)\n\nplt.show()\n```", "```py\ncrawl_depth = 2 process = CrawlerProcess({\n    'LOG_LEVEL': 'ERROR',\n    'DEPTH_LIMIT': crawl_depth\n})\nprocess.crawl(WikipediaSpider)\nspider = next(iter(process.crawlers)).spider\nspider.max_items_per_page = 5 spider.max_crawl_depth = crawl_depth\nprocess.start()\n```", "```py\nDegrees of separation: 1\n Python_(programming_language)\n   C_(programming_language)\n    Dennis_Ritchie\n```", "```py\npath = nx.astar_path(g, \"Python_(programming_language)\", \"Dennis_Ritchie\")\n```", "```py\ndegrees_of_separation = int((len(path) - 1) / 2)\nprint(\"Degrees of separation: {}\".format(degrees_of_separation))\nfor i in range(0, len(path)):\n    print(\" \" * i, path[i])\n```"]