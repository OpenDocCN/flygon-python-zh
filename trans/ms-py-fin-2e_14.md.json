["```py\nIn [ ]:\n    import requests\n\n    BASE_URL = 'https://api-v2.intrinio.com'\n\n    # REPLACE YOUR INTRINIO API KEY HERE!\n    INTRINIO_API_KEY = 'Ojc3NjkzOGNmNDMxMGFiZWZiMmMxMmY0Yjk3MTQzYjdh'\n\n    def query_intrinio(path, **kwargs):   \n        url = '%s%s'%(BASE_URL, path)\n        kwargs['api_key'] = INTRINIO_API_KEY\n        response = requests.get(url, params=kwargs)\n\n        status_code = response.status_code\n        if status_code == 401: \n            raise Exception('API key is invalid!')\n        if status_code == 429: \n            raise Exception('Page limit hit! Try again in 1 minute')\n        if status_code != 200: \n            raise Exception('Request failed with status %s'%status_code)\n\n        return response.json()\n```", "```py\nIn [ ]:\n    import pandas as pd\n    from pandas.io.json import json_normalize\n\n    def get_technicals(ticker, indicator, **kwargs):    \n        url_pattern = '/securities/%s/prices/technicals/%s'\n        path = url_pattern%(ticker, indicator)\n        json_data = query_intrinio(path, **kwargs)\n\n        df = json_normalize(json_data.get('technicals'))    \n        df['date_time'] = pd.to_datetime(df['date_time'])\n        df = df.set_index('date_time')\n        df.index = df.index.rename('date')\n        return df\n```", "```py\nIn [ ]:\n    ticker = 'AAPL'\n    query_params = {'start_date': '2013-01-01', 'page_size': 365*6}\n```", "```py\nIn [ ]:\n    df_rsi = get_technicals(ticker, 'rsi', **query_params)\n    df_wr = get_technicals(ticker, 'wr', **query_params)\n    df_vwap = get_technicals(ticker, 'vwap', **query_params)\n    df_adtv = get_technicals(ticker, 'adtv', **query_params)\n    df_ao = get_technicals(ticker, 'ao', **query_params)\n    df_sma_5d = get_technicals(ticker, 'sma', period=5, **query_params)\n    df_sma_5d = df_sma_5d.rename(columns={'sma':'sma_5d'})\n    df_sma_15d = get_technicals(ticker, 'sma', period=15, **query_params)\n    df_sma_15d = df_sma_15d.rename(columns={'sma':'sma_15d'})\n    df_sma_30d = get_technicals(ticker, 'sma', period=30, **query_params)\n    df_sma_30d = df_sma_30d.rename(columns={'sma':'sma_30d'})\n```", "```py\nIn [ ]:\n    def get_prices(ticker, tag, **params):\n        url_pattern = '/securities/%s/historical_data/%s'\n        path = url_pattern%(ticker, tag)\n        json_data = query_intrinio(path, **params)\n\n        df = json_normalize(json_data.get('historical_data'))    \n        df['date'] = pd.to_datetime(df['date'])\n        df = df.set_index('date')\n        df.index = df.index.rename('date')\n        return df.rename(columns={'value':tag})\n```", "```py\nIn [ ]:\n    df_close = get_prices(ticker, 'adj_close_price', **query_params)\n```", "```py\nIn [ ]:\n    df_target = df_close.shift(1).dropna()\n```", "```py\nIn [ ]:\n    df = df_rsi.join(df_wr).join(df_vwap).join(df_adtv)\\\n         .join(df_ao).join(df_sma_5d).join(df_sma_15d)\\\n         .join(df_sma_30d).join(df_target).dropna()\n```", "```py\nIn [ ]:\n    df_train = df['2017':'2013']\n    df_test = df['2018']\n```", "```py\nIn [ ]:\n    from sklearn.preprocessing import MinMaxScaler\n\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    train_data = scaler.fit_transform(df_train.values)\n    test_data = scaler.transform(df_test.values)\n```", "```py\nIn [ ]:\n    x_train = train_data[:, :-1]\n    y_train = train_data[:, -1]\n```", "```py\nIn [ ]:\n    x_test = test_data[:, :-1]\n```", "```py\nIn [ ]:\n    import tensorflow as tf\n\n    num_features = x_train.shape[1]\n\n    x = tf.placeholder(dtype=tf.float32, shape=[None, num_features])\n    y = tf.placeholder(dtype=tf.float32, shape=[None])\n```", "```py\nIn [ ]:\n    nl_1, nl_2, nl_3, nl_4 = 512, 256, 128, 64\n\n    wi = tf.contrib.layers.variance_scaling_initializer(\n         mode='FAN_AVG', uniform=True, factor=1)\n    zi = tf.zeros_initializer()\n\n    # 4 Hidden layers\n    wt_hidden_1 = tf.Variable(wi([num_features, nl_1]))\n    bias_hidden_1 = tf.Variable(zi([nl_1]))\n\n    wt_hidden_2 = tf.Variable(wi([nl_1, nl_2]))\n    bias_hidden_2 = tf.Variable(zi([nl_2]))\n\n    wt_hidden_3 = tf.Variable(wi([nl_2, nl_3]))\n    bias_hidden_3 = tf.Variable(zi([nl_3]))\n\n    wt_hidden_4 = tf.Variable(wi([nl_3, nl_4]))\n    bias_hidden_4 = tf.Variable(zi([nl_4]))\n\n    # Output layer\n    wt_out = tf.Variable(wi([nl_4, 1]))\n    bias_out = tf.Variable(zi([1]))\n```", "```py\nIn [ ]:\n    hidden_1 = tf.nn.relu(\n        tf.add(tf.matmul(x, wt_hidden_1), bias_hidden_1))\n    hidden_2 = tf.nn.relu(\n        tf.add(tf.matmul(hidden_1, wt_hidden_2), bias_hidden_2))\n    hidden_3 = tf.nn.relu(\n        tf.add(tf.matmul(hidden_2, wt_hidden_3), bias_hidden_3))\n    hidden_4 = tf.nn.relu(\n        tf.add(tf.matmul(hidden_3, wt_hidden_4), bias_hidden_4))\n    out = tf.transpose(tf.add(tf.matmul(hidden_4, wt_out), bias_out))\n```", "```py\nIn [ ]:\n    mse = tf.reduce_mean(tf.squared_difference(out, y))\n```", "```py\nIn [ ]:\n    optimizer = tf.train.AdamOptimizer().minimize(mse)\n```", "```py\nIn [ ]:\n    session = tf.InteractiveSession()\n```", "```py\nIn [ ]:\n    session.run(tf.global_variables_initializer())\n```", "```py\nIn [ ]:\n    from numpy import arange\n    from numpy.random import permutation\n\n    BATCH_SIZE = 100\n    EPOCHS = 100\n\n    for epoch in range(EPOCHS):\n        # Shuffle the training data\n        shuffle_data = permutation(arange(len(y_train)))\n        x_train = x_train[shuffle_data]\n        y_train = y_train[shuffle_data]\n\n        # Mini-batch training\n        for i in range(len(y_train)//BATCH_SIZE):\n            start = i*BATCH_SIZE\n            batch_x = x_train[start:start+BATCH_SIZE]\n            batch_y = y_train[start:start+BATCH_SIZE]\n            session.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n```", "```py\nIn [ ]:\n    [predicted_values] = session.run(out, feed_dict={x: x_test})\n```", "```py\nIn [ ]:\n    predicted_scaled_data = test_data.copy()\n    predicted_scaled_data[:, -1] = predicted_values\n    predicted_values = scaler.inverse_transform(predicted_scaled_data)\n```", "```py\nIn [ ]:\n    predictions = predicted_values[:, -1][::-1]\n    actual = df_close['2018']['adj_close_price'].values[::-1]\n```", "```py\nIn [ ]:\n    %matplotlib inline \n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(12,8))\n    plt.title('Actual and predicted prices of AAPL 2018')\n    plt.plot(actual, label='Actual')\n    plt.plot(predictions, linestyle='dotted', label='Predicted')\n    plt.legend()\n```", "```py\nIn [ ]:\n    import pandas as pd\n\n    df = pd.read_csv('files/chapter11/default_cc_clients.csv')\n```", "```py\nIn [ ]:\n    df.info()\nOut[ ]:\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 30000 entries, 0 to 29999\n    Data columns (total 24 columns):\n    LIMIT_BAL                     30000 non-null int64\n    SEX                           30000 non-null int64\n    EDUCATION                     30000 non-null int64\n    MARRIAGE                      30000 non-null int64\n    AGE                           30000 non-null int64\n    PAY_0                         30000 non-null int64\n    ...\n    PAY_AMT6                      30000 non-null int64\n    default payment next month    30000 non-null int64\n    dtypes: int64(24)\n    memory usage: 5.5 MB\n```", "```py\nIn [ ]:\n    feature_columns= df.columns[:-1]\n    features = df.loc[:, feature_columns]\n    target = df.loc[:, 'default payment next month']\n```", "```py\nIn [ ]:\n    from sklearn.model_selection import train_test_split\n\n    train_features, test_features, train_target, test_target = \\\n        train_test_split(features, target, test_size=0.20, random_state=0)\n```", "```py\nIn [ ]:\n    import numpy as np\n\n    train_x, train_y = np.array(train_features), np.array(train_target)\n    test_x, test_y = np.array(test_features), np.array(test_target)\n```", "```py\nIn [ ]:\n    from sklearn.preprocessing import MinMaxScaler\n\n    scaler = MinMaxScaler()\n    train_scaled_x = scaler.fit_transform(train_x)\n    test_scaled_x = scaler.transform(test_x)\n```", "```py\nIn [ ]:\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.layers import Dropout\n    from keras.layers.normalization import BatchNormalization\n\n    num_features = train_scaled_x.shape[1]\n\n    model = Sequential()\n    model.add(Dense(80, input_dim=num_features, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(80, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(40, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(1, activation='sigmoid'))\n```", "```py\nIn [ ]:\n    model.summary()\nOut[ ]:\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    dense_17 (Dense)             (None, 80)                1920      \n    _________________________________________________________________\n    dropout_9 (Dropout)          (None, 80)                0         \n    _________________________________________________________________\n    dense_18 (Dense)             (None, 80)                6480      \n    _________________________________________________________________\n    dropout_10 (Dropout)         (None, 80)                0         \n    _________________________________________________________________\n    dense_19 (Dense)             (None, 40)                3240      \n    _________________________________________________________________\n    batch_normalization_5 (Batch (None, 40)                160       \n    _________________________________________________________________\n    dense_20 (Dense)             (None, 1)                 41        \n    =================================================================\n    Total params: 11,841\n    Trainable params: 11,761\n    Non-trainable params: 80\n    _________________________________________________________________\n```", "```py\nIn [ ]:\n    import tensorflow as tf\n\n    model.compile(optimizer=tf.train.AdamOptimizer(), \n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n```", "```py\nIn [ ]:\n    from keras.callbacks import History \n\n    callback_history = History()\n\n    model.fit(\n        train_scaled_x, train_y,\n        validation_split=0.2,\n        epochs=100, \n        callbacks=[callback_history]\n    )\nOut [ ]:\n    Train on 19200 samples, validate on 4800 samples\n    Epoch 1/100\n    19200/19200 [==============================] - 2s 106us/step - loss: 0.4209 - acc: 0.8242 - val_loss: 0.4456 - val_acc: 0.8125        \n...\n```", "```py\nIn [ ]:\n    test_loss, test_acc = model.evaluate(test_scaled_x, test_y)\n    print('Test loss:', test_loss)\n    print('Test accuracy:', test_acc)\nOut[ ]:\n    6000/6000 [==============================] - 0s 33us/step\n    Test loss: 0.432878403028\n    Test accuracy: 0.824166666667\n```", "```py\nIn [ ]:\n    predictions = model.predict(test_scaled_x)\n    pred_values = predictions.round().ravel()\n```", "```py\nIn [ ]:\n    from sklearn.metrics import confusion_matrix\n\n    matrix = confusion_matrix(test_y, pred_values)\nIn [ ]:\n    %matplotlib inline\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n\n    flags = ['No', 'Yes']\n    plt.subplots(figsize=(12,8))\n    sns.heatmap(matrix.T, square=True, annot=True, fmt='g', cbar=True,\n        cmap=plt.cm.Blues, xticklabels=flags, yticklabels=flags)\n    plt.xlabel('Actual')\n    plt.ylabel('Predicted')\n    plt.title('Credit card payment default prediction');\n```", "```py\nIn [ ]:\n    from sklearn.metrics import (\n        accuracy_score, precision_score, recall_score, f1_score\n    )\n    actual, predicted = test_y, pred_values\n    print('accuracy_score:', accuracy_score(actual, predicted))\n    print('precision_score:', precision_score(actual, predicted))\n    print('recall_score:', recall_score(actual, predicted))\n    print('f1_score:', f1_score(actual, predicted))    \nOut[ ]:\n    accuracy_score: 0.818666666667\n    precision_score: 0.641025641026\n    recall_score: 0.366229760987\n    f1_score: 0.466143277723\n```", "```py\nIn [ ]:\n    train_acc = callback_history.history['acc']\n    val_acc = callback_history.history['val_acc']\n    train_loss = callback_history.history['loss']\n    val_loss = callback_history.history['val_loss']\n```", "```py\nIn [ ]:\n    %matplotlib inline\n    import matplotlib.pyplot as plt\n\n    epochs = range(1, len(train_acc)+1)\n\n    plt.figure(figsize=(12,6))\n    plt.plot(epochs, train_loss, label='Training')\n    plt.plot(epochs, val_loss, '--', label='Validation')\n    plt.title('Training and validation loss')\n    plt.xlabel('epochs')\n    plt.ylabel('loss')\n    plt.legend();\n```", "```py\nIn [ ]:\n    plt.clf()  # Clear the figure\n    plt.plot(epochs, train_acc, '-', label='Training')\n    plt.plot(epochs, val_acc, '--', label='Validation')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('epochs')\n    plt.ylabel('accuracy')\n    plt.legend();\n```"]