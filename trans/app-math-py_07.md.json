["```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n```", "```py\n          python3.8 -m pip install statsmodels sklearn fbprophet\n\n```", "```py\n          conda install fbprophet\n\n```", "```py\nimport statsmodels.api as sm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.random import default_rng\nrng = default_rng(12345)\n```", "```py\nx = np.linspace(0, 5, 25)\nrng.shuffle(x)\ntrend = 2.0\nshift = 5.0\ny1 = trend*x + shift + rng.normal(0, 0.5, size=25)\ny2 = trend*x + shift + rng.normal(0, 5, size=25)\n```", "```py\nfig, ax = plt.subplots()\nax.scatter(x, y1, c=\"b\", label=\"Good correlation\")\nax.scatter(x, y2, c=\"r\", label=\"Bad correlation\")\nax.legend()\nax.set_xlabel(\"X\"),\nax.set_ylabel(\"Y\")\nax.set_title(\"Scatter plot of data with best fit lines\")\n```", "```py\npred_x = sm.add_constant(x)\n```", "```py\nmodel1 = sm.OLS(y1, pred_x).fit()\nprint(model1.summary())\n```", "```py\nmodel2 = sm.OLS(y2, pred_x).fit()\nprint(model2.summary())\n```", "```py\nmodel_x = sm.add_constant(np.linspace(0, 5))\n```", "```py\nmodel_y1 = model1.predict(model_x)\nmodel_y2 = model2.predict(model_x)\n```", "```py\nax.plot(model_x[:, 1], model_y1, 'b')\nax.plot(model_x[:, 1], model_y2, 'r')\n```", "```py\nfrom numpy.random import default_rng\nrng = default_rng(12345)\n```", "```py\nimport statsmodels.api as sm\n```", "```py\np_vars = pd.DataFrame({\n\"const\": np.ones((100,)),\n\"X1\": rng.uniform(0, 15, size=100),\n\"X2\": rng.uniform(0, 25, size=100),\n\"X3\": rng.uniform(5, 25, size=100)\n})\n\n```", "```py\nresiduals = rng.normal(0.0, 12.0, size=100)\nY = -10.0 + 5.0*p_vars[\"X1\"] - 2.0*p_vars[\"X2\"] + residuals\n```", "```py\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True,\n   tight_layout=True)\nax1.scatter(p_vars[\"X1\"], Y)\nax2.scatter(p_vars[\"X2\"], Y)\nax3.scatter(p_vars[\"X3\"], Y)\n\n```", "```py\nax1.set_title(\"Y against X1\")\nax1.set_xlabel(\"X1\")\nax1.set_ylabel(\"Y\")\nax2.set_title(\"Y against X2\")\nax2.set_xlabel(\"X2\")\nax3.set_title(\"Y against X3\")\nax3.set_xlabel(\"X3\")\n```", "```py\n        The resulting plots can be seen in the following figure:\n\n          ![](assets/151a3a34-831e-40fc-9d29-ec1db98ab665.png)\n\n        Figure 7.2: Scatter plots of the response data against each of the predictor variables\n        As we can see, there appears to be some correlation between the response data and the first two predictor columns, `X1` and `X2`. This is what we expect, given how we generated the data.\n\n          5.  We use the same `OLS` class to perform multilinear regression; that is, providing the response array and the predictor `DataFrame`:\n\n```", "```py\n\n        The output of the `print` statement is as follows:\n\n```", "```py\n\n        In the summary data, we can see that the `X3` variable is not significant since it has a p value of 0.66.\n\n          6.  Since the third predictor variable is not significant, we eliminate this column and perform the regression again:\n\n```", "```py\n\n        This results in a small increase in the goodness of fit statistics.\n        How it works...\n        Multilinear regression works in much the same way as simple linear regression. We follow the same procedure here as in the previous recipe, where we use the statsmodels package to fit a multilinear model to our data. Of course, there are some differences behind the scenes. The model we produce using multilinear regression is very similar in form to the simple linear model from the previous recipe. It has the following form:\n\n          ![](assets/0839eae4-836a-497f-9f90-8dd8f2cd4a17.png)\n\n        Here, *Y* is the response variable, *X[i]* represents the predictor variables, *E* is the error term, and *\u03b2*[*i*] is the parameters to be computed. The same requirements are also necessary for this context: residuals must be independent and normally distributed with a mean of 0 and a common standard deviation.\n        In this recipe, we provided our predictor data as a Pandas `DataFrame` rather than a plain NumPy array. Notice that the names of the columns have been adopted in the summary data that we printed. Unlike the first recipe, *Using basic linear regression*, we included the constant column in this `DataFrame`, rather than using the `add_constant` utility from statsmodels.\n        In the output of the first regression, we can see that the model is a reasonably good fit with an adjusted *R\u00b2* value of 0.762, and is highly significant (we can see this by looking at the regression F statistic p value). However, looking closer at the individual parameters, we can see that both of the first two predictor values are significant, but the constant and the third predictor are less so. In particular, the third predictor parameter, `X3`, is not significantly different from 0 and has a p value of 0.66\\. Given that our response data was constructed without using this variable, this shouldn't come as a surprise. In the final step of the analysis, we repeat the regression without the predictor variable, `X3`, which is a mild improvement to the fit.\n        Classifying using logarithmic regression\n        Logarithmic regression solves a different problem to ordinary linear regression. It is commonly used for classification problems where, typically, we wish to classify data into two distinct groups, according to a number of predictor variables. Underlying this technique is a transformation that's performed using logarithms. The original classification problem is transformed into a problem of constructing a model for the **log-odds***.* This model can be completed with simple linear regression. We apply the inverse transformation to the linear model, which leaves us with a model of the probability that the desired outcome will occur, given the predictor data. The transform we apply here is called the **logistic function**, which gives its name to the method. The probability we obtain can then be used in the classification problem we originally aimed to solve.\n        In this recipe, we will learn how to perform logistic regression and use this technique in classification problems.\n        Getting ready\n        For this recipe, we will need the NumPy package imported as `np`, the Matplotlib `pyplot`module imported as `plt`, the Pandas package imported as `pd`, and an instance of the NumPy default random number generator to be created using the following commands:\n\n```", "```py\n\n        We also need several components from the `scikit-learn` package to perform logistic regression. These can be imported as follows:\n\n```", "```py\n\n        How to do it...\n        Follow these steps to use logistic regression to solve a simple classification problem:\n\n          1.  First, we need to create some sample data that we can use to demonstrate how to use logistic regression. We start by creating the predictor variables:\n\n```", "```py\n\n          2.  Now, we use two of our three predictor variables to create our response variable as a series of Boolean values:\n\n```", "```py\n\n          3.  Next, we scatter plot the points, styled according to the response variable, of the `var3` data against the `var1` data, which are the variables used to construct the response variable:\n\n```", "```py\n\n        The resulting plot can be seen in the following figure:\n\n          ![](assets/e972cdff-197a-4940-8f33-dbe849ddb774.png)\n\n        Figure 7.3: Scatter plot of the var3 data against var1, with classification marked\n\n          4.  Next, we create a `LogisticRegression` object from the `scikit-learn` package and fit the model to our data:\n\n```", "```py\n\n          5.  Next, we prepare some extra data, different from what we used to fit the model, to test the accuracy of our model:\n\n```", "```py\n\n          6.  Then, we generate predicted results based on our logistic regression model:\n\n```", "```py\n\n          7.  Finally, we use the `classification_report` utility from `scikit-learn` to print a summary of predicted classification against known response values to test the accuracy of the model. We print this summary to the Terminal:\n\n```", "```py\n\n        The report that's generated by this routine looks as follows:\n\n```", "```py\n\n        How it works...\n        Logistic regression works by forming a linear model of the *log odds* ratio *(*or *logit*), which, for a single predictor variable, *x*, has the following form:\n\n          ![](assets/ef09118a-1d49-472f-9b70-c9925e410036.png)\n\n        Here, *p*(*x*) represents the probability of a true outcome in response to the given the predictor, *x*. Rearranging this gives a variation of the logistic function for the probability:\n\n          ![](assets/4edf13c3-10f1-42a0-a475-91264d1ea732.png)\n\n        The parameters for the log odds are estimated using a maximum likelihood method. \n        The `LogisticRegression` class from the `linear_model` module in `scikit-learn` is an implementation of logistic regression that is very easy to use. First, we create a new model instance of this class, with any custom parameters that we need, and then use the `fit` method on this object to fit (or train) the model to the sample data. Once this fitting is done, we can access the parameters that have been estimated using the `get_params` method. \n        The `predict` method on the fitted model allows us to pass in new (unseen) data and make predictions about the classification of each sample. We could also get the probability estimates that are actually given by the logistic function using the `predict_proba` method.\n        Once we have built a model for predicting the classification of data, we need to validate the model. This means we have to test the model with some previously unseen data and check whether it correctly classifies the new data. For this, we can use `classification_report`, which takes a new set of data and the predictions generated by the model and computes the proportion of the data that was correctly predicted by the model. This is the *precision* of the model.\n        The classification report we generated using the `scikit-learn` utility performs a comparison between the predicted results and the known response values. This is a common method for validating a model before using it to make actual predictions. In this recipe, we saw that the reported precision for each of the categories (`True` and `False`) was 1.00, indicating that the model performed perfectly in predicting the classification with this data. In practice, it is unlikely that the precision of a model will be 100%. \n        There's more...\n        There are lots of packages that offer tools for using logistic regression for classification problems. The statsmodels package has the `Logit` class for creating logistic regression models. We used the `scikit-learn` package in this recipe, which has a similar interface. `Scikit-learn` is a general-purpose machine learning library and has a variety of other tools for classification problems.\n        Modeling time series data with ARMA\n        Time series, as the name suggests, tracks a value over a sequence of distinct time intervals. They are particularly important in the finance industry, where stock values are tracked over time and used to make predictions \u2013 known as forecasting \u2013 of the value at some future time. Good predictions coming from such data can be used to make better investments. Time series also appear in many other common situations, such as weather monitoring, medicine, and any places where data is derived from sensors over time.\n        Time series, unlike other types of data, do not usually have independent data points. This means that the methods that we use for modeling independent data will not be particularly effective. Thus, we need to use alternative techniques to model data with this property. There are two ways in which a value in a time series can depend on previous values. The first is where there is a direct relationship between the value and one or more previous values. This is the *autocorrelation* property and is modeled by an *autoregressive* model. The second is where the noise that's added to the value depends on one or more previous noise terms. This is modeled by a *moving average* model. The number of terms involved in either of these models is called the *order* of the model.\n        In this recipe, we will learn how to create a model for stationary time series data with ARMA terms.\n        Getting ready\n        For this recipe, we need the Matplotlib `pyplot` module imported as `plt` and the statsmodels package `api` module imported as `sm`. We also need to import the `generate_sample_data` routine from the `tsdata` package from this book's repository, which uses NumPy and Pandas to generate sample data for analysis:\n\n```", "```py\n\n        How to do it...\n        Follow these steps to create an autoregressive moving average model for stationary time series data:\n\n          1.  First, we need to generate the sample data that we will analyze:\n\n```", "```py\n\n          2.  As always, the first step in the analysis is to produce a plot of the data so that we can visually identify any structure:\n\n```", "```py\n\n        The resulting plot can be seen in the following figure. Here, we can see that there doesn't appear to be an underlying trend, which means that the data is likely to be stationary:\n\n          ![](assets/518372fa-7c61-409c-90aa-f0a13459785f.png)\n\n        Figure 7.4: Plot of the time series data that we will analyze. There doesn't appear to be a trend in this data\n\n          3.  Next, we compute the augmented Dickey-Fuller test. The null hypothesis is that the time series is not stationary:\n\n```", "```py\n\n        The reported p value is 0.000376 in this case, so we reject the null hypothesis and conclude that the series is stationary.\n\n          4.  Next, we need to determine the order of the model that we should fit. For this, we'll plot the **autocorrelation function** (**ACF**) and the **partial autocorrelation function**(**PACF**) for the time series:\n\n```", "```py\n\n        The plots of the ACF and PACF for our time series can be seen in the following figure. These plots suggest the existence of both autoregressive and moving average processes:\n\n          ![](assets/093fb316-c237-48f3-b47b-a62afcde7afa.png)\n\n        Figure 7.5: ACF and PACF for the sample time series data\n\n          5.  Next, we create an ARMA model for the data, using the `ARMA` class from statsmodels, `tsa` module. This model will have an order 1 AR and an order 1 MA:\n\n```", "```py\n\n          6.  Now, we fit the model to the data and get the resulting model. We print a summary of these results to the Terminal:\n\n```", "```py\n\n        The summary data given for the fitted model is as follows:\n\n```", "```py\n\n        Here, we can see that both of the estimated parameters for the AR and MA components are significantly different from 0\\. This is because the value in the `P >|z|` column is 0 to 3 decimal places.\n\n          7.  Next, we need to verify that there is no additional structure remaining in the residuals (error) of the predictions from our model. For this, we plot the ACF and PACF of the residuals:\n\n```", "```py\n\n        The ACF and PACF of the residuals can be seen in the following figure. Here, we can see that there are no significant spikes at lags other than 0, so we conclude that there is no structure remaining in the residuals:\n\n          ![](assets/6dc54aaa-0b81-403f-b4d8-2a67cb360f1e.png)\n\n        Figure 7.6: ACF and PACF for the residuals from our model\n\n          8.  Now that we have verified that our model is not missing any structure, we plot the values that are fitted to each data point on top of the actual time series data to see whether the model is a good fit for the data. We plot this model in the plot we created in *step 2*:\n\n```", "```py\n\n        The updated plot can be seen in the following figure:\n\n          ![](assets/67b312d7-3986-4bf8-aba6-a534a0cf9b89.png)\n\n        Figure 7.7: Plot of the fitted time series data over the observed time series data\n        The fitted values give a reasonable approximation of the behavior of the time series, but reduce the noise from the underlying structure.\n        How it works...\n        A time series is stationary if it does not have a trend. They usually have a tendency to move in one direction rather than another. Stationary processes are important because we can usually remove the trend from an arbitrary time series and model the underlying stationary series. The ARMA model that we used in this recipe is a basic means of modeling the behavior of stationary time series. The two parts of an ARMA model are the autoregressive and moving average parts, which model the dependence of the terms and noise, respectively, on previous terms and noise.\n        An order 1 autoregressive model has the following form:\n\n          ![](assets/7a4a2e01-301a-4448-9ad2-32ab71c37a97.png)\n\n        Here, *\u03c6[i]* represents the parameters and *\u03b5[t]* is the noise at a given step. The noise is usually assumed to be normally distributed with a mean of 0 and a standard deviation that is constant across all the time steps. The *Y[t]* value represents the value of the time series at the time step, *t*. In this model, each value depends on the previous value, though it can also depend on some constants and some noise. The model will give rise to a stationary time series precisely when the *\u03c6[1]* parameter lies strictly between -1 and 1.\n        An order 1 moving average model is very similar to an autoregressive model and is given by the following equation:\n\n          ![](assets/c71efac3-1651-4fc9-932c-e3930e7d80e7.png)\n\n        Here, the variants of *\u03b8[i]* are parameters. Putting these two models together gives us an ARMA(1, 1) model, which has the following form:\n\n          ![](assets/fcb7eb3b-9d02-4a2e-b954-b260ba87f8e0.png)\n\n        In general, we can have an ARMA(p, q) model that has an order *p* AR component and an order q MA component. We usually refer to the quantities, *p* and *q*, as the orders of the model.\n        Determining the orders of the AR and MA components is the most tricky aspect of constructing an ARMA model. The ACF and PACF give some information toward this, but even then, it can be quite difficult. For example, an autoregressive process will show some kind of decay or oscillating pattern on the ACF as lag increases, and a small number of peaks on the PACF and values that are not significantly different from 0 beyond that. The number of peaks that appear on the PAF plot can be taken as the order of the process. For a moving average process, the reverse is true. There are usually a small number of significant peaks on the ACF plot, and a decay or oscillating pattern on the PACF plot. Of course, sometimes, this isn't obvious.\n        In this recipe, we plotted the ACF and PACF for our sample time series data. In the autocorrelation plot in *Figure 7.5* (top), we can see that the peaks decay rapidly until they lie within the confidence interval of zero (meaning they are not significant). This suggests the presence of an autoregressive component. On the partial autocorrelation plot in *Figure 7.5* (bottom), we can see that there are only two peaks that can be considered not zero, which suggests an autoregressive process of order 1 or 2\\. You should try to keep the order of the model as small as possible. Due to this, we chose an order 1 autoregressive component. With this assumption, the second peak on the partial autocorrelation plot is indicative of decay (rather than an isolated peak), which suggests the presence of a moving average process. To keep the model simple, we try an order 1 moving average process. This is how the model that we used in this recipe was decided on. Notice that this is not an exact process, and you might have decided differently. \n        We use the augmented Dickey-Fuller test to test the likelihood that the time series that we have observed is stationary. This is a statistical test, such as those seen in [Chapter 6](87b0f91d-3086-41a9-995d-27fe7d364e8b.xhtml), *Working with Data and Statistics*, that generates a test statistic from the data. This test statistic, in turn, determines a p-value that is used to determine whether to accept or reject the null hypothesis. For this test, the null hypothesis is that a unit root is present in the time series that's been sampled. The alternative hypothesis \u2013 the one we are really interested in \u2013 is that the observed time series is (trend) stationary. If the p-value is sufficiently small, then we can conclude with the specified confidence that the observed time series is stationary. In this recipe, the p-value was 0.000 to 3 decimal places, which indicates a strong likelihood that the series is stationary. Stationarity is an essential assumption for using the ARMA model for the data.\n        Once we have determined that the series is stationary, and also decided on the orders of the model, we have to fit the model to the sample data that we have. The parameters of the model are estimated using a maximum likelihood estimator. In this recipe, the learning of the parameters is done using the `fit` method, in *step 6*.\n        The statsmodels package provides various tools for working with time series, including utilities for calculating \u2013 and plotting \u2013ACF and PACF of time series data, various test statistics, and creating ARMA models for time series. There are also some tools for automatically estimating the order of the model.\n        We can use the **Akaike information criterion** (**AIC**), **Bayesian information criterion** (**BIC**), and **Hannan-Quinn Information Criterion** (**HQIC**) quantities to compare this model to other models to see which model best describes the data. A smaller value is better in each case.\n        When using ARMA to model time series data, as in all kinds of mathematical modeling tasks, it is best to pick the simplest model that describes the data to the extent that is needed. For ARMA models, this usually means picking the smallest order model that describes the structure of the observed data.\n        There's more...\n        Finding the best combination of orders for an ARMA model can be quite difficult. Often, the best way to fit a model is to test multiple different configurations and pick the order that produces the best fit. For example, we could have tried ARMA(0, 1) or ARMA(1, 0) in this recipe, and compared it to the ARMA(1, 1) model we used to see which produced the best fit by considering the **Akaike Information Criteria** (**AIC**) statistic reported in the summary. In fact, if we build these models, we will see that the AIC value for ARMA(1, 1) \u2013 the model we used in this recipe \u2013 is the \"best\" of these three models.\n        Forecasting from time series data using ARIMA\n        In the previous recipe, we generated a model for a stationary time series using an ARMA model, which consists of an **autoregressive** (**AR**) component and an **m****oving average** (**MA**) component. Unfortunately, this model cannot accommodate time series that have some underlying trend; that is, they are not stationary time series. We can often get around this by *differencing* the observed time series one or more times until we obtain a stationary time series that can be modeled using ARMA. The incorporation of differencing into an ARMA model is called an ARIMA model, which stands for **Autoregressive** (**AR**) **Integrated** (**I**) **Moving Average** (**MA**).\n        Differencing is the process of computing the difference of consecutive terms in a sequence of data. So, applying first-order differencing amounts to subtracting the value at the current step from the value at the next step (*t[i+1] - t[i]*). This has the effect of removing the underlying upward or downward linear trend from the data. This helps to reduce an arbitrary time series to a stationary time series that can be modeled using ARMA. Higher-order differencing can remove higher-order trends to achieve similar effects.\n        An ARIMA model has three parameters, usually labeled *p*, *d*, and *q*. The *p* and *q* order parameters are the order of the autoregressive component and the moving average component, respectively, just as they are for the ARMA model. The third order parameter, *d*, is the order of differencing to be applied. An ARIMA model with these orders is usually written as ARIMA (*p*, *d*, *q*). Of course, we will need to determine what order differencing should be included before we start fitting the model.\n        In this recipe, we will learn how to fit an ARIMA model to a non-stationary time series and use this model to make forecasts about future values.\n        Getting ready\n        For this recipe, we will need the NumPy package imported as `np`, the Pandas package imported as `pd`, the Matplotlib `pyplot` module as `plt`, and the statsmodels `api` module imported as `sm`. We will also need the utility for creating sample time series data from the `tsdata` module, which is included in this book's repository:\n\n```", "```py\n\n        How to do it...\n        The following steps show you how to construct an ARIMA model for time series data and use this model to make forecasts:\n\n          1.  First, we load the sample data using the `generate_sample_data` routine:\n\n```", "```py\n\n          2.  As usual, the next step is to plot the time series so that we can visually identify the trend of the data:\n\n```", "```py\n\n        The resulting plot can be seen in the following figure. As we can see, there is a clear upward trend in the data, so the time series is certainly not stationary:\n\n          ![](assets/a5081c24-e36c-48ac-86e5-a5ec2cfccee0.png)\n\n        Figure 7.8: Plot of the sample time series. There is an obvious positive trend in the data.\n\n          3.  Next, we difference the series to see if one level of differencing is sufficient to remove the trend:\n\n```", "```py\n\n          4.  Now, we plot the ACF and PACF for the differenced time series: \n\n```", "```py\n\n        The ACF and PACF can be seen in the following figure. We can see that there does not appear to be any trends left in the data and that there appears to be both an autoregressive component and a moving average component:\n\n          ![](assets/dc3a6b97-bdd4-4c6c-822a-d40b00a0a2e8.png)\n\n        Figure 7.9: ACF and PACF for the differenced time series\n\n          5.  Now, we construct the ARIMA model with order 1 differencing, an autoregressive component, and a moving average component. We fit this to the observed time series and print a summary of the model:\n\n```", "```py\n\n        The summary information that's printed looks as follows:\n\n```", "```py\n\n        Here, we can see that all three of our estimated coefficients are significantly different from 0 due to the fact that all three have 0 to 3 decimal places in the `P>|z|` column.\n\n          6.  Now, we can use the `forecast` method to generate predictions of future values. This also returns the standard error and confidence intervals for predictions:\n\n```", "```py\n\n          7.  Next, we plot the forecast values and their confidence intervals on the figure containing the time series data:\n\n```", "```py\n\n          8.  Finally, we add the actual future values to generate, along with the sample in *step 1*, to the plot (it might be easier if you repeat the plot commands from *step 1* to regenerate the whole plot here):\n\n```", "```py\n\n        The final plot containing the time series with the forecast and the actual future values can be seen in the following figure:\n\n          ![](assets/76b414a5-1847-4944-8811-28677d9e3853.png)\n\n        Figure 7.10: Plot of the sample time series with forecast values and actual future values for comparison\n        Here, we can see that the actual future values are within the confidence interval for the forecast values.\n        How it works...\n        The ARIMA model \u2013 with orders *p*, *d*, and *q \u2013* is simply an ARMA (*p*, *q*) model that's applied to a time series. This is obtained by applying differencing of order *d* to the original time series data. It is a fairly simple way to generate a model for time series data. The statsmodels `ARIMA` class handles the creation of a model, while the `fit` method fits this model to the data. We passed the `trend=\"c\"` keyword argument because we know, from *Figure 7.9*, that the time series has a constant trend.\n        The model is fit to the data using a maximum likelihood method and the final estimates for the parameters \u2013 in this case, one parameter for the autoregressive component, one for the moving average component, the constant trend parameter, and the variance of the noise. These parameters are reported in the summary. From this output, we can see that the estimates for the AR coefficient (0.8342) and the MA constant (-0.5204) are very good approximations of the true estimates that were used to generate the data, which were 0.8 for the AR coefficient and -0.5  for the MA coefficient. These parameters are set in the `generate_sample_data` routine from the `tsdata.py` file in the code repository for this chapter. This generates the sample data in *step 1*. You might have noticed that the constant parameter (0.9548) is not 0.2, as specified in the `generate_sample_data` call in *step 1*. In fact, it is not so far from the actual drift of the time series.\n        The `forecast` method on the fitted model (the output of the `fit` method) uses the model to make predictions about the value after a given number of steps. In this recipe, we forecast for up to 50 time steps beyond the range of the sample time series. The output of the `forecast` method is a tuple containing the forecast values, the standard error for the forecasts, and the confidence interval (by default, 95% confidence) for the forecasts. Since we provided the time series as a Pandas series, these are returned as `Series` objects (the confidence interval is a `DataFrame`).\n        When you construct an ARIMA model for time series data, you need to make sure you use the smallest order differencing that removes the underlying trend. Applying more differencing than is necessary is called *overdifferencing* and can lead to problems with the model.\n        Forecasting seasonal data using ARIMA\n        Time series often display periodic behavior so that peaks or dips in the value appear at regular intervals. This behavior is called *seasonality* in the analysis of time series. The methods we have used to far in this chapter to model time series data obviously do not account for seasonality. Fortunately, it is relatively easy to adapt the standard ARIMA model to incorporate seasonality, resulting in what is sometimes called a SARIMA model.\n        In this recipe, we will learn how to model time series data that includes seasonal behavior and use this model to produce forecasts.\n        Getting ready\n        For this recipe, we will need the NumPy package imported as `np`, the Pandas package imported as `pd`, the Matplotlib `pyplot`module as `plt`, and the statsmodels `api`module imported as `sm`. We will also need the utility for creating sample time series data from the `tsdata`module, which is included in this book's repository:\n\n```", "```py\n\n        How to do it...\n        Follow these steps to produce a seasonal ARIMA model for sample time series data and use this model to produce forecasts:\n\n          1.  First, we use the `generate_sample_data` routine to generate a sample time series to analyze:\n\n```", "```py\n\n          2.  As usual, our first step is to visually inspect the data by producing a plot of the sample time series:\n\n```", "```py\n\n        The plot of the sample time series data can be seen in the following figure. Here, we can see that there seem to be periodic peaks in the data:\n\n          ![](assets/f4ae55a8-7e3a-489b-b5bc-987923b794ab.png)\n\n        Figure 7.11: Plot of the sample time series data\n\n          3.  Next, we plot the ACF and PACF for the sample time series:\n\n```", "```py\n\n        The ACF and PACF for the sample time series can be seen in the following figure:\n\n          ![](assets/0d6132a1-0b39-4f7b-bf1a-0da288ad8c3c.png)\n\n        Figure 7.12: ACF and PACF for the sample time series\n        These plots possibly indicate the existence of autoregressive components, but also a significant spike on the PACF with lag 7.\n\n          4.  Next, we difference the time series and produce plots of the ACF and PACF for the differenced series. This should make the order of the model clearer:\n\n```", "```py\n\n        The ACF and PACF for the differenced time series can be seen in the following figure. We can see that there is definitely a seasonal component with lag 7:\n\n          ![](assets/7e2f3382-2b4e-4423-9d38-0e0d4332f9b8.png)\n\n        Figure 7.13: Plot of the ACF and PACF for the differenced time series\n\n          5.  Now, we need to create a `SARIMAX` object that holds the model, with ARIMA order `(1, 1, 1)` and seasonal ARIMA order `(1, 0, 0, 7)`. We fit this model to the sample time series and print summary statistics. We plot the predicted values on top of the time series data:\n\n```", "```py\n\n        The summary statistics that are printed to the Terminal look as follows:\n\n```", "```py\n\n          6.  This model appears to be a reasonable fit, so we move ahead and forecast `50` time steps into the future:\n\n```", "```py\n\n          7.  Finally, we add the forecast values to the plot of the sample time series, along with the confidence interval for these forecasts:\n\n```", "```py\n\n        The final plot of the time series, along with the predictions and the confidence interval for the forecasts, can be seen in the following figure:\n\n          ![](assets/54ae8641-4c49-464e-9183-9c56e95c1145.png)\n\n        Figure 7.14: Plot of the sample time series, along with the forecasts and confidence interval\n        How it works...\n        Adjusting an ARIMA model to incorporate seasonality is a relatively simple task. A seasonal component is similar to an autoregressive component, where the lag starts at some number larger than 1\\. In this recipe, the time series exhibits seasonality with period 7 (weekly), which means that the model is approximately given by the following equation:\n\n          ![](assets/0a72f02a-4314-4245-b1cf-92c84d07882e.png)\n\n        Here *\u03c6[1]* and *\u03a6**[1]**are the parameters and *\u03b5[t]* is the noise at time step *t*. The standard ARIMA model is easily adapted to include this additional lag term.* *The SARIMA model incorporates this additional seasonality into the ARIMA model. It has four additional order terms on top of the three for the underlying ARIMA model. These four additional parameters are the seasonal AR, differencing, and MA components, along with the period of the seasonality. In this recipe, we took the seasonal AR to be order 1, with no seasonal differencing or MA components (order 0), and a seasonal period of 7\\. This gives us the additional parameters (1, 0, 0, 7) that we used in *step 5* of this recipe.\n\nSeasonality is clearly important in modeling time series data that is measured over a period of time covering days, months, or years. It usually incorporates some kind of seasonal component based on the time frame that they occupy. For example, a time series of national power consumption measured hourly over several days would probably have a 24-hour seasonal component since power consumption will likely fall during the night hours.\n\nLong-term seasonal patterns might be hidden if the time series data that you are analyzing does not cover a sufficiently large time period for the pattern to emerge. The same is true for trends in the data. This can lead to some interesting problems when trying to produce long-term forecasts from a relatively short period represented by observed data.\n\nThe `SARIMAX` class from the statsmodels package provides the means of modeling time series data using a seasonal ARIMA model. In fact, it can also model external factors that have an additional effect on the model, sometimes called *exogenous regressors*. (We will not cover these here.) This class works much like the `ARMA` and `ARIMA` classes that we used in the previous recipes. First, we create the model object by providing the data and orders for both the ARIMA process and the seasonal process, and then use the `fit` method on this object to create a fitted model object. We use the `get_forecasts` method to generate an object holding the forecasts and confidence interval data that we can then plot, thus producing the *Figure 7.14*.\n\n## There's more...\n\nThere is a small difference in the interface between the `SARIMAX` class used in this recipe and the `ARIMA` class used in the previous recipe. At the time of writing, the statsmodels package (v0.11) includes a second `ARIMA` class that builds on top of the `SARIMAX` class, thus providing the same interface. However, at the time of writing, this new `ARIMA` class does not offer the same functionality as that used in this recipe.\n\n# Using Prophet to model time series data \n\nThe tools we have seen so far for modeling time series data are very general and flexible methods, but they require some knowledge of time series analysis in order to be set up. The analysis needed to construct a good model that can be used to make reasonable predictions into the future can be intensive and time-consuming, and may not be viable for your application. The Prophet library is designed to automatically model time series data quickly, without the need for input from the user, and make predictions into the future.\n\nIn this recipe, we will learn how to use Prophet to produce forecasts from a sample time series.\n\n## Getting ready\n\nFor this recipe, we will need the Pandas package imported as `pd`, the Matplotlib `pyplot` package imported as `plt`, and the `Prophet` object from the Prophet library, which can be imported using the following command:\n\n```", "```py\n\nWe also need to import the `generate_sample_data` routine from the `tsdata` module, which is included in the code repository for this book:\n\n```", "```py\n\n## How to do it...\n\nThe following steps show you how to use the Prophet package to generate forecasts for a sample time series:\n\n1.  First, we use `generate_sample_data` to generate the sample time series data:\n\n```", "```py\n\n2.  We need to convert the sample data into a `DataFrame` that Prophet expects:\n\n```", "```py\n\n3.  Next, we make a model using the `Prophet` class and fit it to the sample time series:\n\n```", "```py\n\n4.  Now, we create a new `DataFrame` that contains the time intervals for the original time series, plus the additional periods for the forecasts:\n\n```", "```py\n\n5.  Then, we use the `predict` method to produce the forecasts along the time periods we just created:\n\n```", "```py\n\n6.  Finally, we plot the predictions on top of the sample time series data, along with the confidence interval and the true future values:\n\n```", "```py\n\nThe plot of the time series, along with forecasts, can be seen in the following figure:\n\n          ![](assets/9a9b3e93-8bbb-491f-8f99-2357a722754a.png)\n\n        Figure 7.15: Plot of sample time series data, along with forecasts and a confidence interval\n\n## How it works...\n\nProphet is a package that's used to automatically produce models for time series data based on sample data, with little extra input needed from the user. In practice, it is very easy to use; we just need to create an instance of the `Prophet` class, call the `fit` method, and then we are ready to produce forecasts and understand our data using the model.\n\nThe `Prophet` class expects the data in a specific format: a `DataFrame` with columns named `ds` for the date/time index, and `y` for the response data (the time series values). This `DataFrame` should have integer indices. Once the model has been fit, we use `make_future_dataframe` to create a `DataFrame` in the correct format, with appropriate date intervals, and with additional rows for future time intervals. The `predict` method then takes this `DataFrame` and produces values using the model to populate these time intervals with predicted values. We also get other information, such as the confidence intervals, in this forecast's `DataFrame`.\n\n## There's more...\n\nProphet does a fairly good job of modeling time series data without any input from the user. However, the model can be customized using various methods from the `Prophet` class. For example, we could provide information about the seasonality of the data using the `add_seasonality` method of the `Prophet` class, prior to fitting the model.\n\nThere are alternative packages for automatically generating models for time series data. For example, popular machine learning libraries such as TensorFlow can be used to model time series data.\n\n# Further reading\n\nA good textbook on regression in statistics is the book *Probability and Statistics* by Mendenhall, Beaver, and Beaver, as mentioned in [Chapter 6](87b0f91d-3086-41a9-995d-27fe7d364e8b.xhtml), *Working with Data and Statistics*. The following books provide a good introduction to classification and regression in modern data science:\n\n*   *James, G. and Witten, D., 2013\\. An Introduction To Statistical Learning: With Applications In R. New York: Springer.*\n\n*   *M\u00fcller, A. and Guido, S., 2016\\. Introduction To Machine Learning With Python. Sebastopol: O'Reilly Media.*\n\nA good introduction to time series analysis can be found in the following book:\n\n*   *Cryer, J. and Chan, K., 2008\\. Time Series Analysis. New York: Springer.** \n```"]