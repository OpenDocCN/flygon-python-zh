- en: Getting Started with PyCUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用PyCUDA
- en: In the last chapter, we set up our programming environment. Now, with our drivers
    and compilers firmly in place, we will begin the actual GPU programming! We will
    start by learning how to use PyCUDA for some basic and fundamental operations.
    We will first see how to query our GPU—that is, we will start by writing a small
    Python program that will tell us what the characteristics of our GPU are, such
    as the core count, architecture, and memory. We will then spend some time getting
    acquainted with how to transfer memory between Python and the GPU with PyCUDA's
    `gpuarray` class and how to use this class for basic computations. The remainder
    of this chapter will be spent showing how to write some basic functions (which
    we will refer to as **CUDA Kernels**) that we can directly launch onto the GPU.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们设置了编程环境。现在，有了我们的驱动程序和编译器牢固地安装好，我们将开始实际的GPU编程！我们将首先学习如何使用PyCUDA进行一些基本和基础的操作。我们将首先看看如何查询我们的GPU
    - 也就是说，我们将首先编写一个小的Python程序，告诉我们GPU的特性，如核心数量、架构和内存。然后，我们将花一些时间熟悉如何在Python和GPU之间传输内存，使用PyCUDA的`gpuarray`类以及如何使用这个类进行基本计算。本章的其余部分将花在展示如何编写一些基本函数（我们将称之为**CUDA内核**），我们可以直接启动到GPU上。
- en: 'The learning outcomes for this chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习成果如下：
- en: Determining GPU characteristics, such as memory capacity or core count, using
    PyCUDA
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyCUDA确定GPU特性，如内存容量或核心数量
- en: Understanding the difference between host (CPU) and device (GPU) memory and
    how to use PyCUDA's `gpuarray` class to transfer data between the host and device
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解主机（CPU）和设备（GPU）内存之间的区别，以及如何使用PyCUDA的`gpuarray`类在主机和设备之间传输数据
- en: How to do basic calculations using only `gpuarray` objects
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用只有`gpuarray`对象进行基本计算
- en: How to perform basic element-wise operations on the GPU with the PyCUDA `ElementwiseKernel`
    function
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用PyCUDA的`ElementwiseKernel`函数在GPU上执行基本的逐元素操作
- en: Understanding the functional programming concept of reduce/scan operations and
    how to make a basic reduction or scan CUDA kernel
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解函数式编程概念的reduce/scan操作，以及如何制作基本的缩减或扫描CUDA内核
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016 onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0 onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一台安装了现代NVIDIA GPU（2016年以后）的Linux或Windows 10 PC，并安装了所有必要的GPU驱动程序和CUDA Toolkit（9.0以后）。还需要一个合适的Python
    2.7安装（如Anaconda Python 2.7），并安装了PyCUDA模块。
- en: This chapter's code is also available on GitHub at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可以在GitHub上找到：[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)。
- en: For more information about the prerequisites, check the *Preface* of this book;
    for the software and hardware requirements, check the `README` section in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有关先决条件的更多信息，请查看本书的*前言*；有关软件和硬件要求，请查看[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)中的`README`部分。
- en: Querying your GPU
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询您的GPU
- en: 'Before we begin to program our GPU, we should really know something about its
    technical capacities and limits. We can determine this by doing what is known
    as a **GPU query**. A GPU query is a very basic operation that will tell us the
    specific technical details of our GPU, such as available GPU memory and core count.
    NVIDIA includes a command-line example written in pure CUDA-C called `deviceQuery`
    in the `samples` directory (for both Windows and Linux) that we can run to perform
    this operation. Let''s take a look at the output that is produced on the author''s
    Windows 10 laptop (which is a Microsoft Surface Book 2 with a GTX 1050 GPU):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编程GPU之前，我们应该真正了解一些关于其技术能力和限制的知识。我们可以通过进行所谓的**GPU查询**来确定这一点。GPU查询是一个非常基本的操作，它将告诉我们GPU的具体技术细节，如可用的GPU内存和核心数量。NVIDIA在`samples`目录中（适用于Windows和Linux）包含了一个纯CUDA-C编写的命令行示例`deviceQuery`，我们可以运行它来执行此操作。让我们看一下作者的Windows
    10笔记本电脑（Microsoft Surface Book 2，配备了GTX 1050 GPU）上产生的输出：
- en: '![](assets/ef6b22de-9871-49b2-ad73-4e7aff2017ac.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ef6b22de-9871-49b2-ad73-4e7aff2017ac.png)'
- en: 'Let''s look at some of the essentials of all of the technical information displayed
    here. First, we see that there is only one GPU installed, Device 0—it is possible
    that a host computer has multiple GPUs and makes use of them, so CUDA will designate
    each *GPU device* an individual number. There are some cases where we may have
    to be specific about the device number, so it is always good to know. We can also
    see the specific type of device that we have (here, GTX 1050), and which CUDA
    version we are using. There are two more things we will take note of for now:
    the total number of cores (here, 640), and the total amount of global memory on
    the device (in this case, 2,048 megabytes, that is, 2 gigabytes).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这里显示的所有技术信息的一些基本要点。首先，我们看到只安装了一个GPU，设备0 - 可能主机计算机安装了多个GPU并使用它们，因此CUDA将为每个*GPU设备*指定一个独立的编号。有些情况下，我们可能需要明确指定设备编号，所以了解这一点总是很好的。我们还可以看到我们拥有的具体设备类型（这里是GTX
    1050），以及我们正在使用的CUDA版本。现在我们还要注意两件事：核心的总数（这里是640），以及设备上的全局内存总量（在本例中为2,048兆字节，即2千兆字节）。
- en: While you can see many other technical details from `deviceQuery`, the core
    count and amount of memory are usually the first two things your eyes should zero
    in on the first time you run this on a new GPU, since they can give you the most
    immediate idea of the capacity of your new device.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以从`deviceQuery`中看到许多其他技术细节，但核心数量和内存量通常是您第一次在新GPU上运行时应该关注的前两件事，因为它们可以让您最直接地了解新设备的容量。
- en: Querying your GPU with PyCUDA
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyCUDA查询您的GPU
- en: Now, finally, we will begin our foray into the world of GPU programming by writing
    our own version of `deviceQuery` in Python. Here, we will primarily concern ourselves
    with only the amount of available memory on the device, the compute capability,
    the number of multiprocessors, and the total number of CUDA cores.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最后，我们将通过用Python编写我们自己的版本的`deviceQuery`来开始我们的GPU编程之旅。在这里，我们主要关注设备上可用内存的数量，计算能力，多处理器的数量和CUDA核心的总数。
- en: 'We will begin by initializing CUDA as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从以下方式初始化CUDA：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that we will always have to initialize PyCUDA with `pycuda.driver.init()`
    or by importing the PyCUDA `autoinit` submodule with `import pycuda.autoinit`!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将始终需要使用`pycuda.driver.init()`或通过导入PyCUDA的`autoinit`子模块`import pycuda.autoinit`来初始化PyCUDA！
- en: 'We can now immediately check how many GPU devices we have on our host computer
    with this line:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以立即检查我们的主机计算机上有多少个GPU设备：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s type this into IPython and see what happens:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在IPython中输入这个并看看会发生什么：
- en: '![](assets/9c6850ad-552d-48ed-a6d5-4145c4f7407f.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9c6850ad-552d-48ed-a6d5-4145c4f7407f.png)'
- en: Great! So far, I have verified that my laptop does indeed have one GPU in it.
    Now, let's extract some more interesting information about this GPU (and any other
    GPU on the system) by adding a few more lines of code to iterate over each device
    that can be individually accessed with `pycuda.driver.Device` (indexed by number).
    The name of the device (for example, GeForce GTX 1050) is given by the `name`
    function. We then get the **compute capability** of the device with the `compute_capability`
    function and total amount of device memory with the `total_memory` function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！到目前为止，我已经验证了我的笔记本确实有一个GPU。现在，让我们通过添加几行代码来迭代可以通过`pycuda.driver.Device`（按编号索引）单独访问的每个设备，以提取有关此GPU（以及系统上的任何其他GPU）的更多有趣信息。设备的名称（例如，GeForce
    GTX 1050）由`name`函数给出。然后我们使用`compute_capability`函数获取设备的**计算能力**和`total_memory`函数获取设备的总内存量。
- en: '**Compute capability** can be thought of as a *version number* for each NVIDIA
    GPU architecture; this will give us some important information about the device
    that we can''t otherwise query, as we will see in a minute.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算能力**可以被视为每个NVIDIA GPU架构的*版本号*；这将为我们提供一些关于设备的重要信息，否则我们无法查询，我们将在一分钟内看到。'
- en: 'Here''s how we will write it:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这样写：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we are ready to look at some of the remaining attributes of our GPU, which
    PyCUDA yields to us in the form of a Python dictionary type. We will use the following
    lines to convert this into a dictionary that is indexed by strings indicating
    attributes:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备查看PyCUDA以Python字典类型形式提供给我们的GPU的一些剩余属性。我们将使用以下行将其转换为由字符串索引属性的字典：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now determine the number of *multiprocessors* on our device with the
    following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下内容确定设备上的*多处理器*数量：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A GPU divides its individual cores up into larger units known as **Streaming** **Multiprocessors
    (SMs)**; a GPU device will have several SMs, which will each individually have
    a particular number of CUDA cores, depending on the compute capability of the
    device. To be clear: the number of cores per multiprocessor is not indicated directly
    by the GPU—this is given to us implicitly by the compute capability. We will have
    to look up some technical documents from NVIDIA to determine the number of cores
    per multiprocessor (see [http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)),
    and then create a lookup table to give us the number of cores per multiprocessor.
    We do so as such, using the `compute_capability` variable to look up the number
    of cores:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GPU将其各个核心划分为称为**流处理器（SMs）**的较大单元； GPU设备将具有多个SM，每个SM将根据设备的计算能力具有特定数量的CUDA核心。要明确：每个多处理器的核心数并不是由GPU直接指示的-这是由计算能力隐含给我们的。我们将不得不查阅NVIDIA的一些技术文件以确定每个多处理器的核心数（参见[http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)），然后创建一个查找表来给出每个多处理器的核心数。我们使用`compute_capability`变量来查找核心数：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now finally determine the total number of cores on our device by multiplying
    these two numbers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过将这两个数字相乘来最终确定设备上的总核心数：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now can finish up our program by iterating over the remaining keys in our
    dictionary and printing the corresponding values:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过迭代字典中剩余的键并打印相应的值来完成我们的程序：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So, now we finally completed our first true GPU program of the text! (Also
    available at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py)).
    Now, we can run it as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们终于完成了本文的第一个真正的GPU程序！（也可在[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py)找到）。现在，我们可以按如下方式运行它：
- en: '![](assets/59a5907a-0a76-4c08-bfe6-349d9ce48c71.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/59a5907a-0a76-4c08-bfe6-349d9ce48c71.png)'
- en: We can now have a little pride that we can indeed write a program to query our
    GPU! Now, let's actually begin to learn to *use* our GPU, rather than just observe
    it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以有点自豪，因为我们确实可以编写一个程序来查询我们的GPU！现在，让我们真正开始学习*使用*我们的GPU，而不仅仅是观察它。
- en: Using PyCUDA's gpuarray class
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyCUDA的gpuarray类
- en: Much like how NumPy's `array` class is the cornerstone of numerical programming
    within the NumPy environment, PyCUDA's `gpuarray` class plays an analogously prominent
    role within GPU programming in Python. This has all of the features you know and
    love from NumPy—multidimensional vector/matrix/tensor shape structuring, array-slicing,
    array unraveling, and overloaded operators for point-wise computations (for example,
    `+`, `-`, `*`, `/`, and `**`).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 NumPy 的 `array` 类是 NumPy 环境中数值编程的基石一样，PyCUDA 的 `gpuarray` 类在 Python 中的 GPU
    编程中扮演着类似的重要角色。它具有你从 NumPy 中熟悉和喜爱的所有功能——多维向量/矩阵/张量形状结构、数组切片、数组展开，以及用于逐点计算的重载运算符（例如
    `+`、`-`、`*`、`/` 和 `**`）。
- en: '`gpuarray` is really an indispensable tool for any budding GPU programmer.
    We will spend this section going over this particular data structure and gaining
    a strong grasp of it before we move on.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpuarray` 对于任何新手 GPU 程序员来说都是一个不可或缺的工具。在我们继续之前，我们将花费这一部分时间来了解这种特定的数据结构，并对其有一个深入的理解。'
- en: Transferring data to and from the GPU with gpuarray
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 gpuarray 将数据传输到 GPU 和从 GPU 中传输数据
- en: As we note from writing our prior `deviceQuery` program in Python, a GPU has
    its own memory apart from the host computer's memory, which is known as **device
    memory**. (Sometimes this is known more specifically as **global device memory***,*
    to differentiate this from the additional cache memory, shared memory, and register
    memory that is also on the GPU.) For the most part, we treat (global) device memory
    on the GPU as we do dynamically allocated heap memory in C (with the `malloc`
    and `free` functions) or C++ (as with the `new` and `delete` operators); in CUDA
    C, this is complicated further with the additional task of transferring data back
    and forth between the CPU to the GPU (with commands such as `cudaMemcpyHostToDevice`
    and `cudaMemcpyDeviceToHost`), all while keeping track of multiple pointers in
    both the CPU and GPU space and performing proper memory allocations (`cudaMalloc`)
    and deallocations (`cudaFree`).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 Python 中编写的先前的 `deviceQuery` 程序所示，GPU 有自己的内存，与主机计算机的内存分开，这被称为**设备内存**。（有时这更具体地被称为**全局设备内存**，以区分它与
    GPU 上的其他缓存内存、共享内存和寄存器内存。）在大多数情况下，我们将 GPU 上的（全局）设备内存视为我们在 C 中动态分配的堆内存（使用 `malloc`
    和 `free` 函数）或 C++（使用 `new` 和 `delete` 运算符）；在 CUDA C 中，这进一步复杂化，需要在 CPU 和 GPU 空间之间来回传输数据（使用诸如
    `cudaMemcpyHostToDevice` 和 `cudaMemcpyDeviceToHost` 的命令），同时跟踪 CPU 和 GPU 空间中的多个指针，并执行适当的内存分配（`cudaMalloc`）和释放（`cudaFree`）。
- en: Fortunately, PyCUDA covers all of the overhead of memory allocation, deallocation,
    and data transfers with the `gpuarray` class. As stated, this class acts similarly
    to NumPy arrays, using vector/ matrix/tensor shape structure information for the
    data. `gpuarray` objects even perform automatic cleanup based on the lifetime,
    so we do not have to worry about *freeing* any GPU memory stored in a `gpuarray`
    object when we are done with it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，PyCUDA 通过 `gpuarray` 类涵盖了所有的内存分配、释放和数据传输的开销。正如所述，这个类类似于 NumPy 数组，使用矢量/矩阵/张量形状结构信息来处理数据。`gpuarray`
    对象甚至根据其生命周期自动执行清理，因此当我们完成后，我们不必担心释放存储在 `gpuarray` 对象中的任何 GPU 内存。
- en: How exactly do we use this to transfer data from the host to the GPU? First,
    we must contain our host data in some form of NumPy array (let's call it `host_data`),
    and then use the `gpuarray.to_gpu(host_data)` command to transfer this over to
    the GPU and create a new GPU array.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何使用它将数据从主机传输到 GPU 呢？首先，我们必须将我们的主机数据包含在某种形式的 NumPy 数组中（我们称之为 `host_data`），然后使用
    `gpuarray.to_gpu(host_data)` 命令将其传输到 GPU 并创建一个新的 GPU 数组。
- en: 'Let''s now perform a simple computation within the GPU (pointwise multiplication
    by a constant on the GPU), and then retrieve the GPU data into a new with the
    `gpuarray.get` function. Let''s load up IPython and see how this works (note that
    here we will initialize PyCUDA with `import pycuda.autoinit`):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在 GPU 中执行一个简单的计算（在 GPU 上的常数点乘），然后使用 `gpuarray.get` 函数将 GPU 数据检索到一个新的数组中。让我们加载
    IPython 并看看它是如何工作的（请注意，这里我们将使用 `import pycuda.autoinit` 初始化 PyCUDA）：
- en: '![](assets/14eef3e5-273f-45c9-b42f-99d07628f9d8.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/14eef3e5-273f-45c9-b42f-99d07628f9d8.png)'
- en: 'One thing to note is that we specifically denoted that the array on the host
    had its type specifically set to a NumPy `float32` type with the `dtype` option
    when we set up our NumPy array; this corresponds directly with the float type
    in C/C++. Generally speaking, it''s a good idea to specifically set data types
    with NumPy when we are sending data to the GPU. The reason for this is twofold:
    first, since we are using a GPU for increasing the performance of our application,
    we don''t want any unnecessary overhead of using an unnecessary type that will
    possibly take up more computational time or memory, and second, since we will
    soon be writing portions of code in inline CUDA C, we will have to be very specific
    with types or our code won''t work correctly, keeping in mind that C is a statically-typed
    language.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，当我们设置 NumPy 数组时，我们特别指定了主机上的数组类型为 NumPy `float32` 类型，并使用 `dtype` 选项；这与
    C/C++ 中的浮点类型直接对应。一般来说，当我们发送数据到 GPU 时，最好使用 NumPy 明确设置数据类型。原因有两个：首先，由于我们使用 GPU 来提高应用程序的性能，我们不希望使用不必要的类型造成不必要的计算时间或内存开销；其次，由于我们很快将在内联
    CUDA C 中编写代码的部分，我们必须非常具体地指定类型，否则我们的代码将无法正确工作，要记住 C 是一种静态类型语言。
- en: Remember to specifically set data types for NumPy arrays that will be transferred
    to the GPU. This can be done with the `dtype` option in the constructor of the
    `numpy.array` class.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 记得为将要传输到 GPU 的 NumPy 数组明确设置数据类型。这可以在 `numpy.array` 类的构造函数中使用 `dtype` 选项来完成。
- en: Basic pointwise arithmetic operations with gpuarray
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 gpuarray 进行基本的逐点算术运算
- en: 'In the last example, we saw that we can use the (overloaded) Python multiplication
    operator (`*` ) to multiply each element in a `gpuarray` object by a scalar value
    (here it was 2); note that a pointwise operation is intrinsically parallelizable,
    and so when we use this operation on a `gpuarray` object PyCUDA is able to offload
    each multiplication operation onto a single thread, rather than computing each
    multiplication in serial, one after the other (in fairness, some versions of NumPy
    can use the advanced SSE instructions found in modern x86 chips for these computations,
    so in some cases the performance will be comparable to a GPU). To be clear: these
    pointwise operations performed on the GPU are in parallel since the computation
    of one element is not dependent on the computation of any other element.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个示例中，我们看到我们可以使用（重载的）Python乘法运算符（`*`）来将`gpuarray`对象中的每个元素乘以一个标量值（这里是2）；请注意，逐点操作本质上是可并行化的，因此当我们在`gpuarray`对象上使用此操作时，PyCUDA能够将每个乘法操作分配到单个线程上，而不是依次串行计算每个乘法（公平地说，一些版本的NumPy可以使用现代x86芯片中的高级SSE指令进行这些计算，因此在某些情况下性能将与GPU相当）。明确一点：在GPU上执行的这些逐点操作是并行的，因为一个元素的计算不依赖于任何其他元素的计算。
- en: 'To get a feel for how the operators work, I would suggest that the reader load
    up IPython and create a few `gpuarray` objects on the GPU, and then play around
    with these operations for a few minutes to see that these operators do work similarly
    to arrays in NumPy. Here is some inspiration:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这些运算符的工作原理，我建议读者加载IPython并在GPU上创建一些`gpuarray`对象，然后玩几分钟，看看这些运算符是否与NumPy中的数组类似。以下是一些灵感：
- en: '![](assets/fd5469e2-c573-472e-a1a9-6da1dc61ddc5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/fd5469e2-c573-472e-a1a9-6da1dc61ddc5.png)'
- en: Now, we can see that `gpuarray` objects act predictably and are in accordance
    with how NumPy arrays act. (Notice that we will have to pull the output off the
    GPU with the `get` function!) Let's now do some comparison between CPU and GPU
    computation time to see if and when there is any advantage to doing these operations
    on the GPU.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到`gpuarray`对象的行为是可预测的，并且与NumPy数组的行为一致。（请注意，我们将不得不使用`get`函数从GPU中获取输出！）现在让我们比较一下CPU和GPU计算时间，看看在何时是否有任何优势进行这些操作。
- en: A speed test
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 速度测试
- en: 'Let''s write up a little program (`time_calc0.py`) that will do a speed comparison
    test between a scalar multiplication on the CPU and then the same operation on
    the GPU. We will then use NumPy''s `allclose` function to compare the two output
    values. We will generate an array of 50 million random 32-bit floating point values
    (this will amount to roughly 48 megabytes of data, so this should be entirely
    feasible with several gigabytes of memory on any somewhat modern host and GPU
    device), and then we will time how long it takes to scalar multiply the array
    by two on both devices. Finally, we will compare the output values to ensure that
    they are equal. Here''s how it''s done:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个小程序（`time_calc0.py`），对CPU上的标量乘法和GPU上的相同操作进行速度比较测试。然后，我们将使用NumPy的`allclose`函数比较两个输出值。我们将生成一个包含5000万个随机32位浮点值的数组（这将大约占用48兆字节的数据，因此在任何稍微现代的主机和GPU设备上都应该完全可行），然后我们将计算在两个设备上将数组乘以2所需的时间。最后，我们将比较输出值以确保它们相等。操作如下：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: (You can find the `time_calc0.py` file on the repository provided to you earlier.)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: （您可以在之前提供给您的存储库中找到`time_calc0.py`文件。）
- en: 'Now, let''s load up IPython and run this a few times to get an idea of the
    general speed of these, and see if there is any variance. (Here, this is being
    run on a 2017-era Microsoft Surface Book 2 with a Kaby Lake i7 processor and a
    GTX 1050 GPU.):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载IPython并运行几次，以了解这些的一般速度，并查看是否有任何变化。（这里，这是在2017年的微软Surface Book 2上运行的，配备了Kaby
    Lake i7处理器和GTX 1050 GPU。）：
- en: '![](assets/a278a2f1-e099-4907-805f-708f2884a7c3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/a278a2f1-e099-4907-805f-708f2884a7c3.png)'
- en: We first notice that the CPU computation time is about the same for each computation
    (roughly 0.08 seconds). Yet, we notice that the GPU computation time is far slower
    than the CPU computation the first time we run this (1.09 seconds), and it becomes
    much faster in the subsequent run, which remains roughly constant in every following
    run (in the range of 7 or 9 milliseconds). If you exit IPython, and then run the
    program again, the same thing will occur. What is the reason for this phenomenon?
    Well, let's do some investigative work using IPython's built-in `prun` profiler.
    (This works similarly to the `cProfiler` module that was featured in [Chapter
    1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why GPU Programming?*.)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到，每次计算的CPU计算时间大致相同（大约0.08秒）。然而，我们注意到，第一次运行时，GPU计算时间比CPU计算时间慢得多（1.09秒），并且在随后的运行中变得快得多，在每次后续运行中保持大致恒定（在7或9毫秒的范围内）。如果您退出IPython，然后再次运行程序，将发生相同的情况。这种现象的原因是什么？好吧，让我们使用IPython内置的`prun`分析器进行一些调查工作。（这类似于[第1章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)中介绍的`cProfiler`模块，*为什么要进行GPU编程？*）
- en: 'First, let''s load our program as text within IPython with the following lines,
    which we can then run with our profiler via Python''s `exec` command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将我们的程序作为文本加载到IPython中，然后通过Python的`exec`命令运行我们的分析器：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We now type `%prun -s cumulative exec(time_calc_code)` into our IPython console
    (with the leading `%`) and see what operations are taking the most time:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在IPython控制台中键入`%prun -s cumulative exec(time_calc_code)`（带有前导`%`）并查看哪些操作花费了最多的时间：
- en: '![](assets/7dfbfc79-dcc1-4cc8-b7b6-7f11103f54e6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/7dfbfc79-dcc1-4cc8-b7b6-7f11103f54e6.png)'
- en: 'Now, there are a number of suspicious calls to a Python module file, `compiler.py`; these
    take roughly one second total, a little less than the time it takes to do the
    GPU computation here. Now let''s run this again and see if there are any differences:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一些可疑的对Python模块文件`compiler.py`的调用；这些调用总共大约需要一秒钟，比在这里进行GPU计算所需的时间略少。现在让我们再次运行一下，看看是否有任何差异：
- en: '![](assets/da5995b8-f05d-45d7-950c-f921d79b3886.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/da5995b8-f05d-45d7-950c-f921d79b3886.png)'
- en: Notice that this time, there are no calls to `compiler.py`. Why is this? By
    the nature of the PyCUDA library, GPU code is often compiled and linked with NVIDIA's
    `nvcc` compiler the first time it is run in a given Python session; it is then
    cached and, if the code is called again, then it doesn't have to be recompiled.
    This may include even *simple* operations such as this scalar multiply! (We will
    see eventually see that this can be ameliorated by using the pre-compiled code
    in, [Chapter 10](5383b46f-8dc6-4e17-ab35-7f6bd35f059f.xhtml), *Working with Compiled
    GPU Code*, or by using NVIDIA's own linear algebra libraries with the Scikit-CUDA
    module, which we will see in [Chapter 7](55146879-4b7e-4774-9a8b-cc5c80c04ed8.xhtml),
    *Using the CUDA Libraries with Scikit-CUDA*).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一次没有调用`compiler.py`。为什么呢？由于PyCUDA库的性质，GPU代码通常在给定的Python会话中首次运行时使用NVIDIA的`nvcc`编译器进行编译和链接；然后它被缓存，如果再次调用代码，则不必重新编译。这甚至可能包括*简单*的操作，比如标量乘法！（我们最终会看到，通过使用[第10章](5383b46f-8dc6-4e17-ab35-7f6bd35f059f.xhtml)中的预编译代码或使用NVIDIA自己的线性代数库与Scikit-CUDA模块一起使用CUDA库，可以改善这一点，我们将在[第7章](55146879-4b7e-4774-9a8b-cc5c80c04ed8.xhtml)中看到）。
- en: In PyCUDA, GPU code is often compiled at runtime with the NVIDIA `nvcc` compiler
    and then subsequently called from PyCUDA. This can lead to an unexpected slowdown,
    usually the first time a program or GPU operation is run in a given Python session.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyCUDA中，GPU代码通常在运行时使用NVIDIA的`nvcc`编译器进行编译，然后从PyCUDA中调用。这可能会导致意外的减速，通常是在给定的Python会话中首次运行程序或GPU操作时。
- en: Using PyCUDA's ElementWiseKernel for performing pointwise computations
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyCUDA的ElementWiseKernel执行逐点计算
- en: We will now see how to program our own point-wise (or equivalently, *element-wise*)
    operations directly onto our GPU with the help of PyCUDA's `ElementWiseKernel`
    function. This is where our prior knowledge of C/C++ programming will become useful—we'll
    have to write a little bit of *inline code* in CUDA C, which is compiled externally
    by NVIDIA's `nvcc` compiler and then launched at runtime by our code via PyCUDA.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用PyCUDA的`ElementWiseKernel`函数直接在GPU上编写我们自己的逐点（或等效地，*逐元素*）操作。这就是我们之前对C/C++编程的了解将变得有用的地方——我们将不得不在CUDA
    C中编写一点*内联代码*，这些代码是由NVIDIA的`nvcc`编译器在外部编译的，然后通过PyCUDA在运行时由我们的代码启动。
- en: We use the term **kernel** quite a bit in this text; by *kernel*, we always
    mean a function that is launched directly onto the GPU by CUDA. We will use several
    functions from PyCUDA that generate templates and design patterns for different
    types of kernels, easing our transition into GPU programming.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们经常使用术语**kernel**；通过*kernel*，我们总是指的是由CUDA直接启动到GPU上的函数。我们将使用PyCUDA的几个函数来生成不同类型的kernel的模板和设计模式，以便更轻松地过渡到GPU编程。
- en: 'Let''s dive right in; we''re going to start by explicitly rewriting the code
    to multiply each element of a `gpuarray` object by 2 in CUDA-C; we will use the
    `ElementwiseKernel` function from PyCUDA to generate our code. You should try
    typing the following code directly into an IPython console. (The less adventurous
    can just download this from this text''s Git repository, which has the filename
    `simple_element_kernel_example0.py`):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接开始；我们将从头开始重写代码，使用CUDA-C将`gpuarray`对象的每个元素乘以2；我们将使用PyCUDA的`ElementwiseKernel`函数来生成我们的代码。您应该尝试直接在IPython控制台中输入以下代码。（不那么冒险的人可以从本文的Git存储库中下载，文件名为`simple_element_kernel_example0.py`）：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let's take a look at how this is set up; this is, of course, several lines of
    inline C. We first set the input and output variables in the first line ( `"float
    *in, float *out"` ), which will generally be in the form of C pointers to allocated
    memory on the GPU. In the second line, we define our element-wise operation with
    `"out[i] = 2*in[i];"`, which will multiply each point in `in` by two and place
    this in the corresponding index of `out`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是如何设置的；当然，这是几行内联C。我们首先在第一行中设置输入和输出变量（`"float *in, float *out"`），这通常是指指向GPU上已分配内存的C指针的形式。在第二行中，我们使用`"out[i]
    = 2*in[i];"`定义了我们的逐元素操作，它将把`in`中的每个点乘以2，并将其放在`out`的相应索引中。
- en: Note that PyCUDA automatically sets up the integer index `i` for us. When we
    use `i` as our index, `ElementwiseKernel` will automatically parallelize our calculation
    over `i` among the many cores in our GPU. Finally, we give our piece of code its
    internal CUDA C kernel name ( `"gpu_2x_ker"` ). Since this refers to CUDA C's
    namespace and not Python's, it's fine (and also convenient) to give this the same
    name as in Python.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，PyCUDA会自动为我们设置整数索引`i`。当我们使用`i`作为我们的索引时，`ElementwiseKernel`将自动在GPU的许多核心中并行化我们的计算。最后，我们给我们的代码片段起了一个内部CUDA
    C kernel的名称（`"gpu_2x_ker"`）。由于这是指CUDA C的命名空间而不是Python的，因此将其与Python中的名称相同是可以的（也很方便）。
- en: 'Now, let''s do a speed comparison:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行速度比较：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s run this program:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行这个程序：
- en: '![](assets/02db7f9f-e682-41fb-af4c-2833d054a746.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/02db7f9f-e682-41fb-af4c-2833d054a746.png)'
- en: 'Whoa! That doesn''t look good. Let''s run the `speedcomparison()` function
    a few times from IPython:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！看起来不太好。让我们从IPython中运行`speedcomparison()`函数几次：
- en: '![](assets/9514e819-e7cd-42c3-b1af-d5ea832c6864.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9514e819-e7cd-42c3-b1af-d5ea832c6864.png)'
- en: As we can see, the speed increases dramatically after the first time we use
    a given GPU function. Again, as with the prior example, this is because PyCUDA
    compiles our inline CUDA C code the first time a given GPU kernel function is
    called using the `nvcc` compiler. After the code is compiled, then it is cached
    and re-used for the remainder of a given Python session.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，第一次使用给定的GPU函数后，速度显著增加。与前面的例子一样，这是因为PyCUDA在首次调用给定的GPU kernel函数时使用`nvcc`编译器编译我们的内联CUDA
    C代码。代码编译后，它将被缓存并在给定的Python会话的其余部分中重复使用。
- en: 'Now, let''s cover something else important before we move on, which is very
    subtle. The little kernel function we defined operates on C float pointers; this
    means that we will have to allocate some empty memory on the GPU that is pointed
    to by the `out` variable. Take a look at this portion of code again from the `speedcomparison()`
    function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们继续之前，让我们再讨论一些重要的事情，这是非常微妙的。我们定义的小内核函数操作C浮点指针；这意味着我们将不得不在GPU上分配一些空的内存，该内存由`out`变量指向。再次看一下`speedcomparison()`函数中的这部分代码：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As we did before, we send a NumPy array over to the GPU (`host_data`) via the
    `gpuarray.to_gpu` function, which automatically allocates data onto the GPU and
    copies it over from the CPU space. We will plug this into the `in` part of our
    kernel function. In the next line, we allocate empty memory on the GPU with the
    `gpuarray.empty_like` function. This acts as a plain `malloc` in C, allocating
    an array of the same size and data type as `device_data`, but without copying
    anything. We can now use this for the `out` part of our kernel function. We now
    look at the next line in `speedcomparison()` to see how to launch our kernel function
    onto the GPU (ignoring the lines we use for timing):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们通过`gpuarray.to_gpu`函数将一个NumPy数组（`host_data`）发送到GPU，该函数会自动将数据分配到GPU并从CPU空间复制过来。我们将把这个数组插入到我们内核函数的`in`部分。在下一行，我们使用`gpuarray.empty_like`函数在GPU上分配空的内存。这类似于C中的普通`malloc`，分配一个与`device_data`大小和数据类型相同的数组，但不复制任何内容。现在我们可以将其用于内核函数的`out`部分。现在我们来看一下`speedcomparison()`中的下一行，看看如何将我们的内核函数启动到GPU上（忽略我们用于计时的行）：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Again, the variables we set correspond directly to the first line we defined
    with `ElementwiseKernel` (here being, `"float *in, float *out"`).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们设置的变量直接对应于我们用`ElementwiseKernel`定义的第一行（这里是`"float *in, float *out"`）。
- en: Mandelbrot revisited
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 曼德勃罗重新审视
- en: 'Let''s again look at the problem of generating the Mandelbrot set from [Chapter
    1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why GPU Programming?*. The original
    code is available under the `1` folder in the repository, with the filename `mandelbrot0.py`,
    which you should take another look at before we continue. We saw that there were
    two main components of this program: the first being the generation of the Mandelbrot
    set, and the second concerning dumping the Mandelbrot set into a PNG file. In
    the first chapter, we realized that we could parallelize only the generation of
    the Mandelbrot set, and considering that this takes the bulk of the time for the
    program to do, this would be a good candidate for an algorithm to offload this
    onto a GPU. Let''s figure out how to do this. (We will refrain from re-iterating
    over the definition of the Mandelbrot set, so if you need a deeper review, please
    re-read the *Mandelbrot* *revisited* section of [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why
    GPU Programming?*)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次从[第1章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)“为什么使用GPU编程？”中生成曼德勃罗集的问题。原始代码可以在存储库的`1`文件夹中找到，文件名为`mandelbrot0.py`，在继续之前，您应该再次查看一下。我们看到该程序有两个主要组成部分：第一个是生成曼德勃罗集，第二个是将曼德勃罗集转储到PNG文件中。在第一章中，我们意识到我们只能并行生成曼德勃罗集，并且考虑到这占程序运行时间的大部分，这将是一个很好的候选算法，可以将其转移到GPU上。让我们看看如何做到这一点。（我们将避免重复定义曼德勃罗集，因此如果您需要更深入的复习，请重新阅读[第1章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)“为什么使用GPU编程？”中的“曼德勃罗重新审视”部分）
- en: 'First, let''s make a new Python function based on `simple_mandelbrot` from
    the original program. We''ll call it `gpu_mandelbrot`, and this will take in the
    same exact input as before:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们基于原始程序中的`simple_mandelbrot`创建一个新的Python函数。我们将其称为`gpu_mandelbrot`，这将接受与之前完全相同的输入：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We will proceed a little differently from here. We will start by building a
    complex lattice that consists of each point in the complex plane that we will
    analyze.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这里开始以稍微不同的方式进行。我们将首先构建一个复杂的晶格，其中包含我们将分析的复平面中的每个点。
- en: 'Here, we''ll use some tricks with the NumPy matrix type to easily generate
    the lattice, and then typecast the result from a NumPy `matrix` type to a two-dimensional
    NumPy `array` (since PyCUDA can only handle NumPy `array` types, not `matrix`
    types). Notice how we are very carefully setting our NumPy types:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用一些NumPy矩阵类型的技巧轻松生成晶格，然后将结果从NumPy `matrix`类型转换为二维NumPy `array`类型（因为PyCUDA只能处理NumPy
    `array`类型，而不能处理`matrix`类型）。请注意我们非常小心地设置我们的NumPy类型：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'So, we now have a two-dimensional complex array that represents the lattice
    from which we will generate our Mandelbrot set; as we will see, we can operate
    on this very easily within the GPU. Let''s now transfer our lattice to the GPU
    and allocate an array that we will use to represent our Mandelbrot set:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在有一个表示我们将生成曼德勃罗集的晶格的二维复杂数组；正如我们将看到的，我们可以在GPU内非常容易地操作这个数组。现在让我们将我们的晶格传输到GPU，并分配一个数组来表示我们的曼德勃罗集：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To reiterate—the `gpuarray.to_array` function only can operate on NumPy `array`
    types, so we were sure to have type-cast this beforehand before we sent it to
    the GPU. Next, we have to allocate some memory on the GPU with the `gpuarray.empty` function,
    specifying the size/shape of the array and the type. Again, you can think of this
    as acting similarly to `malloc` in C; remember that we won't have to deallocate
    or `free` this memory later, due to the `gpuarray` object destructor taking care
    of memory clean-up automatically when the end of the scope is reached.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下——`gpuarray.to_array`函数只能操作NumPy `array`类型，因此我们在将其发送到GPU之前一定要对其进行类型转换。接下来，我们必须使用`gpuarray.empty`函数在GPU上分配一些内存，指定数组的大小/形状和类型。同样，您可以将其视为类似于C中的`malloc`；请记住，由于`gpuarray`对象析构函数在作用域结束时自动处理内存清理，因此我们不必在以后释放或`free`这些内存。
- en: When you allocate memory on the GPU with the PyCUDA functions `gpuarray.empty`
    or `gpuarray.empty_like`, you do not have to deallocate this memory later due
    to the destructor of the `gpuarray `object managing all memory clean up.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用PyCUDA函数`gpuarray.empty`或`gpuarray.empty_like`在GPU上分配内存时，由于`gpuarray`对象的析构函数管理所有内存清理，因此您不必在以后释放此内存。
- en: We're now ready to launch the kernel; the only change we have to make is to
    change the
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备启动内核；我们唯一需要做的更改是改变
- en: 'We haven''t written our kernel function yet to generate the Mandelbrot set,
    but let''s just write how we want the rest of this function to go:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有编写生成曼德勃罗集的内核函数，但让我们先写出这个函数的其余部分应该是怎样的：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So this is how we want our new kernel to act—the first input will be the complex
    lattice of points (NumPy `complex64` type) we generated, the second will be a
    pointer to a two-dimensional floating point array (NumPy `float32` type) that
    will indicate which elements are members of the Mandelbrot set, the third will
    be an integer indicating the maximum number of iterations for each point, and
    the final input will be the upper bound for each point used for determining membership
    in the Mandelbrot class. Notice that we are *very* careful in typecasting everything
    that goes into the GPU!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们希望我们的新内核行为的方式——第一个输入将是我们生成的复点阵（NumPy `complex64`类型），第二个将是指向二维浮点数组的指针（NumPy
    `float32`类型），它将指示哪些元素是曼德勃罗集的成员，第三个将是一个整数，表示每个点的最大迭代次数，最后一个输入将是用于确定曼德勃罗类成员资格的每个点的上限。请注意，我们在将所有输入传递给GPU时非常小心！
- en: The next line retrieves the Mandelbrot set we generated from the GPU back into
    CPU space, and the end value is returned. (Notice that the input and output of
    `gpu_mandelbrot` is exactly the same as that of `simple_mandelbrot`).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行将从GPU中检索我们生成的曼德勃罗集回到CPU空间，并返回结束值。（请注意，`gpu_mandelbrot`的输入和输出与`simple_mandelbrot`完全相同）。
- en: 'Let''s now look at how to properly define our GPU kernel. First, let''s add
    the appropriate `include` statements to the header:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何正确定义我们的GPU内核。首先，让我们在头部添加适当的`include`语句：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We are now ready to write our GPU kernel! We''ll show it here and then go over
    this line-by-line:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备编写我们的GPU内核！我们将在这里展示它，然后逐行讨论：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: First, we set our input with the first string passed to `ElementwiseKernel`.
    We have to realize that when we are working in CUDA-C, particular C datatypes
    will correspond directly to particular Python NumPy datatypes. Again, note that
    when arrays are passed into a CUDA kernel, they are seen as C pointers by CUDA.
    Here, a CUDA C `int` type corresponds exactly to a NumPy `int32` type, while a
    CUDA C `float` type corresponds to a NumPy `float32` type. An internal PyCUDA
    class template is then used for complex types—here PyCUDA `::complex<float>` corresponds
    to Numpy `complex64`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用传递给`ElementwiseKernel`的第一个字符串设置我们的输入。我们必须意识到当我们在CUDA-C中工作时，特定的C数据类型将直接对应于特定的Python
    NumPy数据类型。再次注意，当数组被传递到CUDA内核时，它们被CUDA视为C指针。在这里，CUDA C `int`类型与NumPy `int32`类型完全对应，而CUDA
    C `float`类型对应于NumPy `float32`类型。然后使用内部PyCUDA类模板进行复杂类型的转换——这里PyCUDA `::complex<float>`对应于Numpy
    `complex64`。
- en: Let's look at the content of the second string, which is deliminated with three
    quotes (`"""`). This allows us to use multiple lines within the string; we will
    use this when we write larger inline CUDA kernels in Python.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第二个字符串的内容，它用三个引号（`"""`）分隔。这使我们能够在字符串中使用多行；当我们在Python中编写更大的内联CUDA内核时，我们将使用这个。
- en: While the arrays we have passed in are two-dimensional arrays in Python, CUDA
    will only see these as being one-dimensional and indexed by `i`. Again, `ElementwiseKernel`
    indexes `i` across multiple cores and threads for us automatically. We initialize
    each point in the output to one with `mandelbrot_graph[i] = 1;`, as `i` will be
    indexed over every single element of our Mandelbrot set; we're going to assume
    that every point will be a member unless proven otherwise. (Again, the Mandelbrot
    set is over two dimensions, real and complex, but `ElementwiseKernel` will automatically
    translate everything into a one-dimensional set. When we interact with the data
    again in Python, the two-dimensional structure of the Mandelbrot set will be preserved.)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们传入的数组在Python中是二维数组，但CUDA只会将它们视为一维数组，并由`i`索引。同样，`ElementwiseKernel`会自动为我们跨多个核心和线程索引`i`。我们将输出中的每个点初始化为1，如`mandelbrot_graph[i]
    = 1;`，因为`i`将在曼德勃罗集的每个元素上进行索引；我们将假设每个点都是成员，除非证明相反。（再次说明，曼德勃罗集是在两个维度上的，实部和虚部，但`ElementwiseKernel`将自动将所有内容转换为一维集合。当我们再次在Python中与数据交互时，曼德勃罗集的二维结构将被保留。）
- en: We set up our `c` value as in Python to the appropriate lattice point with `pycuda::complex<float>
    c = lattice[i];` and initialize our `z` value to `0` with `pycuda::complex<float>
    z(0,0);` (the first zero corresponds to the real part, while the second corresponds
    to the imaginary part). We then perform a loop over a new iterator, `j`, with `for(int
    j = 0; j < max_iters; j++)`. (Note that this algorithm will not be parallelized
    over `j` or any other index—only `i`! This `for` loop will run serially over `j`—but
    the entire piece of code will be parallelized across `i`.)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像在Python中一样为我们的`c`值设置适当的点阵点，如`pycuda::complex<float> c = lattice[i];`，并用`pycuda::complex<float>
    z(0,0);`将我们的`z`值初始化为`0`（第一个零对应于实部，而第二个对应于虚部）。然后我们使用一个新的迭代器`j`进行循环，如`for(int j
    = 0; j < max_iters; j++)`。（请注意，这个算法不会在`j`或任何其他索引上并行化——只有`i`！这个`for`循环将在`j`上串行运行——但整个代码片段将在`i`上并行化。）
- en: We then set the new value of `*z*` with `z = z*z + c;` as per the Mandelbrot
    algorithm. If the absolute value of this element exceeds the upper bound ( `if(abs(z)
    > upper_bound)` ), we set this point to 0 ( `mandelbrot_graph[i] = 0;` ) and break
    out of the loop with the `break` keyword.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`z = z*z + c;`设置`*z*`的新值，按照曼德勃罗算法。如果这个元素的绝对值超过了上限（`if(abs(z) > upper_bound)`），我们将这个点设置为0（`mandelbrot_graph[i]
    = 0;`），并用`break`关键字跳出循环。
- en: In the final string passed into `ElementwiseKernel` we give the kernel its internal
    CUDA C name, here `"mandel_ker"`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在传递给`ElementwiseKernel`的最终字符串中，我们为内核赋予其内部CUDA C名称，这里是`"mandel_ker"`。
- en: 'We''re now ready to launch the kernel; the only change we have to make is to
    change the reference from `simple_mandelbrot` in the main function to `gpu_mandelbrot`,
    and we''re ready to go. Let''s launch this from IPython:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备启动内核；我们唯一需要做的更改是将主函数中的引用从`simple_mandelbrot`更改为`gpu_mandelbrot`，然后我们就可以开始了。让我们从IPython中启动：
- en: '![](assets/f00d5080-4975-4023-9f14-397a8e007ac4.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f00d5080-4975-4023-9f14-397a8e007ac4.png)'
- en: 'Let''s check the dumped image to make sure this is correct:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查转储的图像，以确保这是正确的：
- en: '![](assets/6fa6851a-bcce-46d0-a63d-8023766da21a.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6fa6851a-bcce-46d0-a63d-8023766da21a.png)'
- en: 'This is certainly the same Mandelbrot image that is produced in the first chapter,
    so we have successfully implemented this onto a GPU! Let''s now look at the speed
    increase we''re getting: in the first chapter, it took us 14.61 seconds to produce
    this graph; here, it only took 0.894 seconds. Keep in mind that PyCUDA also has
    to compile and link our CUDA C code at runtime, and the time it takes to make
    the memory transfers to and from the GPU. Still, even with all of that extra overhead,
    it is a very worthwhile speed increase! (You can view the code for our GPU Mandelbrot
    with the file named `gpu_mandelbrot0.py` in the Git repository.)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这肯定是在第一章中生成的相同Mandelbrot图像，所以我们已经成功地将其实现到了GPU上！现在让我们看看我们得到的速度增加：在第一章中，我们花了14.61秒来生成这张图；而在这里，只花了0.894秒。请记住，PyCUDA还必须在运行时编译和链接我们的CUDA
    C代码，并且需要花费时间来进行与GPU的内存传输。即使有了所有这些额外的开销，它仍然是一个非常值得的速度增加！（您可以在Git存储库中找到我们的GPU Mandelbrot的代码，文件名为`gpu_mandelbrot0.py`。）
- en: A brief foray into functional programming
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对函数式编程的简要探讨
- en: Before we continue, let's briefly do a review of two functions available in
    Python for **functional programming***—*`map` and `reduce`. These are both considered
    to be *functional* because they both act on *functions* for their operation. We
    find these interesting because these both correspond to common design patterns
    in programming, so we can swap out different functions in the input to get a multitude
    of different (and useful) operations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们简要回顾一下Python中用于**函数式编程**的两个函数——`map`和`reduce`。它们都被认为是*函数式*，因为它们都对*函数*进行操作。我们发现这些有趣，因为它们都对应于编程中的常见设计模式，所以我们可以替换输入中的不同函数，以获得多种不同（和有用的）操作。
- en: 'Let''s first recall the `lambda` keyword in Python. This allows us to define
    an **anonymous function**—in most cases, these can be thought of as a `throwaway` function
    that we may only wish to use once, or functions that are able to be defined on
    a single line. Let''s open up IPython right now and define a little function that
    squares a number as such—`pow2 = lambda x : x**2`. Let''s test it out on a few
    numbers:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们首先回顾Python中的`lambda`关键字。这允许我们定义一个**匿名函数**——在大多数情况下，这些可以被视为`一次性`函数，或者只希望使用一次的函数，或者可以在一行上定义的函数。让我们现在打开IPython并定义一个将数字平方的小函数，如`pow2
    = lambda x : x**2`。让我们在一些数字上测试一下：'
- en: '![](assets/f7154a51-4486-4292-9ed0-89415e526394.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f7154a51-4486-4292-9ed0-89415e526394.png)'
- en: 'Let''s recall that `map` acts on two input values: a function and a `list`
    of objects that the given function can act on. `map` outputs a list of the function''s
    output for each element in the original list. Let''s now define our squaring operation
    as an anonymous function which we input into map, and a list of the last few numbers
    we checked with the following—`map(lambda x : x**2, [2,3,4])`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们回顾一下`map`作用于两个输入值：一个函数和给定函数可以作用的对象`列表`。`map`输出原始列表中每个元素的函数输出列表。现在让我们将我们的平方操作定义为一个匿名函数，然后将其输入到map中，并使用最后几个数字的列表进行检查，如`map(lambda
    x : x**2, [2,3,4])`：'
- en: '![](assets/c41f370e-9cba-40ba-a5df-e84857044437.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/c41f370e-9cba-40ba-a5df-e84857044437.png)'
- en: 'We see that `map` acts as `ElementwiseKernel`! This is actually a standard
    design pattern in functional programming. Now, let''s look at `reduce`; rather
    than taking in a list and outputting a directly corresponding list, reduce takes
    in a list, performs a recursive binary operation on it, and outputs a singleton.
    Let''s get a notion of this design pattern by typing `reduce(lambda x, y : x +
    y, [1,2,3,4])`. When we type this in IPython, we will see that this will output
    a single number, 10, which is indeed the sum of *1+2+3+4*. You can try replacing
    the summation above with multiplication, and seeing that this indeed works for
    recursively multiplying a long list of numbers together. Generally speaking, we
    use reduce operations with *associative binary operations*; this means that, no
    matter the order we perform our operation between sequential elements of the list,
    will always invariably give the same result, provided that the list is kept in
    order. (This is not to be confused with the *commutative property*.)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '我们看到`map`作为`ElementwiseKernel`！这实际上是函数式编程中的标准设计模式。现在，让我们看看`reduce`；它不是接收一个列表并直接输出相应列表，而是接收一个列表，在其上执行递归二进制操作，并输出一个单例。让我们通过键入`reduce(lambda
    x, y : x + y, [1,2,3,4])`来了解这种设计模式。当我们在IPython中键入这个时，我们将看到这将输出一个单个数字10，这确实是*1+2+3+4*的和。您可以尝试用乘法替换上面的求和，并看到这确实适用于递归地将一长串数字相乘。一般来说，我们使用*可结合的二进制操作*进行缩减操作；这意味着，无论我们在列表的连续元素之间以何种顺序执行操作，都将始终得到相同的结果，前提是列表保持有序。（这与*交换律*不同。）'
- en: We will now see how PyCUDA handles programming patterns akin to `reduce`—with
    **parallel scan** and **reduction kernels**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看PyCUDA如何处理类似于“reduce”的编程模式——使用**并行扫描**和**归约内核**。
- en: Parallel scan and reduction kernel basics
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行扫描和归约内核基础
- en: 'Let''s look at a basic function in PyCUDA that reproduces the functionality
    of reduce—`InclusiveScanKernel`. (You can find the code under the `simple_scankernal0.py` filename.)
    Let''s execute a basic example that sums a small list of numbers on the GPU:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下PyCUDA中一个复制reduce功能的基本函数——`InclusiveScanKernel`。（您可以在`simple_scankernal0.py`文件名下找到代码。）让我们执行一个在GPU上对一小组数字求和的基本示例：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We construct our kernel by first specifying the input/output type (here, NumPy
    `int32`) and in the string, `"a+b"`. Here, `InclusiveScanKernel` sets up elements
    named `a` and `b` in the GPU space automatically, so you can think of this string
    input as being analogous to `lambda a,b: a + b` in Python. We can really put any
    (associative) binary operation here, provided we remember to write it in C.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过首先指定输入/输出类型（这里是NumPy `int32`）和字符串`"a+b"`来构建我们的内核。在这里，`InclusiveScanKernel`自动在GPU空间中设置了名为`a`和`b`的元素，因此您可以将此字符串输入视为Python中的`lambda
    a,b: a + b`的类似物。我们实际上可以在这里放置任何（可结合的）二进制操作，只要我们记得用C语言编写它。'
- en: 'When we run `sum_gpu`, we see that we will get an array of the same size as
    the input array. Each element in the array represents the value for each step
    in the calculation (the NumPy `cumsum` function gives the same output, as we can
    see). The last element will be the final output that we are seeking, which corresponds
    to the output of reduce:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行`sum_gpu`时，我们会得到一个与输入数组大小相同的数组。数组中的每个元素表示计算中的每个步骤的值（我们可以看到，NumPy `cumsum`函数给出了相同的输出）。最后一个元素将是我们正在寻找的最终输出，对应于reduce的输出：
- en: '![](assets/98e28110-698a-4ab5-a827-9c1a8a2e31d4.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/98e28110-698a-4ab5-a827-9c1a8a2e31d4.png)'
- en: 'Let''s try something a little more challenging; let''s find the maximum value
    in a `float32` array:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一些更具挑战性的东西；让我们找到一个`float32`数组中的最大值：
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: (You can find the complete code in the file named `simple_scankernal1.py`.)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: （您可以在名为`simple_scankernal1.py`的文件中找到完整的代码。）
- en: 'Here, the main change we made is to replace the `a + b` string with `a > b
    ? a : b`. (In Python, this would be rendered within a `reduce` statement as `lambda
    a, b:  max(a,b)`). Here, we are using a trick to give the max among `a` and `b`
    with the C language''s `?` operator. We finally display the last value of the
    resulting element in the output array, which will be exactly the last element
    (which we can always retrieve with the `[-1]` index in Python).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们所做的主要更改是用`a > b ? a : b`替换了`a + b`字符串。 （在Python中，这将在`reduce`语句中呈现为`lambda
    a, b: max(a,b)`）。在这里，我们使用了一个技巧，使用C语言的`?`运算符来给出`a`和`b`中的最大值。最后，我们显示了输出数组中结果元素的最后一个值，这将恰好是最后一个元素（我们总是可以用Python中的`[-1]`索引来检索）。'
- en: 'Now, let''s finally look one more PyCUDA function for generating GPU kernels—`ReductionKernel`.
    Effectively, `ReductionKernel` acts like a `ElementwiseKernel` function followed
    by a parallel scan kernel. What algorithm is a good candidate for implementing
    with a `ReductionKernel`? The first that tends to come to mind is the dot product
    from linear algebra. Let''s remember computing the dot product of two vectors
    has two steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们最后再看一个用于生成GPU内核的PyCUDA函数——`ReductionKernel`。实际上，`ReductionKernel`的作用类似于`ElementwiseKernel`函数，后面跟着一个并行扫描内核。哪种算法是使用`ReductionKernel`实现的一个好选择？首先想到的是线性代数中的点积。让我们记住计算两个向量的点积有两个步骤：
- en: Multiply the vectors pointwise
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将向量逐点相乘
- en: Sum the resulting pointwise multiples
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对结果的逐点乘积求和
- en: 'These two steps are also called *multiply and accumulate*. Let''s set up a
    kernel to do this computation now:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤也称为*乘法和累加*。现在让我们设置一个内核来执行这个计算：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: First, note the datatype we use for our kernel (a `float32`). We then set up
    the input arguments to our CUDA C kernel with `arguments`, (here two float arrays
    representing each vector designated with `float *`) and set the pointwise calculation
    with `map_expr`, here it is pointwise multiplication. As with `ElementwiseKernel`,
    this is indexed over `i`. We set up `reduce_expr` the same as with `InclusiveScanKernel`.
    This will take the resulting output from the element-wise operation and perform
    a reduce-type operation on the array. Finally, we set the *neutral element* with
    neutral. This is an element that will act as an identity for `reduce_expr`; here,
    we set `neutral=0`, because `0` is always the identity under addition (under multiplication,
    one is the identity). We'll see why exactly we have to set this up when we cover
    parallel prefix in greater depth later in this book.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意我们为内核使用的数据类型（`float32`）。然后，我们使用`arguments`设置了我们的CUDA C内核的输入参数（这里是两个代表每个向量的浮点数组，用`float
    *`表示），并使用`map_expr`设置了逐点计算，这里是逐点乘法。与`ElementwiseKernel`一样，这是按`i`索引的。我们设置了`reduce_expr`，与`InclusiveScanKernel`一样。这将对数组执行元素操作的结果进行减少类型的操作。最后，我们使用`neutral`设置了*中性元素*。这是一个将作为`reduce_expr`的标识的元素；在这里，我们设置`neutral=0`，因为`0`在加法下始终是标识（在乘法下，1是标识）。稍后在本书中更深入地讨论并行前缀时，我们将看到为什么我们必须设置这个。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We first saw how to query our GPU from PyCUDA, and with this re-create the CUDA
    `deviceQuery` program in Python. We then learned how to transfer NumPy arrays
    to and from the GPU's memory with the PyCUDA `gpuarray` class and its `to_gpu`
    and `get` functions. We got a feel for using `gpuarray` objects by observing how
    to use them to do basic calculations on the GPU, and we learned to do a little
    investigative work using IPython's `prun` profiler. We saw there is sometimes
    some arbitrary slowdown when running GPU functions from PyCUDA for the first time
    in a session, due to PyCUDA launching NVIDIA's `nvcc` compiler to compile inline
    CUDA C code. We then saw how to use the `ElementwiseKernel` function to compile
    and launch element-wise operations, which are automatically parallelized onto
    the GPU from Python. We did a brief review of functional programming in Python
    (in particular the `map` and `reduce` functions), and finally, we covered how
    to do some basic reduce/scan-type computations on the GPU using the `InclusiveScanKernel`
    and `ReductionKernel` functions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先学习了如何从PyCUDA查询我们的GPU，并用此方法在Python中重新创建了CUDA的`deviceQuery`程序。然后我们学习了如何使用PyCUDA的`gpuarray`类及其`to_gpu`和`get`函数将NumPy数组传输到GPU的内存中。我们通过观察如何使用`gpuarray`对象来进行基本的GPU计算来感受了使用`gpuarray`对象的感觉，并且我们学会了使用IPython的`prun`分析器进行一些调查工作。我们发现，由于PyCUDA启动NVIDIA的`nvcc`编译器来编译内联CUDA
    C代码，有时在会话中首次运行PyCUDA的GPU函数时会出现一些任意的减速。然后我们学习了如何使用`ElementwiseKernel`函数来编译和启动逐元素操作，这些操作会自动并行化到GPU上。我们对Python中的函数式编程进行了简要回顾（特别是`map`和`reduce`函数），最后，我们介绍了如何使用`InclusiveScanKernel`和`ReductionKernel`函数在GPU上进行一些基本的reduce/scan类型计算。
- en: Now that we have the absolute basics down about writing and launching kernel
    functions, we should realize that PyCUDA has covered the vast amount of the overhead
    in writing a kernel for us with its templates. We will spend the next chapter
    learning about the principles of CUDA kernel execution, and how CUDA arranges
    concurrent threads in a kernel into abstract **grids** and **blocks**.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了编写和启动内核函数的绝对基础知识，我们应该意识到PyCUDA已经通过其模板为我们覆盖了编写内核的大部分开销。我们将在下一章学习CUDA内核执行的原则，以及CUDA如何将内核中的并发线程排列成抽象的**网格**和**块**。
- en: Questions
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: In `simple_element_kernel_example0.py`, we don't consider the memory transfers
    to and from the GPU in measuring the time for the GPU computation. Try measuring
    the time that the `gpuarray` functions, `to_gpu` and `get`, take with the Python
    time command. Would you say it's worth offloading this particular function onto
    the GPU, with the memory transfer times in consideration?
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`simple_element_kernel_example0.py`中，我们在测量GPU计算时间时不考虑与GPU之间的内存传输。尝试使用Python时间命令测量`gpuarray`函数`to_gpu`和`get`的时间。考虑内存传输时间后，你会认为将这个特定函数卸载到GPU上值得吗？
- en: In [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why GPU Programming?*,
    we had a discussion of Amdahl's Law, which gives us some idea of the gains we
    can potentially get by offloading portions of a program onto a GPU. Name two issues
    that we have seen in this chapter that Amdahl's law does not take into consideration.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[第1章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)中，*为什么进行GPU编程？*，我们讨论了安德尔定律，这让我们对将程序的部分内容卸载到GPU上可能获得的收益有了一些了解。在本章中我们看到的两个问题，安德尔定律没有考虑到的是什么？
- en: Modify `gpu_mandel0.py` to use smaller and smaller lattices of complex numbers,
    and compare this to the same lattices CPU version of the program. Can we choose
    a small enough lattice such that the CPU version is actually faster than the GPU
    version?
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`gpu_mandel0.py`以使用越来越小的复数格点，并将其与程序的CPU版本进行比较。我们可以选择足够小的格点，以至于CPU版本实际上比GPU版本更快吗？
- en: Create a kernel with `ReductionKernel` that takes two `complex64` arrays on
    the GPU of the same length and returns the absolute largest element among both
    arrays.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个使用`ReductionKernel`的内核，该内核在GPU上获取两个相同长度的`complex64`数组，并返回两个数组中的绝对最大元素。
- en: What happens if a `gpuarray` object reaches end-of-scope in Python?
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个`gpuarray`对象在Python中到达作用域的末尾会发生什么？
- en: Why do you think we need to define `neutral` when we use `ReductionKernel`?
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你认为我们在使用`ReductionKernel`时为什么需要定义`neutral`？
- en: 'If in `ReductionKernel` we set `reduce_expr ="a > b ? a : b"`, and we are operating
    on int32 types, then what should we set "`neutral`" to?'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '如果在`ReductionKernel`中我们设置`reduce_expr ="a > b ? a : b"`，并且我们正在操作int32类型，那么我们应该将"`neutral`"设置为什么？'
