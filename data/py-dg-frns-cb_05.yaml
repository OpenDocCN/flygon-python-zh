- en: Networking and Indicators of Compromise Recipes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络和妥协指标食谱
- en: 'The following recipes are covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下食谱：
- en: Getting a jump start with IEF
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用IEF快速入门
- en: Coming into contact with IEF
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接触IEF
- en: It's a beautiful soup
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一种美丽的汤
- en: Going hunting for viruses
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去寻找病毒
- en: Gathering intel
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情报收集
- en: Totally passive
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全被动
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Technology has come a long way and, with it, the extent to which tools are made
    widely available has changed too. As a matter of fact, being cognizant of the
    tools' existence is half the battle due to the sheer volume of tools available
    on the internet. Some of these tools are publicly available and can be bent toward
    forensic purposes. In this chapter, we will learn how to interact with websites
    and identify malware through Python, including an automated review of potentially
    malicious domains, IP addresses, or files.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 技术已经走了很长的路，随之而来的是工具的广泛可用性也发生了变化。事实上，由于互联网上可用的工具数量庞大，知道这些工具的存在已经是一大半的胜利。其中一些工具是公开可用的，并且可以用于取证目的。在本章中，我们将学习如何通过Python与网站互动，并识别恶意软件，包括自动审查潜在恶意域、IP地址或文件。
- en: We start out by taking a look at how to manipulate **Internet Evidence Finder**
    (**IEF**) results and perform additional processing outside of the context of
    the application. We also explore using services such as VirusShare, PassiveTotal,
    and VirusTotal to create HashSets of known malware, query suspicious domain resolutions,
    and identify known bad domains or files, respectively. Between these scripts,
    you will become familiar with using Python to interact with APIs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先看一下如何操纵**Internet Evidence Finder**（**IEF**）的结果，并在应用程序的上下文之外执行额外的处理。我们还探讨了使用VirusShare、PassiveTotal和VirusTotal等服务来创建已知恶意软件的哈希集，查询可疑域名解析，并分别识别已知的恶意域或文件。在这些脚本之间，您将熟悉使用Python与API交互。
- en: 'The scripts in this chapter focus on solving particular problems and are ordered
    by complexity:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的脚本专注于解决特定问题，并按复杂性排序：
- en: Learning to extract data from IEF results
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习从IEF结果中提取数据
- en: Processing cached Yahoo contacts data from Google Chrome
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Google Chrome处理缓存的Yahoo联系人数据
- en: Preserving web pages with Beautiful Soup
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用美丽的汤保存网页
- en: Creating an X-Ways-compatible HashSet from VirusShare
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从VirusShare创建与X-Ways兼容的HashSet
- en: Using PassiveTotal to automate the review of sketchy domains or IP addresses
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PassiveTotal自动审查可疑域名或IP地址
- en: Automating identification of known bad files, domains, or IPs with VirusTotal
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用VirusTotal自动识别已知的恶意文件、域名或IP
- en: Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)下载本章的代码包。
- en: Getting a jump start with IEF
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用IEF快速入门
- en: 'Recipe Difficulty: Easy'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：简单
- en: 'Python Version: 3.5'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：3.5
- en: 'Operating System: Any'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任何
- en: This recipe will act as a quick means of dumping all reports from IEF to a CSV
    file and an introduction to interacting with IEF results. IEF stores data in a
    SQLite database, which we explored rather thoroughly in [Chapter 3](part0097.html#2SG6I0-260f9401d2714cb9ab693c4692308abe),
    *A Deep Dive into Mobile Forensic Recipes*. As IEF can be configured to scan specific
    categories of information, it is not so simple as dumping out set tables for each
    IEF database. Instead, we must determine this information dynamically and then
    interact with said tables. This recipe will dynamically identify result tables
    within the IEF database and dump them to respective CSV files. This process can
    be performed on any SQLite database to quickly dump its contents to a CSV file
    for review.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱将作为一种快速手段，将IEF的所有报告转储到CSV文件，并介绍与IEF结果交互的方法。IEF将数据存储在SQLite数据库中，在[第3章](part0097.html#2SG6I0-260f9401d2714cb9ab693c4692308abe)中我们对此进行了相当彻底的探讨，*移动取证食谱的深入研究*。由于IEF可以配置为扫描特定类别的信息，因此不能简单地为每个IEF数据库转储设置表。相反，我们必须动态确定这些信息，然后与相应的表进行交互。这个食谱将动态识别IEF数据库中的结果表，并将它们转储到相应的CSV文件中。这个过程可以在任何SQLite数据库上执行，以快速将其内容转储到CSV文件进行审查。
- en: Getting started
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: All libraries used in this script are present in Python's standard library.
    For this script, make sure to have an IEF results database generated after executing
    the program. We used IEF version 6.8.9.5774 to generate the database used to develop
    this recipe. After IEF finishes processing the forensic image, for example, you
    should see a file named `IEFv6.db`. This is the database we will interact with
    in this recipe.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本脚本中使用的所有库都包含在Python的标准库中。对于此脚本，请确保在执行程序后生成了IEF结果数据库。我们使用的是IEF版本6.8.9.5774来生成本食谱中使用的数据库。例如，当IEF完成处理取证图像时，您应该会看到一个名为`IEFv6.db`的文件。这是我们将在本食谱中与之交互的数据库。
- en: How to do it...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'We will employ the following steps to extract data from the IEF results database:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用以下步骤从IEF结果数据库中提取数据：
- en: Connect to the database.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到数据库。
- en: Query the database to identify all tables.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询数据库以识别所有表。
- en: Write result tables to an individual CSV file.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果表写入单独的CSV文件。
- en: How it works...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: First, we import the required libraries to handle argument parsing, writing
    spreadsheets, and interacting with SQLite databases.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库来处理参数解析、编写电子表格和与SQLite数据库交互。
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This recipe's command-line handler is relatively straightforward. It accepts
    two positional arguments, `IEF_DATABASE` and `OUTPUT_DIR`, representing the file
    path to the `IEFv6.db` file and the desired output location, respectively.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱的命令行处理程序相对简单。它接受两个位置参数，`IEF_DATABASE`和`OUTPUT_DIR`，分别表示`IEFv6.db`文件的文件路径和期望的输出位置。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We perform the input validation steps as usual prior to calling the `main()`
    function of the script. First, we check the output directory and create it if
    it does not exist. Then, we confirm that the IEF database exists as expected.
    If all is as expected, we execute the `main()` function and supply it with the
    two user-supplied inputs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用脚本的`main()`函数之前，我们像往常一样执行输入验证步骤。首先，我们检查输出目录，如果不存在则创建它。然后，我们确认IEF数据库是否如预期存在。如果一切如预期，我们执行`main()`函数，并向其提供两个用户提供的输入：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `main()` function starts out simply enough. We print a status message to
    the console and create the `sqlite3` connection to the database to execute the
    necessary SQLite queries:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数开始得相当简单。我们在控制台打印一个状态消息，并创建`sqlite3`连接到数据库以执行必要的SQLite查询：'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we need to query the database to identify all tables present. Notice the
    rather complex query we execute to perform this. If you are familiar with SQLite,
    you may shake your head and wonder why we have not executed the `.table` command.
    Unfortunately, in Python, this cannot be done so easily. Rather, one must execute
    the following command to achieve the desired goal.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要查询数据库以识别所有存在的表。请注意，我们执行了相当复杂的查询来执行此操作。如果您熟悉SQLite，您可能会摇摇头，想知道为什么我们没有执行`.table`命令。不幸的是，在Python中，这并不那么容易。相反，我们必须执行以下命令才能实现所需的目标。
- en: As we have seen previously, the `Cursor` returns results as a list of tuples.
    The command we have executed returns a number of details about each table in the
    database. In this case, we are only interested in extracting the name of the table.
    We accomplish this using list comprehension by first fetching all results from
    the cursor object and then appending the second element of each result to the
    tables list if the name matches certain criteria. We have elected to ignore table
    names that start with `_` or end with `_DATA`. From a review of these tables,
    they contained actual cached file content rather than the metadata IEF presents
    for each record.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，`Cursor`以元组列表的形式返回结果。我们执行的命令返回数据库中每个表的许多细节。在这种情况下，我们只对提取表的名称感兴趣。我们通过列表推导来实现这一点，首先从游标对象中获取所有结果，然后如果名称符合特定标准，将每个结果的第二个元素附加到表列表中。我们选择忽略以`_`开头或以`_DATA`结尾的表名。经过对这些表的审查，我们发现它们包含实际的缓存文件内容，而不是IEF为每个记录呈现的元数据。
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With the list of table names in hand, we can now iterate through each one and
    extract their contents into a variable. Prior to that, we print an update status
    message to the console to inform the user of the current execution status of the
    script. In order to write the CSVs, we need to first determine the column names
    for a given table. This is performed, as we saw in Chapter 3, using the `pragma
    table_info` command. With some simple list comprehension, we extract just the
    names of the columns and store them in a variable for later.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有了手头的表名列表，我们现在可以遍历每个表，并将它们的内容提取到一个变量中。在此之前，我们会在控制台打印一个更新状态消息，通知用户脚本的当前执行状态。为了编写CSV文件，我们需要首先确定给定表的列名。这是通过使用`pragma
    table_info`命令来执行的，正如我们在第3章中看到的那样。通过一些简单的列表推导，我们提取列名，并将它们存储在一个变量中以备后用。
- en: 'With that accomplished, we execute the favorite and simplest SQL query and
    select all (`*`) data from each table. Using the `fetchall()` method on the cursor
    object, we store the list of tuples containing the table''s data in its entirety
    in the `table_data` variable:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些工作后，我们执行最喜欢和最简单的SQL查询，并从每个表中选择所有(`*`)数据。通过在游标对象上使用`fetchall()`方法，我们将包含表数据的元组列表以其完整形式存储在`table_data`变量中：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now begin to write the data for each table to its appropriate CSV file.
    To keep things simple, the name of each CSV file is simply the table name and
    an appended `.csv` extension. We use `os.path.join()` to combine the output directory
    with the desired CSV name.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始将每个表的数据写入其相应的CSV文件。为了保持简单，每个CSV文件的名称只是表名和附加的`.csv`扩展名。我们使用`os.path.join()`将输出目录与所需的CSV名称结合起来。
- en: Next, we print a status update to the console and begin the process to write
    each CSV file. This is accomplished by first writing the table column names as
    the header of the spreadsheet followed by the contents of the table. We use the
    **`writerows()`** method to write the list of tuples in one line rather than create
    an unnecessary loop and execute `writerow()` repeatedly for each tuple.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在控制台打印一个状态更新，并开始编写每个CSV文件的过程。首先，我们将表的列名作为电子表格的标题写入，然后是表的内容。我们使用`writerows()`方法将元组列表一次性写入一行，而不是创建一个不必要的循环，并对每个元组执行`writerow()`。
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When we run this script, we can see the discovered artifacts and extract CSV
    reports of the text information:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个脚本时，我们可以看到发现的文物，并提取文本信息的CSV报告：
- en: '![](../images/00049.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00049.jpeg)'
- en: 'Once we have completed the script, we can see information about an artifact
    as seen in the following snippet of a report:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完成脚本后，我们可以看到关于文物的信息，如下面报告片段所示：
- en: '![](../images/00050.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00050.jpeg)'
- en: Coming into contact with IEF
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接触IEF
- en: 'Recipe Difficulty: Medium'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：中等
- en: 'Python Version: 3.5'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：3.5
- en: 'Operating System: Any'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任意
- en: We can take further advantage of the IEF results in the SQLite database by manipulating
    and gleaning, even more, information from artifacts that IEF does not necessarily
    support. This can be particularly important when new artifacts are discovered
    and are unsupported. As the internet, and many businesses using the internet change
    constantly, it is unrealistic for software to keep up with every new artifact.
    In this case, we will look at cached Yahoo Mail contacts that get stored on the
    local system as a byproduct of using Yahoo Mail.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步利用IEF在SQLite数据库中的结果，通过操作和从IEF不一定支持的文物中获取更多信息。当发现并不受支持的新文物时，这可能特别重要。由于互联网和许多使用互联网的企业不断变化，软件无法跟上每个新文物。在这种情况下，我们将查看在使用Yahoo
    Mail时存储在本地系统上的缓存的Yahoo Mail联系人。
- en: Getting started
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: All libraries used in this script are present in Python's standard library.
    Again, as in the previous recipe, if you would like to follow along, you will
    need an IEF results database. We used IEF version 6.8.9.5774 to generate the database
    used to develop this recipe. In addition to that, you will likely need to generate
    Yahoo Mail traffic to create the necessary situation where Yahoo Mail contacts
    are cached. In this example, we used the Google Chrome browser to use Yahoo Mail
    and will, therefore, be looking at Google Chrome cache data. This recipe, while
    specific to Yahoo, illustrates how you can use the IEF results database to further
    process artifacts and identify additional relevant information.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本中使用的所有库都包含在Python的标准库中。与前一个配方一样，如果您想跟着做，您将需要一个IEF结果数据库。我们使用IEF版本6.8.9.5774生成了用于开发此配方的数据库。除此之外，您可能需要生成Yahoo
    Mail流量，以创建必要的情况，其中Yahoo Mail联系人被缓存。在这个例子中，我们使用Google Chrome浏览器使用Yahoo Mail，因此将查看Google
    Chrome缓存数据。这个配方虽然专门针对Yahoo，但说明了您可以使用IEF结果数据库进一步处理工件并识别其他相关信息。
- en: How to do it...
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The recipe follows these basic principles:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该配方遵循以下基本原则：
- en: Connect to the input database.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到输入数据库。
- en: Query the Google Chrome cache table for Yahoo Mail contact records.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询Google Chrome缓存表以获取Yahoo Mail联系人记录。
- en: Process contact cache JSON data and metadata.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理联系人缓存JSON数据和元数据。
- en: Write all relevant data to a CSV.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有相关数据写入CSV。
- en: How it works...
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we import the required libraries to handle argument parsing, writing
    spreadsheets, processing JSON data, and interacting with SQLite databases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库来处理参数解析、编写电子表格、处理JSON数据和与SQLite数据库交互。
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This recipe's command-line handler does not differ from the first recipe. It
    accepts two positional arguments, `IEF_DATABASE` and `OUTPUT_DIR,` representing
    the file paths to the `IEFv6.db` file and the desired output location, respectively.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序与第一个配方没有区别。它接受两个位置参数，`IEF_DATABASE`和`OUTPUT_DIR`，分别表示`IEFv6.db`文件的文件路径和所需的输出位置。
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And again, we perform the same data validation steps as executed in the first
    recipe of this chapter. If it ain't broke, why fix it? After validation, we execute
    the `main()` function and supply it with the two validated inputs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 再次执行与本章第一个配方中执行的相同的数据验证步骤。如果它没有问题，为什么要修复它呢？验证后，执行`main()`函数并向其提供两个经过验证的输入。
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `main()` function starts again by creating a connection to the input SQLite
    database (we promise this recipe isn''t identical to the first one: keep reading).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数再次通过创建与输入SQLite数据库的连接开始（我们承诺这个配方与第一个不同：继续阅读）。'
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can now begin scouring the database for all instances of Yahoo Mail contact
    cache records. Notice that the URL fragment we are looking for is rather specific
    to our purpose. This should ensure that we do not get any false positives. The
    percent sign (`%`) at the end of the URL is the SQLite wildcard equivalent character.
    We execute the query in a `try` and `except` statement in the event the input
    directory does not have the Chrome cache records table, is corrupt, or is encrypted.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始搜索数据库，查找所有Yahoo Mail联系人缓存记录的实例。请注意，我们要寻找的URL片段与我们的目的非常特定。这应该确保我们不会得到任何错误的结果。URL末尾的百分号（`%`）是SQLite通配符的等效字符。我们在`try`和`except`语句中执行查询，以防输入目录没有Chrome缓存记录表，损坏或加密。
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If we were able to execute the query successfully, we store the returned list
    of tuples into the `contact_cache` variable. This variable serves as the only
    input to the `process_contacts()` function, which returns a nested list structure
    convenient for the CSV writer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够成功执行查询，我们将返回的元组列表存储到`contact_cache`变量中。这个变量作为`process_contacts()`函数的唯一输入，该函数返回一个方便CSV写入器的嵌套列表结构。
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `process_contacts()` function starts by printing a status message to the
    console, setting up the `results` list, and iterating through each contact cache
    record. Each record has a number of metadata elements associated with it beyond
    the raw data. This includes the URL, the location of the cache on the filesystem,
    and the timestamps for the first visit, last visit, and last sync time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_contacts()`函数首先通过向控制台打印状态消息，设置`results`列表，并迭代每个联系人缓存记录来开始。每个记录都有一些与之相关的元数据元素，除了原始数据之外。这包括URL、文件系统上缓存的位置以及第一次访问、最后一次访问和最后同步时间的时间戳。'
- en: We use the `json.loads()` method to store the JSON data extracted from the table
    into the `contact_json` variable for further manipulation. The `total` and `count`
    keys from the JSON data, store the total number of Yahoo Mail contacts and the
    count of them present in the JSON cache data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`json.loads()`方法将从表中提取的JSON数据存储到`contact_json`变量中，以便进一步操作。JSON数据中的`total`和`count`键存储了Yahoo
    Mail联系人的总数以及JSON缓存数据中存在的联系人数。
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Before we extract contact data from contact JSON, we need to ensure that it
    has contacts in the first place. If it does not, we continue onto the next cache
    record in the hopes that we find contacts there. If on the other hand, we do have
    contacts, we initialize a number of variables to an empty string. This is achieved
    in one line by bulk-assigning variables to a tuple of empty strings:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在从联系人JSON中提取联系人数据之前，我们需要确保它首先有联系人。如果没有，我们继续到下一个缓存记录，希望在那里找到联系人。另一方面，如果我们有联系人，我们将一些变量初始化为空字符串。通过将变量批量分配给一组空字符串的元组，在一行中实现了这一点：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With these variables initialized, we begin looking for each of them in each
    of the contacts. Sometimes the particular cache record will not retain full contact
    details such as the `"anniversary"` key. For this reason, we initialized these
    variables to avoid referring to variables that do not exist if that particular
    key isn't present in a given cache record.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些初始化的变量，我们开始在每个联系人中查找它们。有时特定的缓存记录不会保留完整的联系人详细信息，比如`"anniversary"`键。因此，我们初始化了这些变量，以避免在给定缓存记录中不存在特定键时引用不存在的变量。
- en: For the `name`, `"anniversary"`, and `"birthday"` keys, we need to perform some
    string concatenation so that they are in a convenient format. The `emails`, `phones`,
    and `links` variables could have more than one result and we, therefore, use list
    comprehension and the `join()` method to create a comma-separated list of those
    respective elements. The great thing about that line of code is that if there
    is only one email, phone number, or link, it will not place a comma after that
    one element unnecessarily.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`name`，`"anniversary"`和`"birthday"`键，我们需要执行一些字符串连接，以便它们以方便的格式。`emails`，`phones`和`links`变量可能有多个结果，因此我们使用列表推导和`join()`方法来创建这些相应元素的逗号分隔列表。这行代码的好处是，如果只有一个电子邮件、电话号码或链接，它不会不必要地在该元素之后放置逗号。
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We handle the `company`, `jobTitle`, and `notes` sections differently by using
    the `get()` method instead. Because these are simple key and value pairs, we do
    not need to do any additional string processing on them. Instead, with the `get()`
    method, we can extract the key's value or, if it isn't present, set the default
    value to an empty string.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用`get()`方法来处理`company`，`jobTitle`和`notes`部分。因为这些是简单的键值对，所以我们不需要对它们进行任何额外的字符串处理。相反，使用`get()`方法，我们可以提取键的值，或者如果不存在，则将默认值设置为空字符串。
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After we have processed the contact data, we append a list of the metadata and
    extracted data elements to the `results` list. Once we have processed each contact
    and each cache record, we return the `results` list back to the `main()` function,
    which gets passed onto the CSV writer function.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们处理完联系数据后，我们将元数据和提取的数据元素的列表附加到`results`列表中。一旦我们处理完每个联系人和每个缓存记录，我们将`results`列表返回到`main()`函数，然后传递给CSV写入函数。
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `write_csv()` method takes the nested `results` list structure and the output
    file path as its inputs. After we print a status message to the console, we employ
    the usual strategy to write the results to the output file. Namely, we first write
    the headers of the CSV followed by the actual contact data. Thanks to the nested
    list structure, we can just use the `writerows()` method to write all of the results
    to the file in one line.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_csv()`方法接受嵌套的`results`列表结构和输出文件路径作为其输入。在我们向控制台打印状态消息后，我们采用通常的策略将结果写入输出文件。换句话说，我们首先写入CSV的标题，然后是实际的联系数据。由于嵌套的列表结构，我们可以使用`writerows()`方法将所有结果一次性写入文件。'
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This screenshot illustrates an example of the type of data that this script
    can extract:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此屏幕截图说明了此脚本可以提取的数据类型的示例：
- en: '![](../images/00051.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00051.jpeg)'
- en: Beautiful Soup
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 美丽的汤
- en: 'Recipe Difficulty: Medium'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 配方难度：中等
- en: 'Python Version: 3.5'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：3.5
- en: 'Operating System: Any'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任何
- en: In this recipe, we create a website preservation tool leveraging the **Beautiful
    Soup** library. This is a library meant to process markup languages, such as HTML
    or XML, and can be used to easily process these types of data structures. We will
    use it to identify and extract all links from a web page in a few lines of code.
    This script is meant to showcase a very simplistic example of a website preservation
    script; it is by no means intended to replace existing software out there on the
    market.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们创建一个网站保存工具，利用**Beautiful Soup**库。这是一个用来处理标记语言（如HTML或XML）的库，可以用来轻松处理这些类型的数据结构。我们将使用它来识别和提取网页中的所有链接，只需几行代码。这个脚本旨在展示一个非常简单的网站保存脚本的例子；它绝不打算取代市场上已有的现有软件。
- en: Getting started
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: This recipe requires the installation of the third-party library `bs4`. This
    module can be installed via the following command. All other libraries used in
    this script are present in Python's standard library.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方需要安装第三方库`bs4`。可以通过以下命令安装此模块。此脚本中使用的所有其他库都包含在Python的标准库中。
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Learn more about the `bs4` library; visit [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于`bs4`库的信息；访问[https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)。
- en: How to do it...
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We will perform the following steps in this recipe:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将执行以下步骤：
- en: Access index web page and identify all initial links.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问索引网页并识别所有初始链接。
- en: 'Recurse through all known links to:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归遍历所有已知链接以：
- en: Find additional links and add them to the queue.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找其他链接并将它们添加到队列中。
- en: Generate `SHA-256` hash of each web page.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成每个网页的`SHA-256`哈希。
- en: Write and then verify web page output to the destination directory.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将网页输出写入目标目录，然后验证。
- en: Log relevant activity and hash results.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录相关活动和哈希结果。
- en: How it works...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we import the required libraries to handle argument parsing, parsing
    HTML data, parsing dates, hashing files, logging data, and interacting with web
    pages. We also setup a variable used to later construct the recipe's logging component.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库来处理参数解析、解析HTML数据、解析日期、哈希文件、记录数据和与网页交互。我们还设置一个变量，用于稍后构建配方的日志组件。
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This recipe's command-line handler takes two positional inputs, `DOMAIN` and
    `OUTPUT_DIR`, which represent the website URL to preserve and the desired output
    directory, respectively. The optional `-l` argument can be used to specify the
    location of the log file path.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的命令行处理程序接受两个位置输入，`DOMAIN`和`OUTPUT_DIR`，分别表示要保存的网站URL和所需的输出目录。可选的`-l`参数可用于指定日志文件路径的位置。
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We will now setup the logging for the script, using the default or user-specified
    path. Using the logging format in *Chapter 1*, we specify a file and stream handler
    to keep the user in the loop and document the acquisition process.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为脚本设置日志记录，使用默认或用户指定的路径。使用*第1章*中的日志格式，我们指定文件和流处理程序，以保持用户在循环中并记录获取过程。
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: After setting up the log, we log a few details about the execution context of
    the script, including the supplied arguments and OS details.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 设置日志后，我们记录了脚本的执行上下文的一些细节，包括提供的参数和操作系统的详细信息。
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We perform some additional input validation on the desired output directory.
    After these steps, we call the `main()` function and pass it the website URL and
    the output directory.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对所需的输出目录进行了一些额外的输入验证。在这些步骤之后，我们调用`main（）`函数并将网站URL和输出目录传递给它。
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `main()` function is used to perform a few tasks. First, it extracts the
    base name of the website by removing any unnecessary elements before the actual
    name. For example, [https://google.com](https://google.com) becomes [google.com](https://google.com).
    We also create the set, `link_queue`, which will hold all unique links found on
    the web page.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`main（）`函数用于执行一些任务。首先，它通过删除实际名称之前的任何不必要元素来提取网站的基本名称。例如，[https://google.com](https://google.com)变成[google.com](https://google.com)。我们还创建了集合`link_queue`，它将保存在网页上找到的所有唯一链接。'
- en: We perform some additional validation on the input URL. During development,
    we ran into some errors when URLs were not preceded by `https://` or `http://`,
    so we check whether that is the case here and exit the script and inform the user
    of the requirement if they are not present. If everything checks out, we are ready
    to access the base web page. To do that, we create the unverified SSL context
    to avoid errors when accessing the web page.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对输入URL进行了一些额外的验证。在开发过程中，当URL没有以`https://`或`http://`开头时，我们遇到了一些错误，因此我们检查这种情况是否存在，并在这种情况下退出脚本并告知用户需求。如果一切正常，我们准备访问基本网页。为此，我们创建未经验证的SSL上下文以避免访问网页时出现错误。
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next, in a `try-except` block, we open a connection to the website with the
    unverified SSL context using the `urlopen()` method and read in the web page data.
    If we receive an error when attempting to access the web page, we print and log
    a status message prior to exiting the script. If we are successful, we log a success
    message and continue script execution.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在一个`try-except`块中，我们使用`urlopen（）`方法打开一个到网站的连接，并使用未经验证的SSL上下文读取网页数据。如果在尝试访问网页时收到错误，我们会在退出脚本之前打印和记录状态消息。如果成功，我们会记录成功消息并继续脚本执行。
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: With this first web page, we call the `write_output()` function to write it
    to the output directory and the `find_links()` function to identify all links
    on the web page. Specifically, this function attempts to identify all internal
    links on the website. We will explore both of these functions momentarily.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个第一个网页，我们调用`write_output（）`函数将其写入输出目录，并调用`find_links（）`函数来识别网页上的所有链接。具体来说，此函数尝试识别网站上的所有内部链接。我们将立即探索这两个函数。
- en: After identifying links on the first page, we print two status messages to the
    console and then call the `recurse_pages()` method to iterate through and discover
    all links on the discovered web pages and add them to the queue set. That completes
    the `main()` function; let's now take a look at the supporting cast of functions,
    starting with the `write_output()` method.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别第一页上的链接后，我们在控制台上打印两条状态消息，然后调用`recurse_pages（）`方法来迭代并发现发现的网页上的所有链接，并将它们添加到队列集合中。这完成了`main（）`函数；现在让我们来看一下支持函数的配角，从`write_output（）`方法开始。
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `write_output()` method takes a few arguments: the URL of the web page,
    its page data, the output directory, and an optional counter argument. By default
    this argument is set to zero if it is not supplied in the function call. The counter
    argument is used to append a loop iteration number to the output file to avoid
    writing over identically named files. We start by removing some unnecessary characters
    in the name of the output file that may cause it to create unnecessary directories.
    We also join the output directory with the URL directories and create them with
    `os.makedirs()`.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_output（）`方法需要一些参数：网页的URL，页面数据，输出目录和一个可选的计数器参数。默认情况下，如果在函数调用中未提供此参数，则将其设置为零。计数器参数用于将循环迭代号附加到输出文件，以避免覆盖同名文件。我们首先删除输出文件名中的一些不必要的字符，这可能会导致创建不必要的目录。我们还将输出目录与URL目录连接起来，并使用`os.makedirs（）`创建它们。'
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now, we log a few details about the web page we are writing. First, we log the
    name and output destination for the file. Then, we log the hash of the data as
    it was read from the web page with the `hash_data()` method. We create the path
    variable for the output file and append the counter string to avoid overwriting
    resources. We then open the output file and write the web page content to it.
    Finally, we log the output file hash by calling the `hash_file()` method.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们记录一些关于我们正在写的网页的细节。首先，我们记录文件的名称和输出目的地。然后，我们记录从网页中读取的数据的哈希值，使用`hash_data（）`方法。我们为输出文件创建路径变量，并附加计数器字符串以避免覆盖资源。然后，我们打开输出文件并将网页内容写入其中。最后，我们通过调用`hash_file（）`方法记录输出文件的哈希值。
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `hash_data()` method is really quite simple. We read in the UTF-8 encoded
    data and then generate the `SHA-256` hash of it using the same methodology as
    seen in previous recipes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`hash_data（）`方法实际上非常简单。我们读取UTF-8编码的数据，然后使用与之前的方法相同的方法生成其`SHA-256`哈希。'
- en: '[PRE30]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `hash_file()` method is just a little more complicated. Before we can hash
    the data, we must first open the file and read its contents into the `SHA-256`
    algorithm. With this complete, we call the `hexdigest()` method and return the
    generated `SHA-256` hash. Let's now shift to the `find_links()` method and how
    we leverage `BeautifulSoup` to quickly find all relevant links.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`hash_file（）`方法稍微复杂一些。在我们可以对数据进行哈希之前，我们必须首先打开文件并将其内容读入`SHA-256`算法中。完成后，我们调用`hexdigest（）`方法并返回生成的`SHA-256`哈希。现在让我们转向`find_links（）`方法以及我们如何利用`BeautifulSoup`快速找到所有相关链接。'
- en: '[PRE31]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `find_links()` method accomplishes a few things in its initial `for` loop.
    First of all, we create a `BeautifulSoup` object out of the web page data. Secondly,
    while creating that object, we specify that we only want to process part of the
    document, specifically, `<a href>` tags. This helps limit CPU cycles and memory
    usage and allows us to focus on only what is relevant. The `SoupStrainer` object
    is a fancy name for a filter and, in this case, filters only `<a href>` tags.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`find_links()` 方法在其初始的 `for` 循环中完成了一些事情。首先，我们从网页数据创建了一个 `BeautifulSoup` 对象。其次，在创建该对象时，我们指定只处理文档的一部分，具体来说是
    `<a href>` 标签。这有助于限制 CPU 周期和内存使用，并且允许我们只关注相关的内容。`SoupStrainer` 对象是一个过滤器的花哨名称，在这种情况下，它只过滤
    `<a href>` 标签。'
- en: With the list of links set up, we then create some logic to test whether they
    are part of this domain. In this case, we accomplish this by checking whether
    the website's URL is part of the link. Any link that passes that test must then
    not start with a "`#`" symbol. During testing, on one of the websites, we found
    this would cause internal page references, or named anchors, to get added as a
    separate page, which was not desirable. After a link passes those tests, it is
    added to the set queue (unless it is already present in the set object). After
    we process all such links, the queue is returned to the calling function. The
    `recurse_pages()` function makes multiple calls to this function to find all links
    in every page we index.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有了链接列表之后，我们创建一些逻辑来测试它们是否属于该域。在这种情况下，我们通过检查网站的 URL 是否属于该链接来实现这一点。通过这个测试的任何链接都不能以“#”符号开头。在测试过程中，我们发现在其中一个网站上，这会导致内部页面引用或命名锚点被添加为单独的页面，这是不可取的。通过这些测试的链接被添加到集合队列中（除非它已经存在于集合对象中）。处理所有这样的链接后，队列将返回到调用函数。`recurse_pages()`
    函数多次调用此函数，以查找我们索引的每个页面中的所有链接。
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `recurse_pages()` function takes as its inputs the website URL, current
    link queue, the unverified SSL context, and the output directory. We start by
    creating a processed list to keep track of the links we have already explored.
    We also set up the loop counter, which we later pass into the `write_output()`
    function to uniquely name the output files.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`recurse_pages()` 函数的输入包括网站 URL、当前链接队列、未经验证的 SSL 上下文和输出目录。我们首先创建一个已处理列表，以跟踪我们已经探索过的链接。我们还设置循环计数器，稍后将其传递给
    `write_output()` 函数，以唯一命名输出文件。'
- en: Next, we begin the dreaded `while True` loop, always a somewhat dangerous way
    of iteration, but it is used in this instance to continue iterating over the queue,
    which becomes progressively larger as we discover more pages. In this loop, we
    increment the counter by `1`, but more importantly, check whether the processed
    list length matches the length of all found links. If that is the case, this loop
    will be broken. However, until that scenario is met, the script will continue
    iterating over all links, looking for more internal links and writing them to
    the output directory.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始可怕的 `while True` 循环，这种迭代方式总是有些危险，但在这种情况下，它用于继续迭代队列，随着我们发现更多页面，队列会变得越来越大。在这个循环中，我们将计数器增加
    `1`，但更重要的是，检查已处理列表的长度是否与所有找到的链接的长度相匹配。如果是这种情况，循环将被中断。但在满足这种情况之前，脚本将继续迭代所有链接，寻找更多内部链接并将它们写入输出目录。
- en: '[PRE33]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We start iterating through a copy of the queue to process each link. We use
    the set `copy()` command so that we can update the queue without generating errors
    during its iterative loops. If the link has already been processed, we continue
    onto the next link to avoid performing redundant tasks. If this is the first time
    the link is being processed, the `continue` command is not executed, and instead,
    we append this link to the processed list so it will not be processed again in
    the future.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始迭代队列的副本，处理每个链接。我们使用 `set` 的 `copy()` 命令，以便我们可以更新队列而不在其迭代循环中生成错误。如果链接已经被处理，我们继续到下一个链接，以避免执行冗余任务。如果这是第一次处理该链接，则不执行
    `continue` 命令，而是将此链接附加到已处理列表中，以便将来不会再次处理。
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We attempt to open and read in the data for each link. If we cannot access the
    web page, we print and log that and continue executing the script. This way, we
    preserve all of the pages that we can access and have a log with details on links
    we were unable to access and preserve.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试打开并读取每个链接的数据。如果我们无法访问网页，我们会打印并记录下来，然后继续执行脚本。这样，我们可以保留所有我们可以访问并且有详细信息的页面，以及我们无法访问和保留的链接的日志。
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Finally, for each link we are able to access, we write its output to a file
    by passing the link name, page data, output directory, and the counter. We also
    set the `queue` object equal to the new set, which will have all elements from
    the old `queue` and any additional new links from the `find_links()` method. Eventually,
    and it may take some time based on the size of the website, we will have processed
    all items in the link queue and will exit the script after printing a status message
    to the console.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于我们能够访问的每个链接，我们通过传递链接名称、页面数据、输出目录和计数器来将其输出到文件。我们还将 `queue` 对象设置为新集合，其中包含旧
    `queue` 的所有元素以及 `find_links()` 方法的任何额外新链接。最终，根据网站的大小可能需要一些时间，我们将处理链接队列中的所有项目，并在打印控制台上的状态消息后退出脚本。
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'When we execute this script, we provide the URL for the website, the output
    folder, and a path to the log file as seen here:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行这个脚本时，我们提供网站的 URL、输出文件夹以及日志文件的路径，如下所示：
- en: '![](../images/00052.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00052.jpeg)'
- en: 'We can then open the output file in a browser and view the preserved content:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以在浏览器中打开输出文件并查看保留的内容：
- en: '![](../images/00053.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00053.jpeg)'
- en: There's more...
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can extend this script in many ways, including:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过许多方式扩展这个脚本，包括：
- en: Collecting CSS, images, and other resources
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集 CSS、图片和其他资源
- en: Screenshotting rendered pages in a browser with selenium
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 selenium 在浏览器中截取渲染的页面
- en: Setting the user-agent to disguise collections
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置用户代理以伪装收集
- en: Going hunting for viruses
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找病毒
- en: 'Recipe Difficulty: Medium'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：中等
- en: 'Python Version: 3.5'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Python 版本：3.5
- en: 'Operating System: Any'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任何
- en: VirusShare is the largest privately owned collection of malware samples, with
    over 29.3 million samples and counting. One of the great benefits of VirusShare,
    besides the literal cornucopia of malware that is every malware researcher's dream,
    is the list of malware hashes which is made freely available. We can use these
    hashes to a create a very comprehensive hash set and leverage that in casework
    to identify potentially malicious files.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: VirusShare是最大的私人拥有的恶意软件样本收集，拥有超过2930万个样本。VirusShare的一个巨大好处，除了每个恶意软件研究人员的梦想——大量的恶意软件之外，还有免费提供的恶意软件哈希列表。我们可以使用这些哈希来创建一个非常全面的哈希集，并在案件调查中利用它来识别潜在的恶意文件。
- en: To learn more about and use `VirusShare`, visit the website [https://virusshare.com/](https://virusshare.com/).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于使用`VirusShare`的信息，请访问网站[https://virusshare.com/](https://virusshare.com/)。
- en: In this recipe, we demonstrate how to automate downloading lists of hashes from
    VirusShare to create a newline-delimited hash list. This list can be used by forensic
    tools, such as X-Ways, to create a HashSet. Other forensic tools, EnCase, for
    example, can use this list as well but require the use of an EnScript to successfully
    import and create the HashSet.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们演示了如何自动下载来自VirusShare的哈希列表，以创建一个以换行符分隔的哈希列表。这个列表可以被法医工具（如X-Ways）使用来创建一个HashSet。其他法医工具，例如EnCase，也可以使用这个列表，但需要使用EnScript来成功导入和创建HashSet。
- en: Getting started
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: This recipe uses the `tqdm` third-party library to create an informative progress
    bar. The `tqdm` module can be installed via the following command. All other libraries
    used in this recipe are native to Python.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例使用了`tqdm`第三方库来创建一个信息丰富的进度条。`tqdm`模块可以通过以下命令安装。这个示例中使用的所有其他库都是Python本身的。
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Learn more about the `tqdm` library; visit [https://github.com/noamraph/tqdm](https://github.com/noamraph/tqdm).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于`tqdm`库的信息；访问[https://github.com/noamraph/tqdm](https://github.com/noamraph/tqdm)。
- en: How to do it...
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We will perform the following steps in this recipe:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将执行以下步骤：
- en: Read the VirusShare hashes page and dynamically identify the most recent hash
    list.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读VirusShare哈希页面并动态识别最新的哈希列表。
- en: Initialize progress bar and download hash lists in the desired range.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化进度条并在所需范围内下载哈希列表。
- en: How it works...
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we import the required libraries to handle argument parsing, creating
    progress bars, and interacting with web pages.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库来处理参数解析、创建进度条和与网页交互。
- en: '[PRE38]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This recipe's command-line handler takes one positional argument, `OUTPUT_HASH`,
    the desired file path for the hash set we will create. An optional argument, `--start`,
    captured as an integer, is the optional starting location for the hash lists.
    VirusShare maintains a page of links to malware hashes, where each link contains
    a list of between `65,536` and `131,072` `MD5` hashes. Rather than downloading
    all hash lists (which can take some time), the user can specify the desired starting
    location. For example, this may come in handy if an individual has previously
    downloaded hashes from VirusShare and now wishes to download the latest few hash
    lists that have been released.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的命令行处理程序接受一个位置参数`OUTPUT_HASH`，即我们将创建的哈希集的所需文件路径。一个可选参数`--start`，作为整数捕获，是哈希列表的可选起始位置。VirusShare维护一个包含恶意软件哈希链接的页面，每个链接包含`65,536`到`131,072`个`MD5`哈希的列表。用户可以指定所需的起始位置，而不是下载所有哈希列表（这可能需要一些时间）。例如，如果一个人之前从VirusShare下载了哈希，现在希望下载最新发布的几个哈希列表，这可能会很方便。
- en: '[PRE39]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We perform the standard input validation steps to ensure the supplied inputs
    will not cause any unexpected errors. We use the `os.path.dirname()` method to
    separate the directory path from the file path and check that it exists. If it
    doesn't, we create the directory now rather than encountering issues trying to
    write to a directory that does not exist. Lastly, we use an `if` statement and
    supply the `main()` function with the `start` argument as a keyword, if it was
    supplied.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行标准的输入验证步骤，以确保提供的输入不会导致任何意外错误。我们使用`os.path.dirname()`方法来从文件路径中分离目录路径并检查其是否存在。如果不存在，我们现在创建目录，而不是在尝试写入不存在的目录时遇到问题。最后，我们使用`if`语句，并将`start`参数作为关键字提供给`main()`函数，如果它被提供的话。
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `main()` function is the only function in this recipe. While it is long,
    the task is relatively straightforward, making additional functions somewhat unnecessary.
    Notice the `**kwargs` argument in the definition of the function. This creates
    a dictionary we can refer to support supplied keyword arguments. Prior to accessing
    the VirusShare website, we set up a few variables and print a status message to
    the console first. We use `ssl._create_unverified_context()` in order to bypass
    an SSL verification error received in Python 3.X.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数是这个示例中唯一的函数。虽然它很长，但任务相对简单，因此额外的函数并不是必要的。请注意函数定义中的`**kwargs`参数。这创建了一个字典，我们可以引用来支持提供的关键字参数。在访问VirusShare网站之前，我们设置了一些变量并首先在控制台打印了一个状态消息。我们使用`ssl._create_unverified_context()`来绕过Python
    3.X中收到的SSL验证错误。'
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We use a `try` and `except` block to open the VirusShare hashes page using the
    `urllib.request.urlopen()` method with the unverified SSL context. We use the
    `read()` method to read the page data and decode it to UTF-8\. If we receive an
    error attempting to access this page, we print a status message to the console
    and exit the script accordingly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`try`和`except`块来使用`urllib.request.urlopen()`方法打开VirusShare哈希页面，并使用未经验证的SSL上下文。我们使用`read()`方法来读取页面数据并解码为UTF-8。如果我们尝试访问这个页面时出现错误，我们会在控制台打印状态消息并相应地退出脚本。
- en: '[PRE42]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The first task with the downloaded page data is to identify the latest hash
    list. We do this by looking for the last instance of an HTML `href` tag to a VirusShare
    hash list. For instance, an example link may look like "`hashes/VirusShare_00288.md5`".
    We use string slicing and methods to separate the hash number (`288` in the previous
    example) from the link. We now check the `kwargs` dictionary to see whether the
    `start` argument was supplied. If it wasn't, we set the `start` variable to zero
    to download the first hash list and all intervening hash lists, up to and including
    the last one, to create the hash set.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下载的页面数据的第一个任务是识别最新的哈希列表。我们通过查找指向VirusShare哈希列表的HTML `href`标签的最后一个实例来实现这一点。例如，一个示例链接可能看起来像"`hashes/VirusShare_00288.md5`"。我们使用字符串切片和方法来从链接中分离哈希数（在前面的示例中为`288`）。现在，我们检查`kwargs`字典，看看是否提供了`start`参数。如果没有，我们将`start`变量设置为零，以下载第一个哈希列表和所有中间的哈希列表，直到并包括最后一个，以创建哈希集。
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Before we begin downloading the hash lists, we perform a sanity check and validate
    the `start` variable. Specifically, we check whether it is less than zero or greater
    than the latest hash list. We are using the `start` and `stop` variables to initialize
    the `for` loop and progress bar and therefore must validate the `start` variable
    to avoid unexpected outcomes. If the user supplied a bad `start` argument, we
    print a status message to the console and exit the script.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始下载哈希列表之前，我们进行一次健全性检查，并验证`start`变量。具体来说，我们检查它是否小于零或大于最新的哈希列表。我们使用`start`和`stop`变量来初始化`for`循环和进度条，因此必须验证`start`变量以避免意外结果。如果用户提供了错误的`start`参数，我们会在控制台打印状态消息并退出脚本。
- en: After the last sanity check, we print a status message to the console and set
    the `hashes_downloaded` counter to zero. We use this counter in a later status
    message to record how many hashes were downloaded and written to the hash list.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的健全性检查之后，我们会在控制台打印状态消息，并将`hashes_downloaded`计数器设置为零。我们将在稍后的状态消息中使用这个计数器来记录下载并写入哈希列表的数量。
- en: '[PRE44]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As discussed in [Chapter 1](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe),
    *Essential Scripting and File Information Recipes*, we can use the `tqdm.trange()`
    method as a substitute for the built-in `range()` method to create a loop and
    also a progress bar. We supply it with the desired `start` and `stop` integers
    and set a scale and a description for the progress bar. We must add `1` to the
    `stop` integer, due to the way `range()` works, to actually download the last
    hash list.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第1章](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe)中讨论的，*基本脚本和文件信息食谱*，我们可以使用`tqdm.trange()`方法作为内置`range()`方法的替代品来创建循环和进度条。我们为其提供所需的`start`和`stop`整数，并为进度条设置一个比例和描述。由于`range()`的工作方式，我们必须将`stop`整数加1，以实际下载最后一个哈希列表。
- en: In the `for` loop, we create a base URL and insert a five-digit number to specify
    the appropriate hash list. We accomplish this by converting the integer to a string
    and using `zfill()` to ensure the digit has five characters by prepending zeroes
    to the front of the string until it is five digits long. Next, as before, we use
    a `try` and `except` to open, read, and decode the hash list. We split on any
    new line characters to quickly create a list of hashes. If we encounter an error
    accessing the web page, we print a status message to the console and continue
    executing rather than exiting from the script.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在`for`循环中，我们创建一个基本的URL，并插入一个五位数来指定适当的哈希列表。我们通过将整数转换为字符串，并使用`zfill()`来确保数字有五个字符，通过在字符串前面添加零直到它有五位数。接下来，和之前一样，我们使用`try`和`except`来打开、读取和解码哈希列表。我们根据任何新行字符来拆分，快速创建一个哈希列表。如果我们遇到访问网页时出现错误，我们会在控制台打印状态消息，并继续执行而不是退出脚本。
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Once we have the hash list, we open the hash set text file in "`a+`" mode to
    append to the bottom of the text file and create the file if it does not already
    exist. Afterward, we only need to iterate through the downloaded hash list and
    write each hash to the file. Note that each hash list starts with a few commented
    lines (denoted by the `#` symbol) and so we implement logic to ignore those lines
    in addition to empty lines. After all hashes have been downloaded and written
    to the text file, we print a status message to the console and indicate the number
    of hashes downloaded.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了哈希列表，我们以“a+”模式打开哈希集文本文件，以便在文本文件底部追加并在文件不存在时创建文件。之后，我们只需要遍历下载的哈希列表，并将每个哈希写入文件。请注意，每个哈希列表都以几行注释开头（由`#`符号表示），因此我们实现逻辑来忽略这些行以及空行。在所有哈希都被下载并写入文本文件后，我们会在控制台打印状态消息，并指示下载的哈希数量。
- en: '[PRE46]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'When we run this script the hashes start downloading locally and are stored
    in the specified file as seen here:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个脚本时，哈希开始在本地下载，并存储在指定的文件中，如下所示：
- en: '![](../images/00054.jpeg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00054.jpeg)'
- en: When previewing the output file, we can see the `MD5` hash values saved as plain
    text. As previously mentioned, we can import this into the forensic tools either
    directly, as with X-Ways, or through a script, as with EnCase ([http://www.forensickb.com/2014/02/enscript-to-create-encase-v7-hash-set.html](http://www.forensickb.com/2014/02/enscript-to-create-encase-v7-hash-set.html)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在预览输出文件时，我们可以看到`MD5`哈希值保存为纯文本。如前所述，我们可以将其直接导入到取证工具中，如X-Ways，或通过脚本导入，如EnCase（[http://www.forensickb.com/2014/02/enscript-to-create-encase-v7-hash-set.html](http://www.forensickb.com/2014/02/enscript-to-create-encase-v7-hash-set.html)）。
- en: '![](../images/00055.jpeg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00055.jpeg)'
- en: Gathering intel
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集情报
- en: 'Recipe Difficulty: Medium'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：中等
- en: 'Python Version: 3.5'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：3.5
- en: 'Operating System: Any'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任意
- en: In this recipe, we use **VirusTotal**, a free online virus, malware, and URL
    scanner, to automate the review of potentially malicious websites or files. VirusTotal
    maintains detailed documentation of their API on their website. We will demonstrate
    how to perform basic queries against their system using their documented API and
    store returned results into a CSV file.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用**VirusTotal**，一个免费的在线病毒、恶意软件和URL扫描程序，来自动化审查潜在恶意网站或文件。VirusTotal在其网站上保留了他们API的详细文档。我们将演示如何使用他们记录的API对其系统执行基本查询，并将返回的结果存储到CSV文件中。
- en: Getting started
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'To follow this recipe, you need to first create an account with VirusTotal
    and decide between the free public API or the private API. The public API has
    request limitations, which the private API does not. For example, with the public
    API, we are limited to 4 requests per minute and 178,560 requests per month. More
    details about the different API types can be found on VirusTotal''s website. We
    will make these API calls with the `requests` library. This library can be installed
    using:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要遵循这个配方，您需要首先在VirusTotal上创建一个帐户，并在免费公共API和私人API之间做出选择。公共API有请求限制，而私人API没有。例如，使用公共API，我们每分钟限制为4次请求，每月限制为178,560次请求。有关不同API类型的更多详细信息可以在VirusTotal的网站上找到。我们将使用`requests`库进行这些API调用。可以使用以下命令安装此库：
- en: '[PRE47]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: To learn more about and use `VirusTotal`, visit the website at [https://www.virustotal.com/](https://www.virustotal.com/).
    [](https://www.virustotal.com/) Learn more about the `VirusTotal` Public API;
    visit [https://www.virustotal.com/en/documentation/public-api/](https://www.virustotal.com/en/documentation/public-api/).
    [](https://www.virustotal.com/en/documentation/public-api/) Learn more about the
    `VirusTotal` Private API; visit [https://www.virustotal.com/en/documentation/private-api/](https://www.virustotal.com/en/documentation/private-api/).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于并使用`VirusTotal`，请访问网站[https://www.virustotal.com/](https://www.virustotal.com/)。了解更多关于`VirusTotal`公共API的信息，请访问[https://www.virustotal.com/en/documentation/public-api/](https://www.virustotal.com/en/documentation/public-api/)。了解更多关于`VirusTotal`私人API的信息，请访问[https://www.virustotal.com/en/documentation/private-api/](https://www.virustotal.com/en/documentation/private-api/)。
- en: To view your API key, which you will need for the script, click on your account
    name in the top-right corner and navigate to My API key. Here you can view details
    of your API key and request a private key. Take a look at the following screenshot
    for additional details. All libraries used in this script are present in Python's
    standard library.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 查看您的API密钥，您将需要用于脚本的，点击右上角的帐户名称，然后导航到我的API密钥。在这里，您可以查看API密钥的详细信息并请求私钥。查看以下屏幕截图以获取更多详细信息。此脚本中使用的所有库都包含在Python的标准库中。
- en: '![](../images/00056.jpeg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00056.jpeg)'
- en: How to do it...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We use the following methodology to accomplish our objective:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下方法来实现我们的目标：
- en: Read in the list of signatures, as either domains and IPs or file paths and
    hashes, to research.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将签名列表读入，作为域和IP或文件路径和哈希进行研究。
- en: Query VirusTotal using the API for domain and IPs or files.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用API查询VirusTotal以获取域和IP或文件。
- en: Flatten results into a convenient format.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果展平成方便的格式。
- en: Write results to a CSV file.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果写入CSV文件。
- en: How it works...
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we import the required libraries to handle argument parsing, creating
    spreadsheets, hashing files, parsing JSON data, and interacting with web pages.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库来处理参数解析、创建电子表格、对文件进行哈希处理、解析JSON数据以及与网页交互。
- en: '[PRE48]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This recipe's command-line handler is a little more complicated than normal.
    It takes three positional arguments, `INPUT_FILE`, `OUTPUT_CSV`, and `API_KEY`,
    which represent the input text file of domains and IPs or file paths, the desired
    output CSV location, and a text file containing the API key to use, respectively.
    In addition to this, there are a few optional arguments, `-t` (or `--type`) and
    `--limit`, to specify the type of data in the input file and file paths or domains
    and to limit requests to comply with public API limitations. By default, the `type`
    argument is configured to the domain value. If the `limit` switch is added, it
    will have the Boolean value of `True`; otherwise, it will be `False`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序比正常情况下要复杂一些。它需要三个位置参数，`INPUT_FILE`，`OUTPUT_CSV`和`API_KEY`，分别代表域和IP或文件路径的输入文本文件，所需的输出CSV位置以及包含要使用的API密钥的文本文件。除此之外，还有一些可选参数，`-t`（或`--type`）和`--limit`，用于指定输入文件和文件路径或域的数据类型，并限制请求以符合公共API的限制。默认情况下，`type`参数配置为域值。如果添加了`limit`开关，它将具有`True`的布尔值；否则，它将是`False`。
- en: '[PRE49]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Next, we perform the standard data validation process on the input file and
    output CSV. If the inputs pass the data validation steps, we pass all arguments
    to the `main()` function or otherwise exit the script.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对输入文件和输出CSV执行标准数据验证过程。如果输入通过了数据验证步骤，我们将所有参数传递给`main()`函数，否则退出脚本。
- en: '[PRE50]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The `main()` function starts by reading the input file into a set called `objects`.
    A set was used here to cut down on duplicate lines and duplicate calls to the
    API. In this manner, we can try to prolong hitting the limitations of the public
    API unnecessarily.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数首先通过将输入文件读入名为`objects`的集合来开始。在这里使用了一个集合，以减少重复的行和对API的重复调用。通过这种方式，我们可以尽量延长不必要地达到公共API的限制。'
- en: '[PRE51]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: After we have read in the data, we check whether the type of data we read in
    is in the domain and IP category or file paths. Depending on the type, we send
    the set of data to the appropriate function, which will return VirusTotal query
    results to the `main()` function. We will then send these results to the `write_csv()`
    method to write the output. Let's look at the `query_domain()` function first.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取数据后，我们检查我们读入的数据类型是否属于域和IP类别或文件路径。根据类型，我们将数据集发送到适当的函数，该函数将返回VirusTotal查询结果给`main()`函数。然后我们将这些结果发送到`write_csv()`方法以写入输出。让我们首先看一下`query_domain()`函数。
- en: '[PRE52]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This function first performs additional input validation, this time on the API
    key file, to ensure the file exists prior to trying to make calls with said key.
    If the file does exist, we read it into the `api` variable. The `json_data` list
    will store returned JSON data from the VirusTotal API calls.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数首先对API密钥文件进行额外的输入验证，以确保在尝试使用该密钥进行调用之前文件存在。如果文件存在，我们将其读入`api`变量中。`json_data`列表将存储从VirusTotal
    API调用返回的JSON数据。
- en: '[PRE53]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: After we print a status message to the console, we begin to loop through each
    domain or IP address in the set. For each item, we increment `count` by one to
    keep track of how many API calls we have made. We create a parameter dictionary
    and store the domain or IP to search and API key and set `scan` to `1`. By setting
    `scan` to `1`, we will automatically submit the domain or IP for review if it
    is not already in the VirusTotal database.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在向控制台打印状态消息后，我们开始循环遍历集合中的每个域名或IP地址。对于每个项目，我们将`count`递增一次以跟踪我们已经进行了多少API调用。我们创建一个参数字典，并存储要搜索的域名或IP和API密钥，并将`scan`设置为`1`。通过将`scan`设置为`1`，如果域名或IP尚未在VirusTotal数据库中，我们将自动提交域名或IP进行审查。
- en: We make the API call with the `requests.post()` method, querying the appropriate
    URL with the parameter dictionary to obtain the results. We use the `json()` method
    on the returned requests object to convert it into easily manipulated JSON data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`requests.post()`方法进行API调用，查询适当的URL并使用参数字典来获取结果。我们使用返回的请求对象上的`json()`方法将其转换为易于操作的JSON数据。
- en: '[PRE54]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: If the API call was successful and the data was found in the VirusTotal database,
    we append the JSON data to the list. If the data was not present in the VirusTotal
    database, we can use the API to retrieve the report after it has been created.
    Here, for simplicity, we assume the data is already present in their database
    and only add results if they were found rather than waiting for the report to
    be generated if the item does not already exist.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果API调用成功并且在VirusTotal数据库中找到了数据，我们将JSON数据附加到列表中。如果在VirusTotal数据库中没有找到数据，我们可以使用API在生成报告后检索报告。在这里，为简单起见，我们假设数据已经存在于他们的数据库中，只有在找到结果时才添加结果，而不是等待报告生成（如果项目不存在）。
- en: '[PRE55]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Next, we check whether `limit` is `True` and the `count` variable is equal to
    3\. If so, we need to wait a minute before continuing the queries to comply with
    the public API limitations. We print status messages to the console so the user
    is aware of what the script is doing and use the `time.sleep()` method to halt
    script execution for a minute. After we have waited a minute, we reset the count
    back to zero and begin querying the remaining domain or IPs in the list. Once
    we have finished this process, we return the list of JSON results back to the
    `main()` function.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查`limit`是否为`True`，并且`count`变量是否等于3。如果是，我们需要等待一分钟，然后才能继续查询以遵守公共API的限制。我们向控制台打印状态消息，以便用户了解脚本正在做什么，并使用`time.sleep()`方法暂停脚本执行一分钟。等待了一分钟后，我们将计数重置为零，并开始查询列表中剩余的域名或IP。完成这个过程后，我们将JSON结果列表返回给`main()`函数。
- en: '[PRE56]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The `query_file()` method is similar to the `query_domain()` method we just
    explored. First, we validate that the API key file exists or exit the script otherwise.
    Once validated, we read in the API key and store it in the `api` variable and
    instantiate the `json_data` list to store the API JSON data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`query_file()`方法类似于我们刚刚探讨的`query_domain()`方法。首先，我们验证API密钥文件是否存在，否则退出脚本。验证通过后，我们读取API密钥并将其存储在`api`变量中，并实例化`json_data`列表以存储API
    JSON数据。'
- en: '[PRE57]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Unlike the `query_domain()` function, we need to perform some additional validation
    and processing on each file path before we can use it. Namely, we need to validate
    that each file path is valid and then we must hash each file, or use the hash
    provided in the signatures file. We hash these files as this is how we will look
    them up in the VirusTotal database. Recall that we are assuming the file is already
    present in the database. We can use the API to submit samples and retrieve reports
    after the file is scanned.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们刚刚探讨的`query_domain()`函数不同，我们需要对每个文件路径进行一些额外的验证和处理才能使用它。换句话说，我们需要验证每个文件路径是否有效，然后我们必须对每个文件进行哈希，或者使用签名文件中提供的哈希。我们对这些文件进行哈希处理，因为这是我们在VirusTotal数据库中查找它们的方式。请记住，我们假设文件已经存在于数据库中。我们可以使用API提交样本并在文件扫描后检索报告。
- en: '[PRE58]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Let's take a quick look at the `file_hash` function. The `hash_file()` method
    is relatively straightforward. This function takes a file path as its only input
    and returns the `SHA-256` hash for the said file. We accomplish this, similar
    to how we did so in [Chapter 1](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe),
    *Essential Scripting and File Information Recipes*, by creating a `hashlib` algorithm
    object, reading the file data into it `1,024` bytes at a time, and then calling
    the `hexdigest()` method to return the calculated hashes. With that covered, let's
    look at the remainder of the `query_file()` method.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下`file_hash`函数。`hash_file()`方法相对简单。这个函数以文件路径作为唯一输入，并返回该文件的`SHA-256`哈希。我们通过创建一个`hashlib`算法对象，类似于我们在[第1章](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe)中所做的方式，读取文件数据，每次读取`1,024`字节，然后调用`hexdigest()`方法返回计算出的哈希值。有了这个，让我们看一下`query_file()`方法的其余部分。
- en: '[PRE59]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The `query_file()` method continues by creating a parameter dictionary with
    the API key and file hash to look up. Again, we use the `requests.post()` and
    `json()` methods to make the API call and convert it into JSON data, respectively.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`query_file()`方法继续通过创建一个带有API密钥和文件哈希的参数字典来查找。同样，我们使用`requests.post()`和`json()`方法进行API调用，并将其转换为JSON数据。'
- en: '[PRE60]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: If the API call was successful and the file was already present in the VirusTotal
    database, we append the JSON data to the list. Once more, we perform checks on
    the count and limit to ensure we comply with the public API limitations. After
    we have completed all of the API calls, we return the list of JSON data back to
    the `main()` function for output.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果API调用成功并且文件已经存在于VirusTotal数据库中，我们将JSON数据附加到列表中。再次，我们对计数和限制进行检查，以确保遵守公共API限制。完成所有API调用后，我们将JSON数据列表返回给`main()`函数进行输出。
- en: '[PRE61]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The `write_csv()` method first checks that the output data actually contains
    API results. If it does not, the script will exit rather than write an empty CSV
    file.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_csv()`方法首先检查输出数据是否实际包含API结果。如果没有，脚本将退出而不是写入空的CSV文件。'
- en: '[PRE62]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: If we do have results, we print a status message to the console and begin by
    flattening the JSON data into a convenient output format. We create a `flatten_data`
    list, which will store each flattened JSON dictionary. The field list maintains
    the list of keys in the flattened JSON dictionary and the desired column headers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有结果，我们会在控制台上打印状态消息，并开始将JSON数据展平为方便的输出格式。我们创建一个`flatten_data`列表，它将存储每个展平的JSON字典。字段列表维护了展平的JSON字典中键的列表和所需的列标题。
- en: We use a few `for` loops to get to the JSON data and append a dictionary with
    this data to the list. After this process is completed, we will have a very simple
    list of dictionary structures to work with. We can use the `csv.DictWriter` class
    as we have previously to easily handle this type of data structure.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用几个`for`循环来获取JSON数据，并将带有这些数据的字典附加到列表中。完成此过程后，我们将拥有一个非常简单的字典结构列表可供使用。我们可以像以前一样使用`csv.DictWriter`类轻松处理这种数据结构。
- en: '[PRE63]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: With the data set ready for output, we open the CSV file and create the `DictWriter`
    class instance. We supply it the file object and the list of headers in the dictionary.
    We write the headers to the spreadsheet before writing each dictionary to a row.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好输出的数据集后，我们打开CSV文件并创建`DictWriter`类实例。我们向它提供文件对象和字典中标题的列表。我们在将每个字典写入行之前将标题写入电子表格。
- en: '[PRE64]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The following screenshot reflects when we run the script against files and
    hashes, and a second for running against domains and IPs:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图反映了当我们针对文件和哈希运行脚本时的情况，以及针对域名和IP运行脚本时的情况：
- en: '![](../images/00057.jpeg)![](../images/00058.jpeg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00057.jpeg)![](../images/00058.jpeg)'
- en: 'Looking at the output, we can learn about the malware classifications for the
    files and hashes and the domain or IP ranking in CSV format:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看输出，我们可以了解文件和哈希的恶意软件分类，以及域名或IP在CSV格式中的排名：
- en: '![](../images/00059.jpeg)![](../images/00060.jpeg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00059.jpeg)![](../images/00060.jpeg)'
- en: Totally passive
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完全被动
- en: 'Recipe Difficulty: Medium'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 教程难度：中等
- en: 'Python Version: 3.5'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：3.5
- en: 'Operating System: Any'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：任何
- en: This recipe explores the PassiveTotal API and how to use it to automate the
    review of domains and IP addresses. This service is particularly useful in viewing
    historical resolution details for a given domain. For example, you may have a
    suspected phishing website and, based on historical resolution patterns, can identify
    how long it has been active and what other domains used to share that IP. This
    then gives you additional domains to review and search for, in your evidence as
    you identify the different means and methods of how the attackers maintained persistence
    as they compromised multiple users across the environment.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程探讨了PassiveTotal API以及如何使用它来自动审查域名和IP地址。这项服务在查看给定域的历史解析详情方面特别有用。例如，您可能有一个被怀疑的钓鱼网站，并且根据历史解析模式，可以确定它已经活跃了多长时间，以及以前有哪些其他域名共享了该IP。然后，这给您提供了额外的域名来审查和搜索，以便在确定攻击者在整个环境中如何维持持久性的不同手段和方法时，您可以找到证据。
- en: Getting started
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: To use the PassiveTotal API, you need to first create a free account on their
    website. Once you are logged in, you can view your API key by navigating to your
    account settings and clicking on the User Show button under the API ACCESS section.
    See the following screenshot for a visual representation of this page.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用PassiveTotal API，您需要首先在他们的网站上创建一个免费帐户。登录后，您可以通过导航到帐户设置并在API ACCESS部分的用户显示按钮下点击查看API密钥。请参考以下截图以直观地了解此页面。
- en: '![](../images/00061.jpeg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00061.jpeg)'
- en: All libraries used in this script are present in Python's standard library.
    However, we do install the PassiveTotal Python API client and follow the installation
    and setup instructions in the README found at [https://github.com/passivetotal/python_api](https://github.com/passivetotal/python_api)
    or with `pip install passivetotal==1.0.30`. We do this to use the PassiveTotal
    command-line `pt-client` application. In this script, we make the API calls through
    this client rather than performing this at a more manual level as we did in the
    previous recipe. More details on the PassiveTotal API, especially if you are interested
    in developing something more advanced, can be found on their website.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本中使用的所有库都包含在Python的标准库中。但是，我们确实安装了PassiveTotal Python API客户端，并按照README中的安装和设置说明在[https://github.com/passivetotal/python_api](https://github.com/passivetotal/python_api)或使用`pip
    install passivetotal==1.0.30`进行安装。我们这样做是为了使用PassiveTotal命令行`pt-client`应用程序。在此脚本中，我们通过此客户端进行API调用，而不是像在上一个教程中那样以更手动的方式执行。如果您对PassiveTotal
    API有更多兴趣，尤其是如果您有兴趣开发更高级的东西，可以在他们的网站上找到更多详细信息。
- en: To learn more about and use `PassiveTotal`, visit the website [https://www.passivetotal.org](https://www.passivetotal.org).
    [](https://www.passivetotal.org) Learn more about the `PassiveTotal` API; visit
    [https://api.passivetotal.org/api/docs](https://api.passivetotal.org/api/docs).
    [](https://api.passivetotal.org/api/docs) Learn more about the `PassiveTotal`
    Python API; visit [https://github.com/passivetotal/python_api](https://github.com/passivetotal/python_api).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于并使用`PassiveTotal`，请访问网站[https://www.passivetotal.org](https://www.passivetotal.org)。[](https://www.passivetotal.org)
    了解更多关于`PassiveTotal` API，请访问[https://api.passivetotal.org/api/docs](https://api.passivetotal.org/api/docs)。[](https://api.passivetotal.org/api/docs)
    了解更多关于`PassiveTotal` Python API，请访问[https://github.com/passivetotal/python_api](https://github.com/passivetotal/python_api)。
- en: How to do it...
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We use the following methodology to accomplish our objective:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下方法来实现我们的目标：
- en: Read in the list of domains to review.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取要审查的域名列表。
- en: Call the command-line `pt-client` using `subprocess` and return results to our
    script for each domain.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`subprocess`调用命令行`pt-client`，并为每个域名将结果返回到我们的脚本。
- en: Write results to a CSV file.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果写入CSV文件。
- en: How it works...
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we import the required libraries to handle argument parsing, creating
    spreadsheets, parsing JSON data, and spawning subprocesses.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库来处理参数解析、创建电子表格、解析JSON数据和生成子进程。
- en: '[PRE65]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: This recipe's command-line handler takes two positional arguments, `INPUT_DOMAINS`
    and `OUTPUT_CSV`, for the input text file containing domains and/or IPs and the
    desired output CSV, respectively.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序接受两个位置参数，`INPUT_DOMAINS`和`OUTPUT_CSV`，分别用于包含域名和/或IP的输入文本文件以及所需的输出CSV。
- en: '[PRE66]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: We perform the standard input validation steps on each of the inputs to avoid
    unexpected errors in the script. With the inputs validated, we called the `main()`
    function and pass it the two inputs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个输入执行标准的输入验证步骤，以避免脚本中出现意外错误。验证输入后，我们调用`main()`函数并传递这两个输入。
- en: '[PRE67]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The `main()` function is rather straightforward, and similar, to that in the
    previous recipe. We again use a set to read in the objects in the input file.
    Once again, this is to avoid redundant API calls to the PassiveTotal API as there
    are daily limitations to the free API. After we read in these objects, we call
    the `query_domains()` function, which uses the `pt-client` application to make
    API calls. Once we have all of the returned JSON data from the API calls, we call
    the `write_csv()` method to write the data to the CSV file.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()` 函数非常简单，与之前的配方类似。我们再次使用集合来读取输入文件中的对象。这是为了避免对PassiveTotal API进行冗余的API调用，因为免费API有每日限制。在读取这些对象之后，我们调用`query_domains()`函数，该函数使用`pt-client`应用程序进行API调用。一旦我们从API调用中获得了所有返回的JSON数据，我们调用`write_csv()`方法将数据写入CSV文件。'
- en: '[PRE68]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The `query_domains()` function starts by creating a `json_data` list to store
    the returned JSON data and printing a status message to the console. We then begin
    to iterate through each object in the input file and remove any "`https://`" or
    "`http://`" substrings. While testing `pt-client`, it was observed to generate
    an internal server error if that substring was present. For example, instead of
    [https://www.google.com](https://www.google.com), the query should just be [www.google.com](https://www.google.com).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`query_domains()` 函数首先创建一个`json_data`列表来存储返回的JSON数据，并在控制台打印状态消息。然后，我们开始遍历输入文件中的每个对象，并删除任何"`https://`"或"`http://`"子字符串。在测试`pt-client`时，观察到如果存在该子字符串，它会生成内部服务器错误。例如，查询应该是[www.google.com](https://www.google.com)而不是[https://www.google.com](https://www.google.com)。'
- en: '[PRE69]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: With the domain or IP address ready to be queried, we use the `subprocess.Popen()`
    method to open a new process and execute the `pt-client` application. Arguments
    to be executed in this process are in a list. The command that will be executed,
    if the domain is [www.google.com](https://www.google.com), would look like `pt-client
    pdns -q www.gooogle.com`. Supplying the `stdout` keyword argument as `subprocess.PIPE`
    creates a new pipe for the process so that we can retrieve results from the query.
    We do exactly that in the following line by calling the `communicate()` method
    and then converting the returned data into a JSON structure that we can then store.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好查询的域名或IP地址后，我们使用`subprocess.Popen()`方法打开一个新进程并执行`pt-client`应用程序。要在此进程中执行的参数在列表中。如果域名是[www.google.com](https://www.google.com)，那么将要执行的命令看起来像`pt-client
    pdns -q www.gooogle.com`。通过将`stdout`关键字参数设置为`subprocess.PIPE`，我们为进程创建了一个新的管道，以便我们可以从查询中检索结果。我们通过调用`communicate()`方法并将返回的数据转换为JSON结构来做到这一点，然后将其存储。
- en: '[PRE70]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: If the `quota_exceeded` message is in the JSON results, then we have exceeded
    the daily API limit and print that to the console and continue executing. We continue
    executing rather than exiting so that we can write any results we did retrieve
    before exceeding the daily API quota.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果JSON结果中包含`quota_exceeded`消息，则表示我们已超过了每日API限制，并将其打印到控制台并继续执行。我们继续执行而不是退出，以便在超过每日API配额之前可以写入我们检索到的任何结果。
- en: '[PRE71]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Next, we set the `result_count` and check if it is equal to zero. If results
    were found for the query, we append the results to the JSON list. We return the
    JSON list, after performing this operation on all domains and/or IPs in the input
    file.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置`result_count`并检查它是否等于零。如果查询找到了结果，我们将结果附加到JSON列表中。在对输入文件中的所有域名和/或IP执行此操作后，我们返回JSON列表。
- en: '[PRE72]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The `write_csv()` method is pretty straightforward. Here we first check that
    we have data to write to the output file. Then, we print a status message to the
    console and create the list of headers and the order in which they should be written.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`write_csv()` 方法非常简单。在这里，我们首先检查是否有数据要写入输出文件。然后，我们在控制台打印状态消息，并创建标题列表以及它们应该被写入的顺序。'
- en: '[PRE73]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: After we have created the list of headers, we use the `csv.DictWriter` class
    to set up the output CSV file, write the header row, and iterate through each
    dictionary in the JSON results and write them to their respective rows.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了标题列表之后，我们使用`csv.DictWriter`类来设置输出CSV文件，写入标题行，并遍历JSON结果中的每个字典，并将它们写入各自的行。
- en: '[PRE74]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Running the script provides insight to the number of responses per item in
    the PassiveTotal lookup:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本可以了解PassiveTotal查找中每个项目的响应数量：
- en: '![](../images/00062.jpeg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00062.jpeg)'
- en: 'The CSV report displays the collected information as seen here:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: CSV报告显示了收集到的信息，如下所示：
- en: '![](../images/00063.jpeg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00063.jpeg)'
