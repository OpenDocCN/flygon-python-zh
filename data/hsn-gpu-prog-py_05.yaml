- en: Streams, Events, Contexts, and Concurrency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流、事件、上下文和并发性
- en: 'In the prior chapters, we saw that there are two primary operations we perform
    from the host when interacting with the GPU:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们看到在与GPU交互时，主机执行的两个主要操作是：
- en: Copying memory data to and from the GPU
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将内存数据从GPU复制到GPU，以及从GPU复制到内存
- en: Launching kernel functions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动内核函数
- en: We know that *within* a single kernel, there is one level of concurrency among
    its many threads; however, there is another level of concurrency *over* multiple
    kernels *and* GPU memory operations that are also available to us. This means
    that we can launch multiple memory and kernel operations at once, without waiting
    for each operation to finish. However, on the other hand, we will have to be somewhat
    organized to ensure that all inter-dependent operations are synchronized; this
    means that we shouldn't launch a particular kernel until its input data is fully
    copied to the device memory, or we shouldn't copy the output data of a launched
    kernel to the host until the kernel has finished execution.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在单个内核中，其许多线程之间存在一定程度的并发性；然而，在多个内核和GPU内存操作之间，还存在另一种并发性。这意味着我们可以同时启动多个内存和内核操作，而无需等待每个操作完成。然而，另一方面，我们必须有一定的组织能力，以确保所有相互依赖的操作都得到同步；这意味着我们不应该在其输入数据完全复制到设备内存之前启动特定的内核，或者在内核执行完成之前，不应该将已启动内核的输出数据复制到主机。
- en: To this end, we have what are known as **CUDA** **streams**—a **stream** is
    a sequence of operations that are run in order on the GPU. By itself, a single
    stream isn't of any use—the point is to gain concurrency over GPU operations issued
    by the host by using multiple streams. This means that we should interleave launches
    of GPU operations that correspond to different streams, in order to exploit this
    notion.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们有所谓的**CUDA** **流**—**流**是在GPU上按顺序运行的一系列操作。单独一个流是没有用的—重点是通过使用多个流在主机上发出的GPU操作来获得并发性。这意味着我们应该交错启动与不同流对应的GPU操作，以利用这个概念。
- en: We will be covering this notion of streams extensively in this chapter. Additionally,
    we will look at **events**, which are a feature of streams that are used to precisely
    time kernels and indicate to the host as to what operations have been completed
    within a given stream.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中广泛涵盖流的概念。此外，我们还将研究**事件**，这是流的一个特性，用于精确计时内核，并指示主机在给定流中已完成哪些操作。
- en: Finally, we will briefly look at CUDA **contexts**. A **context** can be thought
    of as analogous to a process in your operating system, in that the GPU keeps each
    context's data and kernel code *walled off *and encapsulated away from the other
    contexts currently existing on the GPU. We will see the basics of this near the
    end of the chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将简要介绍CUDA **上下文**。**上下文**可以被视为类似于操作系统中的进程，因为GPU将每个上下文的数据和内核代码*隔离*并封装起来，使其与当前存在于GPU上的其他上下文相分离。我们将在本章末尾看到这方面的基础知识。
- en: 'The following are the learning outcomes for this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习成果如下：
- en: Understanding the concepts of device and stream synchronization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解设备和流同步的概念
- en: Learning how to effectively use streams to organize concurrent GPU operations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何有效地使用流来组织并发的GPU操作
- en: Learning how to effectively use CUDA events
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何有效地使用CUDA事件
- en: Understanding CUDA contexts
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解CUDA上下文
- en: Learning how to explicitly synchronize within a given context
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何在给定上下文中显式同步
- en: Learning how to explicitly create and destroy a CUDA context
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何显式创建和销毁CUDA上下文
- en: Learning how to use contexts to allow for GPU usage among multiple processes
    and threads on the host
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用上下文允许主机上的多个进程和线程共享GPU使用
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一台带有现代NVIDIA GPU（2016年以后）的Linux或Windows 10 PC，并安装了所有必要的GPU驱动程序和CUDA Toolkit（9.0及以上）。还需要一个合适的Python
    2.7安装（如Anaconda Python 2.7），并安装了PyCUDA模块。
- en: 'This chapter''s code is also available on GitHub:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)'
- en: For more information about the prerequisites, check the *Preface* of this book,
    and for the software and hardware requirements, check the README in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有关先决条件的更多信息，请查看本书的*前言*，有关软件和硬件要求，请查看[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)中的README。
- en: CUDA device synchronization
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA设备同步
- en: Before we can use CUDA streams, we need to understand the notion of **device
    synchronization**. This is an operation where the host blocks any further execution
    until all operations issued to the GPU (memory transfers and kernel executions)
    have completed. This is required to ensure that operations dependent on prior
    operations are not executed out-of-order—for example, to ensure that a CUDA kernel
    launch is completed before the host tries to read its output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以使用CUDA流之前，我们需要了解**设备同步**的概念。这是一种操作，其中主机阻塞任何进一步的执行，直到发给GPU的所有操作（内存传输和内核执行）都已完成。这是为了确保依赖于先前操作的操作不会被无序执行—例如，确保CUDA内核启动在主机尝试读取其输出之前已完成。
- en: 'In CUDA C, device synchronization is performed with the `cudaDeviceSynchronize`
    function. This function effectively blocks further execution on the host until
    all GPU operations have completed. `cudaDeviceSynchronize` is so fundamental that
    it is usually one of the very first topics covered in most books on CUDA C—we
    haven''t seen this yet, because PyCUDA has been invisibly calling this for us
    automatically as needed. Let''s take a look at an example of CUDA C code to see
    how this is done manually:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA C中，设备同步是通过`cudaDeviceSynchronize`函数执行的。这个函数有效地阻止了主机上的进一步执行，直到所有GPU操作完成。`cudaDeviceSynchronize`是如此基本，以至于它通常是CUDA
    C大多数书籍中最早涉及的主题之一-我们还没有看到这一点，因为PyCUDA已经在需要时自动为我们调用了这个函数。让我们看一个CUDA C代码的例子，看看如何手动完成这个操作：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this block of code, we see that we have to synchronize with the device directly
    after every single GPU operation. If we only have a need to call a single CUDA
    kernel at a time, as seen here, this is fine. But if we want to concurrently launch
    multiple independent kernels and memory operations operating on different arrays
    of data, it would be inefficient to synchronize across the entire device. In this
    case, we should synchronize across multiple streams. We'll see how to do this
    right now.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码块中，我们看到我们必须在每个单独的GPU操作之后直接与设备进行同步。如果我们只需要一次调用单个CUDA内核，就像这里看到的那样，这是可以的。但是，如果我们想要同时启动多个独立的内核和操作不同数据数组的内存操作，跨整个设备进行同步将是低效的。在这种情况下，我们应该跨多个流进行同步。我们现在就来看看如何做到这一点。
- en: Using the PyCUDA stream class
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyCUDA流类
- en: We will start with a simple PyCUDA program; all this will do is generate a series
    of random GPU arrays, process each array with a simple kernel, and copy the arrays
    back to the host. We will then modify this to use streams. Keep in mind this program
    will have no point at all, beyond illustrating how to use streams and some basic
    performance gains you can get. (This program can be seen in the `multi-kernel.py` file,
    under the `5` directory in the GitHub repository.)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的PyCUDA程序开始；它只是生成一系列随机的GPU数组，对每个数组进行简单的内核处理，然后将数组复制回主机。然后我们将修改它以使用流。请记住，这个程序将完全没有任何意义，只是为了说明如何使用流以及一些基本的性能提升。（这个程序可以在GitHub存储库的`5`目录下的`multi-kernel.py`文件中看到。）
- en: 'Of course, we''ll start by importing the appropriate Python modules, as well
    as the `time` function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们将首先导入适当的Python模块，以及`time`函数：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We now will specify how many arrays we wish to process—here, each array will
    be processed by a different kernel launch. We also specify the length of the random
    arrays we will generate, as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将指定要处理多少个数组-在这里，每个数组将由不同的内核启动进行处理。我们还指定要生成的随机数组的长度，如下所示：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now have a kernel that operates on each array; all this will do is iterate
    over each point in the array, and multiply and divide it by 2 for 50 times, ultimately
    leaving the array intact. We want to restrict the number of threads that each
    kernel launch will use, which will help us gain concurrency among many kernel
    launches on the GPU so that we will have each thread iterate over different parts
    of the array with a `for` loop. (Again, remember that this kernel function will
    be completely useless for anything other than for learning about streams and synchronization!)
    If each kernel launch uses too many threads, it will be harder to gain concurrency
    later:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个在每个数组上操作的内核；它只是迭代数组中的每个点，并将其乘以2再除以2，重复50次，最终保持数组不变。我们希望限制每个内核启动将使用的线程数，这将帮助我们在GPU上的许多内核启动之间获得并发性，以便我们可以让每个线程通过`for`循环迭代数组的不同部分。（再次提醒，这个内核函数除了用来学习流和同步之外，完全没有任何用处！）如果每个内核启动使用的线程太多，以后获得并发性将更加困难：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we will generate some random data array, copy these arrays to the GPU,
    iteratively launch our kernel over each array across 64 threads, and then copy
    the output data back to the host and assert that the same with NumPy''s `allclose`
    function. We will time the duration of all operations from start to finish by
    using Python''s `time` function, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将生成一些随机数据数组，将这些数组复制到GPU，迭代地在每个数组上启动我们的内核，然后将输出数据复制回主机，并使用NumPy的`allclose`函数来断言它们是否相同。我们将使用Python的`time`函数来计算从开始到结束的所有操作的持续时间：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are now prepared to run this program. I will run it right now:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备运行这个程序。我现在就要运行它：
- en: '![](assets/e590fec8-0e98-4fce-bd4e-091871758825.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e590fec8-0e98-4fce-bd4e-091871758825.png)'
- en: So, it took almost three seconds for this program to complete. We will make
    a few simple modifications so that our program can use streams, and then see if
    we can get any performance gains (this can be seen in the `multi-kernel_streams.py`
    file in the repository).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这个程序完成需要了将近三秒的时间。我们将进行一些简单的修改，使我们的程序可以使用流，然后看看我们是否可以获得任何性能提升（这可以在存储库中的`multi-kernel_streams.py`文件中看到）。
- en: 'First, we note that for each kernel launch we have a separate array of data
    that it processes, and these are stored in Python lists. We will have to create
    a separate stream object for each individual array/kernel launch pair, so let''s
    first add an empty list, entitled `streams`, that will hold our stream objects:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们注意到对于每个内核启动，我们有一个单独的数据数组进行处理，这些数组存储在Python列表中。我们将不得不为每个单独的数组/内核启动对创建一个单独的流对象，所以让我们首先添加一个空列表，名为`streams`，用来保存我们的流对象：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now generate a series of streams that we will use to organize the kernel
    launches. We can get a stream object from the `pycuda.driver` submodule with the
    `Stream` class. Since we''ve imported this submodule and aliased it as `drv`,
    we can fill up our list with new stream objects, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以生成一系列流，我们将使用它们来组织内核启动。我们可以使用`pycuda.driver`子模块和`Stream`类来获取流对象。由于我们已经导入了这个子模块并将其别名为`drv`，我们可以用以下方式用新的流对象填充我们的列表：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we will have to first modify our memory operations that transfer data
    to the GPU. Consider the following steps for it:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将首先修改将数据传输到GPU的内存操作。考虑以下步骤：
- en: 'Look for the first loop that copies the arrays to the GPU with the `gpuarray.to_gpu`
    function. We will want to switch to the asynchronous and stream-friendly version
    of this function, `gpu_array.to_gpu_async`, instead. (We must now also specify
    which stream each memory operation should use with the `stream` parameter):'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找第一个循环，使用`gpuarray.to_gpu`函数将数组复制到GPU。我们将希望切换到此函数的异步和适用于流的版本`gpu_array.to_gpu_async`。（现在我们还必须使用`stream`参数指定每个内存操作应该使用哪个流）：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now launch our kernels. This is exactly as before, only we must specify
    what stream to use by using the `stream` parameter:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以启动我们的内核。这与以前完全相同，只是我们必须通过使用`stream`参数来指定要使用哪个流：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we need to pull our data off the GPU. We can do this by switching
    the `gpuarray get` function to `get_async`, and again using the `stream` parameter,
    as follows:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要从GPU中取出数据。我们可以通过将`gpuarray get`函数切换为`get_async`来实现这一点，并再次使用`stream`参数，如下所示：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We are now ready to run our stream-friendly modified program:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备运行我们适用于流的修改后的程序：
- en: '![](assets/1e11ea75-4947-459b-a915-363bdad7b241.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1e11ea75-4947-459b-a915-363bdad7b241.png)'
- en: In this case, we have a triple-fold performance gain, which is not too bad considering
    the very few numbers of modifications we had to make. But before we move on, let's
    try to get a deeper understanding as to why this works.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们获得了三倍的性能提升，考虑到我们需要进行的修改非常少，这并不算太糟糕。但在我们继续之前，让我们尝试更深入地理解为什么这样做有效。
- en: 'Let''s consider the case of two CUDA kernel launches. We will also perform
    GPU memory operations corresponding to each kernel before and after we launch
    our kernels, for a total of six operations. We can visualize the operations happening
    on the GPU with respect to time with a graph as such—moving to the right on the
    *x*-axis corresponds to time duration, while the *y*-axis corresponds to operations
    being executed on the GPU at a particular time. This is depicted with the following
    diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑两个CUDA内核启动的情况。在我们启动内核之前和之后，我们还将执行与每个内核相对应的GPU内存操作，总共有六个操作。我们可以通过图表来可视化在GPU上随时间发生的操作，如此—在*x*轴上向右移动对应于时间持续时间，而*y*轴对应于在特定时间执行的GPU上的操作。这可以用以下图表来描述：
- en: '![](assets/9c272bdc-c6e1-4438-96ad-392af521175d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9c272bdc-c6e1-4438-96ad-392af521175d.png)'
- en: 'It''s not too hard to visualize why streams work so well in performance increase—since
    operations in a single stream are blocked until only all *necessary* prior operations
    are competed, we will gain concurrency among distinct GPU operations and make
    full use of our device. This can be seen by the large overlap of concurrent operations.
    We can visualize stream-based concurrency over time as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易想象为什么流在性能提高方面效果如此好——因为单个流中的操作直到所有*必要*的先前操作完成之后才会被阻塞，我们将获得不同GPU操作之间的并发性，并充分利用我们的设备。这可以通过并发操作的大量重叠来看出。我们可以将基于流的并发性随时间的变化可视化如下：
- en: '![](assets/d01793e3-d5a6-4ff8-b3fc-f22c198c7962.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d01793e3-d5a6-4ff8-b3fc-f22c198c7962.png)'
- en: Concurrent Conway's game of life using CUDA streams
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA流进行并发康威生命游戏
- en: We will now see a more interesting application—we will modify the LIFE (Conway's
    *Game of Life*) simulation from the last chapter, so that we will have four independent
    windows of animation displayed concurrently. (It is suggested you look at this
    example from the last chapter, if you haven't yet.)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到一个更有趣的应用——我们将修改上一章的LIFE（康威的*生命游戏*）模拟，以便我们将同时显示四个独立的动画窗口。（建议您查看上一章的这个示例，如果您还没有的话。）
- en: Let's get a copy of the old LIFE simulation from the last chapter in the repository,
    which should be under `conway_gpu.py` in the `4` directory. We will now modify
    this into our new CUDA-stream based concurrent LIFE simulation. (This new streams-based
    simulation that we will see in a moment is also available in the `conway_gpu_streams.py`
    file in this chapter's directory, `5`.)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从上一章的存储库中获取旧的LIFE模拟，应该在`4`目录中的`conway_gpu.py`下。现在我们将把这个修改为基于新的CUDA流的并发LIFE模拟。（我们将在稍后看到的这种基于流的新模拟也可以在本章目录`5`中的`conway_gpu_streams.py`文件中找到。）
- en: 'Go to the main function at the end of the file. We will set a new variable
    that indicates how many concurrent animations we will display at once with `num_concurrent`
    (where `N` indicates the height/width of the simulation lattice, as before). We
    will set it to `4` here, but you can feel free to try other values:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 转到文件末尾的主函数。我们将设置一个新变量，用`num_concurrent`表示我们将同时显示多少个并发动画（其中`N`表示模拟栅格的高度/宽度，与以前一样）。我们在这里将其设置为`4`，但您可以随意尝试其他值：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will now need a collection of `num_concurrent` stream objects, and will
    also need to allocate a collection of input and output lattices on the GPU. We''ll
    of course just store these in lists and initialize the lattices as before. We
    will set up some empty lists and fill each with the appropriate objects over a
    loop, as such (notice how we set up a new initial state lattice on each iteration,
    send it to the GPU, and concatenate it to `lattices_gpu`):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要一组`num_concurrent`流对象，并且还需要在GPU上分配一组输入和输出栅格。当然，我们只需将这些存储在列表中，并像以前一样初始化栅格。我们将设置一些空列表，并通过循环填充每个适当的对象，如下所示（请注意我们如何在每次迭代中设置一个新的初始状态栅格，将其发送到GPU，并将其连接到`lattices_gpu`）：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Since we're only doing this loop once during the startup of our program and
    the virtually all of the computational work will be in the animation loop, we
    really don't have to worry about actually using the streams we just immediately
    generated.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只在程序启动期间执行此循环一次，并且几乎所有的计算工作都将在动画循环中进行，因此我们实际上不必担心实际使用我们刚刚生成的流。
- en: 'We will now set up the environment with Matplotlib using the subplots function;
    notice how we can set up multiple animation plots by setting the `ncols` parameter.
    We will have another list structure that will correspond to the images that are
    required for the animation updates in `imgs`. Notice how we can now set this up
    with `get_async` and the appropriate corresponding stream:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用Matplotlib使用子图函数设置环境；请注意，我们可以通过设置`ncols`参数来设置多个动画图。我们将有另一个列表结构，它将对应于动画更新中所需的图像`imgs`。请注意，我们现在可以使用`get_async`和相应的流来设置这个：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The last thing to change in the main function is the penultimate line starting
    with `ani = animation.FuncAnimation`. Let''s modify the arguments to the `update_gpu`
    function to reflect the new lists we are using and add two more arguments, one
    to pass our `streams` list, plus a parameter to indicate how many concurrent animations
    there should be:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在主函数中要更改的最后一件事是以`ani = animation.FuncAnimation`开头的倒数第二行。让我们修改`update_gpu`函数的参数，以反映我们正在使用的新列表，并添加两个参数，一个用于传递我们的`streams`列表，另一个用于指示应该有多少并发动画的参数：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We now duly make the required modifications to the `update_gpu` function to
    take these extra parameters. Scroll up a bit in the file and modify the parameters
    as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要对`update_gpu`函数进行必要的修改以接受这些额外的参数。在文件中向上滚动一点，并按照以下方式修改参数：
- en: '`def update_gpu(frameNum, imgs, newLattices_gpu, lattices_gpu, N, streams,
    num_concurrent)`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`def update_gpu(frameNum, imgs, newLattices_gpu, lattices_gpu, N, streams,
    num_concurrent)`:'
- en: 'We now need to modify this function to iterate `num_concurrent` times and set
    each element of `imgs` as before, before finally returning the whole `imgs` list:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要修改这个函数，使其迭代`num_concurrent`次，并像以前一样设置`imgs`的每个元素，最后返回整个`imgs`列表：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice the changes we made—each kernel is launched in the appropriate stream,
    while `get` has been switched to a `get_async` synchronized with the same stream.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们所做的更改——每个内核都在适当的流中启动，而`get`已经切换为与相同流同步的`get_async`。
- en: Finally, the last line in the loop copies GPU data from one device array to
    another without any re-allocation. Before, we could use the shorthand slicing
    operator `[:]` to directly copy the elements between the arrays without re-allocating
    any memory on the GPU; in this case, the slicing operator notation acts as an
    alias for the PyCUDA `set` function for GPU arrays. (`set`, of course, is the
    function that copies one GPU array to another of the same size, without any re-allocation.)
    Luckily, there is indeed a stream-synchronized also version of this function, `set_async`,
    but we need to use this specifically to call this function, explicitly specifying
    the array to copy and the stream to use.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在循环中的最后一行将GPU数据从一个设备数组复制到另一个设备数组，而不进行任何重新分配。在此之前，我们可以使用简写切片操作符`[:]`直接在数组之间复制元素，而不在GPU上重新分配任何内存；在这种情况下，切片操作符符号充当PyCUDA
    `set`函数的别名，用于GPU数组。 （当然，`set`是将一个GPU数组复制到另一个相同大小的数组，而不进行任何重新分配的函数。）幸运的是，确实有一个流同步版本的这个函数，`set_async`，但我们需要明确地使用这个函数来调用这个函数，明确指定要复制的数组和要使用的流。
- en: 'We''re now finished and ready to run this. Go to a Terminal and enter `python
    conway_gpu_streams.py` at the command line to enjoy the show:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成并准备运行。转到终端并在命令行输入`python conway_gpu_streams.py`来观看展示：
- en: '![](assets/93d56393-5968-409d-bcab-d56330f6bc91.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/93d56393-5968-409d-bcab-d56330f6bc91.png)'
- en: Events
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件
- en: '**Events** are objects that exist *on the GPU*, whose purpose is to act as
    milestones or progress markers for a stream of operations. Events are generally
    used to provide measure time duration *on the device side* to precisely time operations;
    the measurements we have been doing so far have been with host-based Python profilers
    and standard Python library functions such as `time`. Additionally, events they
    can also be used to provide a status update for the host as to the state of a
    stream and what operations it has already completed, as well as for explicit stream-based
    synchronization.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**事件**是存在于GPU上的对象，其目的是作为操作流的里程碑或进度标记。事件通常用于在设备端精确计时操作的时间持续时间；到目前为止，我们所做的测量都是使用基于主机的Python分析器和标准Python库函数，如`time`。此外，事件还可以用于向主机提供有关流状态和已完成的操作的状态更新，以及用于显式基于流的同步。'
- en: Let's start with an example that uses no explicit streams and uses events to
    measure only one single kernel launch. (If we don't explicitly use streams in
    our code, CUDA actually invisibly defines a default stream that all operations
    will be placed into).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个不使用显式流并使用事件来测量仅一个单个内核启动的示例开始。（如果我们的代码中没有显式使用流，CUDA实际上会隐式定义一个默认流，所有操作都将放入其中）。
- en: 'Here, we will use the same useless multiply/divide loop kernel and header as
    we did at the beginning of the chapter, and modify most of the following contents.
    We want a single kernel instance to run a long time for this example, so we will
    generate a huge array of random numbers for the kernel to process, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用与本章开头相同的无用的乘法/除法循环内核和标题，并修改大部分以下内容。我们希望为此示例运行一个长时间的单个内核实例，因此我们将生成一个巨大的随机数数组供内核处理，如下所示：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We now construct our events using the `pycuda.driver.Event` constructor (where,
    of course, `pycuda.driver` has been aliased as `drv` by our prior import statement).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用`pycuda.driver.Event`构造函数构造我们的事件（当然，`pycuda.driver`已经被我们之前的导入语句别名为`drv`）。
- en: 'We will create two event objects here, one for the start of the kernel launch,
    and the other for the end of the kernel launch, (We will always need *two* event
    objects to measure any single GPU operation, as we will see soon):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里创建两个事件对象，一个用于内核启动的开始，另一个用于内核启动的结束（我们将始终需要*两个*事件对象来测量任何单个GPU操作，很快我们将看到）：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we are about ready to launch our kernel, but first, we have to mark the `start_event`
    instance''s place in the stream of execution with the event record function. We
    launch the kernel and then mark the place of `end_event` in the stream of execution,
    and also with `record`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备启动我们的内核，但首先，我们必须使用事件记录函数标记`start_event`实例在执行流中的位置。我们启动内核，然后标记`end_event`在执行流中的位置，并且使用`record`：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Events have a binary value that indicates whether they were reached or not
    yet, which is given by the function query. Let''s print a status update for both
    events, immediately after the kernel launch:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 事件具有一个二进制值，指示它们是否已经到达，这是由函数query给出的。让我们在内核启动后立即为两个事件打印状态更新：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s run this right now and see what happens:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行这个，看看会发生什么：
- en: '![](assets/b7cd2aa1-cb0c-485a-939b-cf2d7fe35d1e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b7cd2aa1-cb0c-485a-939b-cf2d7fe35d1e.png)'
- en: Our goal here is to ultimately measure the time duration of our kernel execution,
    but the kernel hasn't even apparently launched yet. Kernels in PyCUDA have launched
    asynchronously (whether they exist in a specific stream or not), so we have to
    have to ensure that our host code is properly synchronized with the GPU.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是最终测量内核执行的时间持续，但内核似乎还没有启动。在PyCUDA中，内核是异步启动的（无论它们是否存在于特定流中），因此我们必须确保我们的主机代码与GPU正确同步。
- en: 'Since `end_event` comes last, we can block further host code execution until
    the kernel completes by this event object''s synchronize function; this will ensure
    that the kernel has completed before any further lines of host code are executed.
    Let''s add a line a line of code to do this in the appropriate place:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`end_event`是最后一个，我们可以通过此事件对象的`synchronize`函数阻止进一步的主机代码执行，直到内核完成；这将确保在执行任何进一步的主机代码之前内核已经完成。让我们在适当的位置添加一行代码来做到这一点：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we are ready to measure the execution time of the kernel; we do this
    with the event object''s `time_till` or `time_since `operations to compare to
    another event object to get the time between these two events in milliseconds.
    Let''s use the `time_till `operation of `start_event` on `end_event`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备测量内核的执行时间；我们可以使用事件对象的`time_till`或`time_since`操作来与另一个事件对象进行比较，以获取这两个事件之间的时间（以毫秒为单位）。让我们使用`start_event`的`time_till`操作在`end_event`上：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Time duration can be measured between two events that have already occurred
    on the GPU with the `time_till` and `time_since` functions. Note that these functions
    always return a value in terms of milliseconds!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 时间持续可以通过GPU上已经发生的两个事件之间的`time_till`和`time_since`函数来测量。请注意，这些函数始终以毫秒为单位返回值！
- en: 'Let''s try running our program again now:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次运行我们的程序：
- en: '![](assets/29aadcbf-d395-487a-ac70-f3c422ae6f12.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/29aadcbf-d395-487a-ac70-f3c422ae6f12.png)'
- en: (This example is also available in the `simple_event_example.py` file in the
    repository.)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例也可在存储库中的`simple_event_example.py`文件中找到。）
- en: Events and streams
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件和流
- en: We will now see how to use event objects with respect to streams; this will
    give us a highly intricate level of control over the flow of our various GPU operations,
    allowing us to know exactly how far each individual stream has progressed via
    the `query` function, and even allowing us to synchronize particular streams with
    the host while ignoring the other streams.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到如何在流方面使用事件对象；这将使我们对各种GPU操作的流程具有高度复杂的控制，使我们能够准确了解每个单独流的进展情况，甚至允许我们在忽略其他流的情况下与主机同步特定流。
- en: 'First, though, we have to realize this—each stream has to have its own dedicated
    collection of event objects; multiple streams cannot share an event object. Let''s
    see what this means exactly by modifying the prior example, `multi_kernel_streams.py`.
    After the kernel definition, let''s add two additional empty lists—`start_events`
    and `end_events`. We will fill these lists up with event objects, which will correspond
    to each stream that we have. This will allow us to time one GPU operation in each
    stream, since every GPU operation requires two events:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须意识到这一点——每个流必须有自己专用的事件对象集合；多个流不能共享一个事件对象。让我们通过修改之前的示例`multi_kernel_streams.py`来确切了解这意味着什么。在内核定义之后，让我们添加两个额外的空列表——`start_events`和`end_events`。我们将用事件对象填充这些列表，这些事件对象将对应于我们拥有的每个流。这将允许我们在每个流中计时一个GPU操作，因为每个GPU操作都需要两个事件：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we can time each kernel launch individually by modifying the second loop
    to use the record of the event at the beginning and end of the launch. Notice
    that here, since there are multiple streams, we have to input the appropriate
    stream as a parameter to each event object''s `record` function. Also, notice
    that we can capture the end events in a second loop; this will still allow us
    to capture kernel execution duration perfectly, without any delay in launching
    the subsequent kernels. Now consider the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过修改第二个循环来逐个计时每个内核启动，使用事件的开始和结束记录。请注意，由于这里有多个流，我们必须将适当的流作为参数输入到每个事件对象的`record`函数中。还要注意，我们可以在第二个循环中捕获结束事件；这仍然可以完美地捕获内核执行持续时间，而不会延迟启动后续内核。现在考虑以下代码：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we''re going to extract the duration of each individual kernel launch.
    Let''s add a new empty list after the iterative assert check, and fill it with
    the duration by way of the `time_till` function:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将提取每个单独内核启动的持续时间。让我们在迭代断言检查之后添加一个新的空列表，并通过`time_till`函数将其填充为持续时间：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s now add two `print` statements at the very end, to tell us the mean
    and standard deviation of the kernel execution times:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在最后添加两个`print`语句，告诉我们内核执行时间的平均值和标准偏差：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can now run this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行这个：
- en: '![](assets/40d38973-beee-4d08-8e0b-3847ae757c8e.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/40d38973-beee-4d08-8e0b-3847ae757c8e.png)'
- en: (This example is also available as `multi-kernel_events.py` in the repository.)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例也可在存储库中的`multi-kernel_events.py`中找到。）
- en: We see that there is a relatively low degree of standard deviation in kernel
    duration, which is good, considering each kernel processes the same amount of
    data over the same block and grid size—if there were a high degree of deviation,
    then that would mean that we were making highly uneven usage of the GPU in our
    kernel executions, and we would have to re-tune parameters to gain a greater level
    of concurrency.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到内核持续时间的标准偏差相对较低，这是好事，考虑到每个内核在相同的块和网格大小上处理相同数量的数据——如果存在较高的偏差，那将意味着我们在内核执行中使用GPU的不均匀程度很高，我们将不得不重新调整参数以获得更高的并发水平。
- en: Contexts
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文
- en: A CUDA **context** is usually described as being analogous to a process in an
    operating system. Let's review what this means—a process is an instance of a single
    program running on a computer; all programs outside of the operating system kernel
    run in a process. Each process has its own set of instructions, variables, and
    allocated memory, and is, generally speaking, blind to the actions and memory
    of other processes. When a process ends, the operating system kernel performs
    a cleanup, ensuring that all memory that the process allocated has been de-allocated,
    and closing any files, network connections, or other resources the process has
    made use of. (Curious Linux users can view the processes running on their computer
    with the command-line `top` command, while Windows users can view them with the
    Windows Task Manager).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA上下文通常被描述为类似于操作系统中的进程。让我们来看看这意味着什么——进程是计算机上运行的单个程序的实例；操作系统内核之外的所有程序都在一个进程中运行。每个进程都有自己的一组指令、变量和分配的内存，一般来说，它对其他进程的操作和内存是盲目的。当一个进程结束时，操作系统内核会进行清理，确保进程分配的所有内存都已被释放，并关闭进程使用的任何文件、网络连接或其他资源。（好奇的Linux用户可以使用命令行`top`命令查看计算机上运行的进程，而Windows用户可以使用Windows任务管理器查看）。
- en: Similar to a process, a context is associated with a single host program that
    is using the GPU. A context holds in memory all CUDA kernels and allocated memory
    that is making use of and is blind to the kernels and memory of other currently
    existing contexts. When a context is destroyed (at the end of a GPU based program,
    for example), the GPU performs a cleanup of all code and allocated memory within
    the context, freeing resources up for other current and future contexts. The programs
    that we have been writing so far have all existed within a single context, so
    these operations and concepts have been invisible to us.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于进程，上下文与正在使用GPU的单个主机程序相关联。上下文在内存中保存了所有正在使用的CUDA内核和分配的内存，并且对其他当前存在的上下文的内核和内存是盲目的。当上下文被销毁（例如，在基于GPU的程序结束时），GPU会清理上下文中的所有代码和分配的内存，为其他当前和未来的上下文释放资源。到目前为止，我们编写的程序都存在于一个单一的上下文中，因此这些操作和概念对我们来说是不可见的。
- en: Let's also remember that a single program starts as a single process, but it
    can fork itself to run across multiple processes or threads. Analogously, a single
    CUDA host program can generate and use multiple CUDA contexts on the GPU. Usually,
    we will create a new context when we want to gain host-side concurrency when we
    fork new processes or threads of a host process. (It should be emphasized, however,
    that there is no exact one-to-one relation between host processes and CUDA contexts).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也记住，一个单独的程序开始时是一个单独的进程，但它可以分叉自身以在多个进程或线程中运行。类似地，一个单独的CUDA主机程序可以在GPU上生成和使用多个CUDA上下文。通常，当我们分叉主机进程的新进程或线程时，我们将创建一个新的上下文以获得主机端的并发性。（然而，应该强调的是，主机进程和CUDA上下文之间没有确切的一对一关系）。
- en: As in many other areas of life, we will start with a simple example. We will
    first see how to access a program's default context and synchronize across it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与生活中的许多其他领域一样，我们将从一个简单的例子开始。我们将首先看看如何访问程序的默认上下文并在其间进行同步。
- en: Synchronizing the current context
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同步当前上下文
- en: We're going to see how to explicitly synchronize our device within a context
    from within Python as in CUDA C; this is actually one of the most fundamental
    skills to know in CUDA C, and is covered in the first or second chapters in most
    other books on the topic. So far, we have been able to avoid this topic, since
    PyCUDA has performed most synchronizations for us automatically with `pycuda.gpuarray`
    functions such as `to_gpu` or `get`; otherwise, synchronization was handled by
    streams in the case of the `to_gpu_async` or `get_async` functions, as we saw
    at the beginning of this chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如何在Python中显式同步我们的设备上下文，就像在CUDA C中一样；这实际上是CUDA C中最基本的技能之一，在其他大多数关于这个主题的书籍的第一章或第二章中都有涵盖。到目前为止，我们已经能够避免这个话题，因为PyCUDA自动使用`pycuda.gpuarray`函数（如`to_gpu`或`get`）执行大多数同步；否则，在本章开头我们看到的`to_gpu_async`或`get_async`函数的情况下，同步是由流处理的。
- en: We will be humble and start by modifying the program we wrote in [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml),
    *Getting Started with PyCUDA, *which generates an image of the Mandelbrot set
    using explicit context synchronization. (This is available here as the file `gpu_mandelbrot0.py`
    under the `3` directory in the repository.)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将谦卑地开始修改我们在[第3章](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)中编写的程序，*使用PyCUDA入门*，它使用显式上下文同步生成Mandelbrot集的图像。（这在存储库的`3`目录下的文件`gpu_mandelbrot0.py`中可用。）
- en: We won't get any performance gains over our original Mandelbrot program here;
    the only point of this exercise is just to help us understand CUDA contexts and
    GPU synchronization.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会获得任何性能提升，与我们最初的Mandelbrot程序相比；这个练习的唯一目的是帮助我们理解CUDA上下文和GPU同步。
- en: Looking at the header, we, of course, see the `import pycuda.autoinit` line.
    We can access the current context object with `pycuda.autoinit.context`, and we
    can synchronize in our current context by calling the `pycuda.autoinit.context.synchronize()` function.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下头部，我们当然看到了`import pycuda.autoinit`一行。我们可以使用`pycuda.autoinit.context`访问当前上下文对象，并且可以通过调用`pycuda.autoinit.context.synchronize()`函数在当前上下文中进行同步。
- en: 'Now let''s modify the `gpu_mandelbrot` function to handle explicit synchronization.
    The first GPU-related line we see is this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们修改`gpu_mandelbrot`函数以处理显式同步。我们看到的第一行与GPU相关的代码是这样的：
- en: '`mandelbrot_lattice_gpu = gpuarray.to_gpu(mandelbrot_lattice)`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`mandelbrot_lattice_gpu = gpuarray.to_gpu(mandelbrot_lattice)`'
- en: 'We can now change this to be explicitly synchronized. We can copy to the GPU
    asynchronously with `to_gpu_async`, and then synchronize as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将其更改为显式同步。我们可以使用`to_gpu_async`异步将数据复制到GPU，然后进行同步，如下所示：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We then see the next line allocates memory on the GPU with the `gpuarray.empty`
    function. Memory allocation in CUDA is, by the nature of the GPU architecture,
    automatically synchronized; there is no *asynchronous* memory allocation equivalent
    here. Hence, we keep this line as it was before.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到下一行使用`gpuarray.empty`函数在GPU上分配内存。由于GPU架构的性质，CUDA中的内存分配始终是自动同步的；这里没有*异步*内存分配的等价物。因此，我们保持这行与之前一样。
- en: Memory allocation in CUDA is always synchronized!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA中的内存分配始终是同步的！
- en: 'We now see the next two lines—our Mandelbrot kernel is launched with an invocation
    to `mandel_ker`, and we copy the contents of our Mandelbrot `gpuarray` object
    with an invocation to `get`. We synchronize after the kernel launch, switch `get`
    to `get_async`, and finally synchronize one last line:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到接下来的两行 - 我们的Mandelbrot内核通过调用`mandel_ker`启动，并且我们通过调用`get`复制了Mandelbrot
    `gpuarray`对象的内容。在内核启动后我们进行同步，将`get`切换为`get_async`，最后进行最后一行同步：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can now run this, and it will produce a Mandelbrot image to disk, exactly
    as in [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml), *Getting Started
    with PyCUDA.*
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行这个程序，它将像[第3章](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)中的*Getting
    Started with PyCUDA*一样在磁盘上生成一个Mandelbrot图像。
- en: (This example is also available as `gpu_mandelbrot_context_sync.py` in the repository.)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例也可在存储库中的`gpu_mandelbrot_context_sync.py`中找到。）
- en: Manual context creation
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动上下文创建
- en: So far, we have been importing `pycuda.autoinit` at the beginning of all of
    our PyCUDA programs; this effectively creates a context at the beginning of our
    program and has it destroyed at the end.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在所有PyCUDA程序的开头导入`pycuda.autoinit`；这实际上在程序开始时创建一个上下文，并在结束时销毁它。
- en: Let's try doing this manually. We will make a small program that just copies
    a small array to the GPU, copies it back to the host, prints the array, and exits.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试手动操作。我们将制作一个小程序，只是将一个小数组复制到GPU，然后将其复制回主机，打印数组，然后退出。
- en: 'We start with the imports:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入开始：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'First, we initialize CUDA with the `pycuda.driver.init` function, which is
    here aliased as `drv`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`pycuda.driver.init`函数初始化CUDA，这里被别名为`drv`：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we choose which GPU we wish to work with; this is necessary for the cases
    where one has more than one GPU. We can select a specific GPU with  `pycuda.driver.Device`;
    if you only have one GPU, as I do, you can access it with `pycuda.driver.Device(0)`,
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们选择要使用的GPU；在拥有多个GPU的情况下是必要的。我们可以使用`pycuda.driver.Device`选择特定的GPU；如果您只有一个GPU，就像我一样，可以使用`pycuda.driver.Device(0)`访问它，如下所示：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can now create a new context on this device with `make_context`, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`make_context`在此设备上创建一个新上下文，如下所示：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that we have a new context, this will automatically become the default
    context. Let''s copy an array into the GPU, copy it back to the host, and print
    it:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个新的上下文，这将自动成为默认上下文。让我们将一个数组复制到GPU，然后将其复制回主机并打印出来：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now we are done. We can destroy the context by calling the `pop` function:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们完成了。我们可以通过调用`pop`函数销毁上下文：
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: That's it! We should always remember to destroy contexts that we explicitly
    created with `pop` before our program exists.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们应该始终记得在程序退出之前销毁我们显式创建的上下文。
- en: (This example can be seen in the `simple_context_create.py` file under this
    chapter's directory in the repository.)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例可以在存储库中的本章目录下的`simple_context_create.py`文件中找到。）
- en: Host-side multiprocessing and multithreading
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主机端多进程和多线程
- en: Of course, we may seek to gain concurrency on the host side by using multiple
    processes or threads on the host's CPU. Let's make the distinction right now between
    a host-side operating system process and thread with a quick overview.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以通过在主机的CPU上使用多个进程或线程来实现并发。让我们现在明确区分主机端操作系统进程和线程。
- en: Every host-side program that exists outside the operating system kernel is executed
    as a process, and can also exist in multiple processes. A process has its own
    address space, as it runs concurrently with, and independently of, all other processes.
    A process is, generally speaking, blind to the actions of other processes, although
    multiple processes can communicate through sockets or pipes. In Linux and Unix,
    new processes are spawned with the fork system call.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 每个存在于操作系统内核之外的主机端程序都作为一个进程执行，并且也可以存在于多个进程中。进程有自己的地址空间，因为它与所有其他进程同时运行并独立运行。进程通常对其他进程的操作视而不见，尽管多个进程可以通过套接字或管道进行通信。在Linux和Unix中，使用fork系统调用生成新进程。
- en: In contrast, a host-side thread exists within a single process, and multiple
    threads can also exist within a single process. Multiple threads in a single process
    run concurrently. All threads in the same process share the same address space
    within the process and have access to the same shared variables and data. Generally,
    resource locks are used for accessing data among multiple threads, so as to avoid
    race conditions. In compiled languages such as C, C++, or Fortran, multiple process
    threads are usually managed with the Pthreads or OpenMP APIs.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，主机端线程存在于单个进程中，多个线程也可以存在于单个进程中。单个进程中的多个线程并发运行。同一进程中的所有线程共享进程内的相同地址空间，并且可以访问相同的共享变量和数据。通常，资源锁用于在多个线程之间访问数据，以避免竞争条件。在编译语言（如C、C++或Fortran）中，多个进程线程通常使用Pthreads或OpenMP
    API进行管理。
- en: Threads are much more lightweight than processes, and it is far faster for an
    operating system kernel to switch tasks between multiple threads in a single process,
    than to switch tasks between multiple processes. Normally, an operating system
    kernel will automatically execute different threads and processes on different
    CPU cores to establish true concurrency.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 线程比进程更轻量级，操作系统内核在单个进程中的多个线程之间切换任务要比在多个进程之间切换任务快得多。通常，操作系统内核会自动在不同的CPU核心上执行不同的线程和进程，以建立真正的并发性。
- en: A peculiarity of Python is that while it supports multi-threading through the
    `threading` module, all threads will execute on the same CPU core. This is due
    to technicalities of Python being an interpreted scripting language, and is related
    to Python's Global Identifier Lock (GIL). To achieve true multi-core concurrency
    on the host through Python, we, unfortunately, must spawn multiple processes with
    the `multiprocessing` module. (Unfortunately, the multiprocessing module is currently
    not fully functional under Windows, due to how Windows handles processes. Windows
    users will sadly have to stick to single-core multithreading here if they want
    to have any form of host-side concurrency.)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Python的一个特点是，虽然它通过`threading`模块支持多线程，但所有线程都将在同一个CPU核心上执行。这是由于Python是一种解释脚本语言的技术细节所致，与Python的全局标识符锁（GIL）有关。要通过Python在主机上实现真正的多核并发，不幸的是，我们必须使用`multiprocessing`模块生成多个进程。（不幸的是，由于Windows处理进程的方式，`multiprocessing`模块目前在Windows下并不完全可用。Windows用户将不幸地不得不坚持单核多线程，如果他们想要在主机端实现任何形式的并发。）
- en: We will now see how to use both threads in Python to use GPU based operations;
    Linux users should note that this can be easily extended to processes by switching
    references of `threading` to `multiprocessing`, and references to `Thread` to
    `Process`, as both modules look and act similarly. By the nature of PyCUDA, however,
    we will have to create a new CUDA context for every thread or process that we
    will use that will make use of the GPU. Let's see how to do this right now.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到如何在Python中使用两个线程来使用基于GPU的操作；Linux用户应该注意，通过将`threading`的引用切换到`multiprocessing`，将`Thread`的引用切换到`Process`，可以很容易地将其扩展到进程，因为这两个模块看起来和行为都很相似。然而，由于PyCUDA的性质，我们将不得不为每个将使用GPU的线程或进程创建一个新的CUDA上下文。让我们立即看看如何做到这一点。
- en: Multiple contexts for host-side concurrency
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主机并发的多个上下文
- en: 'Let''s first briefly review how to create a single host thread in Python that
    can return a value to the host with a simple example. (This example can also be
    seen in the `single_thread_example.py` file under `5` in the repository.) We will
    do this by using the `Thread` class in the `threading` module to create a subclass
    of `Thread`, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先简要回顾一下如何在Python中创建一个可以通过简单示例返回值给主机的单个主机线程。（这个例子也可以在存储库中的`single_thread_example.py`文件的`5`下看到。）我们将使用`threading`模块中的`Thread`类来创建`Thread`的子类，如下所示：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We now set up our constructor. We call the parent class''s constructor and
    set up an empty variable within the object that will be the return value from
    the thread:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们设置我们的构造函数。我们调用父类的构造函数，并在对象中设置一个空变量，这将是线程返回的值：
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We now set up the run function within our thread class, which is what will
    be executed when the thread is launched. We''ll just have it print a line and
    set the return value:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在我们的线程类中设置运行函数，这是在启动线程时将被执行的内容。我们只需要让它打印一行并设置返回值：
- en: '[PRE35]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We finally have to set up the join function. This will allow us to receive
    a return value from the thread:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要设置join函数。这将允许我们从线程中接收一个返回值：
- en: '[PRE36]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now we are done setting up our thread class. Let''s start an instance of this
    class as the `NewThread` object, spawn the new thread by calling the `start` method,
    and then block execution and get the output from the host thread by calling `join`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了我们的线程类。让我们将这个类的一个实例作为`NewThread`对象启动，通过调用`start`方法来生成新的线程，然后通过调用`join`来阻塞执行并从主机线程获取输出：
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now let''s run this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行这个：
- en: '![](assets/9dc8f524-03ac-4f2c-a21c-8736c1feb1cf.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9dc8f524-03ac-4f2c-a21c-8736c1feb1cf.png)'
- en: Now, we can expand this idea among multiple concurrent threads on the host to
    launch concurrent CUDA operations by way of multiple contexts and threading. We
    will now look at one last example. Let's re-use the pointless multiply/divide
    kernel from the beginning of this chapter and launch it within each thread that
    we spawn.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在主机上的多个并发线程之间扩展这个想法，通过多个上下文和线程来启动并发的CUDA操作。现在我们来看最后一个例子。让我们重新使用本章开头的无意义的乘法/除法内核，并在我们生成的每个线程中启动它。
- en: 'First, let''s look at the imports. Since we are making explicit contexts, remember
    to remove `pycuda.autoinit` and add an import `threading` at the end:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下导入部分。由于我们正在创建显式上下文，请记住删除`pycuda.autoinit`，并在最后添加一个`threading`导入：
- en: '[PRE38]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We will use the same array size as before, but this time we will have a direct
    correspondence between the number of the threads and the number of the arrays.
    Generally, we don''t want to spawn more than 20 or so threads on the host, so
    we will only go for `10` arrays. So, consider now the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与之前相同的数组大小，但这次线程的数量和数组的数量将直接对应。通常情况下，我们不希望在主机上生成超过20个左右的线程，所以我们只会使用`10`个数组。因此，现在考虑以下代码：
- en: '[PRE39]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we will store our old kernel as a string object; since this can only be
    compiled within a context, we will have to compile this in each thread individually:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将把旧的内核存储为一个字符串对象；由于这只能在一个上下文中编译，我们将不得不在每个线程中单独编译这个内核：
- en: '[PRE40]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we can begin setting up our class. We will make another subclass of `threading.Thread`
    as before, and set up the constructor to take one parameter as the input array.
    We will initialize an output variable with `None`, as we did before:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始设置我们的类。我们将像以前一样创建`threading.Thread`的另一个子类，并设置构造函数以接受一个输入数组作为参数。我们将用`None`初始化一个输出变量，就像以前一样：
- en: '[PRE41]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can now write the `run` function. We choose our device, create a context
    on that device, compile our kernel, and extract the kernel function reference.
    Notice the use of the `self` object:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写`run`函数。我们选择我们的设备，在该设备上创建一个上下文，编译我们的内核，并提取内核函数引用。注意`self`对象的使用：
- en: '[PRE42]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We now copy the array to the GPU, launch the kernel, and copy the output back
    to the host. We then destroy the context:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将数组复制到GPU，启动内核，然后将输出复制回主机。然后我们销毁上下文：
- en: '[PRE43]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we set up the join function. This will return `output_array` to the
    host:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们设置了join函数。这将把`output_array`返回到主机：
- en: '[PRE44]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We are now done with our subclass. We will set up some empty lists to hold
    our random test data, thread objects, and thread output values, similar to before.
    We will then generate some random arrays to process and set up a list of kernel
    launcher threads that will operate on each corresponding array:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了我们的子类。我们将设置一些空列表来保存我们的随机测试数据、线程对象和线程输出值，与之前类似。然后我们将生成一些随机数组进行处理，并设置一个核发射器线程列表，这些线程将分别操作每个数组：
- en: '[PRE45]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We will now launch each thread object, and extract its output into the `gpu_out`
    list by using `join`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将启动每个线程对象，并通过使用`join`将其输出提取到`gpu_out`列表中：
- en: '[PRE46]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we just do a simple assert on the output arrays to ensure they are
    the same as the input:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只需对输出数组进行简单的断言，以确保它们与输入相同：
- en: '[PRE47]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This example can be seen in the `multi-kernel_multi-thread.py` file in the repository.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例可以在存储库中的`multi-kernel_multi-thread.py`文件中看到。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter by learning about device synchronization and the importance
    of synchronization of operations on the GPU from the host; this allows dependent
    operations to allow antecedent operations to finish before proceeding. This concept
    has been hidden from us, as PyCUDA has been handling synchronization for us automatically
    up to this point. We then learned about CUDA streams, which allow for independent
    sequences of operations to execute on the GPU simultaneously without synchronizing
    across the entire GPU, which can give us a big performance boost; we then learned
    about CUDA events, which allow us to time individual CUDA kernels within a given
    stream, and to determine if a particular operation in a stream has occurred. Next,
    we learned about contexts, which are analogous to processes in a host operating
    system. We learned how to synchronize across an entire CUDA context explicitly
    and then saw how to create and destroy contexts. Finally, we saw how we can generate
    multiple contexts on the GPU, to allow for GPU usage among multiple threads or
    processes on the host.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过学习设备同步和从主机上同步GPU操作的重要性开始了本章；这允许依赖操作在进行之前完成。到目前为止，这个概念一直被PyCUDA自动处理，然后我们学习了CUDA流，它允许在GPU上同时执行独立的操作序列而无需在整个GPU上进行同步，这可以大大提高性能；然后我们学习了CUDA事件，它允许我们在给定流中计时单个CUDA内核，并确定流中的特定操作是否已发生。接下来，我们学习了上下文，它类似于主机操作系统中的进程。我们学习了如何在整个CUDA上下文中显式同步，然后看到了如何创建和销毁上下文。最后，我们看到了如何在GPU上生成多个上下文，以允许主机上的多个线程或进程使用GPU。
- en: Questions
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: In the launch parameters for the kernel in the first example, our kernels were
    each launched over 64 threads. If we increase the number of threads to and beyond
    the number of cores in our GPU, how does this affect the performance of both the
    original to the stream version?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个示例中内核的启动参数中，我们的内核每个都是在64个线程上启动的。如果我们将线程数增加到并超过GPU中的核心数，这会如何影响原始版本和流版本的性能？
- en: Consider the CUDA C example that was given at the very beginning of this chapter,
    which illustrated the use of `cudaDeviceSynchronize`. Do you think it is possible
    to get some level of concurrency among multiple kernels without using streams
    and only using `cudaDeviceSynchronize`?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑本章开头给出的CUDA C示例，它演示了`cudaDeviceSynchronize`的使用。您认为在不使用流，只使用`cudaDeviceSynchronize`的情况下，是否可能在多个内核之间获得一定程度的并发？
- en: If you are a Linux user, modify the last example that was given to operate over
    processes rather than threads.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您是Linux用户，请修改上一个示例，使其在进程上运行而不是线程。
- en: Consider the `multi-kernel_events.py` program; we said it is good that there
    was a low standard deviation of kernel execution durations. Why would it be bad
    if there were a high standard deviation?
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑`multi-kernel_events.py`程序；我们说内核执行持续时间的标准偏差很低是件好事。如果标准偏差很高会有什么坏处？
- en: We only used 10 host-side threads in the last example. Name two reasons why
    we have to use a relatively small number of threads or processes for launching
    concurrent GPU operations on the host.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们只使用了10个主机端线程。列出两个原因，说明为什么我们必须使用相对较少数量的线程或进程来在主机上启动并发的GPU操作。
