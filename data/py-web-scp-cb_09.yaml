- en: Creating a Simple Data API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个简单的数据API
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Creating a REST API with Flask-RESTful
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Flask-RESTful创建REST API
- en: Integrating the REST API with scraping code
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将REST API与抓取代码集成
- en: Adding an API to find the skills for a job listing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个用于查找工作列表技能的API
- en: Storing data in Elasticsearch as the result of a scraping request
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据存储在Elasticsearch中作为抓取请求的结果
- en: Checking Elasticsearch for a listing before scraping
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在抓取之前检查Elasticsearch中的列表
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: We have now reached an exciting inflection point in our learning about scraping. 
    From this point on, we will learn about making scrapers as a service using several
    APIs, microservice, and container tools, all of which will allow the running of
    the scraper either locally or in the cloud, and to give access to the scraper
    through standardized REST APIs.60;
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经达到了学习抓取的一个激动人心的转折点。从现在开始，我们将学习使用几个API、微服务和容器工具将抓取器作为服务运行，所有这些都将允许在本地或云中运行抓取器，并通过标准化的REST
    API访问抓取器。
- en: We will start this new journey in this chapter with the creation of a simple
    REST API using Flask-RESTful which we will eventually use to make requests to
    the service to scrape pages on demand.  We will connect this API to a scraper
    function implemented in a Python module that reuses the concepts for scraping
    StackOverflow jobs, as discussed in [Chapter  7](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml),
    *Text Wrangling and Analysis*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中开始这个新的旅程，使用Flask-RESTful创建一个简单的REST API，最终我们将使用它来对服务进行页面抓取请求。我们将把这个API连接到一个Python模块中实现的抓取器功能，该模块重用了在[第7章](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml)中讨论的从StackOverflow工作中抓取的概念，*文本整理和分析*。
- en: The final few recipes will focus on using Elasticsearch as a cache for these
    results, storing documents we retrieve from the scraper, and then looking for
    them first within the cache.  We will examine more elaborate uses of ElasticCache,
    such as performing searches for jobs with a given set of skills, later in [Chapter
    11](05a687e5-e5d7-4ffb-aa28-6078fbab5fea.xhtml), *Making the Scraper as a Service
    Real*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几个食谱将重点介绍将Elasticsearch用作这些结果的缓存，存储我们从抓取器中检索的文档，然后首先在缓存中查找它们。我们将在[第11章](05a687e5-e5d7-4ffb-aa28-6078fbab5fea.xhtml)中进一步研究ElasticCache的更复杂用法，比如使用给定技能集进行工作搜索，*使抓取器成为真正的服务*。
- en: Creating a REST API with Flask-RESTful
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Flask-RESTful创建REST API
- en: We start with the creation of a simple REST API using Flask-RESTful.  This initial
    API will consist of a single method that lets the caller pass an integer value
    and which returns a JSON blob.  In this recipe, the parameters and their values,
    as well as the return value, are not important at this time as we want to first
    simply get an API up and running using Flask-RESTful.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从使用Flask-RESTful创建一个简单的REST API开始。这个初始API将由一个单一的方法组成，让调用者传递一个整数值，并返回一个JSON块。在这个食谱中，参数及其值以及返回值在这个时候并不重要，因为我们首先要简单地使用Flask-RESTful来运行一个API。
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Flask is a web microframework that makes creating simple web application functionality
    incredibly easy.  Flask-RESTful is an extension to Flask which does the same for
    making REST APIs just as simple.  You can get Flask and read more about it at
    `flask.pocoo.org`. Flask-RESTful can be read about at `https://flask-restful.readthedocs.io/en/latest/`. Flask
    can be installed into your Python environment using `pip install flask`. and Flask-RESTful
    can also be installed with `pip install flask-restful`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Flask是一个Web微框架，可以让创建简单的Web应用功能变得非常容易。Flask-RESTful是Flask的一个扩展，可以让创建REST API同样简单。您可以在`flask.pocoo.org`上获取Flask并了解更多信息。Flask-RESTful可以在`https://flask-restful.readthedocs.io/en/latest/`上了解。可以使用`pip
    install flask`将Flask安装到您的Python环境中。Flask-RESTful也可以使用`pip install flask-restful`进行安装。
- en: 'The remainder of the recipes in the book will be in a subfolder of the chapter''s
    directory.  This is because most of these recipes either require multiple files
    to operate, or use the same filename (ie: `apy.py`).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中其余的食谱将在章节目录的子文件夹中。这是因为这些食谱中的大多数要么需要多个文件来操作，要么使用相同的文件名（即：`apy.py`）。
- en: How to do it
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'The initial API is implemented in `09/01/api.py`.  The API itself and the logic
    of the API is implemented in this single file: `api.py`.  The API can be run in
    two manners, the first of which is by simply executing the file as a Python script.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 初始API实现在`09/01/api.py`中。API本身和API的逻辑都在这个单一文件`api.py`中实现。API可以以两种方式运行，第一种方式是简单地将文件作为Python脚本执行。
- en: 'The API can then be launched with the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用以下命令启动API：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When run, you will initially see output similar to the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时，您将首先看到类似以下的输出：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This program exposes a REST API on `127.0.0.1:5000`, and we can make requests
    for job listings using a `GET` request to the path `/joblisting/<joblistingid>`. 
    We can try this with curl:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序在`127.0.0.1:5000`上公开了一个REST API，我们可以使用`GET`请求到路径`/joblisting/<joblistingid>`来请求工作列表。我们可以使用curl尝试一下：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result of this command will be the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令的结果将如下：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And just like that, we have a REST API up and running.  Now let's see how it
    is implemented.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这样，我们有一个正在运行的REST API。现在让我们看看它是如何实现的。
- en: How it works
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: There really isn't a lot of code, which is the beauty of Flask-RESTful.  The
    code begins with importing of `flask` and `flask_restful`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上并没有太多的代码，这就是Flask-RESTful的美妙之处。代码以导入`flask`和`flask_restful`开始。
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'These are followed with code to set up the initial configuration of Flask-RESTful:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是用于设置Flask-RESTful的初始配置的代码：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next comes a definition of a class which represents the implementation of our
    API:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一个代表我们API实现的类的定义：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: What we will have Flask-RESTful do is map HTTP requests to methods in this class. 
    Specifically, by convention `GET` requests will be mapped to member functions
    named `get`.  There will be a mapping of the values passed as part of the URL
    to the `jobListingId` parameter of the function.  This function then returns a
    Python dictionary, which Flask-RESTful converts to JSON for us.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Flask-RESTful将映射HTTP请求到这个类的方法。具体来说，按照惯例，`GET`请求将映射到名为`get`的成员函数。将URL的值映射到函数的`jobListingId`参数。然后，该函数返回一个Python字典，Flask-RESTful将其转换为JSON。
- en: 'The next line of code tells Flask-RESTful how to map portions of the URL to
    our class:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行代码告诉Flask-RESTful如何将URL的部分映射到我们的类：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This defines that URLs with paths beginning with `/joblisting` will be mapped
    to our `JobListing` class, and that the next portion of the URL represents a string
    to be passed to the `jobListingId` parameter of the `get` method.  The GET HTTP
    verb is implied as no other verb has been defined in this mapping.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了以`/joblisting`开头的路径的URL将映射到我们的`JobListing`类，并且URL的下一部分表示要传递给`get`方法的`jobListingId`参数的字符串。由于在此映射中未定义其他动词，因此假定使用GET
    HTTP动词。
- en: Finally, we have code that specifies that when the file is run as a script that
    we simply execute `app.run()` (passing in this case a parameter to give us debug
    output).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一段代码，指定了当文件作为脚本运行时，我们只需执行`app.run()`（在这种情况下传递一个参数以便获得调试输出）。
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Flask-RESTful then finds our class and sets of the mappings, starts listening
    on `127.0.0.1:5000` (the default), and forwarding requests to our class and method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Flask-RESTful找到我们的类并设置映射，开始在`127.0.0.1:5000`（默认值）上监听，并将请求转发到我们的类和方法。
- en: There's more...
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The default for Flask-RESTful is to run on port `5000`.  This can be changed
    using alternate forms of `app.run()`.  We will be fine with leaving it at 5000
    for our recipes. Ultimately, you would run this service in something like a container
    and front it with a reverse proxy such as NGINX and perform a public port mapping
    to the internal service port.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Flask-RESTful的默认运行端口是`5000`。可以使用`app.run()`的替代形式来更改。对于我们的食谱，将其保留在5000上就可以了。最终，您会在类似容器的东西中运行此服务，并在前面使用诸如NGINX之类的反向代理，并执行公共端口映射到内部服务端口。
- en: Integrating the REST API with scraping code
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将REST API与抓取代码集成
- en: In this recipe, we will integrate code that we wrote for scraping and getting
    a clean job listing from StackOverflow with our API.  This will result in a reusable
    API that can be used to perform on-demand scrapes without the client needing any
    knowledge of the scraping process.  Essentially, we will have created a *scraper
    as a service*, a concept we will spend much time with in the remaining recipes
    of the book.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将把我们为从StackOverflow获取干净的工作列表编写的代码与我们的API集成。这将导致一个可重用的API，可以用来执行按需抓取，而客户端无需了解抓取过程。基本上，我们将创建一个*作为服务的抓取器*，这是我们在本书的其余食谱中将花费大量时间的概念。
- en: Getting ready
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The first part of this process is to create a module out of our preexisting
    code that was written in [Chapter 7](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml), *Text
    Wrangling and Analysis* so that we can reuse it.  We will reuse this code in several
    recipes throughout the remainder of the book.  Let's briefly examine the structure
    and contents of this module before going and integrating it with the API.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的第一部分是将我们在[第7章](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml)中编写的现有代码创建为一个模块，以便我们可以重用它。我们将在本书的其余部分中的几个食谱中重用这段代码。在将其与API集成之前，让我们简要地检查一下这个模块的结构和内容。
- en: The code for the module is in the `sojobs` (for StackOverflow Jobs) module in
    the project's modules folder.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块的代码位于项目的模块文件夹中的`sojobs`（用于StackOverflow职位）模块中。
- en: '![](assets/c192716d-ac40-476d-936a-36091d78b2fb.png)The sojobs folder'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/c192716d-ac40-476d-936a-36091d78b2fb.png)sojobs文件夹'
- en: 'For the most part, these files are copied from those used in [Chapter 7](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml), *Text
    Wrangling and Analysis*.  The primary file for reuse is `scraping.py`, which contains
    several functions that facilitate scraping.  The function that we will use in
    this recipe is `get_job_listing_info`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，这些文件是从[第7章](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml)中使用的文件复制而来，即*文本整理和分析*。可重用的主要文件是`scraping.py`，其中包含几个函数，用于方便抓取。在这个食谱中，我们将使用的函数是`get_job_listing_info`：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Heading back to the code in [Chapter 7](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml), *Text
    Wrangling and Analysis*, you can see that this code is reused code that we created
    in those recipes. A difference is that instead of reading a single local `.html`
    file, this function is passed the identifier for a job listing, then constructs
    the URL for that job listing, reads the content with requests, performs several
    analyses, and then returns the results.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[第7章](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml)中的代码，您可以看到这段代码是我们在那些食谱中创建的重用代码。不同之处在于，这个函数不是读取单个本地的`.html`文件，而是传递了一个工作列表的标识符，然后构造了该工作列表的URL，使用requests读取内容，执行了几项分析，然后返回结果。
- en: Note that the function returns a Python dictionary that consists of the requested
    job ID, the original HTML, the text of the listing, and the list of cleaned words.
    This API is returning to the caller an aggregate of these results, with the `ID`
    so it is easy to know the requested job, as well as all of the other results that
    we did to perform various clean ups.  Hence, we have created a value-added service
    for job listings instead of just getting the raw HTML.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该函数返回一个Python字典，其中包含请求的工作ID、原始HTML、列表的文本和清理后的单词列表。该API将这些结果聚合返回给调用者，其中包括`ID`，因此很容易知道请求的工作，以及我们执行各种清理的所有其他结果。因此，我们已经创建了一个增值服务，用于工作列表，而不仅仅是获取原始HTML。
- en: Make sure that you either have your PYTHONPATH environment variable pointing
    to the modules directory, or that you have set up your Python IDE to find modules
    in this directory.  Otherwise, you will get errors that this module cannot be
    found.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的PYTHONPATH环境变量指向模块目录，或者你已经设置好你的Python IDE以在这个目录中找到模块。否则，你将会得到找不到这个模块的错误。
- en: How to do it
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'We proceed with the recipe as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下步骤进行食谱：
- en: 'The code of the API for this recipe is in `09/02/api.py`.  This extends the
    code in the previous recipe to make a call to this function in the `sojobs` module. 
    The code for the service is the following:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个食谱的API代码在`09/02/api.py`中。这扩展了上一个食谱中的代码，以调用`sojobs`模块中的这个函数。服务的代码如下：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that the main difference is the import of the function from the module,
    and the call to the function and return of the data from the result.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，主要的区别是从模块导入函数，并调用函数并从结果返回数据。
- en: The service is run by executing the script with Python `api.py`.  We can then
    test the API using `curl`.  The following requests the SpaceX job listing we have
    previously examined.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行带有Python `api.py`的脚本来运行服务。然后我们可以使用`curl`测试API。以下请求我们之前检查过的SpaceX工作列表。
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in quite a bit of output.  The following is some of the beginning
    of the response:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这导致了相当多的输出。以下是部分响应的开头：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Adding an API to find the skills for a job listing
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加一个API来查找工作列表的技能
- en: In this recipe, we add an additional operation to our API which will allow us
    to request the skills associated with a job listing.  This demonstrates a means
    of being able to retrieve only a subset of the data instead of the entire content
    of the listing.  While we will only do this for the skills, the concept can be
    easily extended to any other subsets of the data, such as the location of the
    job, title, or almost any other content that makes sense for the user of your
    API.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们向我们的API添加了一个额外的操作，允许我们请求与工作列表相关的技能。这演示了一种能够检索数据的子集而不是整个列表内容的方法。虽然我们只对技能做了这个操作，但这个概念可以很容易地扩展到任何其他数据的子集，比如工作的位置、标题，或者几乎任何对API用户有意义的其他内容。
- en: Getting ready
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The first thing that we will do is add a scraping function to the `sojobs`
    module.  This function will be named `get_job_listing_skills`.  The following
    is the code for this function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是向`sojobs`模块添加一个爬取函数。这个函数将被命名为`get_job_listing_skills`。以下是这个函数的代码：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This function retrieves the job listing, extracts the JSON provided by StackOverflow,
    and then only returns the `skills` property of the JSON.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数检索工作列表，提取StackOverflow提供的JSON，然后只返回JSON的`skills`属性。
- en: Now, let's see how to add a method to the REST API to call it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何添加一个方法来调用REST API。
- en: How to do it
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'We proceed with the recipe as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下步骤进行食谱：
- en: 'The code of the API for this recipe is in `09/03/api.py`. This script adds
    an additional class, `JobListingSkills`, with the following implementation:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个食谱的API代码在`09/03/api.py`中。这个脚本添加了一个额外的类`JobListingSkills`，具体实现如下：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This implementation is similar to that of the previous recipe, except that it
    calls the new function for getting skills.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现与上一个食谱类似，只是调用了获取技能的新函数。
- en: We still need to add a statement to inform Flask-RESTful how to map URLs to
    this classes' `get` method.  Since we are actually looking at retrieving a sub-property
    of the larger job listing, we will extend our URL scheme to include an additional
    segment representing the sub-property of the overall job listing resource.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们仍然需要添加一个语句来告诉Flask-RESTful如何将URL映射到这个类的`get`方法。因为我们实际上是在检索整个工作列表的子属性，我们将扩展我们的URL方案，包括一个额外的段代表整体工作列表资源的子属性。
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we can retrieve just the skills using the following curl:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下curl仅检索技能：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Which gives us the following result:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了以下结果：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Storing data in Elasticsearch as the result of a scraping request
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据存储在Elasticsearch中作为爬取请求的结果
- en: In this recipe, we extend our API to save the data we received from the scraper
    into Elasticsearch.  We will use this later (in the next recipe) to be able to
    optimize requests by using the content in Elasticsearch as a cache so that we
    do not repeat the scraping process for jobs listings already scraped.  Therefore,
    we can play nice with StackOverflows servers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们扩展了我们的API，将我们从爬虫那里收到的数据保存到Elasticsearch中。我们稍后会使用这个（在下一个食谱中）来通过使用Elasticsearch中的内容来优化请求，以便我们不会重复爬取已经爬取过的工作列表。因此，我们可以与StackOverflow的服务器友好相处。
- en: Getting ready
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Make sure you have Elasticsearch running locally, as the code will access Elasticsearch
    at `localhost:9200`.  There a good quick-start available at  [https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html),
    or you can check out the docker Elasticsearch recipe in [Chapter 10](ae07f922-9f75-4b94-9902-e5a5ebeec167.xhtml), *Creating
    Scraper Microservices with Docker* if you'd like to run it in Docker.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的Elasticsearch在本地运行，因为代码将访问`localhost:9200`上的Elasticsearch。有一个很好的快速入门可用于
    [https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html)，或者你可以在
    [第10章](ae07f922-9f75-4b94-9902-e5a5ebeec167.xhtml) 中查看Docker Elasticsearch食谱，*使用Docker创建爬虫微服务*，如果你想在Docker中运行它。
- en: 'Once installed, you can check proper installation with the following `curl`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，你可以使用以下`curl`检查正确的安装：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If installed properly, you will get output similar to the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果安装正确，你将得到类似以下的输出：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You will also need to have elasticsearch-py installed.  This is available at [https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html),
    but can be quickly installed using `pip install elasticsearch`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要安装elasticsearch-py。它可以在[https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html)找到，但可以使用`pip
    install elasticsearch`快速安装。
- en: How to do it
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到的
- en: We will make a few small changes to our API code.  The code from the previous
    recipe has been copied into `09/04/api.py`, with the few modifications made.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对我们的API代码进行一些小的更改。之前的代码已经复制到`09/04/api.py`中，并进行了一些修改。
- en: 'First, we add an import for elasticsearch-py:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们为elasticsearch-py添加了一个导入：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we make a quick modification to the `get` method of the `JobListing` class
    (I''ve done the same in JobListingSkills, but it''s omitted here for brevity):'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们对`JobListing`类的`get`方法进行了快速修改（我在JobListingSkills中也做了同样的修改，但出于简洁起见，这里省略了）：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The two new lines create an `Elasticsearch` object, and then insert the resulting
    document into ElasticSearch.  Before the first time of calling the API, we can
    see that there is no content, nor a `''joblistings''` index, by using the following
    curl:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这两行新代码创建了一个`Elasticsearch`对象，然后将结果文档插入到ElasticSearch中。在第一次调用API之前，我们可以通过以下curl看到没有内容，也没有`'joblistings'`索引：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Given we just installed Elasticsearch, this will result in the following error.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑到我们刚刚安装了Elasticsearch，这将导致以下错误。
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now start up the API by using `python api.py`.  Then issue the `curl` to get
    the job listing (`curl localhost:5000/joblisting/122517`).  This will result in
    output similar to the previous recipes. The difference now is that this document
    will be stored in Elasticsearch.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在通过`python api.py`启动API。然后发出`curl`以获取作业列表（`curl localhost:5000/joblisting/122517`）。这将导致类似于之前的配方的输出。现在的区别是这个文档将存储在Elasticsearch中。
- en: 'Now reissue the previous curl for the index:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在重新发出先前的curl以获取索引：
- en: '[PRE24]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And now you will get the following result (only the first few lines shown):'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你会得到以下结果（只显示前几行）：
- en: '[PRE25]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: There has been an index created, named `joblistings`, and this result demonstrates
    the index structure that Elasticsearch has identified by examining the document.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 已经创建了一个名为`joblistings`的索引，这个结果展示了Elasticsearch通过检查文档识别出的索引结构。
- en: While Elasticsearch is schema-less, it does examine the documents submitted
    and build indexes based upon what it finds.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Elasticsearch是无模式的，但它会检查提交的文档并根据所找到的内容构建索引。
- en: 'The specific document that we just stored can be retrieved by using the following
    curl:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们刚刚存储的特定文档可以通过以下curl检索：
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Which will give us the following result (again, just the beginning of the content
    shown):'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将给我们以下结果（同样，只显示内容的开头）：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: And just like that, with two lines of code, we have the document stored in our
    Elasticsearch database.  Now let's briefly examine how this worked.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这样，只用两行代码，我们就将文档存储在了Elasticsearch数据库中。现在让我们简要地看一下这是如何工作的。
- en: How it works
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'The storing of the document was performed using the following line:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下行执行了文档的存储：
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Let's examine what each of these parameters does relative to storing this document.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查每个参数相对于存储这个文档的作用。
- en: The `index` parameter specifies which Elasticsearch index we want to store the
    document within. That is named `joblistings`.  This also becomes the first portion
    of the URL used to retrieve the documents.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`index`参数指定我们要将文档存储在其中的Elasticsearch索引。它的名称是`joblistings`。这也成为用于检索文档的URL的第一部分。'
- en: Each Elasticsearch index can also have multiple document 'types', which are
    logical collections of documents that can represent different types of documents
    within the index.  We used `'job-listing'`, and that value also forms the second
    part of our URL for retrieving a specific document.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Elasticsearch索引也可以有多个文档“类型”，这些类型是逻辑上的文档集合，可以表示索引内不同类型的文档。我们使用了`'job-listing'`，这个值也构成了用于检索特定文档的URL的第二部分。
- en: Elasticsearch does not require that an indentifier be specified for each document,
    but if we provide one we can look up specific documents without having to do a
    search.  We will use the job listing ID for the document ID.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch不要求为每个文档指定标识符，但如果我们提供一个，我们可以查找特定的文档而不必进行搜索。我们将使用文档ID作为作业列表ID。
- en: The final parameter, body, specifies the actual content of the document.  This
    code simply passed the result received from the scraper.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个参数`body`指定文档的实际内容。这段代码只是传递了从爬虫接收到的结果。
- en: There's more...
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Let's look briefly at a few more facets of what Elasticsearch has done for us
    by looking at the results of this document retrieval.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地看一下Elasticsearch通过查看文档检索的结果为我们做了什么。
- en: 'First, we can see the index, document type, and ID in the first few lines of
    the result:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以在结果的前几行看到索引、文档类型和ID：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This makes a retrieval of a document very efficient when using these three values
    as we did in this query.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用这三个值进行查询时，文档的检索非常高效。
- en: There is also a version stored for each document, which in this case is 1.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文档也存储了一个版本，这种情况下是1。
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If we do this same query with the code remaining as it is, then this document
    will be stored again with the same index, doc type, and ID, and hence will have
    the version incremented.  Trust me, do the curl on the API again, and you will
    see this increment to 2.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用相同的代码进行相同的查询，那么这个文档将再次存储，具有相同的索引、文档类型和ID，因此版本将增加。相信我，再次对API进行curl，你会看到这个版本增加到2。
- en: Now examine the content of the first few properties of the `"JSON"` property.
    We assigned this property of the result from the API to be the JSON of the job
    description returned by StackOverflow embedded within the HTML.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在检查`“JSON”`属性的前几个属性的内容。我们将API返回的结果的此属性分配为嵌入在HTML中的StackOverflow作业描述的JSON。
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This is some of the beauty of a web site like StackOverflow giving us structured
    data, and with using a tools like Elasticsearch as we get nicely structured data. 
    We can, and will, leverage this for great effect with very small amounts of code. 
    We can easily perform queries using Elasticsearch to identify job listing based
    upon specific skills (we'll do this in an upcoming recipe), industry, job benefits,
    and other attributes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是像StackOverflow这样的网站给我们提供结构化数据的美妙之处，使用Elasticsearch等工具，我们可以得到结构良好的数据。我们可以并且将利用这一点，只需很少量的代码就可以产生很大的效果。我们可以轻松地使用Elasticsearch执行查询，以识别基于特定技能（我们将在即将到来的示例中执行此操作）、行业、工作福利和其他属性的工作列表。
- en: 'The result of our API also returned a property ''`CleanedWords`'', which was
    the result of several of our NLP processes extracting high-value words and terms. 
    The following is an excerpt of the values that ended up in Elasticsearch:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的API的结果还返回了一个名为`CleanedWords`的属性，这是我们的几个NLP过程提取高价值词语和术语的结果。以下是最终存储在Elasticsearch中的值的摘录：
- en: '[PRE32]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: And again, we will be able to use these to perform rich queries that can help
    us find specific matches based upon these specific words.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，我们将能够使用这些来执行丰富的查询，帮助我们根据这些特定词语找到特定的匹配项。
- en: Checking Elasticsearch for a listing before scraping
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在爬取之前检查Elasticsearch中是否存在列表
- en: Now lets leverage Elasticsearch as a cache by checking to see if we already
    have stored a job listing and hence do not need to hit StackOverflow again. We
    extend the API for performing a scrape of a job listing to first search Elasticsearch,
    and if the result is found there we return that data.  Hence, we optimize the
    process by making Elasticsearch a job listings cache.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过检查是否已经存储了工作列表来利用Elasticsearch作为缓存，因此不需要再次访问StackOverflow。我们扩展API以执行对工作列表的爬取，首先搜索Elasticsearch，如果结果在那里找到，我们返回该数据。因此，我们通过将Elasticsearch作为工作列表缓存来优化这个过程。
- en: How to do it
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'We proceed with the recipe as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下步骤进行：
- en: 'The code for this recipe is within `09/05/api.py`.  The `JobListing` class
    now has the following implementation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的代码在`09/05/api.py`中。`JobListing`类现在有以下实现：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Before calling the scraper code, the API checks to see if the document already
    exists in Elasticsearch.  This is performed by the appropriately named '`exists`'
    method, which we pass the index, doc type and ID we are trying to get.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用爬虫代码之前，API会检查文档是否已经存在于Elasticsearch中。这是通过名为`exists`的方法执行的，我们将要获取的索引、文档类型和ID传递给它。
- en: If that returns true, then the document is retrieved using the `get` method
    of the Elasticsearch object,  which is also given the same parameters.  This returns
    a Python dictionary representing the Elasticsearch document, not the actual data
    that we stored.  That actual data/document is referenced by accessing the `'_source'`
    key of the dictionary.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果返回true，则使用Elasticsearch对象的`get`方法检索文档，该方法也具有相同的参数。这将返回一个表示Elasticsearch文档的Python字典，而不是我们存储的实际数据。实际的数据/文档是通过访问字典的`'_source'`键来引用的。
- en: There's more...
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The `JobListingSkills` API implementation follows a slightly different pattern.
    The following is its code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`JobListingSkills` API 实现遵循了稍微不同的模式。以下是它的代码：'
- en: '[PRE34]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This implementation only uses ElasticSearch to the extent of checking if the
    document already is in ElasticSearch.  It does not try to save a newly retrieved
    document from the scraper.  That is because the result of the `get_job_listing`
    scraper is only a list of the skills and not the entire document.  So, this implementation
    can use the cache, but it adds no new data.  This is one of the design decisions
    of having different a method for scraping which returns only a subset of the actual
    document that is scraped.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现仅在检查文档是否已经存在于ElasticSearch时使用ElasticSearch。它不会尝试保存从爬虫中新检索到的文档。这是因为`get_job_listing`爬虫的结果只是技能列表，而不是整个文档。因此，这个实现可以使用缓存，但不会添加新数据。这是设计决策之一，即对爬取方法进行不同的设计，返回的只是被爬取文档的子集。
- en: A potential solution to this is to have this API method call `get_job_listing_info`
    instead, then save the document, and finally only return the specific subset (in
    this case the skills).  Again, this is ultimately a design consideration around
    what types of methods your users of the sojobs module need.  For purposes of these
    initial recipes, it was considered better to have two different functions at that
    level to return the different sets of data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对此的一个潜在解决方案是，将这个API方法调用`get_job_listing_info`，然后保存文档，最后只返回特定的子集（在这种情况下是技能）。再次强调，这最终是围绕sojobs模块的用户需要哪些类型的方法的设计考虑。出于这些初始示例的目的，考虑到在该级别有两个不同的函数返回不同的数据集更好。
