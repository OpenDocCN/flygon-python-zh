- en: Assessment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Chapter 1, Why GPU Programming?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章，为什么要进行GPU编程？
- en: The first two `for` loops iterate over every pixel, whose outputs are invariant
    to each other; we can thus parallelize over these two `for` loops. The third `for`
    loop calculates the final value of a particular pixel, which is intrinsically
    recursive.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前两个`for`循环遍历每个像素，它们的输出彼此不变；因此我们可以在这两个`for`循环上并行化。第三个`for`循环计算特定像素的最终值，这是固有的递归。
- en: Amdahl's Law doesn't account for the time it takes to transfer memory between
    the GPU and the host.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阿姆达尔定律没有考虑在GPU和主机之间传输内存所需的时间。
- en: 512 x 512 amounts to 262,144 pixels. This means that the first GPU can only
    calculate the outputs of half of the pixels at once, while the second GPU can
    calculate all of the pixels at once; this means the second GPU will be about twice
    as fast as the first here. The third GPU has more than sufficient cores to calculate
    all pixels at once, but as we saw in problem 1, the extra cores will be of no
    use to us here. So the second and third GPUs will be equally fast for this problem.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 512 x 512等于262,144像素。这意味着第一个GPU一次只能计算一半像素的输出，而第二个GPU可以一次计算所有像素；这意味着第二个GPU在这里将比第一个GPU快大约两倍。第三个GPU有足够的核心来一次计算所有像素，但正如我们在问题1中看到的那样，额外的核心在这里对我们没有用。因此，对于这个问题，第二个和第三个GPU的速度将是相同的。
- en: One issue with generically designating a certain segment of code as parallelizable
    with regards to Amdahl's Law is that this makes the assumption that the computation
    time for this piece of code will be close to 0 if the number of processors, *N*,
    is very large. As we can see from the last problem, this is not the case.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将某个代码段通用地指定为与阿姆达尔定律相关的并行化存在一个问题，即假设这段代码的计算时间在处理器数量*N*非常大时接近0。正如我们从上一个问题中看到的，情况并非如此。
- en: First, using *time* consistently can be cumbersome, and it might not zero in
    on the bottlenecks of your program. Second, a profiler can tell you the exact
    computation time of all of your code from the perspective of Python, so you can
    tell whether some library function or background activity of your operating system
    is at fault rather than your code.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，一致使用*时间*可能很麻烦，并且可能无法找出程序的瓶颈。其次，分析器可以告诉您从Python的角度来看，所有代码的确切计算时间，因此您可以确定某些库函数或操作系统的后台活动是否有问题，而不是您的代码。
- en: Chapter 2, Setting Up Your GPU Programming Environment
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章，设置GPU编程环境
- en: No, CUDA only supports Nvidia GPUs, not Intel HD or AMD Radeon
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，CUDA只支持Nvidia GPU，不支持Intel HD或AMD Radeon
- en: This book only uses Python 2.7 examples
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本书只使用Python 2.7示例
- en: Device Manager
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设备管理器
- en: '`lspci`'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: lspci
- en: '`free`'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`free`'
- en: '`.run`'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.run`'
- en: Chapter 3, Getting Started with PyCUDA
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章，开始使用PyCUDA
- en: Yes.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的。
- en: Memory transfers between host/device, and compilation time.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主机/设备之间的内存传输和编译时间。
- en: You can, but this will vary depending on your GPU and CPU setup.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以，但这将取决于你的GPU和CPU设置。
- en: Do this using the C `?` operator for both the point-wise and reduce operations.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用C `?`运算符来执行点对点和减少操作。
- en: If a `gpuarray` object goes out of scope its destructor is called, which will
    deallocate (free) the memory it represents on the GPU automatically.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`gpuarray`对象超出范围，其析构函数将被调用，这将自动在GPU上释放它所代表的内存。
- en: '`ReductionKernel` may perform superfluous operations, which may be necessary
    depending on how the underlying GPU code is structured. A *neutral element* will
    ensure that no values are altered as a result of these superfluous operations.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ReductionKernel`可能执行多余的操作，这可能取决于底层GPU代码的结构。*中性元素*将确保没有值因这些多余的操作而改变。'
- en: We should set `neutral` to the smallest possible value of a signed 32-bit integer.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该将`neutral`设置为带符号32位整数的最小可能值。
- en: Chapter 4, Kernels, Threads, Blocks, and Grids
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章，内核，线程，块和网格
- en: Try it.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 试试看。
- en: All of the threads don't operate on the GPU simultaneously. Much like a CPU
    switching between tasks in an OS, the individual cores of the GPU switch between
    the different threads for a kernel.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并非所有线程都同时在GPU上运行。就像CPU在操作系统中在任务之间切换一样，GPU的各个核心在内核的不同线程之间切换。
- en: O( n/640 log n), that is, O(n log n).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: O(n/640 log n)，即O(n log n)。
- en: Try it.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 试试看。
- en: There is actually no internal grid-level synchronization in CUDA—only block-level
    (with `__syncthreads).` We have to synchronize anything above a single block with
    the host.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际上，CUDA中没有内部网格级同步，只有块级（使用`__syncthreads`）。我们必须将单个块以上的任何内容与主机同步。
- en: 'Naive: 129 addition operations. Work-efficient: 62 addition operations.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 天真：129次加法运算。高效工作：62次加法运算。
- en: Again, we can't use `__syncthreads` if we need to synchronize over a large grid
    of blocks. We can also launch over fewer threads on each iteration if we synchronize
    on the host, freeing up more resources for other operations.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，如果我们需要在大量块的网格上进行同步，我们不能使用`__syncthreads`。如果我们在主机上进行同步，我们还可以在每次迭代中启动更少的线程，从而为其他操作释放更多资源。
- en: In the case of a naive parallel sum, we will likely be working with only a small
    number of data points that should be equal to or less than the total number of
    GPU cores, which can likely fit in the maximum size of a block (1032); since a
    single block can be synchronized internally, we should do so. We should use the
    work-efficient algorithm only if the number of data points are far greater than
    the number of available cores on the GPU.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在天真的并行求和的情况下，我们可能只会处理一小部分数据点，这些数据点应该等于或少于GPU核心的总数，这些核心可能适合于块的最大大小（1032）；由于单个块可以在内部同步，我们应该这样做。只有当数据点的数量远远大于GPU上可用核心的数量时，我们才应该使用高效的工作算法。
- en: Chapter 5, Streams, Events, Contexts, and Concurrency
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章，流，事件，上下文和并发性
- en: The performance improves for both; as we increase the number of threads, the
    GPU reaches peak utilization in both cases, reducing the gains made through using
    streams.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两者的性能都会提高；随着线程数量的增加，GPU在两种情况下都达到了峰值利用率，减少了使用流所获得的收益。
- en: Yes, you can launch an arbitrary number of kernels asynchronously and synchronize
    them to with `cudaDeviceSynchronize`.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，你可以异步启动任意数量的内核，并使用`cudaDeviceSynchronize`进行同步。
- en: Open up your text editor and try it!
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的文本编辑器，试试看！
- en: High standard deviation would mean that the GPU is being used unevenly, overwhelming
    the GPU at some points and under-utilizing it at others. A low standard deviation
    would mean that all launched operations are running generally smoothly.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高标准差意味着GPU的使用不均匀，有时会过载GPU，有时会过度利用它。低标准差意味着所有启动的操作通常都运行顺利。
- en: i. The host can generally handle far fewer concurrent threads than a GPU. ii.
    Each thread requires its own CUDA context. The GPU can become overwhelmed with
    excessive contexts, since each has its own memory space and has to handle its
    own loaded executable code.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: i. 主机通常可以处理的并发线程远远少于GPU。ii. 每个线程都需要自己的CUDA上下文。GPU可能会因为上下文过多而不堪重负，因为每个上下文都有自己的内存空间，并且必须处理自己加载的可执行代码。
- en: Chapter 6, Debugging and Profiling Your CUDA Code
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章，调试和分析CUDA代码
- en: Memory allocations are automatically synchronized in CUDA.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CUDA中，内存分配会自动同步。
- en: The `lockstep` property only holds in single blocks of size 32 or less. Here,
    the two blocks would properly diverge without any `lockstep`.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`lockstep`属性仅在大小为32或更小的单个块中有效。在这里，两个块将在没有任何`lockstep`的情况下正确分歧。'
- en: The same thing would happen here. This 64-thread block would actually be split
    into two 32-thread warps.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里也会发生同样的事情。这个64线程块实际上会被分成两个32线程的warp。
- en: Nvprof can time individual kernel launches, GPU utilization, and stream usage;
    any host-side profiler would only see CUDA host functions being launched.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nvprof可以计时单个内核启动、GPU利用率和流使用；任何主机端的分析器只会看到CUDA主机函数的启动。
- en: Printf is generally easier to use for small-scale projects with relatively short,
    inline kernels. If you write a very involved CUDA kernel with thousands of lines,
    then probably you would want to use the IDE to step through and debug your kernel
    line by line.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Printf通常更容易用于规模较小、内联内核相对较短的项目。如果你编写了一个非常复杂的CUDA内核，有数千行代码，那么你可能会想使用IDE逐行步进和调试你的内核。
- en: This tells CUDA which GPU we want to use.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这告诉CUDA我们要使用哪个GPU。
- en: '`cudaDeviceSynchronize` will ensure that interdependent kernel launches and
    mem copies are indeed synchronized, and that they won''t launch before all necessary
    operations have finished.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cudaDeviceSynchronize`将确保相互依赖的内核启动和内存复制确实是同步的，并且它们不会在所有必要的操作完成之前启动。'
- en: Chapter 7, Using the CUDA Libraries with Scikit-CUDA
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章，使用Scikit-CUDA库
- en: SBLAH starts with an S, so this function uses 32-bit real floats. ZBLEH starts
    with a Z, which means it works with 128-bit complex floats.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SBLAH以S开头，因此该函数使用32位实数浮点数。ZBLEH以Z开头，这意味着它使用128位复数浮点数。
- en: 'Hint: set `trans = cublas._CUBLAS_OP[''T'']`'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示：设置`trans = cublas._CUBLAS_OP['T']`
- en: 'Hint: use the Scikit-CUDA wrapper to the dot product, `skcuda.cublas.cublasSdot`'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示：使用Scikit-CUDA包装器进行点积，`skcuda.cublas.cublasSdot`
- en: 'Hint: build upon the answer to the last problem.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示：建立在上一个问题的答案基础上。
- en: You can put the cuBLAS operations in a CUDA stream and use event objects with
    this stream to precisely measure the computation times on the GPU.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以将cuBLAS操作放在一个CUDA流中，并使用该流的事件对象来精确测量GPU上的计算时间。
- en: Since the input appears as being complex to cuFFT, it will calculate all of
    the values as NumPy.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于输入看起来对cuFFT来说很复杂，它将计算所有值作为NumPy。
- en: The dark edge is due to the zero-buffering around the image. This can be mitigated
    by *mirroring* the image on its edges rather than by using a zero-buffer.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 暗边是由于图像周围的零缓冲造成的。这可以通过在边缘上*镜像*图像来缓解，而不是使用零缓冲。
- en: Chapter 8, The CUDA Device Function Libraries and Thrust
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章，CUDA设备函数库和Thrust
- en: Try it. (It's actually more accurate than you'd think.)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 试试看。(它实际上比你想象的更准确。)
- en: 'One application: a Gaussian distribution can be used to add `white noise` to
    samples to augment a dataset in machine learning.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个应用：高斯分布可以用于向样本添加“白噪声”以增加机器学习中的数据集。
- en: No, since they are from different seeds, these lists may have a strong correlation
    if we concatenate them together. We should use subsequences of the same seed if
    we plan to concatenate them together.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，因为它们来自不同的种子，如果我们将它们连接在一起，这些列表可能会有很强的相关性。如果我们打算将它们连接在一起，我们应该使用相同种子的子序列。
- en: Try it.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 试试看。
- en: 'Hint: remember that matrix multiplication can be thought of as a series of
    matrix-vector multiplications, while matrix-vector multiplication can be thought
    of as a series of dot products.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示：记住矩阵乘法可以被看作是一系列矩阵-向量乘法，而矩阵-向量乘法可以被看作是一系列点积。
- en: '`Operator()` is used to define the actual function.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Operator()`用于定义实际函数。'
- en: Chapter 9, Implementation of a Deep Neural Network
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章，实现深度神经网络
- en: One problem could be that we haven't normalized our training inputs. Another
    could be that the training rate was too large.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个问题可能是我们没有对训练输入进行归一化。另一个可能是训练速率太大。
- en: With a small training rate a set of weights might converge very slowly, or not
    at all.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用较小的训练速率，一组权重可能会收敛得非常缓慢，或者根本不会收敛。
- en: A large training rate can lead to a set of weights being over-fit to particular
    batch values or this training set. Also, it can lead to numerical overflows/underflows
    as in the first problem.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大的训练速率可能导致一组权重过度拟合特定的批处理值或该训练集。此外，它可能导致数值溢出/下溢，就像第一个问题一样。
- en: Sigmoid.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sigmoid。
- en: Softmax.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Softmax。
- en: More updates.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更多更新。
- en: Chapter 10, Working with Compiled GPU Code
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章，使用已编译的GPU代码
- en: Only the EXE file will have the host functions, but both the PTX and EXE will
    contain the GPU code.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有EXE文件将包含主机函数，但PTX和EXE都将包含GPU代码。
- en: '`cuCtxDestory`.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cuCtxDestory`。'
- en: '`printf` with arbitrary input parameters. (Try looking up the `printf` prototype.)'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`printf`带有任意输入参数。(尝试查找`printf`原型。)'
- en: With a Ctypes `c_void_p` object.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Ctypes的`c_void_p`对象。
- en: This will allow us to link to the function with its original name from Ctypes.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将允许我们使用原始名称从Ctypes链接到函数。
- en: Device memory allocations and memcopies between device/host are automatically
    synchronized by CUDA.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CUDA会自动同步设备内存分配和设备/主机之间的内存复制。
- en: Chapter 11, Performance Optimization in CUDA
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章，CUDA性能优化
- en: The fact that `atomicExch` is thread-safe doesn't guarantee that all threads
    will execute this function at the same time (which is not the case since different
    blocks in a grid can be executed at different times).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`atomicExch`是线程安全的事实并不保证所有线程将同时执行此函数（因为在网格中不同的块可以在不同的时间执行）。'
- en: A block of size 100 will be executed over multiple warps, which will not be
    synchronized within the block unless we use `__syncthreads`. Thus, `atomicExch`
    may be called at multiple times.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大小为100的块将在多个warp上执行，除非我们使用`__syncthreads`，否则块内部将不会同步。因此，`atomicExch`可能会被多次调用。
- en: Since a warp executes in lockstep by default, and blocks of size 32 or less
    are executed with a single warp, `__syncthreads` would be unnecessary.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于warp默认以锁步方式执行，并且大小为32或更小的块将使用单个warp执行，因此`__syncthreads`是不必要的。
- en: We use a naïve parallel sum within the warp, but otherwise, we are doing as
    many sums with`atomicAdd` as we would do with a serial sum. While CUDA automatically
    parallelizes many of these `atomicAdd` invocations, we could reduce the total
    number of required `atomicAdd` invocations by implementing a work-efficient parallel
    sum.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在warp内使用天真的并行求和，但除此之外，我们使用`atomicAdd`进行了许多求和，就像我们使用串行求和一样。虽然CUDA自动并行化了许多这些`atomicAdd`调用，但我们可以通过实现高效的并行求和来减少所需的`atomicAdd`调用总数。
- en: Definitely `sum_ker`. It's clear that PyCUDA's sum doesn't use the same hardware
    tricks as we do since ours performs better on smaller arrays, but by scaling the
    size to be much larger, the only explanation as to why PyCUDA's version is better
    is that it performs fewer addition operations.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 肯定是`sum_ker`。很明显PyCUDA的求和没有使用与我们相同的硬件技巧，因为我们在较小的数组上表现更好，但通过扩大尺寸，PyCUDA版本更好的唯一解释是它执行的加法操作更少。
- en: Chapter 12, Where to Go from Here
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章，下一步去哪里
- en: 'Two examples: DNA analysis and physics simulations.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个例子：DNA分析和物理模拟。
- en: 'Two examples: OpenACC, Numba.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个例子：OpenACC，Numba。
- en: TPUs are only used for machine learning operations and lack the components required
    to render graphics.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TPU仅用于机器学习操作，缺少呈现图形所需的组件。
- en: Ethernet.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以太网。
