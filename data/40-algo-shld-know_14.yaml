- en: Data Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据算法
- en: 'This chapter is about data-centric algorithms and, in particular, it focuses
    on three aspects of data-centric algorithms: storage, streaming, and compression.
    This chapter starts with a brief overview of data-centric algorithms, then we
    will discuss various strategies that can be used in data storage. Next, how to
    apply algorithms to streaming data is described, then different methodologies
    to compress data are discussed. Finally, we will learn how we can use the concepts
    developed in this chapter to monitor the speeds of cars traveling on a highway
    using a state-of-the-art sensor network.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了数据中心算法的三个方面：存储、流式处理和压缩。本章首先简要概述了数据中心算法，然后讨论了可以用于数据存储的各种策略。接下来，描述了如何将算法应用于流式数据，然后讨论了压缩数据的不同方法。最后，我们将学习如何使用本章中开发的概念来使用最先进的传感器网络监测高速公路上行驶车辆的速度。
- en: By the end of this chapter, you should be able to understand the concepts and
    trade-offs involved in the design of various data-centric algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的学习，您应该能够理解设计各种数据中心算法所涉及的概念和权衡。
- en: 'This chapter discusses the  following concepts:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了以下概念：
- en: Data classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分类
- en: Data storage algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储算法
- en: How to use algorithms to compress data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用算法来压缩数据
- en: How to use algorithms to stream data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用算法来流式处理数据
- en: Let's first introduce the basic concepts.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先介绍基本概念。
- en: Introduction to data algorithms
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据算法简介
- en: Whether we realize it or not, we are living in an era of big data. Just to get
    an idea about how much data is constantly being generated, just look into some
    of the numbers published by Google for 2019\. As we know, Google Photos is the
    multimedia repository for storing photos created by Google. In 2019, an average
    of 1.2 billion photos and videos were uploaded to Google Photos every day. Also,
    an average of 400 hours of video (amounting to 1 PB of data) were uploaded every
    minute each day to YouTube. We can safely say the amount of data that is being
    generated has simply exploded.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是否意识到，我们生活在一个大数据时代。只需了解一下不断产生的数据量，看一下谷歌在2019年发布的一些数字就可以了。众所周知，Google Photos是谷歌创建的存储照片的多媒体库。2019年，平均每天有12亿张照片和视频上传到Google
    Photos。此外，每天平均有400小时的视频（相当于1 PB的数据）上传到YouTube。我们可以肯定地说，正在产生的数据量简直是爆炸式增长。
- en: The current interest in data-driven algorithms is driven by the fact that data
    contains valuable information and patterns. If used in the right way, data can
    become the basis of policy-making decisions, marketing, governance, and trend
    analysis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当前对数据驱动算法的兴趣是因为数据包含有价值的信息和模式。如果以正确的方式使用，数据可以成为政策制定、营销、治理和趋势分析的基础。
- en: For obvious reasons, algorithms that deal with data are becoming more and more
    important. Designing algorithms that can process data is an active area of research.  There
    is no doubt that exploring the best ways to use data to provide some quantifiable
    benefit is the focus of various organizations, businesses, and governments all
    over the world. But data in its raw form is seldom useful. To mine the information
    from the raw data, it needs to be processed, prepared, and analyzed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于处理数据的算法变得越来越重要，有明显的原因。设计能够处理数据的算法是一个活跃的研究领域。毫无疑问，探索最佳利用数据以提供一些可量化的好处是全世界各种组织、企业和政府的关注焦点。但原始形式的数据很少有用。要从原始数据中挖掘信息，需要对其进行处理、准备和分析。
- en: For that, we first need to store it somewhere. Efficient methodologies to store
    the data are becoming more and more important. Note that due to the physical storage
    limitations of single-node systems, big data can only be stored in distributed
    storage consisting of more than one node connected by high-speed communication
    links. So, it makes sense that, for learning data algorithms, we start by looking
    at different data storage algorithms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们首先需要将其存储在某个地方。高效存储数据的方法变得越来越重要。请注意，由于单节点系统的物理存储限制，大数据只能存储在由高速通信链路连接的多个节点组成的分布式存储中。因此，对于学习数据算法来说，首先看不同的数据存储算法是有意义的。
- en: First, let's classify data into various categories.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将数据分类为各种类别。
- en: Data classification
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分类
- en: Let's look into how we can classify data in the context of designing data algorithms.
    As discussed in [Chapter 2](04672393-683c-406b-8dd1-4dab5b5d9c4f.xhtml), *Data
    Structures Used in Algorithms*, quantifying the volume, variety, and velocity
    of the data can be used to classify it. This classification can become a basis
    to design data algorithms that can be used for its storage and processing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看在设计数据算法的背景下如何对数据进行分类。正如在[第2章](04672393-683c-406b-8dd1-4dab5b5d9c4f.xhtml)中讨论的那样，*算法中使用的数据结构*，量化数据的容量、多样性和速度可以用来对其进行分类。这种分类可以成为设计数据算法的基础，用于其存储和处理。
- en: 'Let''s look into these characteristics one by one in the context of data algorithms:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在数据算法的背景下逐个查看这些特征：
- en: '**Volume**  quantifies the amount of data that needs to be stored and processed
    in an algorithm. As the volume increases, the task becomes data-intensive and
    requires provisioning enough resources to store, cache, and process data. Big
    data is a term that vaguely defines a large volume of data that cannot be handled
    by a single node.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容量** 量化了需要在算法中存储和处理的数据量。随着容量的增加，任务变得数据密集，并需要足够的资源来存储、缓存和处理数据。大数据是一个模糊定义的术语，用来描述无法由单个节点处理的大量数据。'
- en: '**Velocity**  defines the rate at which new data is being generated. Usually,
    high-velocity data is called "hot data" or a "hot stream" and low-velocity data
    is called a "cold stream" or simply "cold data". In many applications, data will
    be a mix of hot and cold streams that will first need to be prepared and combined
    into a single table before it can be used with the algorithm.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**定义了新数据生成的速率。通常，高速数据被称为“热数据”或“热流”，低速数据被称为“冷流”或简单地称为“冷数据”。在许多应用中，数据将是热流和冷流的混合，首先需要准备并合并到一个表中，然后才能与算法一起使用。'
- en: '**Variety**  refers to different types of structured and unstructured data
    that needs to be combined into a single table before it can be used by the algorithm.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**指的是需要将不同类型的结构化和非结构化数据合并到一个表中，然后才能被算法使用。'
- en: The next section will help us to understand the trade-offs involved and will
    present various design choices when designing storage algorithms.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将帮助我们理解涉及的权衡，并在设计存储算法时提出各种设计选择。
- en: Presenting data storage algorithms
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据存储算法介绍
- en: A reliable and efficient data repository is the heart of a distributed system.
    If this data repository is created for analytics, then it is also called a data
    lake. A data repository brings together data from different domains into a single
    location. Let's start with first understanding different issues related to the
    storage of data in a distributed repository.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠和高效的数据存储库是分布式系统的核心。如果这个数据存储库是为分析而创建的，那么它也被称为数据湖。数据存储库将来自不同领域的数据汇集到一个地方。让我们首先了解分布式存储库中与数据存储相关的不同问题。
- en: Understanding data storage strategies
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解数据存储策略
- en: In the initial years of digital computing, the usual way of designing a data
    repository was by using a single node architecture. With the ever-increasing sizes
    of datasets, distributed storage of data has now become mainstream. The right
    strategy to store data in a distributed environment depends on the type of data
    and its expected usage pattern as well as its non-functional requirements. To
    further analyze the requirements of a distributed data store, let's start with
    the  **Consistency Availability Partition-Tolerance (CAP)**  theorem, which provides
    us with the basis of devising a data storage strategy for a distributed system.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字计算的最初几年，设计数据存储库的常规方式是使用单节点架构。随着数据集的不断增大，数据的分布式存储现在已经成为主流。在分布式环境中存储数据的正确策略取决于数据的类型、预期的使用模式以及其非功能性需求。为了进一步分析分布式数据存储的需求，让我们从**一致性可用性分区容忍（CAP）**定理开始，这为我们提供了制定分布系统数据存储策略的基础。
- en: Presenting the CAP theorem
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍CAP定理
- en: In 1998, Eric Brewer proposed a theorem that later became famous as the CAP
    theorem. It highlights the various trade-offs involved in designing a distributed
    storage system.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 1998年，Eric Brewer提出了一个定理，后来被称为CAP定理。它突出了设计分布式存储系统涉及的各种权衡。
- en: 'To understand the CAP theorem, first, let''s define the following three characteristics
    of distributed storage systems: consistency, availability, and partition tolerance.
    CAP is, in fact, an acronym made up of these three characteristics:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解CAP定理，首先让我们定义分布式存储系统的以下三个特性：一致性、可用性和分区容忍。CAP实际上是由这三个特性组成的首字母缩写：
- en: '**Consistency** (or simply C): The distributed storage consists of various
    nodes. Any of these nodes can be used to read, write, or update records in the
    data repository. Consistency guarantees that at a certain time, *t[1]*, independent
    of which node we use to read the data, we will get the same result. Every *read*
    operation either returns the latest data that is consistent across the distributed
    repository or gives an error message.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**（简称C）：分布式存储由各种节点组成。这些节点中的任何一个都可以用于读取、写入或更新数据存储库中的记录。一致性保证在某个时间*t[1]*，无论我们使用哪个节点来读取数据，我们都会得到相同的结果。每个*读*操作要么返回跨分布式存储库一致的最新数据，要么给出错误消息。'
- en: '**Availability** (or simply A):Availability guarantees that any node in the
    distributed storage system will be able to immediately handle the request with
    or without consistency.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**（简称A）：可用性保证分布式存储系统中的任何节点都能立即处理请求，无论是否具有一致性。'
- en: '**Partition Tolerance** (or simply P): In a distributed system, multiple nodes
    are connected via a communication network. Partition tolerance guarantees that,
    in the event of communication failure between a small subset of nodes (one or
    more), the system remains operational. Note that to guarantee partition tolerance,
    data needs to be replicated across a sufficient number of nodes.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区容忍**（简称P）：在分布式系统中，多个节点通过通信网络连接。分区容忍保证在一小部分节点（一个或多个）之间的通信失败的情况下，系统仍然可以正常运行。请注意，为了保证分区容忍，数据需要在足够数量的节点上复制。'
- en: 'Using these three characteristics, the CAP theorem carefully summarizes the
    trade-offs involved in the architecture and design of a distributed system. Specifically,
    the CAP theorem states that, in a storage system, we can only have two of the
    following characteristics: consistency or C, availability or A, and partition
    tolerance or P.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这三种特性，CAP定理仔细总结了分布系统的架构和设计中涉及的权衡。具体来说，CAP定理规定，在存储系统中，我们只能拥有以下两种特性中的两种：一致性或C，可用性或A，以及分区容忍性或P。
- en: 'This is shown in the following diagram:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下图表中显示：
- en: '![](assets/aed77c3c-5f64-4ccd-9799-36306f2bb941.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aed77c3c-5f64-4ccd-9799-36306f2bb941.png)'
- en: 'The CAP theorem also means that we can have three types of distributed storage
    systems:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CAP定理也意味着我们可以有三种类型的分布式存储系统：
- en: A CA system (implementing Consistency-Availability)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CA系统（实现一致性-可用性）
- en: An AP system (implementing Availability-Partition Tolerance)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AP系统（实现可用性-分区容忍）
- en: A CP system (implementing Consistency-Partition Tolerance)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CP系统（实现一致性-分区容忍）
- en: Let's look into them, one by one.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们依次来看看它们。
- en: CA systems
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CA系统
- en: Traditional single-node systems are CA systems. This is because if we do not
    have a distributed system, then we do not need to worry about partition tolerance.
    In that case, we can have a system that has both consistency and availability,
    that is, a CA system.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的单节点系统是CA系统。这是因为如果我们没有分布式系统，那么我们就不需要担心分区容忍性。在这种情况下，我们可以拥有既有一致性又有可用性的系统，即CA系统。
- en: Traditional single-node databases such as Oracle or MySQL are all examples of
    CA systems.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的单节点数据库，如Oracle或MySQL，都是CA系统的例子。
- en: AP systems
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AP系统
- en: AP systems are the distributed storage systems tuned for availability. Designed
    as highly-responsive systems, they can sacrifice consistency, if needed, to accommodate
    high-velocity data. This means that these are distributed storage systems that
    are designed to immediately handle requests from users. Typical user requests
    are to read or write fast-changing data. Typical AP systems are used in real-time
    monitoring systems such as sensor networks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: AP系统是为可用性调整的分布式存储系统。设计为高度响应的系统，它们可以牺牲一致性，以适应高速数据。这意味着这些是设计为立即处理用户请求的分布式存储系统。典型的用户请求是读取或写入快速变化的数据。典型的AP系统用于实时监控系统，如传感器网络。
- en: High-velocity distributed databases, such as Cassandra, are good examples of
    AP systems.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 高速分布式数据库，如Cassandra，是AP系统的良好例子。
- en: Let's look into where an AP system can be used. If Transport Canada wants to
    monitor traffic on one of the highways in Ottawa through a network of sensors
    installed at different locations on the highway, an AP system is recommended for
    implementing distributed data storage.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看AP系统可以在哪些地方使用。如果加拿大交通部想要通过在渥太华一条高速公路上安装的传感器网络监控交通情况，建议使用AP系统来实现分布式数据存储。
- en: CP systems
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CP系统
- en: CP systems have both consistency and partition tolerance. This means that these
    are the distributed storage systems that are tuned to guarantee consistency before
    a read process can fetch a value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CP系统具有一致性和分区容忍性。这意味着这些是调整为在读取过程可以获取值之前保证一致性的分布式存储系统。
- en: A typical use case for CP systems is when we want to store document files in
    JSON format. Document datastores such as MongoDB are CP systems tuned for consistency
    in a distributed environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CP系统的典型用例是当我们想要以JSON格式存储文档文件时。像MongoDB这样的文档数据存储系统是为分布式环境中的一致性而调整的CP系统。
- en: Distributed data storage is increasingly becoming the most important part of
    a modern IT infrastructure. Distributed data storage should be carefully designed
    based on the characteristics of the data and the requirements of the problem we
    want to solve. Classifying data storage into CA, AP, and CP systems help us to
    understand the various trade-offs involved when designing data storage systems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式数据存储越来越成为现代IT基础设施中最重要的部分。分布式数据存储应该根据数据的特性和我们想要解决的问题的要求进行精心设计。将数据存储分类为CA、AP和CP系统有助于我们理解在设计数据存储系统时涉及的各种权衡。
- en: Now, let's look into streaming data algorithms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看流数据算法。
- en: Presenting streaming data algorithms
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 呈现流数据算法
- en: Data can be categorized as bounded or unbounded. Bounded data is data at rest
    and is usually processed through a batch process. Streaming is basically data
    processing on unbounded data. Let's look into an example. Let's assume that we
    are analyzing fraudulent transactions at a bank. If we want to look for fraud
    transactions 7 days ago, we have to look at the data at rest; this is an example
    of a batch process.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以被分类为有界或无界。有界数据是静态数据，通常通过批处理过程处理。流式处理基本上是对无界数据进行数据处理。让我们看一个例子。假设我们正在分析银行的欺诈交易。如果我们想要查找7天前的欺诈交易，我们必须查看静态数据；这是一个批处理的例子。
- en: n the other hand, if we want to detect fraud in real-time, that is an example
    of streaming. Hence, streaming data algorithms are those algorithms that deal
    with processing data streams. The fundamental idea is to divide the input data
    stream into batches, which are then processed by the processing node. Streaming
    algorithms need to be fault-tolerant and should be able to handle the incoming
    velocity of data. As the demand for real-time trend analysis is increasing, the
    demand for stream processing is also increasing these days. Note that, for streaming
    to work, data has to be processed fast and while designing algorithms, this needs
    to be always kept in mind.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们想要实时检测欺诈，那就是流式处理的一个例子。因此，流数据算法是处理数据流的算法。其基本思想是将输入数据流分成批次，然后由处理节点处理。流算法需要具有容错能力，并且应该能够处理数据的传入速度。随着对实时趋势分析的需求增加，对流处理的需求也在这些天增加。请注意，为了使流处理工作，数据必须快速处理，而在设计算法时，这一点必须始终牢记在心。
- en: Applications of streaming
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流应用
- en: 'There are many applications of streaming data and its utilization in a meaningful
    way. Some of the applications are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据及其有意义的利用有许多应用。一些应用如下：
- en: Fraud detection
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈检测
- en: System monitoring
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统监控
- en: Smart order routing
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能订单路由
- en: Live dashboards
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时仪表板
- en: Traffic sensors along highways
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高速公路上的交通传感器
- en: Credit card transactions
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用卡交易
- en: User moves in multi-user online gaming
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户在多用户在线游戏中移动
- en: Now, let's see how we can implement streaming using Python.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用Python实现流处理。
- en: Presenting data compression algorithms
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 呈现数据压缩算法
- en: Data compression algorithms are involved in the process of reducing the size
    of data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩算法参与了减小数据大小的过程。
- en: In this chapter, we will delve into a specific data compression algorithm named
    the lossless compression algorithm.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将深入研究一种名为无损压缩算法的特定数据压缩算法。
- en: Lossless compression algorithms
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无损压缩算法
- en: 'These are algorithms that are capable of compressing data in such a way that
    it can be decompressed without any loss of information. They are used when it
    is important to retrieve the exact original files after decompression. Typical
    uses of lossless compression algorithms are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法能够以一种可以在解压缩时不丢失信息的方式压缩数据。当重要的是在解压缩后检索到确切的原始文件时，它们被使用。无损压缩算法的典型用途如下：
- en: To compress documents
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩文件
- en: To compress and package source code and  executable files
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩和打包源代码和可执行文件
- en: To convert a large number of small files into a small number of large files
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将大量小文件转换为少量大文件
- en: Understanding the basic techniques of lossless compression
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解无损压缩的基本技术
- en: Data compression is based on the principle that the majority of data is known
    to be using more bits than its entropy indicates is optimal. Recall that entropy
    is a term used to specify the information that the data carries. It means that
    a more optimal bit representation of the same information is possible. Exploring
    and formulating more efficient bit representation becomes the basis for devising
    compression algorithms. Lossless data compression takes advantage of this redundancy
    to compress data without losing any information. In the late '80s, Ziv and Lempel
    proposed dictionary-based data compression techniques that can be used to implement
    lossless data compression. These techniques were an instant hit due to their speed
    and good compression rate. These techniques were used to create the popular Unix-based
    *compress* tool. Also, the ubiquitous `gif` image format uses these compression
    techniques, which proved to be popular as they could be used to represent the
    same information in a lesser number of bits, saving space and communication bandwidth.
    These techniques later became the basis of developing the `zip` utility and its
    variants. The compression standard, V.44, used in modems is also based on it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩是基于这样一个原则，即大多数数据使用的位数比其熵所指示的最佳位数多。回想一下，熵是一个用来指定数据所携带信息的术语。这意味着可以有更优化的位表示相同信息。探索和制定更有效的位表示成为设计压缩算法的基础。无损数据压缩利用这种冗余来压缩数据而不丢失任何信息。在80年代后期，Ziv和Lempel提出了基于字典的数据压缩技术，可以用于实现无损数据压缩。由于其速度和良好的压缩率，这些技术一炮而红。这些技术被用于创建流行的基于Unix的*compress*工具。此外，普遍存在的`gif`图像格式使用了这些压缩技术，因为它们可以用较少的位数表示相同的信息，节省了空间和通信带宽。这些技术后来成为开发`zip`实用程序及其变体的基础。调制解调器中使用的压缩标准V.44也是基于它。
- en: Let's now look at the techniques one by one in the upcoming sections.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们逐一查看即将到来的部分中的技术。
- en: Huffman coding
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Huffman编码
- en: Huffman coding is one of the oldest methods of compressing data and is based
    on creating a Huffman tree, which is used to both encode and decode data. Huffman
    coding can represent data content in a more compact form by exploiting the fact
    that some data (for instance certain characters of the alphabet) appears more
    frequently in a data stream. By using encodings of different length (shorter for
    the most frequent characters and longer for the least frequent ones), the data
    consumes less space.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Huffman编码是压缩数据的最古老方法之一，它基于创建Huffman树，用于对数据进行编码和解码。Huffman编码可以通过利用某些数据（例如字母表的某些字符）在数据流中更频繁地出现这一事实，以更紧凑的形式表示数据内容。通过使用不同长度的编码（对于最常见的字符较短，对于最不常见的字符较长），数据占用的空间更少。
- en: 'Now, let''s learn a few terminologies related to Huffman coding:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习一些与Huffman编码相关的术语：
- en: '**Coding**: Coding in the context of data represents the method of representing
    data from one form to another. We would like the resultant form to be concise.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码：**在数据的上下文中，编码表示将数据从一种形式表示为另一种形式的方法。我们希望结果形式简洁。'
- en: '**Codeword**:  A particular character in encoded form is called a codeword.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**码字：**编码形式中的特定字符称为码字。'
- en: '**Fixed-length coding**:  This is when each  encoded  character, that is, codeword,
    uses the same number of bits.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定长度编码：**每个编码字符，即码字，使用相同数量的位。'
- en: '**Variable-length coding**: Codewords can use a different number of bits.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可变长度编码：**码字可以使用不同数量的位。'
- en: '**Evaluation of code**:  This is the expected number of bits per codeword.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码评估：**这是每个码字的预期位数。'
- en: '**Prefix free codes**: This means that no codeword is a prefix of any other
    codeword.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前缀自由码：**这意味着没有码字是任何其他码字的前缀。'
- en: '**Decoding**: This means that a variable-length code must be free from any
    prefix.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码：**这意味着可变长度编码必须不受任何前缀的限制。'
- en: 'For understanding the last two terms, you need to have a look at this table
    first:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解最后两个术语，您首先需要查看此表：
- en: '| **character** | **frequency** | **fixed length code** | **variable length
    code** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '**字符** | **频率** | **固定长度编码** | **可变长度编码**'
- en: '| L | .45 | 000 | 0 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| L | .45 | 000 | 0 |'
- en: '| M | .13 | 001 | 101 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| M | .13 | 001 | 101 |'
- en: '| N | .12 | 010 | 100 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| N | .12 | 010 | 100 |'
- en: '| X | .16 | 011 | 111 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| X | .16 | 011 | 111 |'
- en: '| Y | .09 | 100 | 1101 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Y | .09 | 100 | 1101 |'
- en: '| Z | .05 | 101 | 1100 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Z | .05 | 101 | 1100 |'
- en: 'Now, we can infer the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以推断以下内容：
- en: '**Fixed length code:** The fixed-length code for this table is 3.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定长度编码：**该表的固定长度编码为3。'
- en: '**Variable length code:** The variable-length code for this table is *45(1)
    + .13(3) + .12(3) + .16(3) + .09(4) + .05(4) = 2.24*.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可变长度编码：**该表的可变长度编码为*45(1) + .13(3) + .12(3) + .16(3) + .09(4) + .05(4) =
    2.24*。'
- en: 'The following diagram shows the Huffman tree created from the preceding example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了从上面的例子创建的Huffman树：
- en: '![](assets/87919c43-e9b2-4df9-8d42-adf6b96c29d2.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/87919c43-e9b2-4df9-8d42-adf6b96c29d2.png)'
- en: Note that Huffman encoding is about converting data into a Huffman tree that
    enables compression. Decoding or decompression brings the data back to the original
    format.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Huffman编码是将数据转换为Huffman树以实现压缩。解码或解压缩将数据恢复到原始格式。
- en: A practical example – Twitter real-time sentiment analysis
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个实际的例子- Twitter实时情感分析
- en: 'Twitter is said to have almost 7,000 tweets every second on a wide variety
    of topics. Let''s try to build a sentiment analyzer that can capture the emotions
    of the news from different news sources in real time. We will start by importing
    the required packages:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 据说Twitter每秒有近7000条关于各种话题的推文。让我们尝试构建一个情感分析器，可以实时捕捉来自不同新闻来源的新闻的情绪。我们将从导入所需的包开始：
- en: 'Import the needed packages:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that we are using the following two packages:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用以下两个包：
- en: '**VADER** sentiment analysis, which stands for **Valence Aware Dictionary and
    Sentiment Reasoner**. It is one of the popular rule-based sentiment analysis tools
    that is developed for social media. If you have never used it before, then you
    will first have to run the following:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**VADER**情感分析，代表**Valence Aware Dictionary and Sentiment Reasoner**。这是一种流行的基于规则的情感分析工具，专为社交媒体开发。如果您以前从未使用过它，那么您首先需要运行以下命令：'
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Tweepy`, which is a Python-based API to access Twitter. Again, if you have
    never used it before, you need to first run this:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Tweepy`，这是一个用于访问Twitter的基于Python的API。同样，如果您以前从未使用过它，您需要首先运行这个：'
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next step is a bit tricky. You need to make a request to create a developer
    account with Twitter to get access to the live stream of tweets. Once you have
    the API keys, you can represent them with the following variables:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步有点棘手。您需要向Twitter发出请求，创建一个开发者帐户，以获取对推文的实时流的访问权限。一旦您获得API密钥，您可以用以下变量表示它们：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s then configure the `Tweepy` API authentication. For that, we need to
    provide the previously created variables:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们配置`Tweepy` API的身份验证。为此，我们需要提供先前创建的变量：
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now comes the interesting part. We will choose the Twitter handles of the news
    sources that we want to monitor for sentiment analysis. For this example, we have
    chosen the following news sources:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是有趣的部分。我们将选择我们想要监控情感分析的新闻来源的Twitter句柄。在这个例子中，我们选择了以下新闻来源：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let''s create the main loop. This loop will start with an empty array
    called `array_sentiments` to hold the sentiments. Then, we will loop through all
    five news sources and collect 100 tweets each. Then, for each tweet, we will calculate
    its polarity:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建主循环。这个循环将从一个名为`array_sentiments`的空数组开始，用于保存情感。然后，我们将循环遍历所有五个新闻来源，并收集每个100条推文。然后，对于每条推文，我们将计算其极性：
- en: '![](assets/0270c5ec-21f7-4aa3-b229-4544795df1c4.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/0270c5ec-21f7-4aa3-b229-4544795df1c4.png)'
- en: 'Now, let''s create a graph that shows the polarity of the news from these individual
    news sources:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个图表，显示来自这些个别新闻来源的新闻的极性：
- en: '![](assets/85779482-ee6d-485a-b4f2-d432095bffba.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/85779482-ee6d-485a-b4f2-d432095bffba.png)'
- en: Note that each of the news sources is represented by a different color.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个新闻来源都用不同的颜色表示。
- en: 'Now, let''s look at the summary statistics:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看一下摘要统计信息：
- en: '![](assets/8f9fb7d1-bc46-4e29-8c1e-3aa6d7390809.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8f9fb7d1-bc46-4e29-8c1e-3aa6d7390809.png)'
- en: The preceding numbers summarize the trends of the sentiments. For example, the
    sentiments of BBC are found to be the most positive, and the Canadian news channel,
    CTVnews, seems to be carrying the most negative emotions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数字总结了情感的趋势。例如，BBC的情感被发现是最积极的，而加拿大新闻频道CTVnews似乎带有最消极的情绪。
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we looked at the design of data-centric algorithms. We focused
    on three aspects of data-centric algorithms: storage, compression, and streaming.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了以数据为中心的算法的设计。我们关注了数据为中心算法的三个方面：存储、压缩和流式处理。
- en: We looked into how the characteristics of data can dictate the data storage
    design. We looked into two different types of data compression algorithms. Then,
    we studied a practical example of how data streaming algorithms can be used to
    count words from a stream of textual data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了数据特征如何决定数据存储设计。我们研究了两种不同类型的数据压缩算法。然后，我们研究了一个实际示例，说明了如何使用数据流算法来计算文本数据流中的单词计数。
- en: In the next chapter, we will look at cryptographic algorithms. We will learn
    how we can use the power of these algorithms to secure  exchanged and stored messages.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究加密算法。我们将学习如何利用这些算法的力量来保护交换和存储的消息。
