- en: Searching, Mining and Visualizing Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搜索、挖掘和可视化数据
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Geocoding an IP address
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP地址地理编码
- en: Collecting IP addresses of Wikipedia edits
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集维基百科编辑的IP地址
- en: Visualizing contributor location frequency on Wikipedia
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在维基百科上可视化贡献者位置频率
- en: Creating a word cloud from a StackOverflow job listing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从StackOverflow工作列表创建词云
- en: Crawling links on Wikipedia
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在维基百科上爬取链接
- en: Visualizing page relationships on Wikipedia
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在维基百科上可视化页面关系
- en: Calculating degrees of separation between Wikipedia pages
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算维基百科页面之间的分离度
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter we will examine how to search web content, derive analytical
    results, and also visualize those results. We will learn how to locate posters
    of content an visualize the distribution of their locations.  Then we will examine
    how to scrape, model, and visualize the relationships between pages on Wikipedia.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究如何搜索Web内容，推导分析结果，并可视化这些结果。我们将学习如何定位内容的发布者并可视化其位置的分布。然后，我们将研究如何爬取、建模和可视化维基百科页面之间的关系。
- en: Geocoding an IP address
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IP地址地理编码
- en: Geocoding is the process of converting an address into geographic coordinates.
    These addresses can be actual street addresses, which can be geocoded with various
    tools such as the Google maps geocoding API ([https://developers.google.com/maps/documentation/geocoding/intro](https://developers.google.com/maps/documentation/geocoding/intro)).
    IP addresses can be, and often are, geocoded by various applications to determine
    where computers, and their users, are located. A very common and valuable use
    is analyzing web server logs to determine the source of users of your website.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 地理编码是将地址转换为地理坐标的过程。这些地址可以是实际的街道地址，可以使用各种工具进行地理编码，例如Google地图地理编码API（[https://developers.google.com/maps/documentation/geocoding/intro](https://developers.google.com/maps/documentation/geocoding/intro)）。
    IP地址可以通过各种应用程序进行地理编码，以确定计算机及其用户的位置。一个非常常见和有价值的用途是分析Web服务器日志，以确定您网站的用户来源。
- en: This is possible because an IP address does not only represent an address of
    the computer in terms of being able to communicate with that computer, but often
    can also be converted into an approximate physical location by looking it up in
    IP address / location databases. There are many of these databases available,
    all of which are maintained by various registrars (such as ICANN). There are also
    other tools that can report geographic locations for public IP addresses.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可能的，因为IP地址不仅代表计算机的地址，可以与该计算机进行通信，而且通常还可以通过在IP地址/位置数据库中查找来转换为大致的物理位置。有许多这些数据库可用，所有这些数据库都由各种注册机构（如ICANN）维护。还有其他工具可以报告公共IP地址的地理位置。
- en: There are a number of free services for IP geolocation. We will examine one
    that is quite easy to use, freegeoip.net.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多免费的IP地理位置服务。我们将研究一个非常容易使用的服务，即freegeoip.net。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Freegeoip.net is a free geocoding service. If you go to [http://www.freegeoip.net](http://www.freegeoip.net)
    in your browser, you will be presented with a page similar to the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Freegeoip.net是一个免费的地理编码服务。如果您在浏览器中转到[http://www.freegeoip.net](http://www.freegeoip.net)，您将看到一个类似以下的页面：
- en: '![](assets/9f56e0c3-c452-4ab1-b951-aa839cfdc831.png)The freegeoip.net home
    page'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/9f56e0c3-c452-4ab1-b951-aa839cfdc831.png)freegeoip.net主页'
- en: The default page reports your public IP address, and also gives you the geolocation
    of the IP address according to their database. This isn't accurate to the actual
    address of my house, and is actually quite a few miles off, but the general location
    in the world is fairly accurate. We can do important things with data that is
    at this resolution and even lower. Often just knowing the country origin for web
    requests is enough for many purposes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 默认页面报告您的公共IP地址，并根据其数据库给出IP地址的地理位置。这并不准确到我家的实际地址，实际上相差几英里，但在世界上的一般位置是相当准确的。我们可以使用这种分辨率甚至更低的数据做重要的事情。通常，只知道Web请求的国家来源对于许多目的已经足够了。
- en: Freegeoip lets you make 15000 calls per hour. Each page load counts as one call,
    and as we will see, each API call also counts as one.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Freegeoip允许您每小时进行15000次调用。每次页面加载都算一次调用，正如我们将看到的，每次API调用也算一次。
- en: How to do it
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'We could scrape this page to get this information but fortunately, freegeoip.net
    gives us a convenient REST API to use. Scrolling further down the page, we can
    see the API documentation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以爬取这个页面来获取这些信息，但幸运的是，freegeoip.net为我们提供了一个方便的REST API来使用。在页面下方滚动，我们可以看到API文档：
- en: '![](assets/89c10af1-bbcf-4f28-ba1f-5d13fb0f706f.png)The freegeoio.net API documentation'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/89c10af1-bbcf-4f28-ba1f-5d13fb0f706f.png)freegeoio.net API文档'
- en: 'We can simply use the requests library to make a GET request using the properly
    formatted URL. As an example, just entering the following URL in the browser returns
    a JSON representation of the geocoded data for the given IP address:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地使用requests库使用正确格式的URL进行GET请求。例如，只需在浏览器中输入以下URL，即可返回给定IP地址的地理编码数据的JSON表示：
- en: '![](assets/f87e77ef-910e-4011-b99f-c7942469aa52.png)Sample JSON for an IP address'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/f87e77ef-910e-4011-b99f-c7942469aa52.png)IP地址的示例JSON'
- en: 'A Python script to demonstrate this is available in `08/01_geocode_address.py`.
    The is simple and consists of the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Python脚本，用于演示这一点，可以在`08/01_geocode_address.py`中找到。这很简单，包括以下内容：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This has the following output:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这有以下输出：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that your output for this IP address may vary, and surely will with different
    IP addresses.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于这个IP地址，您的输出可能会有所不同，并且不同的IP地址肯定会有所不同。
- en: How to collect IP addresses of Wikipedia edits
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何收集维基百科编辑的IP地址
- en: Processing aggregate results of geocoded IP addresses can provide valuable insights.
    This is very common for server logs and can also be used in many other situations.
    Many websites include the IP address of contributors of content. Wikipedia provides
    a history of changes on all of their pages. Edits created by someone that is not
    a registered user of Wikipedia have their IP address published in the history.
    We will examine how to create a scraper that will navigate the history of a given
    Wikipedia topic and collect the IP addresses of unregistered edits.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 处理地理编码IP地址的聚合结果可以提供有价值的见解。这在服务器日志中非常常见，也可以在许多其他情况下使用。许多网站包括内容贡献者的IP地址。维基百科提供了他们所有页面的更改历史。由维基百科未注册用户创建的编辑在历史中公布其IP地址。我们将研究如何创建一个爬虫，以浏览给定维基百科主题的历史，并收集未注册编辑的IP地址。
- en: Getting ready
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will examine the edits made to the Web scraping page in Wikipedia. This
    page is available at:  [https://en.wikipedia.org/wiki/Web_scraping](https://en.wikipedia.org/wiki/Web_scraping).
    The following shows a small part of this page:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究对维基百科的Web抓取页面所做的编辑。此页面位于：[https://en.wikipedia.org/wiki/Web_scraping](https://en.wikipedia.org/wiki/Web_scraping)。以下是此页面的一小部分：
- en: '![](assets/533c554f-85a6-4f21-bea7-c382ae64db39.png)The view history tab'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/533c554f-85a6-4f21-bea7-c382ae64db39.png)查看历史选项卡'
- en: 'Note View history in the upper-right. Clicking on that link gives you access
    to the history for the edits:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意右上角的查看历史。单击该链接可访问编辑历史：
- en: '![](assets/11286403-7354-45f0-9786-49148c1f52dc.png)Inspecting an IP address'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/11286403-7354-45f0-9786-49148c1f52dc.png)检查IP地址'
- en: I've scrolled this down a little bit to highlight an anonymous edit.  Note that
    we can identify these anonymous edit entries using the `mw-userling mw-anonuserlink`
    class in the source.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我把这个滚动了一点，以突出一个匿名编辑。请注意，我们可以使用源中的`mw-userling mw-anonuserlink`类来识别这些匿名编辑条目。
- en: 'Notice also that you can specify the number of edits per page to be listed,
    which can be specified by adding a parameter to the URL. The following URL will
    give us the 500 most recent edits:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，您可以指定要列出的每页编辑的数量，可以通过向URL添加参数来指定。以下URL将给我们最近的500次编辑：
- en: '[https://en.wikipedia.org/w/index.php?title=Web_scraping&offset=&limit=500&action=history](https://en.wikipedia.org/w/index.php?title=Web_scraping&offset=&limit=500&action=history)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/w/index.php?title=Web_scraping&offset=&limit=500&action=history](https://en.wikipedia.org/w/index.php?title=Web_scraping&offset=&limit=500&action=history)'
- en: So instead of crawling a number of different pages, walking through them 50
    at a time, we'll just do one page of 500.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不是爬行多个不同的页面，每次走50个，而是只做一个包含500个页面。
- en: How to do it
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作方法
- en: 'We proceed with the recipe as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下步骤进行：
- en: 'The code to perform the scraping is in the script file, `08/02_geocode_wikipedia_edits.py`.
    Running the script produces the following output (truncated to the first few geo
    IPs):'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行抓取的代码在脚本文件`08/02_geocode_wikipedia_edits.py`中。运行脚本会产生以下输出（截断到前几个地理IP）：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The script also writes the geo IPs to the `geo_ips.json` file. The next recipe
    will use this file instead of making all the page requests again.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本还将地理IP写入`geo_ips.json`文件。下一个示例将使用该文件，而不是再次进行所有页面请求。
- en: How it works
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: 'The explanation is as follows. The script begins by executing the following
    code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 解释如下。脚本首先执行以下代码：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A call is made to `collect_geo_ips` which will request the page with the specified
    topic and up to 500 edits. These geo IPs are then printed to the console, and
    also written to the `geo_ips.json` file.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`collect_geo_ips`，该函数将请求指定主题的页面和最多500次编辑。然后将这些地理IP打印到控制台，并写入`geo_ips.json`文件。
- en: 'The code for `collect_geo_ips` is the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`collect_geo_ips`的代码如下：'
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This function first makes a call to `get_history_ips`, reports the quantity
    found, and then makes repeated requests to `get_geo_ips` for each IP address.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数首先调用`get_history_ips`，报告找到的数量，然后对每个IP地址重复请求`get_geo_ips`。
- en: 'The code for `get_history_ips` is the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_history_ips`的代码如下：'
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This formulates the URL for the history page, retrieves the page, and then pulls
    out all distinct IP addresses with the `mw-anonuserlink` class.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数构建了历史页面的URL，检索页面，然后提取所有具有`mw-anonuserlink`类的不同IP地址。
- en: '`get_geo_ips` then takes this set of IP addresses and calls `freegeoip.net`
    on each for the data.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`get_geo_ips`获取这组IP地址，并对每个IP地址调用`freegeoip.net`以获取数据。
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There's more...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While this data is useful, in our next recipe we will read in the data written
    to `geo_ips.json` (using pandas) and visualize the distribution of the users by
    country using a bar chart.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些数据很有用，但在下一个示例中，我们将读取写入`geo_ips.json`的数据（使用pandas），并使用条形图可视化用户按国家的分布。
- en: Visualizing contributor location frequency on Wikipedia
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在维基百科上可视化贡献者位置频率
- en: We can use the collected data to determine the frequency of edits of Wikipedia
    articles from countries around the world. This can be done by grouping the captured
    data by country and counting the number of edits related to each country. Then
    we will sort the data and create a bar chart to see the results.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用收集的数据来确定来自世界各地的维基百科文章的编辑频率。这可以通过按国家对捕获的数据进行分组并计算与每个国家相关的编辑数量来完成。然后，我们将对数据进行排序并创建一个条形图来查看结果。
- en: How to do it
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作方法
- en: This is a very simple task to perform with pandas. The code of the example is
    in `08/03_visualize_wikipedia_edits.py`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用pandas执行的非常简单的任务。示例的代码在`08/03_visualize_wikipedia_edits.py`中。
- en: 'The code begins by importing pandas and `matplotlib.pyplot`:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码开始导入pandas和`matplotlib.pyplot`：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The data file we created in the previous recipe is already in a format that
    can be read directly by pandas. This is one of the benefits of using JSON as a
    data format; pandas has built-in support for reading and writing data from JSON.
    The following reads in the data using the `pd.read_json()` function and displays
    the first five rows on the console:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在上一个示例中创建的数据文件已经以可以直接被pandas读取的格式。这是使用JSON作为数据格式的好处之一；pandas内置支持从JSON读取和写入数据。以下使用`pd.read_json()`函数读取数据并在控制台上显示前五行：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For our immediate purpose we only require the `country_code` column, which
    we can extract with the following (and shows the first five rows in that result):'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的直接目的，我们只需要`country_code`列，我们可以用以下方法提取它（并显示该结果中的前五行）：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we can group the rows in this series using `.groupby(''country_code'')`,
    and on the result, `call .count()` will return the number of items in each of
    those groups. The code also sorts the results from the largest to lowest values
    by `calling .sort_values()`:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用`.groupby('country_code')`来对这个系列中的行进行分组，然后在结果上，`调用.count()`将返回每个组中的项目数。该代码还通过`调用.sort_values()`将结果从最大到最小值进行排序：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see from just these results that the US definitely leads in edits, with
    India being the second most popular.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 仅从这些结果中，我们可以看出美国在编辑方面绝对领先，印度是第二受欢迎的。
- en: 'This data can easily be visualized as a bar graph:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据可以很容易地可视化为条形图：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in the following bar graph showing overall distribution for all
    of the countries:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下条形图显示所有国家的总体分布：
- en: '![](assets/3c71176f-4d78-4576-9b9d-63f6df781f6a.png)Histogram of the edit frequencies'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/3c71176f-4d78-4576-9b9d-63f6df781f6a.png)编辑频率的直方图'
- en: Creating a word cloud from a StackOverflow job listing
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从StackOverflow职位列表创建词云
- en: Now lets look at creating a word cloud.  Word clouds are an image that demonstrate
    the frequency of key words within a set of text.  The larger the word in the image,
    the more apparent significance it has in the body of text.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看如何创建一个词云。词云是一种展示一组文本中关键词频率的图像。图像中的单词越大，它在文本中的重要性就越明显。
- en: Getting ready
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the Word Cloud  library to create our word cloud. The source for
    the library is available at [https://github.com/amueller/word_cloud](https://github.com/amueller/word_cloud). 
    This library can be installed into your Python environment using  `pip install
    wordcloud`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Word Cloud库来创建我们的词云。该库的源代码可在[https://github.com/amueller/word_cloud](https://github.com/amueller/word_cloud)上找到。这个库可以通过`pip
    install wordcloud`安装到你的Python环境中。
- en: How to do it
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: The script to create the word cloud is in the `08/04_so_word_cloud.py` file. 
    This recipe continues on from the stack overflow recipes from chapter 7 to provide
    a visualization of the data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 创建词云的脚本在`08/04_so_word_cloud.py`文件中。这个示例是从第7章的堆栈溢出示例中继续提供数据的可视化。
- en: 'Start by importing the word cloud and the frequency distribution function from
    NLTK:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先从NLTK中导入词云和频率分布函数：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The word cloud is then generated from the probability distribution of the words
    we collected from the job listing:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，词云是从我们从职位列表中收集的单词的概率分布生成的：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we just need to display the word cloud:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需要显示词云：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And the resulting word cloud is the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的词云如下：
- en: '![](assets/02a14763-b72f-4386-ae29-ebed2d7d7219.png)The word cloud for the
    job listing'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/02a14763-b72f-4386-ae29-ebed2d7d7219.png)职位列表的词云'
- en: The positioning and size have some built-in randomness, so the result you get
    may differ.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 位置和大小都有一些内置的随机性，所以你得到的结果可能会有所不同。
- en: Crawling links on Wikipedia
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在维基百科上爬取链接
- en: In this recipe we will write a small program to utilize the crawl the links
    on a Wikipedia page through several levels of depth. During this crawl we will
    gather the relationships between the pages and those referenced from each page.
    During this we will build a relationship amongst these pages the we will ultimately
    visualize in the next recipe.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将编写一个小程序来利用爬取维基百科页面上的链接，通过几个深度级别。在这个爬取过程中，我们将收集页面之间以及每个页面引用的页面之间的关系。在此过程中，我们将建立这些页面之间的关系，最终在下一个示例中进行可视化。
- en: Getting ready
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The code for this example is in the `08/05_wikipedia_scrapy.py`. It references
    code in a module in the `modules`/`wikipedia` folder of the code samples, so make
    sure that is in your Python path.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的代码在`08/05_wikipedia_scrapy.py`中。它引用了代码示例中`modules`/`wikipedia`文件夹中的一个模块的代码，所以确保它在你的Python路径中。
- en: How to do it
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: You can the sample Python script.  It will crawl a single Wikipedia page using
    Scrapy.  The page it will crawl is the Python page at [https://en.wikipedia.org/wiki/Python_(programming_language)](https://en.wikipedia.org/wiki/Python_(programming_language)),
    and collect relevant links on that page.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用示例Python脚本。它将使用Scrapy爬取单个维基百科页面。它将爬取的页面是Python页面，网址为[https://en.wikipedia.org/wiki/Python_(programming_language)](https://en.wikipedia.org/wiki/Python_(programming_language))，并收集该页面上的相关链接。
- en: 'When run you will see the similar output to the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时，你将看到类似以下的输出：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first part of output is from the Scrapy crawler and shows the pages that
    are passed to the parse method.  These are pages that start with our initial page
    and through the first five most common links from that page.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的第一部分来自Scrapy爬虫，并显示传递给解析方法的页面。这些页面以我们的初始页面开头，并通过该页面的前五个最常见的链接。
- en: The second part of this output is a representation of the page that is crawled
    and the links found on that page that are considered for future processing.  The
    first number is the level of the crawl the crawl that the relationship was found,
    followed by the parent page and the link found on that page.  For every page /
    link found, there is a separate entry.  Since this is a one depth crawl, we just
    show pages found from the initial page.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出的第二部分是对被爬取的页面以及在该页面上找到的链接的表示，这些链接被认为是未来处理的。第一个数字是找到关系的爬取级别，然后是父页面和在该页面上找到的链接。对于每个找到的页面/链接，都有一个单独的条目。由于这是一个深度爬取，我们只显示从初始页面找到的页面。
- en: How it works
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'Lets start with the code in them main script file, `08/05_wikipedia_scrapy.py`. 
    This starts with creating a `WikipediaSpider` object and running the crawl:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主脚本文件`08/05_wikipedia_scrapy.py`中的代码开始。这是通过创建一个`WikipediaSpider`对象并运行爬取开始的：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This tells Scrapy that we want to run it for one level of depth, and we get
    an instance of the crawler as we want to inspect its properties which are the
    result of the crawl.  The results are then printed with the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉Scrapy我们希望运行一层深度，我们得到一个爬虫的实例，因为我们想要检查其属性，这些属性是爬取的结果。然后用以下方法打印结果：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Each result from the crawler is stored in the `linked_pages` property.  Each
    of those objects is represented by several properties including the title of the
    page (the last portion of the wikipedia URL) and the title of each page found
    within the content of the HTML of that page.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫的每个结果都存储在`linked_pages`属性中。每个对象都由几个属性表示，包括页面的标题（维基百科URL的最后部分）和在该页面的HTML内容中找到的每个页面的标题。
- en: 'Now let''s walk through how the crawler functions.  The code for the spider
    is in `modules/wikipedia/spiders.py`. The crawler starts off by defining a sub-class
    of a Scrapy `Spider`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下爬虫的功能。爬虫的代码在`modules/wikipedia/spiders.py`中。爬虫首先定义了一个Scrapy `Spider`的子类：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We are starting on the Python page in Wikipedia.  Next are the definition of
    a few class level variable to define how the crawl operates and for the results
    to be retrieved:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从维基百科的Python页面开始。接下来是定义一些类级变量，以定义爬取的操作方式和要检索的结果：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Each page of this crawl will the processed by the parse method of the spider. 
    Let''s walk through it.  It starts with the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这次爬取的每个页面都将由爬虫的解析方法处理。让我们来看一下。它从以下开始：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In each Wikipedia page we look for links what start with `/wiki`.  There are
    other links in the page but these are the ones this this crawl will consider important.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个维基百科页面中，我们寻找以“/wiki”开头的链接。页面中还有其他链接，但这些是这次爬取将考虑的重要链接。
- en: This crawler implements an algorithm where all found links on the page are counted
    for similarity.  There are quite a few repeat links.  Some of these are spurious.
    Others represent a true importance of linking multiple times to other pages.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个爬虫实现了一个算法，其中页面上找到的所有链接都被计算为相似。有相当多的重复链接。其中一些是虚假的。其他代表了多次链接到其他页面的真正重要性。
- en: The `max_items_per_page` defines how many links on the current page we will
    further investigate.  There will be quite a few links on each page, and this algorithm
    counts all the similar links and puts them into buckets.  It then follows the
    `max_items_per_page`  most popular links.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_items_per_page`定义了我们将进一步调查当前页面上有多少链接。每个页面上都会有相当多的链接，这个算法会计算所有相似的链接并将它们放入桶中。然后它会跟踪`max_items_per_page`最受欢迎的链接。'
- en: 'This process is managed though the use of the `links_counter` variable.  This
    is a dictionary of mappings between the current page and all links found on the
    page. For each link we decide to follow We count the number of times it is referenced
    on the page.  This variable is a map between that URL and and instance of the
    following object that counts the number of references:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程是通过使用`links_counter`变量来管理的。这是当前页面和页面上找到的所有链接之间的映射字典。对于我们决定跟踪的每个链接，我们计算它在页面上被引用的次数。这个变量是该URL和计数引用次数的对象之间的映射：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The code then walks through all the identified links:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代码遍历所有识别的链接：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This examines every link and only considers them for further crawling based
    upon the stated rules (no ':' in the link, nor 'International' as it is quite
    popular so we exclude it, and finally we don't include the start URL).  If the
    link passes this, then a new `LinkReferenceCounter` object is created (if this
    link as not been seen before), or its reference count is incremented.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法检查每个链接，并根据规则（链接中没有“：”，也没有“国际”因为它非常受欢迎所以我们排除它，最后我们不包括起始URL）只考虑它们进行进一步的爬取。如果链接通过了这一步，那么就会创建一个新的`LinkReferenceCounter`对象（如果之前没有看到这个链接），或者增加它的引用计数。
- en: 'Since there are likely repeat links on each page, we want to consider only
    the `max_items_per_page` most common links.  The code does this by the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个页面上可能有重复的链接，我们只想考虑`max_items_per_page`最常见的链接。代码通过以下方式实现了这一点：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Out of the `link_counter` dictionary we pull all of the `LinkReferenceCounter`
    objects and sort them by the count, and then select the top `max_items_per_page`
    items.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从`link_counter`字典中，我们提取所有的`LinkReferenceCounter`对象，并按计数排序，然后选择前`max_items_per_page`个项目。
- en: 'The next step is for each of these qualifying items we recored them in the
    `linked_pages` field of the class.  Each object in this list of the the type `PageToPageMap`. 
    This class has the following definition:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对这些符合条件的项目进行记录，记录在类的`linked_pages`字段中。这个列表中的每个对象都是`PageToPageMap`类型。这个类有以下定义：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Fundamentally this object represents a source page URL to a linked page URL,
    and it also tracks the current level of the crawl.  The title properties are the
    URL decoded forms of the last part of the Wikipedia URL, and represent a more
    human-friendly version of the URL.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上说，这个对象表示一个源页面URL到一个链接页面URL，并跟踪爬取的当前级别。标题属性是维基百科URL最后部分的URL解码形式，代表了URL的更加人性化的版本。
- en: Finally, the code yields to Scrapy new pages to crawl to.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，代码将新的页面交给Scrapy进行爬取。
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Theres more...
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: This crawler / algorithm also keeps track of the current level of **depth** in
    the crawl.  If a new link is considered to be beyond the maximum depth of the
    crawl. While this can be controlled to a point by Scrapy, this code still needs
    to not include links beyond the maximum depth.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个爬虫/算法还跟踪爬取中当前的**深度**级别。如果认为新链接超出了爬取的最大深度。虽然Scrapy可以在一定程度上控制这一点，但这段代码仍然需要排除超出最大深度的链接。
- en: 'This is controlled by using the depth field of the `PageToPageMap` object. 
    For each page of the crawl, we check to see if the response has meta-data, a property
    which represents the `"parent"` PageToPageMap object for an given page.  We find
    this with the following code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过使用`PageToPageMap`对象的深度字段来控制的。对于每个爬取的页面，我们检查响应是否具有元数据，这是表示给定页面的“父”`PageToPageMap`对象的属性。我们可以通过以下代码找到这个：
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This code in the page parser looks to see if there is a parent object.  Only
    the first page of the crawl does not have a parent page. If there is an instance,
    the depth of this crawl is considered one higher.  When the new `PageToPageMap`
    object is created, this value is passed to it and stored.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 页面解析器中的此代码查看是否有父对象。只有爬取的第一个页面没有父页面。如果有一个实例，这个爬取的深度被认为是更高的。当创建新的`PageToPageMap`对象时，这个值被传递并存储。
- en: 'The code passes this object to the next level of the crawl by using the meta
    property of the request object:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 代码通过使用请求对象的meta属性将此对象传递到爬取的下一级别：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this way we can pass data from one level of a crawl in a Scrapy spider to
    the next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以将数据从Scrapy蜘蛛的一个爬取级别传递到下一个级别。
- en: Visualizing page relationships on Wikipedia
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在维基百科上可视化页面关系
- en: In this recipe we take the data we collected in the previous recipe and create
    a force-directed network visualization of the page relationships using the NetworkX
    Python library.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用之前收集的数据，并使用NetworkX Python库创建一个力导向网络可视化页面关系。
- en: Getting ready
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: NetworkX is software for modeling, visualizing, and analyzing complex network
    relationships. You can find more information about it at: [https://networkx.github.io](https://networkx.github.io/).
    It can be installed in your Python environment using `pip install networkx`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkX是用于建模、可视化和分析复杂网络关系的软件。您可以在[https://networkx.github.io](https://networkx.github.io/)找到更多关于它的信息。它可以通过`pip
    install networkx`在您的Python环境中安装。
- en: How to do it
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'The script for this example is in the `08/06_visualizze_wikipedia_links.py`
    file. When run it produces a graph of the links found on the initial Python page
    in Wikipedia:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的脚本位于`08/06_visualizze_wikipedia_links.py`文件中。运行时，它会生成维基百科上初始Python页面上找到的链接的图表：
- en: '![](assets/54f24aea-344a-42b8-b02b-a68c1a8287c4.png)Graph of the links'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/54f24aea-344a-42b8-b02b-a68c1a8287c4.png)链接的图表'
- en: Now we can see the relationships between the pages!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到页面之间的关系了！
- en: How it works
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: 'The crawl starts with defining a one level of depth crawl:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 爬取从定义一级深度爬取开始：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This information is similar to the previous recipe, and new we need to convert
    it into a model that NetworkX can use for a graph. This starts with creating a
    NetworkX graph model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息与之前的示例类似，现在我们需要将其转换为NetworkX可以用于图的模型。这始于创建一个NetworkX图模型：
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A NetworkX graph consists of nodes and edges.  From the data collected we must
    crate a set of unique nodes (the pages) and the edges (the fact that a page references
    another page).  This performed with the following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkX图由节点和边组成。从收集的数据中，我们必须创建一组唯一的节点（页面）和边（页面引用另一个页面的事实）。可以通过以下方式执行：
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This iterates through all the results from out crawl and identifies all the
    unique nodes (the distinct pages), and then all of the links between any pages
    and other pages.  For each node and edge, we register those with NetworkX.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过遍历我们爬取的所有结果，并识别所有唯一节点（不同的页面），以及页面之间的所有链接。对于每个节点和边，我们使用NetworkX进行注册。
- en: 'Next we create the plot with Matplotlib and tell NetworkX how to create the
    visuals in the plot:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用Matplotlib创建绘图，并告诉NetworkX如何在绘图中创建可视化效果：
- en: '[PRE31]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The important parts of this are first the use of NetworkX to form a spring layout
    on the nodes.  That calculates the actual positions of the nodes but does not
    render them or the edges. That is the purpose of the next two lines which give
    NetworkX the instructions on how to render both the nodes and edges. and finally,
    we need to put labels on the nodes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 其中重要的部分首先是使用NetworkX在节点上形成弹簧布局。这计算出节点的实际位置，但不渲染节点或边。这是接下来的两行的目的，它们给出了NetworkX如何渲染节点和边的指令。最后，我们需要在节点上放置标签。
- en: There's more...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This crawl only did a single depth crawl.  The crawl can be increased with
    the following change to the code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这次爬取只进行了一级深度的爬取。可以通过对代码进行以下更改来增加爬取的深度：
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Fundamentally the only change is to increase the depth one level. This then
    results in the following graph (there is randomization in any spring graph so
    the actual results have a different layout):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上唯一的变化是增加一级深度。然后得到以下图表（任何弹簧图都会有随机性，因此实际结果会有不同的布局）：
- en: '![](assets/6ddc2576-434d-411d-9d3f-15e9482e48f6.png)Spider graph of the links'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/6ddc2576-434d-411d-9d3f-15e9482e48f6.png)链接的蜘蛛图'
- en: This begins to be interesting as we now start to see inter-relationships and
    cyclic relationships between pages.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这开始变得有趣，因为我们现在开始看到页面之间的相互关系和循环关系。
- en: I dare you to further increase the depth and number of links per page.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我敢你进一步增加深度和每页的链接数。
- en: Calculating degrees of separation
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算分离度
- en: Now let's calculate the degrees of separation between any two pages.  This answers
    the question of how many pages we need to go through from a source page to find
    another page.  This could be a non-trivial graph traversal problem as there can
    be multiple paths between the two pages.  Fortunately for us, NetworkX, using
    the exact same graph model, has built in function to solve this with the exact
    same model from the previous recipe.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算任意两个页面之间的分离度。这回答了从源页面到另一个页面需要浏览多少页面的问题。这可能是一个非平凡的图遍历问题，因为两个页面之间可能有多条路径。幸运的是，对于我们来说，NetworkX使用完全相同的图模型，具有内置函数来解决这个问题。
- en: How to do it
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'The script for this example is in the `08/07_degrees_of_separation.py`. The
    code is identical to the previous recipe, with a 2-depth crawl, except that it
    omits the graph and asks NetworkX to solve the degrees of separation between `Python_(programming_language)`
    and `Dennis_Ritchie`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的脚本在`08/07_degrees_of_separation.py`中。代码与之前的示例相同，进行了2层深度的爬取，只是省略了图表，并要求NetworkX解决`Python_(programming_language)`和`Dennis_Ritchie`之间的分离度：
- en: '[PRE33]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This tells us that to go from `Python_(programming_language)` to `Dennis_Ritchie`
    we have to go through one other page: `C_(programming_language)`.  Hence, one
    degree of separation. If we went directly to `C_(programming_language), ` it would
    be 0 degrees of separation.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，要从`Python_(programming_language)`到`Dennis_Ritchie`，我们必须通过另一个页面：`C_(programming_language)`。因此，一度分离。如果我们直接到`C_(programming_language)`，那么就是0度分离。
- en: How it works
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'The solution of this problem is solved by an algorithm known as **A***. The
    **A*** algorithm determines the shortest path between two nodes in a graph.  Note
    that this path can be multiple paths of different lengths and that the correct
    result is the shortest path.  A good thing for us is that NetworkX has a built
    in function to do this for us.  It is done with one simple statement:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的解决方案是由一种称为**A***的算法解决的。**A***算法确定图中两个节点之间的最短路径。请注意，这条路径可以是不同长度的多条路径，正确的结果是最短路径。对我们来说好消息是，NetworkX有一个内置函数来为我们做这个。它可以用一条简单的语句完成：
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'From this we report on the actual path:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们报告实际路径：
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: There's more...
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: For more information on the **A*** algorithm check out this page at[ ](https://en.wikipedia.org/wiki/A*_search_algorithm)[https://en.wikipedia.org/A*_search_algorithm](https://en.wikipedia.org/A*_search_algorithm).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有关**A***算法的更多信息，请查看[此页面](https://en.wikipedia.org/wiki/A*_search_algorithm)。
