- en: Exploring Windows Forensic Artifacts Recipes - Part I
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Windows取证工件配方-第一部分
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下配方：
- en: One man's trash is a forensic examiner's treasure
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个人的垃圾是取证人员的宝藏
- en: A sticky situation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个棘手的情况
- en: Reading the registry
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读注册表
- en: Gathering user activity
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集用户活动
- en: The missing link
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失的链接
- en: Searching high and low
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四处搜寻
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Windows has long reigned supreme as the operating system of choice in the PC
    market. In fact, Windows makes up approximately 47 percent of the users visiting
    government websites, with the second most popular PC operating system, macOS,
    making up only 8.5 percentage. There is no reason to suspect that this will be
    changing anytime soon, especially with the warm reception that Windows 10 has
    received. Therefore, it is exceedingly likely that future investigations will
    continue to require the analysis of Windows artifacts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，Windows一直是PC市场上的首选操作系统。事实上，Windows约占访问政府网站的用户的47%，而第二受欢迎的PC操作系统macOS仅占8.5%。没有理由怀疑这种情况会很快改变，特别是考虑到Windows
    10受到的热烈欢迎。因此，未来的调查很可能会继续需要分析Windows工件。
- en: 'This chapter covers many types of artifacts and how to interpret them with
    Python, using various first and third-party libraries, directly from forensic
    evidence containers. We will leverage the framework we developed in [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container Recipes* to process these artifacts
    directly from forensic acquisitions. In this manner, we can provide captured raw
    or EWF images to our code and not worry about the process of extracting the required
    files or mounting the image prior to processing the data. Specifically, we will
    cover:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了许多类型的工件以及如何使用Python和各种第一方和第三方库直接从取证证据容器中解释它们。我们将利用我们在[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)中开发的框架，*处理取证证据容器配方*，直接处理这些工件，而不用担心提取所需文件或挂载镜像的过程。具体来说，我们将涵盖：
- en: Interpreting `$I` files to learn more about files sent to the Recycle Bin
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释`$I`文件以了解发送到回收站的文件的更多信息
- en: Reading content and metadata from Sticky Notes on Window 7 systems
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Windows 7系统的便笺中读取内容和元数据
- en: Extracting values from the registry to learn about the operating system version
    and other configuration details
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从注册表中提取值，以了解操作系统版本和其他配置细节
- en: Revealing user activity related to searches, typed paths, and run commands
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭示与搜索、输入路径和运行命令相关的用户活动
- en: Parsing LNK files to learn about historical and recent file access
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析LNK文件以了解历史和最近的文件访问
- en: Examining `Windows.edb` for information about indexed files, folders, and messages
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查`Windows.edb`以获取有关索引文件、文件夹和消息的信息
- en: To view more interesting metrics, visit [https://analytics.usa.gov/](https://analytics.usa.gov/).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看更多有趣的指标，请访问[https://analytics.usa.gov/](https://analytics.usa.gov/)。
- en: Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)下载本章的代码包。
- en: One man's trash is a forensic examiner's treasure
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个人的垃圾是取证人员的宝藏
- en: 'Recipe difficulty: Medium'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 配方难度：中等
- en: 'Python version: 2.7'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating system: Linux'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: While that may not be the exact saying, forensic examination of deleted files
    residing in the Recycle Bin is an important step in most investigations. The non-technical
    custodian likely does not understand that these files sent to the Recycle Bin
    are still present and that we can learn a good deal about the original file, such
    as its original file path and the time that it was sent to the Recycle Bin. While
    the specific artifacts vary between versions of Windows, this recipe focuses on
    the Windows 7 version of the Recycle Bin's `$I` and `$R` files.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能不是确切的说法，但是对于大多数调查来说，取证检查回收站中已删除文件是一个重要的步骤。非技术保管人可能不明白这些发送到回收站的文件仍然存在，我们可以了解到原始文件的很多信息，比如原始文件路径以及发送到回收站的时间。虽然特定的工件在不同版本的Windows中有所不同，但这个配方侧重于Windows
    7版本的回收站的`$I`和`$R`文件。
- en: Getting started
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'This recipe requires the installation of three third-party modules to function:
    `pytsk3`, `pyewf`, and `unicodecsv`. *Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    Working with Forensic Evidence Container Recipes* for a detailed explanation of
    installing the `pytsk3` and `pyewf` modules*.* All other libraries used in this
    script are present in Python''s standard library'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方需要安装三个第三方模块才能运行：`pytsk3`、`pyewf`和`unicodecsv`。*有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，处理取证证据容器配方*。*此脚本中使用的所有其他库都包含在Python的标准库中*
- en: 'Because we are developing these recipes in Python 2.x, we are likely to encounter
    Unicode encode and decode errors. To account for that, we use the `unicodecsv`
    library to write all CSV output in this chapter. This third-party module takes
    care of Unicode support, unlike Python 2.x''s standard `csv` module, and will
    be put to great use here. As usual, we can use `pip` to install `unicodecsv`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们正在用Python 2.x开发这些配方，我们很可能会遇到Unicode编码和解码错误。为了解决这个问题，我们使用`unicodecsv`库来写这一章节中的所有CSV输出。这个第三方模块负责Unicode支持，不像Python
    2.x的标准`csv`模块，并且在这里将得到很好的应用。和往常一样，我们可以使用`pip`来安装`unicodecsv`：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To learn more about the `unicodecsv` library, visit [https://github.com/jdunck/python-unicodecsv](https://github.com/jdunck/python-unicodecsv).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`unicodecsv`库的信息，请访问[https://github.com/jdunck/python-unicodecsv](https://github.com/jdunck/python-unicodecsv)。
- en: In addition to these, we'll continue to use the `pytskutil` module developed
    from [Chapter 8](https://cdp.packtpub.com/python_digital_forensics_cookbook/wp-admin/post.php?post=260&action=edit#post_218),
    *Working with Forensic Evidence Container recipes**,* to allow interaction with
    forensic acquisitions. This module is largely similar to what we previously wrote,
    with some minor changes to better suit our purposes. You can review the code by
    navigating to the utility directory within the code package.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们将继续使用从[第8章](https://cdp.packtpub.com/python_digital_forensics_cookbook/wp-admin/post.php?post=260&action=edit#post_218)开发的`pytskutil`模块，*与取证证据容器配方一起工作*，以允许与取证获取进行交互。这个模块在很大程度上类似于我们之前编写的内容，只是对一些细微的更改以更好地适应我们的目的。您可以通过导航到代码包中的实用程序目录来查看代码。
- en: How to do it...
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To parse the `$I` and `$R` files from a Windows 7 machine, we will need to:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要解析来自Windows 7机器的`$I`和`$R`文件，我们需要：
- en: Recurse through the `$Recycle.bin` folder in the evidence file, selecting all
    files starting with `$I`.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归遍历证据文件中的`$Recycle.bin`文件夹，选择所有以`$I`开头的文件。
- en: Read the contents of the files and parse the available metadata structures.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取文件的内容并解析可用的元数据结构。
- en: Search for the associated `$R` file and check if it is a file or folder.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索相关的`$R`文件并检查它是文件还是文件夹。
- en: Write the results into a CSV file for review.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果写入CSV文件进行审查。
- en: How it works...
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We import the `argparse`, `datetime`, `os`, and `struct` built-in libraries
    to assist with running the script and interpreting the binary data within these
    files. We also bring in our Sleuth Kit utilities for handling the evidence files,
    reading the content, and iterating through folders and files. Lastly, we import
    the `unicodecsv` library to assist with writing the CSV report:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入`argparse`，`datetime`，`os`和`struct`内置库来帮助运行脚本并解释这些文件中的二进制数据。我们还引入了我们的Sleuth
    Kit实用程序来处理证据文件，读取内容，并遍历文件夹和文件。最后，我们导入`unicodecsv`库来帮助编写CSV报告。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This recipe's command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `CSV_REPORT`, which represent the path to the evidence file,
    the type of evidence file, and the desired output path to the CSV report, respectively.
    These three arguments are passed to the `main()` function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序接受三个位置参数，`EVIDENCE_FILE`，`IMAGE_TYPE`和`CSV_REPORT`，分别代表证据文件的路径，证据文件的类型和所需的CSV报告输出路径。这三个参数被传递给`main()`函数。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `main()` function handles the necessary interactions with the evidence
    file to identify and provide any `$I` files for processing. To access the evidence
    file, one must provide the path to the container and the image type. This initiates
    the `TSKUtil` instance, which we use to search for files and folders within the
    image. To find the `$I` files, we call the `recurse_files()` method on the `tsk_util`
    instance, specifying the file name pattern to look for, the `path` to start the
    search in, and the string `logic` used to find the filename. The `logic` keyword
    argument accepts the following values which correspond to string operations: `startswith`,
    `endswith`, `contains`, and `equals`. These dictate the string operation used
    to search for our `$I` pattern within the scanned file and folder names.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数处理与证据文件的必要交互，以识别和提供任何用于处理的`$I`文件。要访问证据文件，必须提供容器的路径和图像类型。这将启动`TSKUtil`实例，我们使用它来搜索图像中的文件和文件夹。要找到`$I`文件，我们在`tsk_util`实例上调用`recurse_files()`方法，指定要查找的文件名模式，开始搜索的`path`和用于查找文件名的字符串`logic`。`logic`关键字参数接受以下值，这些值对应于字符串操作：`startswith`，`endswith`，`contains`和`equals`。这些指定了用于在扫描的文件和文件夹名称中搜索我们的`$I`模式的字符串操作。'
- en: 'If any `$I` files are found, we pass this list to the `process_dollar_i()`
    function along with the `tsk_util` object. After they are all processed, we write
    the extracted metadata to a CSV report with the `write_csv()` method:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找到任何`$I`文件，我们将此列表传递给`process_dollar_i()`函数，以及`tsk_util`对象。在它们都被处理后，我们使用`write_csv()`方法将提取的元数据写入CSV报告：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `process_dollar_i()` function accepts as its input, the `tsk_util` object
    and the list of discovered `$I` files. We iterate through this list and inspect
    each of these files. Each element within the `dollar_i_files` list is itself a
    list of tuples, where each tuple element contains (in order) the file''s name,
    relative path, handle to access the file''s content, and filesystem identifier.
    With these available attributes, we will call our `read_dollar_i()` function and
    provide it the third tuple, the file object handle. If this is a valid `$I` file,
    this method returns a dictionary of extracted metadata from the raw file, otherwise,
    it returns `None`. If the file is valid, we continue processing it by adding the
    file path to the `$I` file to the `file_attribs` dictionary:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_dollar_i()`函数接受`tsk_util`对象和发现的`$I`文件列表作为输入。我们遍历这个列表并检查每个文件。`dollar_i_files`列表中的每个元素本身都是一个元组列表，其中每个元组元素依次包含文件的名称、相对路径、用于访问文件内容的句柄和文件系统标识符。有了这些可用的属性，我们将调用我们的`read_dollar_i()`函数，并向其提供第三个元组，文件对象句柄。如果这是一个有效的`$I`文件，该方法将从原始文件中返回提取的元数据字典，否则返回`None`。如果文件有效，我们将继续处理它，将文件路径添加到`$I`文件的`file_attribs`字典中：'
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we search for the associated `$R` file within the image. In preparation
    of this, we join base path to the `$I` file (including the `$Recycle.bin` and
    the `SID` folders) to reduce the amount of time required to search for the corresponding
    `$R` file. On Windows 7, the `$I` and `$R` files have a similar file name, where
    the first two letters are `$I` and `$R`, respectively, followed by a shared identifier.
    By using that identifier in our search and specifying the specific folder we expect
    to find the `$R` file, we have reduced the likelihood of false positives. Using
    these patterns, we query our evidence file again with the `startswith` logic:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在图像中搜索相关的`$R`文件。为此，我们将基本路径与`$I`文件（包括`$Recycle.bin`和`SID`文件夹）连接起来，以减少搜索相应`$R`文件所需的时间。在Windows
    7中，`$I`和`$R`文件具有类似的文件名，前两个字母分别是`$I`和`$R`，后面是一个共享标识符。通过在我们的搜索中使用该标识符，并指定我们期望找到`$R`文件的特定文件夹，我们已经减少了误报的可能性。使用这些模式，我们再次使用`startswith`逻辑查询我们的证据文件：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If the search for the `$R` files is unsuccessful, we try to query for a directory
    with the same information. If this query is also unsuccessful, we append dictionary
    values that the `$R` file was not found and that we are unsure if it was a file
    or directory. If, however, we do find a matching directory, we log the path of
    the directory and set the `is_directory` attribute to `True`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果搜索`$R`文件失败，我们尝试查询具有相同信息的目录。如果此查询也失败，我们将附加字典值，指出未找到`$R`文件，并且我们不确定它是文件还是目录。然而，如果我们找到匹配的目录，我们会记录目录的路径，并将`is_directory`属性设置为`True`：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If the search for the `$R` file returned one or more hits, we create a list
    of the matched files, using list comprehension, to store in the CSV, delimited
    by semicolons, and mark the `is_directory` attribute as `False`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果搜索`$R`文件返回一个或多个命中，我们使用列表推导创建一个匹配文件的列表，存储在以分号分隔的CSV中，并将`is_directory`属性标记为`False`。
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Prior to exiting the loop, we append the `file_attribs` dictionary to the `processed_files`
    list which stores all `$I` processed dictionaries. This list of dictionaries is
    returned to the `main()` function where it is used in the reporting process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在退出循环之前，我们将`file_attribs`字典附加到`processed_files`列表中，该列表存储了所有`$I`处理过的字典。这个字典列表将被返回到`main()`函数，在报告过程中使用。
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let's briefly look at the `read_dollar_i()` method, used to parse metadata from
    the binary file with `struct`. We start by checking the file header, using the
    Sleuth Kit's `read_random()` method to read the signature's first eight bytes.
    If the signature does not match, we return `None` to alert that the `$I` failed
    validation and is an invalid file format.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地看一下`read_dollar_i()`方法，用于使用`struct`从二进制文件中解析元数据。我们首先通过使用Sleuth Kit的`read_random()`方法来检查文件头，读取签名的前八个字节。如果签名不匹配，我们返回`None`来警告`$I`未通过验证，是无效的文件格式。
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we detect a valid file, we continue to read and unpack values from the `$I`
    file. The first is the file size attribute, which is located at byte offset `8`
    and is `8` bytes long. We unpack this with `struct` and store the integer in a
    temporary variable. The next attribute, deletion time, is stored at byte offset
    `16` and `8` bytes long. This is a Windows `FILETIME` object and we will borrow
    some old code to later process it into a human-readable timestamp. The last attribute
    is the former file path, which we read from byte `24` to the end of the file:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检测到一个有效的文件，我们继续从`$I`文件中读取和解压值。首先是文件大小属性，位于字节偏移`8`，长度为`8`字节。我们使用`struct`解压缩这个值，并将整数存储在一个临时变量中。下一个属性是删除时间，存储在字节偏移`16`和`8`字节长。这是一个Windows
    `FILETIME`对象，我们将借用一些旧代码来稍后将其处理为可读的时间戳。最后一个属性是以前的文件路径，我们从字节`24`读取到文件的末尾：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With these values extracted, we interpret the integers into human-readable
    values. We use the `sizeof_fmt()` function to convert the file size integer into
    a human-readable size, containing size prefixes such as MB or GB. Next, we interpret
    the timestamp using the logic from our date parsing recipe from [Chapter 7](part0212.html#6A5N80-260f9401d2714cb9ab693c4692308abe),
    *Log-Based Artifact Recipes* (after adapting the function to work only with integers).
    Lastly, we decode the path as UTF-16 and remove null-byte values. These refined
    details are then returned as a dictionary to the calling function:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 提取了这些值后，我们将整数解释为可读的值。我们使用`sizeof_fmt()`函数将文件大小整数转换为可读的大小，包含诸如MB或GB的大小前缀。接下来，我们使用来自[第7章](part0212.html#6A5N80-260f9401d2714cb9ab693c4692308abe)的日期解析配方的逻辑来解释时间戳（在适应该函数仅使用整数后）。最后，我们将路径解码为UTF-16并删除空字节值。然后将这些精细的细节作为字典返回给调用函数：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Our `sizeof_fmt()` function is borrowed from [StackOverflow.com](https://stackoverflow.com/),
    a site filled with many solutions to programming problems. While we could have
    drafted our own, this code is well formed for our purpose. It takes the integer
    `num` and iterates through the listed unit suffixes. If the number is less than
    `1024`, the number, unit, and suffix are joined into a string and returned; otherwise,
    the number is divided by `1024` and run through the next iteration. If the number
    is larger than a zettabyte, it returns the information in terms of yottabytes.
    For your sake, we hope the number is never that large.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`sizeof_fmt()`函数是从[StackOverflow.com](https://stackoverflow.com/)借来的，这是一个充满了许多编程问题解决方案的网站。虽然我们可以自己起草，但这段代码对我们的目的来说形式良好。它接受整数`num`并遍历列出的单位后缀。如果数字小于`1024`，则数字、单位和后缀被连接成一个字符串并返回；否则，数字除以`1024`并通过下一次迭代。如果数字大于1
    zettabyte，它将以yottabytes的形式返回信息。为了你的利益，我们希望数字永远不会那么大。
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our next support function is `parse_windows_filetime()`, adapted from the previous
    date-parsing recipe in [Chapter 7](part0212.html#6A5N80-260f9401d2714cb9ab693c4692308abe),
    *Log-Based Artifact Recipes*. We borrow the logic and condense the code to only
    interpret integers into a formatted date that is then returned to the calling
    function. Generic functions, like the two we just discussed, are handy to keep
    in your arsenal as you never know when you may need this logic.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个支持函数是`parse_windows_filetime()`，改编自[第7章](part0212.html#6A5N80-260f9401d2714cb9ab693c4692308abe)中的先前日期解析配方，*基于日志的证据配方*。我们借用这个逻辑并将代码压缩为只解释整数并返回给调用函数的格式化日期。像我们刚刚讨论的这两个通用函数一样，它们在你的工具库中是很方便的，因为你永远不知道什么时候会需要这个逻辑。
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we are ready to write the processed results to a CSV file. As you
    have no doubt come to expect, this function is similar to all of our other CSV
    functions. The only difference is that it is using the `unicodecsv` library under
    the hood, though the method and function names used here are the same:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备将处理后的结果写入CSV文件。毫无疑问，这个函数与我们所有其他的CSV函数类似。唯一的区别是它在底层使用了`unicodecsv`库，尽管这里使用的方法和函数名称是相同的：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the following two screenshots, we can see examples of the type of data this
    recipe extracts from `$I` and `$R` files:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的两个屏幕截图中，我们可以看到这个配方从`$I`和`$R`文件中提取的数据的示例：
- en: '![](../images/00096.jpeg)![](../images/00097.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00096.jpeg)![](../images/00097.jpeg)'
- en: A sticky situation
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个棘手的情况
- en: 'Recipe difficulty: Medium'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 配方难度：中等
- en: 'Python version: 2.7'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating system: Linux'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: Computers have replaced pen and paper. We have transferred many processes and
    habits, one relegated solely to the confines of paper, to these machines, including
    taking notes and making lists. One feature that replicates a real-world habit
    is Windows Sticky Notes. These sticky notes allow persistent notes to float on
    the desktop, with options for color, fonts, and more. This recipe will allow us
    to explore these sticky notes and add them to our investigative workflow.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机已经取代了纸和笔。我们已经将许多过程和习惯转移到了这些机器上，其中一个仅限于纸张的习惯，包括做笔记和列清单。一个复制真实习惯的功能是Windows的便利贴。这些便利贴可以让持久的便签漂浮在桌面上，可以选择颜色、字体等选项。这个配方将允许我们探索这些便利贴，并将它们添加到我们的调查工作流程中。
- en: Getting started
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始
- en: 'This recipe requires the installation of four third-party modules to function:
    `olefile`, `pytsk3`, `pyewf`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *One man''s trash is a forensic examiner''s treasure*
    recipe for details on installing `unicodecsv`. All other libraries used in this
    script are present in Python''s standard library.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方需要安装四个第三方模块才能运行：`olefile`，`pytsk3`，`pyewf`和`unicodecsv`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，*使用法证证据容器*
    *配方*。同样，有关安装`unicodecsv`的详细信息，请参阅*一个人的垃圾是法医检查员的宝藏*配方中的*入门*部分。此脚本中使用的所有其他库都包含在Python的标准库中。
- en: 'The Windows Sticky Note file is stored as an `OLE` file. Therefore, we will
    leverage the `olefile` library to interact with and extract data from Windows
    Sticky Notes. The `olefile` library can be installed with `pip`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Windows的便利贴文件存储为`OLE`文件。因此，我们将利用`olefile`库与Windows的便利贴进行交互并提取数据。`olefile`库可以通过`pip`安装：
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To learn more about the `olefile` library, visit [https://olefile.readthedocs.io/en/latest/index.html](https://olefile.readthedocs.io/en/latest/index.html).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`olefile`库的信息，请访问[https://olefile.readthedocs.io/en/latest/index.html](https://olefile.readthedocs.io/en/latest/index.html)。
- en: How to do it...
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To properly form this recipe, we need to take the following steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确制作这个配方，我们需要采取以下步骤：
- en: Open the evidence file and find all `StickyNote.snt` files across the user profiles.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开证据文件并找到所有用户配置文件中的`StickyNote.snt`文件。
- en: Parse metadata and content from the OLE streams.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析OLE流中的元数据和内容。
- en: Write the RTF content to files.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将RTF内容写入文件。
- en: Create a CSV report of the metadata.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建元数据的CSV报告。
- en: How it works...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This script, like the others, begins with import statements of the libraries
    required for its execution. The two new libraries here are `olefile` which, as
    we discussed, parses the Windows Sticky Note OLE streams and `StringIO`, a built-in
    library used to interpret a string of data as a file-like object. This library
    will be used to convert the `pytsk` file object into a stream the `olefile` library
    can interpret:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '这个脚本，就像其他脚本一样，以导入所需库的导入语句开始执行。这里的两个新库是`olefile`，正如我们讨论的，它解析Windows的便利贴OLE流，以及`StringIO`，一个内置库，用于将数据字符串解释为类似文件的对象。这个库将用于将`pytsk`文件对象转换为`olefile`库可以解释的流： '
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We specify a global variable, `REPORT_COLS`, which represent the report columns.
    These static columns will be used across several functions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定一个全局变量，`REPORT_COLS`，代表报告列。这些静态列将在几个函数中使用。
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This recipe''s command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `REPORT_FOLDER`, which represent the path to the evidence file,
    the type of evidence file, and the desired output directory path, respectively.
    This is similar to the previous recipe, with the exception of the `REPORT_FOLDER`,
    which is a directory that we will write the Sticky Note RTF files to:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序需要三个位置参数，`EVIDENCE_FILE`，`IMAGE_TYPE`和`REPORT_FOLDER`，它们分别代表证据文件的路径，证据文件的类型和期望的输出目录路径。这与之前的配方类似，唯一的区别是`REPORT_FOLDER`，这是一个我们将写入便利贴RTF文件的目录：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our main function starts similarly to the last, by handling the evidence file
    and searching for the files we seek to parse. In this case, we are looking for
    the `StickyNotes.snt` file, which is found within each user''s `AppData` directory.
    For this reason, we limit the search to the `/Users` folder and look for a file
    matching the exact name:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要函数开始方式与上一个类似，处理证据文件并搜索我们要解析的文件。在这种情况下，我们正在寻找`StickyNotes.snt`文件，该文件位于每个用户的`AppData`目录中。因此，我们将搜索限制为`/Users`文件夹，并寻找与确切名称匹配的文件：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We then iterate through the resulting files, splitting out the user's home directory
    name and setting up the file-like object required for processing by the `olefile`
    library. Next, we call the `parse_snt_file()` function to process the file and
    return a list of results to iterate through. At this point, if the `note_data`
    is not `None`, we write the RTF file with the `write_note_rtf()` method. Additionally,
    we append the processed the processed data from the `prep_note_report()` to the
    `report_details` list. Once the `for` loop completes, we write the CSV report
    with the `write_csv()` method by providing the report name, report columns, and
    the list we have built of the sticky note information.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遍历生成的文件，分离用户的主目录名称，并设置`olefile`库所需的类文件对象。接下来，我们调用`parse_snt_file()`函数处理文件，并返回一个结果列表进行遍历。在这一点上，如果`note_data`不是`None`，我们使用`write_note_rtf()`方法写入RTF文件。此外，我们将从`prep_note_report()`处理的数据附加到`report_details`列表中。一旦`for`循环完成，我们使用`write_csv()`方法写入CSV报告，提供报告名称、报告列和我们构建的粘贴便笺信息列表。
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `create_file_like_obj()` function takes our `pytsk` file object and reads
    the size of the file. This size is used in the `read_random()` function to read
    the entire sticky note content into memory. We feed the `file_content` into the
    `StringIO()` class to convert it into a file-like object the `olefile` library
    can read before returning it to the parent function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_file_like_obj()`函数获取我们的`pytsk`文件对象并读取文件的大小。这个大小在`read_random()`函数中用于将整个粘贴便笺内容读入内存。我们将`file_content`传递给`StringIO()`类，将其转换为`olefile`库可以读取的类文件对象，然后将其返回给父函数：'
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `parse_snt_file()` function accepts the file-like object as its input and
    is used to read and interpret the sticky note file. We begin by validating that
    the file-like object is an OLE file, returning `None` if it is not. If it is,
    we open the file-like object using the `OleFileIO()` method. This provides a list
    of streams, allowing us to iterate through each element of each sticky note. As
    we iterate over the list, we check if the stream contains three dashes, as this
    suggests that the stream contains a unique identifier for a sticky note. This
    file can contain one or more sticky notes, each identified by the unique IDs.
    The sticky note data is either read directly as RTF data or UTF-16 encoded data
    based on the value of the element in the first index of the stream.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse_snt_file()`函数接受类文件对象作为输入，并用于读取和解释粘贴便笺文件。我们首先验证类文件对象是否是OLE文件，如果不是，则返回`None`。如果是，我们使用`OleFileIO()`方法打开类文件对象。这提供了一个流列表，允许我们遍历每个粘贴便笺的每个元素。在遍历列表时，我们检查流是否包含三个破折号，因为这表明流包含粘贴便笺的唯一标识符。该文件可以包含一个或多个粘贴便笺，每个粘贴便笺由唯一的ID标识。粘贴便笺数据根据流的第一个索引元素的值，直接读取为RTF数据或UTF-16编码数据。'
- en: 'We also read the created and modified information from the stream using the
    `getctime()` and `getmtime()` functions, respectively. Next, we extract the sticky
    note RTF or UTF-16 encoded data to the `content` variable. Note, we must decode
    the UTF-16 encoded data prior to storing it. If there is content to save, we add
    it to the `note` dictionary and continue processing all remaining streams. Once
    all streams are processed, the `note` dictionary is returned to the parent function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用`getctime()`和`getmtime()`函数从流中读取创建和修改的信息。接下来，我们将粘贴便笺的RTF或UTF-16编码数据提取到`content`变量中。注意，我们必须在存储之前解码UTF-16编码的数据。如果有内容要保存，我们将其添加到`note`字典中，并继续处理所有剩余的流。一旦所有流都被处理，`note`字典将返回给父函数：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To create the RTF files, we pass the dictionary of note data to the `write_note_rtf()`
    function. If the report folder does not exist, we use the `os` library to create
    it. At this point, we iterate through the `note_data` dictionary, splitting the
    `note_id` keys from `stream_data` values. The `note_id` is used to create the
    output RTF filename prior to opening it.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建RTF文件，我们将便笺数据字典传递给`write_note_rtf()`函数。如果报告文件夹不存在，我们使用`os`库来创建它。在这一点上，我们遍历`note_data`字典，分离`note_id`键和`stream_data`值。在打开之前，`note_id`用于创建输出RTF文件的文件名。
- en: 'The data stored in stream zero is then written to the ouput RTF file before
    it is closed and the next sticky note is handled:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将存储在流零中的数据写入输出的RTF文件，然后关闭文件并处理下一个粘贴便笺：
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: With the content of the sticky notes written, we now move onto the CSV report
    itself which is handled a little differently by the `prep_note_report()` function.
    This translates the nested dictionary into a flat list of dictionaries that are
    more conducive and appropriate for a CSV spreadsheet. We flatten it by including
    the `note_id` key and naming the fields using the keys specified in the global
    `REPORT_COLS` list.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 将粘贴便笺上的内容写好后，我们现在转向`prep_note_report()`函数处理的CSV报告本身，这个函数处理方式有点不同。它将嵌套字典转换为一组更有利于CSV电子表格的扁平字典。我们通过包括`note_id`键来扁平化它，并使用全局`REPORT_COLS`列表中指定的键来命名字段。
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Lastly, in the `write_csv()` method, we create a `csv.Dictwriter` object to
    create an overview report of the sticky note data. This CSV writer also uses the
    `unicodecsv` library and writes the list of dictionaries to the file, using the
    `REPORT_COLS` list of columns as the `fieldnames`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在`write_csv()`方法中，我们创建一个`csv.Dictwriter`对象来创建粘贴便笺数据的概述报告。这个CSV写入器还使用`unicodecsv`库，并将字典列表写入文件，使用`REPORT_COLS`列的`fieldnames`。
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can then view the output as we have a new directory containing the exported
    sticky notes and report:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以查看输出，因为我们有一个包含导出的粘贴便笺和报告的新目录：
- en: '![](../images/00098.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00098.jpeg)'
- en: 'Opening our report, we can view the note metadata and gather some of the internal
    content, though most spreadsheet viewers have difficulty with non-ASCII character
    interpretations:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 打开我们的报告，我们可以查看注释元数据并收集一些内部内容，尽管大多数电子表格查看器在处理非ASCII字符解释时会遇到困难：
- en: '![](../images/00099.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00099.jpeg)'
- en: 'Lastly, we can open the output RTF files and view the raw content:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以打开输出的RTF文件并查看原始内容：
- en: '![](../images/00100.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00100.jpeg)'
- en: Reading the registry
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取注册表
- en: 'Recipe Difficulty: Medium'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：中等
- en: 'Python Version: 2.7'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating System: Linux'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: The Windows registry contains many important details related to the operating
    system configuration, user activity, software installation and usage, and so much
    more. These files are often heavily scrutinized and researched due to the number
    of artifacts they contain and their relevance to Windows systems. Parsing registry
    files gives us access to the keys and values that can reveal basic operating system
    information, access to folders and files, application usage, USB devices, and
    more. In this recipe, we focus on accessing common baseline information from the
    `SYSTEM` and `SOFTWARE` hives.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Windows注册表包含许多与操作系统配置、用户活动、软件安装和使用等相关的重要细节。由于它们包含的文物数量和与Windows系统的相关性，这些文件经常受到严格审查和研究。解析注册表文件使我们能够访问可以揭示基本操作系统信息、访问文件夹和文件、应用程序使用情况、USB设备等的键和值。在这个食谱中，我们专注于从`SYSTEM`和`SOFTWARE`注册表文件中访问常见的基线信息。
- en: Getting started
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'This recipe requires the installation of three third-party modules to function:
    `pytsk3`, `pyewf`, and `Registry`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes,* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. All other libraries used in this
    script are present in Python''s standard library.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此食谱需要安装三个第三方模块才能正常运行：`pytsk3`，`pyewf`和`Registry`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，*使用取证证据容器*
    *食谱*。此脚本中使用的所有其他库都包含在Python的标准库中。
- en: 'In this recipe, we use the `Registry` module to interact with registry hives
    in an object-oriented manner. Critically, this module can be used to interact
    with external and standalone registry files. The `Registry` module can be installed
    with `pip`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用`Registry`模块以面向对象的方式与注册表文件进行交互。重要的是，该模块可用于与外部和独立的注册表文件进行交互。可以使用`pip`安装`Registry`模块：
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: To learn more about the `Registry` library, visit [https://github.com/williballenthin/python-registry](https://github.com/williballenthin/python-registry).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关`Registry`库的更多信息，请访问[https://github.com/williballenthin/python-registry](https://github.com/williballenthin/python-registry)。
- en: How to do it...
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To build our registry system overview script, we will need to:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们的注册表系统概述脚本，我们需要：
- en: Find the registry hives to process by name and path.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过名称和路径查找要处理的注册表文件。
- en: Open these files using the `StringIO` and `Registry` modules.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`StringIO`和`Registry`模块打开这些文件。
- en: Process each hive, printing the parsed values to the console for interpretation.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理每个注册表文件，将解析的值打印到控制台以进行解释。
- en: How it works...
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The imports overlap with the other recipes in this chapter. These modules allow
    us to handle argument parsing, date manipulation, read our files into memory for
    the `Registry` library, and unpack and interpret binary data we extract from registry
    values. We also import the `TSKUtil()` class and the `Registry` module to process
    registry files.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 导入与本章其他食谱重叠的导入。这些模块允许我们处理参数解析，日期操作，将文件读入内存以供`Registry`库使用，并解压和解释我们从注册表值中提取的二进制数据。我们还导入`TSKUtil()`类和`Registry`模块以处理注册表文件。
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This recipe''s command-line handler takes two positional arguments, `EVIDENCE_FILE`
    and `IMAGE_TYPE`, which represent the path to the evidence file and the type of
    evidence file, respectively:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此食谱的命令行处理程序接受两个位置参数，`EVIDENCE_FILE`和`IMAGE_TYPE`，分别表示证据文件的路径和证据文件的类型：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `main()` function starts by creating a `TSKUtil` object from the evidence
    and searches for the `SYSTEM` and `SOFTWARE` hives within the `/Windows/System32/config`
    folder. We create `Registry()` class instances of these hives with the `open_file_as_reg()`
    function before they are passed to their respective processing functions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数首先通过从证据中创建一个`TSKUtil`对象，并在`/Windows/System32/config`文件夹中搜索`SYSTEM`和`SOFTWARE`注册表文件。在将它们传递给各自的处理函数之前，我们使用`open_file_as_reg()`函数创建这些注册表文件的`Registry()`类实例。'
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To open the registry files, we need to gather the size of the file from the
    `pytsk` metadata and read the entire file, from byte zero to the end of the file
    into a variable. We then provide this variable to a `StringIO()` instance which
    allows us to open the file-like object with the `Registry()` class. We return
    the `Registry` class instance to the calling function for further processing:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开注册表文件，我们需要从`pytsk`元数据中收集文件的大小，并将整个文件从字节零到文件末尾读入变量中。然后，我们将此变量提供给`StringIO()`实例，该实例允许我们使用`Registry()`类打开类似文件的对象。我们将`Registry`类实例返回给调用函数进行进一步处理：
- en: '[PRE30]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s start with the `SYSTEM` hive processing. This hive holds the majority
    of its information within control sets. The `SYSTEM` hive generally has two or
    more control sets that act as a backup system for the configurations they store.
    For simplicity, we will only read the current control set. To identify the current
    control set, we get our foothold within the hive with the `root` key and use the
    `find_key()` method to get the `Select` key. Within this key, we read the `Current`
    value, using the `value()` method to select it and the `value()` method on the
    `value` object to present the content of the value. While the method naming is
    a little ambiguous, the values within a key are named, so we first need to select
    them by name before then calling out the content that they hold. Using this information,
    we select the current control set key, passing an appropriately padded integer
    for the current control set (such as `ControlSet0001`). This object will be used
    through the remainder of the function to navigate to specific `subkeys` and `values`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`SYSTEM` hive处理开始。这个hive主要包含在控制集中的大部分信息。`SYSTEM` hive通常有两个或更多的控制集，它们充当存储的配置的备份系统。为了简单起见，我们只读取当前的控制集。为了识别当前的控制集，我们通过`root`键在hive中找到我们的立足点，并使用`find_key()`方法获取`Select`键。在这个键中，我们读取`Current`值，使用`value()`方法选择它，并在`value`对象上使用`value()`方法来呈现值的内容。虽然方法的命名有点模糊，但键中的值是有名称的，所以我们首先需要按名称选择它们，然后再调用它们所持有的内容。使用这些信息，我们选择当前的控制集键，传递一个适当填充的整数作为当前控制集（如`ControlSet0001`）。这个对象将在函数的其余部分用于导航到特定的`subkeys`和`values`：
- en: '[PRE31]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The first piece of information we will extract from the `SYSTEM` hive is the
    shutdown time. We read the `Control\Windows\ShutdownTime` value from the current
    control set and pass the hexadecimal value into `struct` to convert it to a `64-bit`
    integer. We then provide this integer to the Windows `FILETIME` parser to obtain
    a human-readable date string which we print to the console.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`SYSTEM` hive中提取的第一条信息是关机时间。我们从当前控制集中读取`Control\Windows\ShutdownTime`值，并将十六进制值传递给`struct`来将其转换为`64位`整数。然后我们将这个整数提供给Windows
    `FILETIME`解析器，以获得一个可读的日期字符串，然后将其打印到控制台上。
- en: '[PRE32]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we will ascertain the time zone information for the machine. This is
    found within the `Control\TimeZoneInformation\TimeZoneKeyName` value. This returns
    a string value that we can print directly to the console:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将确定机器的时区信息。这可以在`Control\TimeZoneInformation\TimeZoneKeyName`值中找到。这将返回一个字符串值，我们可以直接打印到控制台上：
- en: '[PRE33]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Following that, we gather the machine''s hostname. This is found under the
    `Control\ComputerName\ComputerName` key in the `ComputerName` value. The extracted
    value is a string that we can print to the console:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们收集机器的主机名。这可以在`Control\ComputerName\ComputerName`键的`ComputerName`值下找到。提取的值是一个字符串，我们可以打印到控制台上：
- en: '[PRE34]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Pretty easy so far, right? Lastly, for the `System` hive, we parse information
    about the last access timestamp configuration. This `registry` key determines
    if the NTFS volume''s last access timestamp is maintained, and is generally disabled
    by default on systems. To confirm this, we look for the `NtfsDisableLastAccessUpdate`
    value in the `Control\FileSystem` key and see if it is equal to `1`. If it is,
    the last access timestamp is not maintained and marked as disabled before printing
    to the console. Notice the one-liner `if-else` statement, while perhaps a little
    more difficult to read it does have its uses:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，还是相当容易的，对吧？最后，对于`System` hive，我们解析关于最后访问时间戳配置的信息。这个`registry`键确定了NTFS卷的最后访问时间戳是否被维护，并且通常在系统上默认情况下是禁用的。为了确认这一点，我们查找`Control\FileSystem`键中的`NtfsDisableLastAccessUpdate`值，看它是否等于`1`。如果是，最后访问时间戳就不会被维护，并且在打印到控制台之前标记为禁用。请注意这个一行的`if-else`语句，虽然可能有点难以阅读，但它确实有它的用途：
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Our Windows `FILETIME` parser borrows logic from our former date-parsing recipe,
    accepting an integer that we convert into a human-readable date string. We also
    borrowed the logic for the `Unix` epoch date parser from the same date-parsing
    recipe and will use it to interpret dates from the `Software` hive.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Windows `FILETIME`解析器从以前的日期解析配方中借用逻辑，接受一个整数，我们将其转换为可读的日期字符串。我们还从相同的日期解析配方中借用了`Unix`
    epoch日期解析器的逻辑，并将用它来解释来自`Software` hive的日期。
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Our last function processes the `SOFTWARE` hive, presenting information to users
    in the console window. This function also begins by gathering the root of the
    hive and then selecting the `Microsoft\Windows NT\CurrentVersion` key. This key
    contains values about OS installation metadata and other useful subkeys. In this
    function, we will extract the `ProductName`, `CSDVersion`, `CurrentBuild number`,
    `RegisteredOwner`, `RegisteredOrganization`, and `InstallDate` values. While most
    of these values are strings we can print directly to the console, we need to use
    the `Unix` epoch converter to interpret the installation date value prior to printing
    it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一个函数处理`SOFTWARE` hive，在控制台窗口向用户呈现信息。这个函数也是通过收集hive的根开始，然后选择`Microsoft\Windows
    NT\CurrentVersion`键。这个键包含有关OS安装元数据和其他有用的子键的值。在这个函数中，我们将提取`ProductName`、`CSDVersion`、`CurrentBuild
    number`、`RegisteredOwner`、`RegisteredOrganization`和`InstallDate`值。虽然这些值大多是我们可以直接打印到控制台的字符串，但在打印之前我们需要使用`Unix`
    epoch转换器来解释安装日期值。
- en: '[PRE37]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'When we run this script, we can learn about the information stored in the keys
    we interpreted:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个脚本时，我们可以了解到我们解释的键中存储的信息：
- en: '![](../images/00101.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00101.jpeg)'
- en: There's more...
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们提供了一个或多个以下建议：
- en: Add logic to handle the situation where the `SYSTEM` or `SOFTWARE` hives are
    not found in the initial search
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加逻辑来处理在初始搜索中找不到`SYSTEM`或`SOFTWARE` hive的情况
- en: Consider adding support for `NTUSER.DAT` files, pulling basic information about
    mount points and shell bags queries
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑添加对`NTUSER.DAT`文件的支持，提取有关挂载点和shell bags查询的基本信息
- en: List basic USB device information from the `System` hive
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`System` hive列出基本的USB设备信息
- en: Parse the `SAM` hive to show user and group information
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析`SAM` hive以显示用户和组信息
- en: Gathering user activity
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集用户活动
- en: 'Recipe Difficulty: Medium'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 配方难度：中等
- en: 'Python Version: 2.7'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating System: Linux'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: Windows stores a plethora of information about user activity, and like other
    registry hives, the `NTUSER.DAT` file is a great resource to be relied upon during
    an investigation. This hive lives within each user's profile and stores information
    and configurations as they relate to the specific user's on the system.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Windows存储了大量关于用户活动的信息，就像其他注册表hive一样，`NTUSER.DAT`文件是调查中可以依赖的重要资源。这个hive存在于每个用户的配置文件中，并存储与特定用户在系统上相关的信息和配置。
- en: In this recipe, we cover multiple keys within `NTUSER.DAT` that throw light
    on the actions of a user on a system. This includes the prior searches run in
    Windows Explorer, paths typed into Explorer's navigation bar, and the recently
    used statements in the Windows `run` command. These artifacts better illustrate
    how the user interacted with the system and may give insight into what normal,
    or abnormal, usage of the system looked like for the user.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们涵盖了`NTUSER.DAT`中的多个键，这些键揭示了用户在系统上的操作。这包括在Windows资源管理器中运行的先前搜索、输入到资源管理器导航栏的路径以及Windows“运行”命令中最近使用的语句。这些工件更好地说明了用户如何与系统进行交互，并可能揭示用户对系统的正常或异常使用看起来是什么样子。
- en: Getting started
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始
- en: 'This recipe requires the installation of four third-party modules to function:
    `jinja2`, `pytsk3`, `pyewf`, and `Registry`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes,* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Reading the registry* recipe for details on installing
    `Registry`. All other libraries used in this script are present in Python''s standard
    library.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方需要安装四个第三方模块才能正常工作：`jinja2`、`pytsk3`、`pyewf`和`Registry`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，*使用取证证据容器*
    *配方*。同样，有关安装`Registry`的详细信息，请参阅*入门*部分*读取注册表*配方。此脚本中使用的所有其他库都包含在Python的标准库中。
- en: 'We will reintroduce `jinja2`, first introduced in [Chapter 2](part0071.html#23MNU0-260f9401d2714cb9ab693c4692308abe),
    *Creating Artifact Report* R*ecipes*, to build an HTML report. This library is
    a template language that allows us to build text files programmatically using
    a Pythonic syntax. As a reminder, we can use `pip` to install this library:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重新介绍`jinja2`，这是在[第2章](part0071.html#23MNU0-260f9401d2714cb9ab693c4692308abe)中首次介绍的，*创建工件报告*
    *配方*，用于构建HTML报告。这个库是一个模板语言，允许我们使用Python语法以编程方式构建文本文件。作为提醒，我们可以使用`pip`来安装这个库：
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How to do it...
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To extract these values from `NTUSER.DAT` files within the image, we must:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要从图像中的`NTUSER.DAT`文件中提取这些值，我们必须：
- en: Search for all `NTUSER.DAT` files across the system.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在系统中搜索所有`NTUSER.DAT`文件。
- en: Parse the `WordWheelQuery` key for each `NTUSER.DAT` file.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析每个`NTUSER.DAT`文件的`WordWheelQuery`键。
- en: Read the `TypedPath` key for each `NTUSER.DAT` file.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取每个`NTUSER.DAT`文件的`TypedPath`键。
- en: Extract the `RunMRU` key for each `NTUSER.DAT` file.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取每个`NTUSER.DAT`文件的`RunMRU`键。
- en: Write each of the processed artifacts to an HTML report.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个处理过的工件写入HTML报告。
- en: How it works...
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Our imports start in the same manner as our prior recipe, adding in the `jinja2`
    module:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的导入方式与之前的配方相同，添加了`jinja2`模块：
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This recipe's command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `REPORT`, which represent the path to the evidence file, the
    type of evidence file, and the desired output path to the HTML report, respectively.
    These three arguments are passed to the `main()` function.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的命令行处理程序接受三个位置参数，`EVIDENCE_FILE`、`IMAGE_TYPE`和`REPORT`，分别代表证据文件的路径、证据文件的类型和HTML报告的期望输出路径。这三个参数被传递给`main()`函数。
- en: '[PRE40]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `main()` function begins by reading the evidence file and searching for
    all `NTUSER.DAT` files. Following this, we set up a dictionary object, `nt_rec`,
    which, while complex, is designed in a manner that eases the HTML report generation
    process. We then begin iterating through the discovered hives and parse out the
    username from the path for reference in the processing functions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数首先通过读取证据文件并搜索所有`NTUSER.DAT`文件来开始。随后，我们设置了一个字典对象`nt_rec`，虽然复杂，但设计得可以简化HTML报告生成过程。然后，我们开始迭代发现的hive，并从路径中解析出用户名以供处理函数参考。'
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Next, we pass the `pytsk` file handle to be opened as a `Registry` object. This
    resulting object is used to gather the `root` key in common with all of the desired
    values (`Software\Microsoft\Windows\CurrentVersion\Explorer`). If this key path
    is not found, we continue to the next `NTUSER.DAT` file.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`pytsk`文件句柄传递给`Registry`对象以打开。得到的对象用于收集所有所需值（`Software\Microsoft\Windows\CurrentVersion\Explorer`）中的`root`键。如果未找到此键路径，我们将继续处理下一个`NTUSER.DAT`文件。
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If they key is found, we call the three processing functions responsible for
    each artifact and provide the shared key object and username. The returned data
    is stored in the respective data key within the dictionary. We can easily extend
    the number of artifacts parsed by the code by expanding the storage object definition
    and adding a new function with the same profile as the others shown here:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找到了键，我们调用负责每个工件的三个处理函数，并提供共享键对象和用户名。返回的数据存储在字典中的相应数据键中。我们可以通过扩展存储对象定义并添加一个与这里显示的其他函数具有相同配置文件的新函数，轻松扩展代码解析的工件数量：
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: After iterating through the `NTUSER.DAT` files, we set up the headers for each
    of the record types by extracting the key list of the first item on our data list.
    Since all of the dictionary objects in our data list have uniform keys, we can
    use this method to reduce the number of arguments or variables passed around.
    These statements are also easily extensible.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在遍历`NTUSER.DAT`文件之后，我们通过提取数据列表中第一项的键列表来为每种记录类型设置标题。由于数据列表中的所有字典对象都具有统一的键，我们可以使用这种方法来减少传递的参数或变量的数量。这些语句也很容易扩展。
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Lastly, we take our completed dictionary object and pass it, along with the
    path to the report file, to our `write_html()` method:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将完成的字典对象和报告文件的路径传递给我们的`write_html()`方法：
- en: '[PRE45]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We've seen the `open_file_as_reg()` method before in the previous recipe. As
    a reminder, it takes the `pytsk` file handle and reads it into the `Registry`
    class via the `StringIO` class. The returned `Registry` object allows us to interact
    and read the registry in an object-oriented manner.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在上一个示例中见过`open_file_as_reg()`方法。作为提醒，它接受`pytsk`文件句柄，并通过`StringIO`类将其读入`Registry`类。返回的`Registry`对象允许我们以面向对象的方式与注册表交互和读取。
- en: '[PRE46]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The first processing function handles the `WordWheelQuery` key, which stores
    information about searches run by a user within Windows Explorer. We can parse
    this artifact by accessing the key by name from our `explorer_key` object. If
    the key does not exist, we will return an empty list as we do not have any values
    to extract.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个处理函数处理`WordWheelQuery`键，它存储了用户在Windows资源管理器中运行的搜索的信息。我们可以通过从`explorer_key`对象中按名称访问键来解析这个遗物。如果键不存在，我们将返回一个空列表，因为我们没有任何值可以提取。
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'On the other hand, should the key exist, we iterate through the `MRUListEx`
    value, which holds a list of integers that contain the order of the searches.
    Each number in the list matches a value of the same number in the key. For this
    reason, we read the order of the list and interpret the remaining values in the
    order they appear. Each value name is stored as a two-byte integer, and so we
    split this list into two-byte chunks and read the integers with `struct`. We then
    append this value to the list after checking that it does not exist. If it does
    exist in the list, and is `\x00` or `\xFF`, we have reached the end of the `MRUListEx`
    data and break out of the loop:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果这个键存在，我们遍历`MRUListEx`值，它包含一个包含搜索顺序的整数列表。列表中的每个数字都与键中相同数字的值相匹配。因此，我们读取列表的顺序，并按照它们出现的顺序解释剩余的值。每个值的名称都存储为两个字节的整数，所以我们将这个列表分成两个字节的块，并用`struct`读取整数。然后在检查它不存在后，将这个值追加到列表中。如果它存在于列表中，并且是`\x00`或`\xFF`，那么我们已经到达了`MRUListEx`数据的末尾，并且跳出循环：
- en: '[PRE48]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Using our ordered value list, we iterate through it to extract the search terms
    in the order they were run. Since we know the order of use, we can associate the
    last write time of the `WordWheelQuery` key as the timestamp for the search term.
    This timestamp is only associated with the most recently run search. All other
    searches are given the value of `N/A`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们排序后的值列表，我们遍历它以提取按顺序运行的搜索词。由于我们知道使用的顺序，我们可以将`WordWheelQuery`键的最后写入时间作为搜索词的时间戳。这个时间戳只与最近运行的搜索相关联。所有其他搜索都被赋予值`N/A`。
- en: '[PRE49]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Afterwards, we build out the dictionary within the `append` statement, adding
    the time value, username, order (as the count integer), the value's name, and
    the search content. To properly display the search content, we will need to provide
    the key name as a string and decode the text as UTF-16\. This text, once stripped
    of null termination, is ready for the report. The list is built out until all
    values are processed and then ultimately returned.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，在`append`语句中构建字典，添加时间值、用户名、顺序（作为计数整数）、值的名称和搜索内容。为了正确显示搜索内容，我们需要将键名提供为字符串并解码文本为UTF-16。这个文本一旦去除了空终止符，就可以用于报告。直到所有值都被处理并最终返回为止，列表将被构建出来。
- en: '[PRE50]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The next processing function handles the typed paths key, taking the same arguments
    as the prior processing function. We access the key in the same manner and return
    the empty list in case the `TypedPaths` subkey is not found.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个处理函数处理输入的路径键，与之前的处理函数使用相同的参数。我们以相同的方式访问键，并在`TypedPaths`子键未找到时返回空列表。
- en: '[PRE51]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This key does not have an MRU value ordering the typed paths, so we read all
    of its values and add them directly to the list. We can gather the value's name
    and path from this key, adding the username value for additional context. We finish
    this function by returning the list of dictionary values to the `main()` function.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个键没有MRU值来排序输入的路径，所以我们读取它的所有值并直接添加到列表中。我们可以从这个键中获取值的名称和路径，并为了额外的上下文添加用户名值。我们通过将字典值的列表返回给`main()`函数来完成这个函数。
- en: '[PRE52]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Our last processing function handles the `RunMRU` key. If it does not exist
    in the `explorer_key`, we return an empty list as seen before.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一个处理函数处理`RunMRU`键。如果它在`explorer_key`中不存在，我们将像之前一样返回一个空列表。
- en: '[PRE53]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Since this key can be empty, we first check if there are values for us to parse
    and, if there are not, return an empty list to prevent any unnecessary processing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个键可能是空的，我们首先检查是否有值可以解析，如果没有，就返回一个空列表，以防止进行任何不必要的处理。
- en: '[PRE54]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Similar to the `WordWheelQuery`, this key also has an MRU value, which we process
    to learn the correct order of the other values. This list stores items differently,
    as its values are letters as opposed to integers. This makes our job quite simple
    as we directly query for the necessary values using these characters without additional
    processing. We append the order of values to a list and move on.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 与`WordWheelQuery`类似，这个键也有一个MRU值，我们处理它以了解其他值的正确顺序。这个列表以不同的方式存储项目，因为它的值是字母而不是整数。这使得我们的工作非常简单，因为我们直接使用这些字符查询必要的值，而无需额外的处理。我们将值的顺序追加到列表中并继续进行。
- en: '[PRE55]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: As we iterate through the order of values, we begin to build out our dictionary
    of results. First, we handle the timestamps in the same manner as our `WordWheelQuery`
    processor, by assigning a default `N/A` value and updating it with the key's last
    written time if it is the first entry in our ordered list. Following this, we
    append a dictionary containing the relevant entries, such as the username, the
    value order, value name, and value content. This list of dictionaries is returned
    once we have processed all remaining values in the `Run` key.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们遍历值的顺序时，我们开始构建我们的结果字典。首先，我们以与我们的`WordWheelQuery`处理器相同的方式处理时间戳，通过分配默认的`N/A`值并在我们有序列表中的第一个条目时更新它的键的最后写入时间。在此之后，我们附加一个包含相关条目的字典，例如用户名、值顺序、值名称和值内容。一旦我们处理完`Run`键中的所有剩余值，我们将返回这个字典列表。
- en: '[PRE56]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The last function handles the creation of the HTML report. This function starts
    by preparing the path of the code and the `jinja2` environment class. This class
    is used to store shared resources within the library, and we use it to point the
    library to the directory it should search for template files. In our case, we
    want it to look for template HTML files in the current directory, so we use the
    `os` library to get the current working directory and provide it to the `FileSystemLoader()`
    class.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个函数处理HTML报告的创建。这个函数首先准备代码的路径和`jinja2`环境类。这个类用于在库中存储共享资源，并且我们用它来指向库应该搜索模板文件的目录。在我们的情况下，我们希望它在当前目录中查找模板HTML文件，所以我们使用`os`库获取当前工作目录并将其提供给`FileSystemLoader()`类。
- en: '[PRE57]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: With the environment configured, we call the template we would like to use and
    then the `render()` method to create an HTML file with our passed dictionary.
    The `render` function returns a string representing the rendered HTML output with
    the results of the processed data inserted which we write to the output file.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境配置好后，我们调用我们想要使用的模板，然后使用`render()`方法创建一个带有我们传递的字典的HTML文件。`render`函数返回一个表示渲染的HTML输出的字符串，其中包含处理数据插入的结果，我们将其写入输出文件。
- en: '[PRE58]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Let's look at the template file, it starts as any HTML document with the `html`,
    `head`, and `body` tags. While we've included scripts and style sheets in our
    `head` tag, we have omitted the unrelated material here. This information is available
    for review in full in the code bundle.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下模板文件，它像任何HTML文档一样以`html`、`head`和`body`标签开头。虽然我们在`head`标签中包含了脚本和样式表，但我们在这里省略了不相关的材料。这些信息可以在代码包中完整查看。
- en: We start the HTML document with a `div` that holds the processed data tables
    and section headers. To simplify the amount of HTML we need to write, we use a
    `for` loop to gather each of the nested dictionaries from the `nt_data` values.
    The `jinja2` template language allows us to still use Python loops as long as
    they are wrapped in curly brackets, a percentage symbol, and a space character.
    We can also reference properties and methods of objects, allowing us to iterate
    through the values of the `nt_data` dictionary without extra code.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个包含处理过的数据表和部分标题的`div`开始HTML文档。为了简化我们需要编写的HTML量，我们使用一个`for`循环来收集`nt_data`值中的每个嵌套字典。`jinja2`模板语言允许我们仍然使用Python循环，只要它们被包裹在花括号、百分号和空格字符中。我们还可以引用对象的属性和方法，这使我们能够在不需要额外代码的情况下遍历`nt_data`字典的值。
- en: The other commonly used template syntax is shown within the `h2` tag, where
    we access the title attribute we set in the `main()` function. Variables we would
    like the `jinja2` engine to interpret (versus show as literal strings) need to
    be enclosed in double curly brackets and a space character. This will now print
    the section header for each section in our `nt_data` dictionary.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的模板语法显示在`h2`标签中，我们在其中访问了`main()`函数中设置的title属性。我们希望`jinja2`引擎解释的变量（而不是显示为字面字符串）需要用双花括号和空格字符括起来。现在这将为我们的`nt_data`字典中的每个部分打印部分标题。
- en: '[PRE59]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Within this loop, we set up our data table using the `data` tag and create
    a new row to hold the table headers. To generate the headers, we step through
    each of the headers we gathered and assign the value in a nested `for` loop. Notice
    how we need to specify the end of the loop with the `endfor` statement; this is
    required by the templating engine, as (unlike Python) it is not sensitive to indents:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个循环中，我们使用`data`标签设置我们的数据表，并创建一个新行来容纳表头。为了生成表头，我们遍历收集到的每个表头，并在嵌套的`for`循环中分配值。请注意，我们需要使用`endfor`语句指定循环的结束；这是模板引擎所要求的，因为（与Python不同）它对缩进不敏感：
- en: '[PRE60]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Following the table headers, we enter a separate loop to iterate through each
    dictionary in our data list. Inside each table row, we use similar logic as the
    table headers to create another `for` loop to write each value into a cell in
    the row:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在表头之后，我们进入一个单独的循环，遍历我们数据列表中的每个字典。在每个表行内，我们使用与表头相似的逻辑来创建另一个`for`循环，将每个值写入行中的单元格：
- en: '[PRE61]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now that the HTML data table is populated, we close the `for` loop for the
    current data point: we draw a horizontal line and start writing the next artifact''s
    data table. Once we completely iterate through those, we close the outer `for`
    loop and the tags we opened at the start of the HTML report.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在HTML数据表已经填充，我们关闭当前数据点的`for`循环：我们画一条水平线，并开始编写下一个工件的数据表。一旦我们完全遍历了这些，我们关闭外部的`for`循环和我们在HTML报告开头打开的标签。
- en: '[PRE62]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Our generated report is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的报告如下：
- en: '![](../images/00102.jpeg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00102.jpeg)'
- en: There's more...
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们提供了以下一个或多个建议：
- en: Add additional `NTUser` or other easy to review artifacts to the dashboard to
    provide more useful information at a glance
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在仪表板上添加额外的`NTUser`或其他易于审查的工件，以便一目了然地提供更多有用的信息
- en: Add charts, a timeline, or other interactive elements to this dashboard using
    various JavaScript and CSS elements
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种JavaScript和CSS元素在仪表板上添加图表、时间轴或其他交互元素
- en: Provide export options from the dashboard into CSV or Excel spreadsheets with
    additional JavaScript
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从仪表板提供导出选项到CSV或Excel电子表格，并附加JavaScript
- en: The missing link
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失的链接
- en: 'Recipe Difficulty: Medium'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：中等
- en: 'Python Version: 2.7'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating System: Linux'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: Shortcut files, also known as link files, are common across operating system
    platforms. They enable the user to use one file to reference another, located
    elsewhere on the system. On the Windows platform, these link files also record
    historical access to the files they reference. Generally, the creation time of
    a link file represents the first access time of a file with that name, and the
    modification time represents the most recent access time of the file with that
    name. Using this, we can extrapolate a window of activity and learn about how,
    and where, these files were accessed.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 快捷方式文件，也称为链接文件，在操作系统平台上很常见。它们使用户可以使用一个文件引用另一个文件，该文件位于系统的其他位置。在Windows平台上，这些链接文件还记录了对它们引用的文件的历史访问。通常，链接文件的创建时间代表具有该名称的文件的第一次访问时间，修改时间代表具有该名称的文件的最近访问时间。利用这一点，我们可以推断出一个活动窗口，并了解这些文件是如何以及在哪里被访问的。
- en: Getting started
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'This recipe requires the installation of three third-party modules to function:
    `pytsk3`, `pyewf`, and `pylnk`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. All other libraries used in this
    script are present in Python''s standard library.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 此食谱需要安装三个第三方模块才能正常运行：`pytsk3`、`pyewf`和`pylnk`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)，*使用法证证据容器*
    *食谱*。此脚本中使用的所有其他库都包含在Python的标准库中。
- en: 'Navigate to the GitHub repository and download the desired release of the `pylnk`
    library. This recipe was developed using the `pylnk-alpha-20170111` release. Next,
    once the contents of the release are extracted, open a terminal and navigate to
    the extracted directory and execute the following commands:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到GitHub存储库并下载所需版本的`pylnk`库。此处使用的是`pylnk-alpha-20170111`版本。接下来，一旦提取了发布的内容，打开终端并导航到提取的目录，执行以下命令：
- en: '[PRE63]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: To learn more about the `pylnk` library, visit [https://github.com/libyal/liblnk](https://github.com/libyal/liblnk).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`pylnk`库的信息，请访问[https://github.com/libyal/liblnk](https://github.com/libyal/liblnk)。
- en: Lastly, we can check our library's installation by opening a Python interpreter,
    importing `pylnk`, and running the `gpylnk.get_version()` method to ensure we
    have the correct release version.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过打开Python解释器，导入`pylnk`，并运行`gpylnk.get_version()`方法来检查我们的库的安装，以确保我们有正确的发布版本。
- en: How to do it...
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This script will leverage the following steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本将利用以下步骤：
- en: Search for all `lnk` files within the system.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在系统中搜索所有`lnk`文件。
- en: Iterate through discovered `lnk` files and extract relevant attributes.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历发现的`lnk`文件并提取相关属性。
- en: Write all artifacts to a CSV report.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有工件写入CSV报告。
- en: How it works...
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Starting with the imports, we bring in the Sleuth Kit utilities and `pylnk`
    library. We also bring in libraries for argument parsing, writing the CSV reports,
    and `StringIO` to read the Sleuth Kit objects as files:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 从导入开始，我们引入Sleuth Kit实用程序和`pylnk`库。我们还引入了用于参数解析、编写CSV报告和`StringIO`读取Sleuth Kit对象作为文件的库：
- en: '[PRE64]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: This recipe's command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `CSV_REPORT`, which represent the path to the evidence file,
    the type of evidence file, and the desired output path to the CSV report, respectively.
    These three arguments are passed to the `main()` function.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此食谱的命令行处理程序接受三个位置参数，`EVIDENCE_FILE`、`IMAGE_TYPE`和`CSV_REPORT`，分别代表证据文件的路径、证据文件的类型和CSV报告的期望输出路径。这三个参数将传递给`main()`函数。
- en: '[PRE65]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The `main()` function begins with creating the `TSKUtil` object used to interpret
    the evidence file and iterate through the filesystem to find files ending in `lnk`.
    If there are not any `lnk` files found on the system, the script alerts the user
    and exits. Otherwise, we specify columns representing the data attributes we want
    to store for each of the `lnk` files. While there are other attributes available,
    these are some of the more relevant ones we extract in this recipe:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数从创建`TSKUtil`对象开始，该对象用于解释证据文件并遍历文件系统以查找以`lnk`结尾的文件。如果在系统上找不到任何`lnk`文件，则脚本会提醒用户并退出。否则，我们指定代表我们要为每个`lnk`文件存储的数据属性的列。虽然还有其他可用的属性，但这些是我们在此食谱中提取的一些更相关的属性：'
- en: '[PRE66]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Next, we iterate through the discovered `lnk` files, opening each as a file
    using the `open_file_as_lnk()` function. The returned object is an instance of
    the `pylnk` library, ready for us to read the attributes from. We initialize the
    attribute dictionary with the file's name and path and then iterate through the
    columns we specified in the `main()` function. For each of the columns, we try
    to read the specified attribute value, and, if we are unable to, store an "`N/A`"
    value otherwise. These attributes are stored in the `lnk_data` dictionary which
    is appended to the `parsed_lnks` list once all attributes are extracted. After
    this process completes for each `lnk` file, we pass this list, along with the
    output path, and column names, to the `write_csv()` method.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们遍历发现的`lnk`文件，使用`open_file_as_lnk()`函数将每个文件作为文件打开。返回的对象是`pylnk`库的一个实例，可以让我们读取属性。我们使用文件的名称和路径初始化属性字典，然后遍历我们在`main()`函数中指定的列。对于每个列，我们尝试读取指定的属性值，如果无法读取，则存储“N/A”值。这些属性存储在`lnk_data`字典中，一旦提取了所有属性，就将其附加到`parsed_lnks`列表中。完成每个`lnk`文件的这个过程后，我们将此列表与输出路径和列名一起传递给`write_csv()`方法。
- en: '[PRE67]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'To open our `pytsk` file object as a `pylink` object, we use the `open_file_as_lnk()`
    function which operates like other similarly named functions throughout this chapter.
    This function reads the entire file, using the `read_random()` method and file
    size property, into a `StringIO` buffer that is then passed into a `pylnk` file
    object. Reading in this manner allows us to read the data as a file without needing
    to cache it to the disk. Once we have loaded the file into our `lnk` object, we
    return it to the `main()` function:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的`pytsk`文件对象作为`pylink`对象打开，我们使用`open_file_as_lnk()`函数，该函数类似于本章中的其他同名函数。此函数使用`read_random()`方法和文件大小属性将整个文件读入`StringIO`缓冲区，然后将其传递给`pylnk`文件对象。以这种方式读取允许我们以文件的形式读取数据，而无需将其缓存到磁盘。一旦我们将文件加载到我们的`lnk`对象中，我们将其返回给`main()`函数：
- en: '[PRE68]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The last function is the common CSV writer, which uses the `csv.DictWriter`
    class to iterate through the data structure and write the relevant fields to a
    spreadsheet. The order of the columns list defined in the `main()` function determines
    their order here as the `fieldnames` argument. One could change that order, if
    desired, to modify the order in which they are displayed in the resulting spreadsheet.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个函数是常见的CSV写入器，它使用`csv.DictWriter`类来遍历数据结构，并将相关字段写入电子表格。在`main()`函数中定义的列列表的顺序决定了它们在这里作为`fieldnames`参数的顺序。如果需要，可以更改该顺序，以修改它们在生成的电子表格中显示的顺序。
- en: '[PRE69]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'After running the script, we can view the results in a single CSV report as
    seen in the following two screenshots. Since there are many visible columns, we
    have elected to display only a few for the readability sake:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，我们可以在单个CSV报告中查看结果，如下两个屏幕截图所示。由于有许多可见列，我们选择仅显示一些以便阅读：
- en: '![](../images/00103.jpeg)![](../images/00104.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00103.jpeg)![](../images/00104.jpeg)'
- en: There's more...
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们提供了一个或多个建议如下：
- en: Add checks to see if the target file still exists
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加检查以查看目标文件是否仍然存在
- en: Identify target locations on remote or removable volumes
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别远程或可移动卷上的目标位置
- en: Add support for parsing jumplists
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加对解析跳转列表的支持
- en: Searching high and low
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 四处搜寻
- en: 'Recipe difficulty: Hard'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱难度：困难
- en: 'Python version: 2.7'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Python版本：2.7
- en: 'Operating system: Linux'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：Linux
- en: Most modern operating systems maintain an index of files and other data content
    stored on the system. These indexes allow for more efficient searches across file
    formats, emails, and other content found on the system's volumes. On Windows,
    such an index is found in the `Windows.edb` file. This database is stored in the
    **Extensible Storage Engine** (**ESE**) file format and found within the `ProgramData`
    directory. We will leverage another library from the `libyal` project to parse
    this file to extract information about the indexed content on the system.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代操作系统都维护着系统中存储的文件和其他数据内容的索引。这些索引允许在系统卷上更有效地搜索文件格式、电子邮件和其他内容。在Windows上，这样的索引可以在`Windows.edb`文件中找到。这个数据库以**可扩展存储引擎**（**ESE**）文件格式存储，并位于`ProgramData`目录中。我们将利用`libyal`项目的另一个库来解析这个文件，以提取有关系统上索引内容的信息。
- en: Getting started
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'This recipe requires the installation of four third-party modules to function:
    `pytsk3`, `pyewf`, `pyesedb`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    on installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *One man''s trash is a forensic examiner''s treasure*
    recipe for details on installing `unicodecsv`. All other libraries used in this
    script are present in Python''s standard library.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此食谱需要安装四个第三方模块才能运行：`pytsk3`、`pyewf`、`pyesedb`和`unicodecsv`。有关安装`pytsk3`和`pyewf`模块的详细说明，请参阅[第8章](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe)中的*使用取证证据容器*
    *食谱*。同样，有关安装`unicodecsv`的详细信息，请参阅*一个人的垃圾是取证人员的宝藏*食谱中的*入门*部分。此脚本中使用的所有其他库都包含在Python的标准库中。
- en: 'Navigate to the GitHub repository and download the desired release for each
    library. This recipe was developed using the `libesedb-experimental-20170121`
    release. Once the contents of the release are extracted, open a terminal, navigate
    to the extracted directory, and execute the following commands:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 转到GitHub存储库，并下载每个库的所需版本。此食谱是使用`libesedb-experimental-20170121`版本开发的。提取版本的内容后，打开终端，转到提取的目录，并执行以下命令：
- en: '[PRE70]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: To learn more about the `pyesedb` library, visit [**https://github.com/libyal/libesedb**](https://github.com/libyal/libesedb)**.**
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`pyesedb`库的信息，请访问[**https://github.com/libyal/libesedb**](https://github.com/libyal/libesedb)**。**
- en: Lastly, we can check our library's installation by opening a Python interpreter,
    importing `pyesedb`, and running the `epyesedb.get_version()` method to ensure
    we have the correct release version.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过打开Python解释器，导入`pyesedb`，并运行`epyesedb.get_version()`方法来检查我们的库安装是否正确。
- en: How to do it...
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'To draft this script we will need to:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 起草此脚本，我们需要：
- en: Recurse the `ProgramData` directory to search for the `Windows.edb` file.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归搜索`ProgramData`目录，查找`Windows.edb`文件。
- en: Iterate through discovered `Windows.edb` files (though there should really only
    be one) and open the files using the `pyesedb` library.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历发现的`Windows.edb`文件（虽然实际上应该只有一个），并使用`pyesedb`库打开文件。
- en: Process each of the files to extract key columns and attributes.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理每个文件以提取关键列和属性。
- en: Write these key columns and attributes to the report.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些关键列和属性写入报告。
- en: How it works...
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The imports here include those libraries we''ve used for most recipes in the
    chapter for argument parsing, string buffer file-like objects, and the `TSK` utilities.
    We also import the `unicodecsv` library to handle any Unicode objects in the CSV
    report, the `datetime` library to assist with timestamp parsing, and the `struct`
    module to help make sense of the binary data we read. Additionally, we define
    a global variable, `COL_TYPES`, that aliases the column types from the `pyesedb`
    library, used to help identify the types of data that we will extract later in
    the code:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这里导入的库包括我们在本章大多数配方中使用的用于参数解析、字符串缓冲文件样对象和`TSK`实用程序的库。我们还导入`unicodecsv`库来处理CSV报告中的任何Unicode对象，`datetime`库来辅助时间戳解析，以及`struct`模块来帮助理解我们读取的二进制数据。此外，我们定义了一个全局变量`COL_TYPES`，它将`pyesedb`库中的列类型别名，用于帮助识别我们稍后在代码中将提取的数据类型：
- en: '[PRE71]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This recipe's command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `CSV_REPORT`, which represent the path to the evidence file,
    the type of evidence file, and the desired output path to the CSV report, respectively.
    These three arguments are passed to the `main()` function.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 该配方的命令行处理程序接受三个位置参数，`EVIDENCE_FILE`，`IMAGE_TYPE`和`CSV_REPORT`，它们分别表示证据文件的路径，证据文件的类型以及所需的CSV报告输出路径。这三个参数被传递给`main()`函数。
- en: '[PRE72]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The `main()` function opens the evidence and searches for the `Windows.edb`
    file within the `ProgramData` directory. If one or more files are found, we iterate
    through the list and open each ESE database for further processing with the `process_windows_search()`
    function. This function returns the spreadsheet column headers to use and a list
    of dictionaries containing the data to include in the report. This information
    is then written to the output CSV for review by the `write_csv()` method:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数打开证据并搜索`ProgramData`目录中的`Windows.edb`文件。如果找到一个或多个文件，我们会遍历列表并打开每个ESE数据库，以便使用`process_windows_search()`函数进行进一步处理。该函数返回要使用的电子表格列标题以及包含报告中要包含的数据的字典列表。然后将此信息写入输出CSV，供`write_csv()`方法审查：'
- en: '[PRE73]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Reading the responsive ESE database requires the `open_file_as_esedb()` function.
    This code block uses similar logic to the previous recipes to read the file into
    a `StringIO` object and open the file-like object with the library. Note, this
    could cause errors on your system if the file is rather large or your machine
    has lower amounts of memory. You can use the built-in `tempfile` library to cache
    the file to a temporary location on disk, reading from there if you would prefer.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 读取响应的ESE数据库需要`open_file_as_esedb()`函数。此代码块使用与之前配方类似的逻辑，将文件读入`StringIO`对象并使用库打开文件样对象。请注意，如果文件相当大或您的计算机内存较少，这可能会在您的系统上引发错误。您可以使用内置的`tempfile`库将文件缓存到磁盘上的临时位置，然后从那里读取，如果您愿意的话。
- en: '[PRE74]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Our `process_windows_search()` function starts with column definitions. While
    our previous recipe used a simple list of columns, the `pyesedb` library takes
    a column index as an input to retrieve a value from a row within a table. For
    this reason, our column list must consist of tuples, where the first element is
    a number (the index) and the second is the string description. Since the description
    isn''t used in the function to select columns, we name these in the manner we
    would like them displayed in the report. For this recipe, we have defined the
    following column indexes and names:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`process_windows_search()`函数从列定义开始。虽然我们之前的配方使用了一个简单的列列表，但`pyesedb`库需要一个列索引作为输入，以从表中的行中检索值。因此，我们的列列表必须由元组组成，其中第一个元素是数字（索引），第二个元素是字符串描述。由于描述在函数中未用于选择列，我们将其命名为我们希望它们在报告中显示的方式。对于本配方，我们已定义了以下列索引和名称：
- en: '[PRE75]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: After we define the columns of interest, we access the `SystemIndex_0A` table,
    which contains the indexed file, mail, and other entries. We iterate through the
    records within the table, building a `record_info` dictionary of the column values
    for each record that will eventually be appended to the `table_data` list. A second
    loop iterates through the columns we defined earlier and attempts to extract the
    value and value type for each column in the record.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义感兴趣的列之后，我们访问`SystemIndex_0A`表，其中包含索引文件、邮件和其他条目。我们遍历表中的记录，为每个记录构建一个`record_info`字典，其中包含每个记录的列值，最终将其附加到`table_data`列表中。第二个循环遍历我们之前定义的列，并尝试提取每个记录中的列的值和值类型。
- en: '[PRE76]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Using the `COL_TYPES` global variable we defined earlier, we can reference the
    various data types and ensure we are interpreting the values correctly. The logic
    in the following code block focuses on interpreting the values correctly based
    on their data type. First, we handle dates, which may be stored as Windows `FILETIME`
    values. We attempt to convert the `FILETIME` value, if possible, or present the
    date value in hexadecimal if not. The next statement checks for text values, interpreting
    the value with the `pyesedb` `get_value_data_as_string()` function or as a UTF-16
    big-endian and replacing any unrecognized character for completeness.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前定义的`COL_TYPES`全局变量，我们可以引用各种数据类型，并确保我们正确解释值。以下代码块中的逻辑侧重于根据其数据类型正确解释值。首先，我们处理日期，日期可能存储为Windows
    `FILETIME`值。我们尝试转换`FILETIME`值（如果可能），或者如果不可能，则以十六进制呈现日期值。接下来的语句检查文本值，使用`pyesedb`的`get_value_data_as_string()`函数或作为UTF-16大端，并替换任何未识别的字符以确保完整性。
- en: 'We then individually handle integer and Boolean data type interpretation using
    the `pyesedb` `get_value_data_as_integer()` function and a simple comparison statement,
    respectively. Specifically, we check if the `rec_val` is equal to `"\x01"` and
    allow `rec_val` to be set `True` or `False` based on that comparison. If none
    of these data types are valid, we interpret the value as hex and store it with
    the associated column name before appending the value to the table:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`pyesedb`的`get_value_data_as_integer()`函数和一个简单的比较语句分别处理整数和布尔数据类型的解释。具体来说，我们检查`rec_val`是否等于`"\x01"`，并允许根据该比较将`rec_val`设置为`True`或`False`。如果这些数据类型都不合法，我们将该值解释为十六进制，并在将该值附加到表之前将其与相关列名一起存储：
- en: '[PRE77]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: We then return a tuple to our calling function, where the first element is the
    list of names of the columns in the `report_cols` dictionary and the second is
    a list of data dictionaries.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将一个元组返回给我们的调用函数，其中第一个元素是`report_cols`字典中列的名称列表，第二个元素是数据字典的列表。
- en: '[PRE78]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Borrowing our logic from our date-parsing recipe in [Chapter 7](part0212.html#6A5N80-260f9401d2714cb9ab693c4692308abe),
    *Log-Based Artifact Recipes*, we implement a function to parse the Windows `FILETIME`
    value into a human-readable state. This accepts an integer value as input and
    returns a human-readable string:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴我们在[第7章](part0212.html#6A5N80-260f9401d2714cb9ab693c4692308abe)中日期解析食谱中的逻辑，*基于日志的工件食谱*，我们实现了一个将Windows
    `FILETIME`值解析为可读状态的函数。这个函数接受一个整数值作为输入，并返回一个可读的字符串：
- en: '[PRE79]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The last function is the CSV report writer, which writes the columns and the
    rows of collected information to the open CSV spreadsheet using the `DictWriter`
    class. While we selected a subset of the available columns at the outset, there
    are many more to choose from that may be useful in varying case types. Therefore,
    we recommend taking a look at all available columns to better understand this
    recipe and what columns may or may not be useful for you.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个函数是CSV报告编写器，它使用`DictWriter`类将收集到的信息的列和行写入到打开的CSV电子表格中。虽然我们在一开始选择了一部分可用的列，但还有许多可供选择的列，可能对不同的案例类型有用。因此，我们建议查看所有可用的列，以更好地理解这个食谱，以及哪些列对您可能有用或无用。
- en: '[PRE80]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'After running the recipe, we can review the output CSV shown here. As there
    are many columns to this report, we have highlighted a few interesting ones in
    the following two screenshots:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 运行食谱后，我们可以查看这里显示的输出CSV。由于这份报告有很多列，我们在接下来的两个屏幕截图中突出显示了一些有趣的列：
- en: '![](../images/00105.jpeg)![](../images/00106.jpeg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00105.jpeg)![](../images/00106.jpeg)'
- en: There's more...
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可以进一步改进。我们提供了一个或多个以下建议：
- en: Add support to check for the existence of referenced files and folders
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加支持以检查引用文件和文件夹的存在。
- en: Write our `Windows.edb` file to a temporary location to relieve memory pressure
    when parsing large databases with the Python `tempfile` library
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python的`tempfile`库将我们的`Windows.edb`文件写入临时位置，以减轻解析大型数据库时的内存压力
- en: Add more columns or create separate (targeted) reports using more of the over
    300 available columns in the table
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在表中添加更多列或创建单独的（有针对性的）报告，使用表中超过300个可用列中的更多列
