- en: Algorithms for Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理算法
- en: This chapter introduces algorithms for **natural language processing** (**NLP**).
    This chapter proceeds from the theoretical to the practical in a progressive manner.
    It will first introduce the fundamentals of NLP, followed by the basic algorithms.
    Then, it will look at one of the most popular neural networks that is widely used
    to design and implement solutions for important use cases for textual data. We
    will then look at the limitations of NLP before finally learning how we can use
    NLP to train a machine learning model that can predict the polarity of movie reviews.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了**自然语言处理**（**NLP**）的算法。本章从理论到实践逐步进行。它将首先介绍NLP的基础知识，然后介绍基本算法。然后，它将研究最流行的神经网络之一，该网络被广泛用于设计和实施文本数据的重要用例的解决方案。最后，我们将研究NLP的局限性，最后学习如何使用NLP来训练一个可以预测电影评论极性的机器学习模型。
- en: 'This chapter will consist of the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将包括以下部分：
- en: Introducing NLP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍NLP
- en: Bag-of-words-based (BoW-based) NLP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于词袋（BoW）的NLP
- en: Introduction to word embedding
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入介绍
- en: Use of recurrent neural networks for NLP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用递归神经网络进行NLP
- en: Using NLP for sentiment analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NLP进行情感分析
- en: 'Case study: movie review sentiment analysis'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究：电影评论情感分析
- en: By the end of this chapter, you will understand the basic techniques that are
    used for NLP. You should be able to understand how NLP can be used to solve some
    interesting real-world problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章结束时，您将了解用于NLP的基本技术。您应该能够理解NLP如何用于解决一些有趣的现实世界问题。
- en: Let's start with the basic concepts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基本概念开始。
- en: Introducing NLP
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍NLP
- en: 'NLP is used to investigate methodologies to formalize and formulate the interactions
    between computers and human (natural) languages. NLP is a comprehensive subject,
    and involves using computer linguistics algorithms and human–computer interaction
    technologies and methodologies to process complex unstructured data. NLP can be
    used for a variety of cases, including the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: NLP用于研究形式化和规范化计算机与人类（自然）语言之间的交互。NLP是一个综合性的学科，涉及使用计算机语言学算法和人机交互技术和方法来处理复杂的非结构化数据。NLP可以用于各种情况，包括以下情况：
- en: '**Topic identification**:To discover topics in a text repository and classify
    the documents in the repository according to the discovered topics'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题识别**：发现文本存储库中的主题，并根据发现的主题对存储库中的文档进行分类'
- en: '**Sentiment analysis**: To classify the text according to the positive or negative
    sentiments that it contains'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：根据文本中包含的积极或消极情感对文本进行分类'
- en: '**Machine translation**: To translate the text from one spoken human language
    to another'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：将文本从一种口头人类语言翻译成另一种口头人类语言'
- en: '**Text to speech**: To convert spoken words into text'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本转语音**：将口头语言转换为文本'
- en: '**Subjective interpretation**: To intelligently interpret a question and answer
    it using the information available'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主观解释**：智能地解释问题并利用可用信息回答问题'
- en: '**Entity recognition**:To identify entities (such as a person, place, or thing)
    from text'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实体识别**：从文本中识别实体（如人、地点或物品）'
- en: '**Fake news detection**: To flag fake news based on the content'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假新闻检测**：根据内容标记假新闻'
- en: Let's start by looking at some of the terminology that is used when discussing
    NLP.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一些在讨论NLP时使用的术语。
- en: Understanding NLP terminology
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解NLP术语
- en: NLP is a comprehensive subject. In the literature surrounding a certain field,
    we will observe that, sometimes, different terms are  used to specify the same
    thing. We will start by looking at some of the basic terminology related to NLP.
    Let's start with normalization, which is one of the basic kinds of NLP processing,
    usually performed on the input data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是一个综合性的学科。在围绕某一领域的文献中，我们会观察到，有时会使用不同的术语来指定相同的事物。我们将从一些与NLP相关的基本术语开始。让我们从规范化开始，这是一种基本的NLP处理，通常在输入数据上执行。
- en: Normalization
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规范化
- en: 'Normalization is performed on the input text data to improve its quality in
    the context of training a machine learning model. Normalization usually involves
    the following processing steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化是对输入文本数据进行的处理，以提高其在训练机器学习模型的情况下的质量。规范化通常包括以下处理步骤：
- en: Converting all text to uppercase or lowercase
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有文本转换为大写或小写
- en: Removing punctuation
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除标点符号
- en: Removing numbers
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除数字
- en: Note that although the preceding processing steps are typically needed, the
    actual processing steps depend on the problem that we want to solve. They will
    vary from use case to use case—for example, if the numbers in the text represent
    something that may have some value in the context of the problem that we are trying
    to solve, then we may not need to remove the numbers from the text in the normalization
    phase.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管通常需要前面的处理步骤，但实际的处理步骤取决于我们想要解决的问题。它们会因用例而异，例如，如果文本中的数字代表了在我们尝试解决的问题的情境中可能具有一些价值的东西，那么我们在规范化阶段可能就不需要从文本中去除数字。
- en: Corpus
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语料库
- en: The group of input documents that we are using to solve the problem in question
    is called the **corpus**. The corpus acts as the input data for the NLP problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来解决问题的输入文档组称为**语料库**。语料库充当NLP问题的输入数据。
- en: Tokenization
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化
- en: 'When we are working with NLP, the first job is to divide the text into a list
    of tokens. This process is called **tokenization**. The granularity of the resulting
    tokens will vary based on the objective—for example, each token can consist of
    the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用NLP时，第一项工作是将文本分成一个标记列表。这个过程称为**标记化**。由于目标的不同，生成的标记的粒度也会有所不同，例如，每个标记可以包括以下内容：
- en: A word
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个词
- en: A combination of words
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组单词的组合
- en: A sentence
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个句子
- en: A paragraph
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个段落
- en: Named entity recognition
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: In NLP, there are many use cases where we need to identify certain words and
    numbers from unstructured data as belonging to predefined categories, such as
    phone numbers, postal codes, names, places, or countries. This is used to provide
    a structure to the unstructured data. This process is called **named entity recognition**
    (**NER**).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，有许多用例需要从非结构化数据中识别特定的单词和数字，这些单词和数字属于预定义的类别，如电话号码、邮政编码、姓名、地点或国家。这用于为非结构化数据提供结构。这个过程称为**命名实体识别**（**NER**）。
- en: Stopwords
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停用词
- en: After word-level tokenization, we have a list of words that are used in the
    text. Some of these words are common words that are expected to appear in almost
    every document. These words do not provide any additional insight into the documents
    that they appear in. These words are called **stopwords**. They are usually removed
    in the data-processing phase. Some examples of stopwords are *was*, *we,* and
    *the*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在单词级别的标记化之后，我们得到了文本中使用的单词列表。其中一些单词是常见单词，预计几乎会出现在每个文档中。这些单词不会为它们出现在的文档提供任何额外的见解。这些单词被称为**停用词**。它们通常在数据处理阶段被移除。一些停用词的例子是*was*、*we*和*the*。
- en: Sentiment analysis
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: Sentimental analysis, or opinion mining, is the process of extracting positive
    or negative sentiments from the text.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析，或者称为意见挖掘，是从文本中提取正面或负面情感的过程。
- en: Stemming and lemmatization
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取和词形还原
- en: In textual data, most words are likely to be present in slightly different forms.
    Reducing each word to its origin or stem in a family of words is called **stemming**.
    It is used to group words based on their similar meanings to reduce the total
    number of words that need to be analyzed. Essentially, stemming reduces the overall
    conditionality of the problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本数据中，大多数单词可能以稍微不同的形式存在。将每个单词减少到其原始形式或词干所属的词族中称为**词干提取**。它用于根据它们的相似含义对单词进行分组，以减少需要分析的单词总数。基本上，词干提取减少了问题的整体条件性。
- en: For example, {use, used, using, uses} => use.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，{use, used, using, uses} => use。
- en: The most common algorithm for stemming English is the Porter algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 英语词干提取的最常见算法是波特算法。
- en: Stemming is a crude process that can result in chopping off the ends of words.
    This may result in words that are misspelled. For many use cases, each word is
    just an identifier of a level in our problem space, and misspelled words do not
    matter. If correctly spelled words are required, then lemmatization should be
    used instead of stemming.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是一个粗糙的过程，可能会导致词尾被截断。这可能导致拼写错误的单词。对于许多用例来说，每个单词只是我们问题空间中的一个级别的标识符，拼写错误的单词并不重要。如果需要正确拼写的单词，那么应该使用词形还原而不是词干提取。
- en: Algorithms lack common sense. For the human brain, treating similar words the
    same is straightforward. For an algorithm, we have to guide it and provide the
    grouping criteria.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 算法缺乏常识。对于人类大脑来说，将类似的单词视为相同是很简单的。对于算法，我们必须引导它并提供分组标准。
- en: 'Fundamentally, there are three different ways of implementing NLP. These three
    techniques, which are different in terms of sophistication, are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，有三种不同的NLP实现方法。这三种技术在复杂性方面有所不同，如下所示：
- en: '**Bag-of-words**-based (Bo**W**-based) NLP'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于词袋模型（Bo**W**-based）的NLP
- en: Traditional NLP classifiers
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的NLP分类器
- en: Using deep learning for NLP
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习进行自然语言处理
- en: NLTK
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLTK
- en: The **Natural Language Toolkit** (**NLTK**) is the most widely utilized package
    for handling NLP tasks in Python. NLTK is one of the oldest and most popular Python
    libraries used for NLP. NLTK is great because it basically provides a jumpstart
    to building any NLP process by giving you the basic tools that you can then chain
    together to accomplish your goal rather than building all those tools from scratch.
    A lot of tools are packaged into NLTK, and in the next section, we will download
    the package and explore some of those tools.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言工具包**（**NLTK**）是Python中处理NLP任务最广泛使用的包。NLTK是用于NLP的最古老和最流行的Python库之一。NLTK非常好，因为它基本上为构建任何NLP流程提供了一个起点，它为您提供了基本工具，然后您可以将它们链接在一起以实现您的目标，而不是从头开始构建所有这些工具。许多工具都打包到了NLTK中，在下一节中，我们将下载该包并探索其中的一些工具。'
- en: Let's look at BoW-based NLP.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看基于词袋模型的NLP。
- en: BoW-based NLP
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于词袋模型的NLP
- en: The representation of input text as a bag of tokens is called **BoW-based processing**.
    The drawback of using BoW is that we discard most of the grammar and tokenization,
    which sometimes results in losing the context of the words. In the BoW approach,
    we first quantify the importance of each word in the context of each document
    that we want to analyze.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入文本表示为一组标记的过程称为**基于词袋模型的处理**。使用词袋模型的缺点是我们丢弃了大部分语法和标记化，这有时会导致丢失单词的上下文。在词袋模型的方法中，我们首先量化要分析的每个文档中每个单词的重要性。
- en: 'Fundamentally, there are three different ways of quantifying the importance
    of the words in the context of each document:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，有三种不同的方法来量化每个文档中单词的重要性：
- en: '**Binary**: A feature will have a value of 1 if the word appears in the text
    or 0 otherwise.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二进制**：如果单词出现在文本中，则特征的值为1，否则为0。'
- en: '**Count**: A feature will have the number of times the word appears in the
    text as its value or 0 otherwise.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数**：特征将以单词在文本中出现的次数作为其值，否则为0。'
- en: '**Term frequency/Inverse document frequency**: The value of the feature will
    be a ratio of how unique a word is in a single document to how unique it is in
    the entire corpus of documents. Obviously, for common words such as the, in, and
    so on (known as stop words), the **term frequency–inverse document frequency**
    (**TF-IDF**) score will be low. For more unique words—for instance, domain-specific
    terms—the score will be higher.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词项频率/逆文档频率**：特征的值将是单个文档中单词的独特程度与整个文档语料库中单词的独特程度的比率。显然，对于常见单词，如the、in等（称为停用词），**词项频率-逆文档频率**（**TF-IDF**）得分将很低。对于更独特的单词，例如领域特定术语，得分将更高。'
- en: Note that by using BoW, we are throwing away information—namely, the order of
    the words in our text. This often works, but may lead to reduced accuracy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过使用词袋模型，我们丢失了信息——即文本中单词的顺序。这通常有效，但可能会导致准确性降低。
- en: Let's look at a specific example. We will train a model that can classify the
    reviews of a restaurant as negative or positive. The input file is a strutted
    file where the reviews will be classified as positive or negative.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子。我们将训练一个模型，可以将餐厅的评论分类为负面或正面。输入文件是一个结构化文件，其中评论将被分类为正面或负面。
- en: For this, let's first process the input data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，让我们首先处理输入数据。
- en: 'The processing steps are defined in the following diagram:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 处理步骤在下图中定义：
- en: '![](assets/70cf5585-8206-45a8-9b86-8aa31f914b37.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/70cf5585-8206-45a8-9b86-8aa31f914b37.png)'
- en: 'Let''s implement this processing pipeline by going through the following steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤实现这个处理流程：
- en: 'First, we import the packages that we need:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入我们需要的包：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we import the dataset from a `CSV` file:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们从`CSV`文件中导入数据集：
- en: '![](assets/99fd1e80-976c-43b6-bf61-3e878651d157.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/99fd1e80-976c-43b6-bf61-3e878651d157.png)'
- en: 'Next, we clean the data:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们清理数据：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let''s define the features (represented by `y`) and the label (represented
    by `X`):'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们定义特征（用`y`表示）和标签（用`X`表示）：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s divide the data into testing and training data:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将数据分成测试数据和训练数据：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For training the model, we are using the naive Bayes algorithm:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于训练模型，我们使用朴素贝叶斯算法：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s predict the test set results:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们预测测试集的结果：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The confusion matrix looks like this:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混淆矩阵如下所示：
- en: '![](assets/d3c55c12-09ac-460b-a3c2-915340b89103.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d3c55c12-09ac-460b-a3c2-915340b89103.png)'
- en: Looking at the confusion matrix, we can estimate the misclassification.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察混淆矩阵，我们可以估计误分类情况。
- en: Introduction to word embedding
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入简介
- en: In the preceding section, we studied how we can perform NLP by using BoW as
    the abstraction for the input text data. One of the major advancements in NLP
    is our ability to create a meaningful numeric representation of words in the form
    of dense vectors. This technique is called word embedding. Yoshua Bengio first
    introduced the term in his paper *A Neural Probabilistic Language Model*. Each
    word in an NLP problem can be thought of as a categorical object. Mapping each
    of the words to a list of numbers represented as a vector is called word embedding.
    In other words, the methodologies that are used to convert words into real numbers
    are called word embedding. A differentiating feature of embedding is that it uses
    a dense vector, instead of using traditional approaches that use sparse matrix
    vectors.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们学习了如何使用词袋模型作为输入文本数据的抽象来执行NLP。NLP的一个主要进展是我们能够以密集向量的形式创建单词的有意义的数值表示能力。这种技术称为词嵌入。Yoshua
    Bengio首次在他的论文《神经概率语言模型》中引入了这个术语。NLP问题中的每个词都可以被视为一个分类对象。将每个词映射到表示为向量的数字列表称为词嵌入。换句话说，用于将单词转换为实数的方法称为词嵌入。嵌入的一个区别特征是它使用密集向量，而不是使用传统方法使用稀疏矩阵向量。
- en: 'There are basically two problems with using BoW for NLP:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词袋模型进行NLP存在两个基本问题：
- en: '**Loss of semantic context**: When we tokenize the data, its context is lost.
    A word may have different meanings based on where it is used in the sentence;
    this becomes even more important when interpreting complex human expressions,
    such as humor or satire.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义上下文的丢失**：当我们对数据进行标记化时，它的上下文就丢失了。一个词可能根据它在句子中的使用位置有不同的含义；当解释复杂的人类表达时，比如幽默或讽刺，这变得更加重要。'
- en: '**Sparse input**: When we tokenize, each word becomes a feature. As we saw
    in the preceding example, each word is a feature. It results in sparse data structures.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏输入**：当我们进行标记化时，每个单词都成为一个特征。正如我们在前面的例子中看到的，每个单词都是一个特征。这导致了稀疏的数据结构。'
- en: The neighborhood of a word
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个词的邻域
- en: A key insight into how to present textual data (specifically, individual words,
    or lexemes) to an algorithm comes from linguistics. In word embedding, we pay
    attention to the neighborhood of each word and use it to establish its meaning
    and importance. Neighbourhood of a word is the set of words surrounds a particular
    word. The context of a word is determined by its neighborhood.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如何向算法呈现文本数据（特别是单词或词元）的关键见解来自语言学。在词嵌入中，我们关注每个词的邻域，并用它来确定其含义和重要性。一个词的邻域是围绕特定词的一组词。一个词的上下文是由它的邻域决定的。
- en: Note that in BoW, a word loses its context, as its context is from the neighborhood
    that it is in.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在词袋模型中，一个词失去了它的上下文，因为它的上下文来自它所在的邻域。
- en: Properties of word embeddings
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入的特性
- en: 'Good word embeddings exhibit the following four properties:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的词嵌入具有以下四个特性：
- en: '**They are dense**: In fact, embeddings are essentially factor models. As such,
    each component of the embedding vector represents a quantity of a (latent) feature.
    We typically do not know what that feature represents; however, we will have very
    few—if any—zeros that will cause a sparse input.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它们是密集的**：实际上，嵌入本质上是因子模型。因此，嵌入向量的每个组件代表一个（潜在）特征的数量。通常我们不知道该特征代表什么；但是，我们将有非常少的（如果有的话）零值，这将导致稀疏输入。'
- en: '**They are low dimensional**: An embedding has a predefined dimensionality
    (chosen as a hyperparameter). We saw earlier that in the BoW representation we
    needed |*V*| inputs for each word, so that the total size of the input was |*V*|
    * *n* where *n* is the number of words we use as input. With word embeddings,
    our input size will be *d* * *n*, where *d* is typically between 50 and 300\.
    Considering the fact that large text corpora are often much larger than 300 words,
    this means that we have large savings in input size, which we saw can lead to
    better accuracy for a smaller total number of data instances.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它们是低维的**：嵌入具有预定义的维度（作为超参数选择）。我们之前看到，在BoW表示中，我们需要为每个单词输入|*V*|，因此输入的总大小为|*V*|
    * *n*，其中*n*是我们用作输入的单词数。使用单词嵌入，我们的输入大小将是*d* * *n*，其中*d*通常在50到300之间。考虑到大型文本语料库通常远大于300个单词，这意味着我们在输入大小上有很大的节省，我们看到这可能导致更小的数据实例总数的更高准确性。'
- en: '**They embed domain semantics**: This property is probably the most surprising,
    but also the most useful. When properly trained, embeddings learn about the meaning
    of their domain.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它们嵌入领域语义**：这个属性可能是最令人惊讶的，但也是最有用的。当正确训练时，嵌入会学习关于其领域的含义。'
- en: '**Generalize easily**: Finally, web embedding is capable of picking up generalized
    abstract patterns—for example, we can train on (the embeddings of) cat, deer,
    dog, and so on, and the model will understand that we mean animals. Note that
    the model was never trained for sheep, and yet the model will still correctly
    classify it. By using embedding, we can expect to receive the correct answer.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于泛化**：最后，网络嵌入能够捕捉到一般的抽象模式——例如，我们可以对（嵌入的）猫、鹿、狗等进行训练，模型将理解我们指的是动物。请注意，模型从未接受过对羊的训练，但模型仍然会正确分类它。通过使用嵌入，我们可以期望得到正确的答案。'
- en: Now let us explore, how we can use RNNs for Natural Language Processing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨一下，我们如何使用RNN进行自然语言处理。
- en: Using RNNs for NLP
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN进行NLP
- en: 'An RNN is a traditional feed-forward network with feedback. A simple way of
    thinking about an RNN is that it is a neural network with states. RNNs are used
    with any type of data for generating and predicting various sequences of data.
    Training an RNN model is about formulating these sequences of data. RNNs can be
    used for text data as sentences are just sequences of words. When we use RNNs
    for NLP, we can use them for the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是一个具有反馈的传统前馈网络。对RNN的一种简单思考方式是，它是一个带有状态的神经网络。RNN可用于任何类型的数据，用于生成和预测各种数据序列。训练RNN模型是关于构建这些数据序列。RNN可用于文本数据，因为句子只是单词序列。当我们将RNN用于NLP时，我们可以用它来进行以下操作：
- en: Predicting the next word when typing
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输入时预测下一个单词
- en: 'Generating new text, following the style already used in the text:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成新的文本，遵循文本中已经使用的风格：
- en: '![](assets/8aba997b-e588-4c5d-8307-d33b6ad5c15b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8aba997b-e588-4c5d-8307-d33b6ad5c15b.png)'
- en: Remember the combination of words that resulted in their correct prediction?
    The learning process of RNNs is based on the text that is found in the corpus.
    They are trained by reducing the error between the predicted next word and the
    actual next word.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得导致它们正确预测的单词组合吗？RNN的学习过程是基于语料库中的文本。它们通过减少预测的下一个单词和实际的下一个单词之间的错误来进行训练。
- en: Using NLP for sentiment analysis
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NLP进行情感分析
- en: The approach presented in this section is based on the use case of classifying
    a high rate of incoming stream tweets. The task at hand is to extract the embedded
    sentiments within the tweets about a chosen topic. The sentiment classification
    quantifies the polarity in each tweet in real time and then aggregate the total
    sentiments from all tweets to capture the overall sentiments about the chosen
    topic. To face the challenges posed by the content and behavior of Twitter stream
    data and perform the real-time analytics efficiently, we use NLP by using a trained
    classifier. The trained classifier is then plugged into the Twitter stream to
    determine the polarity of each tweet (positive, negative, or neutral), followed
    by the aggregation and determination of the overall polarity of all tweets about
    a certain topic. Let's see how this is done step by step.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍的方法是基于对分类高速流推文的使用情况。手头的任务是提取关于所选主题的推文中嵌入的情绪。情感分类实时量化每条推文中的极性，然后聚合所有推文的总情感，以捕捉关于所选主题的整体情感。为了应对Twitter流数据的内容和行为带来的挑战，并有效地执行实时分析，我们使用NLP使用训练过的分类器。然后将训练过的分类器插入Twitter流中，以确定每条推文的极性（积极、消极或中性），然后聚合并确定关于某一主题的所有推文的整体极性。让我们一步一步地看看这是如何完成的。
- en: 'First, we have to train the classifier. In order to train the classifier, we
    needed an already-prepared dataset that has historical Twitter data and follows
    the patterns and trends of the real-time data. Therefore, we used a dataset from
    the website [www.sentiment140.com](http://www.sentiment140.com/), which comes
    with a human-labeled corpus (a large collection of texts upon which the analysis
    is based) with over 1.6 million tweets. The tweets within this dataset have been
    labeled with one of three polarities: zero for negative, two for neutral, and
    four for positive. In addition to the tweet text, the corpus provides the tweet
    ID, date, flag, and the user who tweeted. Now let''s see each of the operations
    that are performed on the live tweet before it reaches the *trained* classifier:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须训练分类器。为了训练分类器，我们需要一个已经准备好的数据集，其中包含有历史的Twitter数据，并且遵循实时数据的模式和趋势。因此，我们使用了来自网站[www.sentiment140.com](http://www.sentiment140.com/)的数据集，该数据集带有一个人工标记的语料库（基于该分析的大量文本集合），其中包含超过160万条推文。该数据集中的推文已经被标记为三种极性之一：零表示负面，两表示中性，四表示正面。除了推文文本之外，语料库还提供了推文ID、日期、标志和推文用户。现在让我们看看在*训练*分类器之前对实时推文执行的每个操作：
- en: Tweets are first split into individual words called tokens (tokenization).
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先将推文分割成称为标记的单词（标记化）。
- en: The output from tokenization creates a BoW, which is a collection of individual
    words in the text.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记化的输出创建了一个BoW，其中包含文本中的单个单词。
- en: These tweets are further filtered by removing numbers, punctuations, and stop
    words (stop word removal). Stop words are words that are extremely common, such
    as *is*, *am*, *are*, and *the*. As they hold no additional information, these
    words are removed.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些推文进一步通过去除数字、标点和停用词（停用词去除）进行过滤。停用词是非常常见的词，如*is*、*am*、*are*和*the*。由于它们没有额外的信息，这些词被移除。
- en: Additionally, nonalphabetical characters, such as *#**@* and numbers, are removed
    using pattern matching, as they hold no relevance in the case of sentiment analysis.
    Regular expressions are used to match alphabetical characters only and the rest
    are ignored. This helps to reduce the clutter from the Twitter stream.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，非字母字符，如*#**@*和数字，使用模式匹配进行删除，因为它们在情感分析的情况下没有相关性。正则表达式用于仅匹配字母字符，其余字符将被忽略。这有助于减少Twitter流的混乱。
- en: The outcomes of the prior phase are taken to the stemming phase. In this phase,
    the derived words are reduced to their roots—for example, a word like *fish* has
    the same roots as *fishing* and *fishes*. For this, we use the library of standard
    NLP, which provides various algorithms, such as Porter stemming.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 先前阶段的结果被用于词干处理阶段。在这个阶段，派生词被减少到它们的词根-例如，像*fish*这样的词与*fishing*和*fishes*具有相同的词根。为此，我们使用标准NLP库，它提供各种算法，如Porter词干处理。
- en: Once the data is processed, it is converted into a structure called a **term
    document matrix** (**TDM**). The TDM represents the term and frequency of each
    work in the filtered corpus.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据被处理，它被转换成一个称为**术语文档矩阵**（**TDM**）的结构。TDM表示过滤后语料库中每个词的术语和频率。
- en: From the TDM, the tweet reaches the trained classifier (as it is trained, it
    can process the tweet), which calculates the **sentimental polarity importance**
    (**SPI**) of each word which is a number from -5 to +5\. The positive or negative
    sign specifies the type of emotions represented by that particular word, and its
    magnitude represents the strength of sentiment. This means that the tweet can
    be classified as either positive or negative (refer to the following image). Once
    we calculate the polarity of the individual tweets, we sum their overall SPI to
    find the aggregated sentiment of the source—for example, an overall polarity greater
    than one indicates that the aggregated sentiment of the tweets in our observed
    period of time is positive.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从TDM中，推文到达训练过的分类器（因为它经过训练，可以处理推文），它计算每个词的**情感极性重要性**（**SPI**），这是一个从-5到+5的数字。正负号指定了该特定词所代表的情绪类型，其大小表示情感的强度。这意味着推文可以被分类为正面或负面（参考下图）。一旦我们计算了个别推文的极性，我们将它们的总体SPI相加，以找到来源的聚合情感-例如，总体极性大于一表示我们观察时间内推文的聚合情感是积极的。
- en: To retrieve the real-time raw tweets, we use the Scala library *Twitter4J*,
    a Java library that provides a package for a real-time Twitter streaming API.
    The API requires the user to register a developer account with Twitter and fill
    in some authentication parameters. This API allows you to either get random tweets
    or filter tweets using chosen keywords. We used filters to retrieve tweets related
    to our chosen keywords.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取实时原始推文，我们使用Scala库*Twitter4J*，这是一个提供实时Twitter流API包的Java库。该API要求用户在Twitter上注册开发者帐户并填写一些认证参数。该API允许您获取随机推文或使用选择的关键词过滤推文。我们使用过滤器来检索与我们选择的关键词相关的推文。
- en: 'The overall architecture is shown in the following figure:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总体架构如下图所示：
- en: '![](assets/3dc16321-97c8-4e5e-b815-3584e3e61f97.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3dc16321-97c8-4e5e-b815-3584e3e61f97.png)'
- en: The sentiment analysis have various applications. It can be applied to classify
    the feedback from customers. Social media polarity analysis can be used by governments
    to find the effectiveness of their policies. It can also quantify the success
    of various advertisement campaigns.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析有各种应用。它可以用来分类客户的反馈。政府可以利用社交媒体极性分析来找到他们政策的有效性。它还可以量化各种广告活动的成功。
- en: In the next section, we will learn how we can actually apply sentiment analysis
    to predict the sentiments of movie reviews.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将学习如何实际应用情感分析来预测电影评论的情感。
- en: 'Case study: movie review sentiment analysis'
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：电影评论情感分析
- en: 'Let''s use NLP to conduct a movie review sentiment analysis. For this, we will
    use some open source movie review data available at [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用NLP进行电影评论情感分析。为此，我们将使用一些开放的电影评论数据，可在[http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)上找到：
- en: 'First, we will import the dataset that contains the movie reviews:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将导入包含电影评论的数据集：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, let us load the movies' data and print the first few rows to observe its
    structure.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们加载电影数据并打印前几行以观察其结构。
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](assets/d613c48f-67c9-44c0-97b0-db32f368deae.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d613c48f-67c9-44c0-97b0-db32f368deae.png)'
- en: Note that the dataset has `2000` movie reviews. Out of these, half are negative
    and half are positive.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意数据集有`2000`条电影评论。其中一半是负面的，一半是正面的。
- en: Now, let's start preparing the dataset for training the model. First, let us
    drop any missing values that are in the data
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始准备数据集以训练模型。首先，让我们删除数据中的任何缺失值
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we need to remove the whitespaces as well. Whitespaces are not null but
    need to be removed. For this, we need to iterate over each row in the input `DataFrame`.
    We will use `.itertuples()` to access every field:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要移除空格。空格不是空的，但需要被移除。为此，我们需要遍历输入`DataFrame`中的每一行。我们将使用`.itertuples()`来访问每个字段：
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that we have used `i`, `lb`, and `rv` to the index, label, and review columns.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经使用`i`，`lb`和`rv`来索引、标签和评论列。
- en: 'Let''s split the data into test and train datasets:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据分割成测试和训练数据集：
- en: 'The first step is to specify the features and the label and then split the
    data into the train and test sets:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是指定特征和标签，然后将数据分割成训练集和测试集：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we have the testing and training datasets.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有测试和训练数据集。
- en: 'Now let''s divide the sets into train and test:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们将数据集分成训练集和测试集：
- en: '[PRE11]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that we are using `tfidf` to quantify the importance of a datapoint in
    a collection.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在使用`tfidf`来量化集合中数据点的重要性。
- en: Next, let's train the model using the naive Bayes algorithm, and then test the
    trained model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用朴素贝叶斯算法来训练模型，然后测试训练好的模型。
- en: 'Let''s follow these steps to train the model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤来训练模型：
- en: 'Now let''s train the model using the testing and training datasets that we
    created:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们使用我们创建的测试和训练数据集来训练模型：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let us run the predictions and analyze the results:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行预测并分析结果：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let us now look into the performance of our model by printing the confusion
    matrix. We will also look at *precision*, *recall*, *f1-score*, and *accuracy*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过打印混淆矩阵来查看模型的性能。我们还将查看*精确度*、*召回率*、*F1分数*和*准确度*。
- en: '![](assets/774aa10e-3479-403d-a3a5-949c7179ab32.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/774aa10e-3479-403d-a3a5-949c7179ab32.png)'
- en: These performance metrics give us a measure of the quality of the predictions.
    With a 0.78 accuracy, now we have successfully trained a model that can predict
    what type of review we can predict for that particular movie.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些性能指标为我们提供了预测质量的度量。准确率为0.78，现在我们已经成功训练了一个可以预测特定电影评论类型的模型。
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the algorithms related to NLP. First, we looked
    at the terminology related to NLP. Next, we looked at the BoW methodology of implementing
    an NLP strategy. Then we looked at the concept of word embedding and the use of
    neural networks in NLP. Finally, we looked at an actual example where we used
    the concepts developed in this chapter to predict the sentiments of movie reviews
    based on their text. Having gone through this chapter, the user should be able
    to use NLP for text classification and sentiment analysis.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了与自然语言处理相关的算法。首先，我们研究了与自然语言处理相关的术语。接下来，我们研究了实施自然语言处理策略的BoW方法。然后，我们研究了词嵌入的概念以及在自然语言处理中使用神经网络。最后，我们看了一个实际的例子，我们在这一章中使用了开发的概念来根据电影评论的文本来预测情感。通过学习本章内容，用户应该能够将自然语言处理用于文本分类和情感分析。
- en: In the next chapter, we will look at recommendation engines. We will study different
    types of recommendation engine and how they can be used to solve some real-world
    problems.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究推荐引擎。我们将研究不同类型的推荐引擎以及它们如何用于解决一些现实世界的问题。
