- en: Kernels, Threads, Blocks, and Grids
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心、线程、块和网格
- en: In this chapter, we'll see how to write effective **CUDA kernels***.* In GPU
    programming, a**kernel **(which we interchangeably use with terms such as *CUDA
    kernel* or *kernel function*) is a parallel function that can be launched directly
    from the **host **(the CPU) onto the **device** (the GPU), while a** device function**
    is a function that can only be called from a kernel function or another device
    function. (Generally speaking, device functions look and act like normal serial
    C/C++ functions, only they are running on the GPU and are called in parallel from
    kernels.)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将看到如何编写有效的**CUDA核心**。在GPU编程中，**核心**（我们可以互换使用术语**CUDA核心**或**核心函数**）是一个可以直接从**主机**（CPU）启动到**设备**（GPU）的并行函数，而**设备函数**是一个只能从核心函数或另一个设备函数调用的函数。（一般来说，设备函数看起来和行为像普通的串行C/C++函数，只是它们在GPU上运行，并且从核心函数并行调用。）
- en: We'll then get an understanding of how CUDA uses the notion of **threads**, **blocks**,
    and **grids** to abstract away some of the underlying technical details of the
    GPU (such as cores, warps, and streaming multiprocessors, which we'll cover later
    in this book), and how we can use these notions to ease the cognitive overhead
    in parallel programming. We'll learn about thread synchronization (both block-level
    and grid-level), and intra-thread communication in CUDA using both **global**
    and **shared** **memory**. Finally, we'll delve into the technical details of
    how to implement our own parallel prefix type algorithms on the GPU (that is,
    the scan/reduce type functions we covered in the last chapter), which allow us
    to put all of the principles we'll learn in this chapter into practice.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将了解CUDA如何使用**线程**、**块**和**网格**的概念来抽象GPU的一些基础技术细节（例如核心、warp和流多处理器，我们将在本书的后面部分介绍），以及我们如何使用这些概念来减轻并行编程中的认知负担。我们将学习关于线程同步（块级和网格级），以及在CUDA中使用**全局**和**共享****内存**进行线程间通信。最后，我们将深入了解如何在GPU上实现我们自己的并行前缀类型算法（即我们在上一章中介绍的扫描/归约类型函数），这使我们能够将本章学到的所有原则付诸实践。
- en: 'The learning outcomes for this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习成果如下：
- en: Understanding the difference between a kernel and a device function
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解核心和设备函数之间的区别
- en: How to compile and launch a kernel in PyCUDA and use a device function within
    a kernel
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在PyCUDA中编译和启动核心，并在核心内使用设备函数
- en: Effectively using threads, blocks, and grids in the context of launching a kernel
    and how to use `threadIdx` and `blockIdx` within a kernel
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在启动核心的上下文中有效使用线程、块和网格，以及如何在核心内使用`threadIdx`和`blockIdx`
- en: How and why to synchronize threads within a kernel, using both `__syncthreads()`
    for synchronizing all threads among a single block and the host to synchronize
    all threads among an entire grid of blocks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何以及为什么在核心内同步线程，使用`__syncthreads()`来同步单个块中的所有线程，以及主机来同步整个块网格中的所有线程
- en: How to use device global and shared memory for intra-thread communication
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用设备全局和共享内存进行线程间通信
- en: How to use all of our newly acquired knowledge about kernels to properly implement
    a GPU version of the parallel prefix sum
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用我们新获得的关于核心的所有知识来正确实现并行前缀和的GPU版本
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016 onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0 onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一台带有现代NVIDIA GPU（2016年以后）的Linux或Windows 10 PC，并安装了所有必要的GPU驱动程序和CUDA Toolkit（9.0及以上）。还需要一个合适的Python
    2.7安装（如Anaconda Python 2.7），并安装了PyCUDA模块。
- en: 'This chapter''s code is also available on GitHub at:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)'
- en: For more information about the prerequisites, check the *Preface* of this book;
    for the software and hardware requirements, check the `README` section in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有关先决条件的更多信息，请查看本书的*前言*；有关软件和硬件要求，请查看[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)中的`README`部分。
- en: Kernels
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心
- en: As in the last chapter, we'll be learning how to write CUDA kernel functions
    as inline CUDA C in our Python code and launch them onto our GPU using PyCUDA.
    In the last chapter, we used templates provided by PyCUDA to write kernels that
    fall into particular design patterns; in contrast, we'll now see how to write
    our own kernels from the ground up, so that we can write a versatile variety of
    kernels that may not fall into any particular design pattern covered by PyCUDA,
    and so that we may get a more fine-tuned control over our kernels. Of course,
    these gains will come at the expense of greater complexity in programming; we'll
    especially have to get an understanding of **threads**, **blocks**, and **grids**
    and their role in kernels, as well as how to **synchronize** the threads in which
    our kernel is executing, as well as understand how to exchange data among threads.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章一样，我们将学习如何在Python代码中以内联CUDA C编写CUDA核心函数，并使用PyCUDA将它们启动到我们的GPU上。在上一章中，我们使用PyCUDA提供的模板来编写符合特定设计模式的核心，相比之下，我们现在将看到如何从头开始编写我们自己的核心，以便我们可以编写各种各样的核心，这些核心可能不属于PyCUDA涵盖的任何特定设计模式，并且我们可以更精细地控制我们的核心。当然，这些收益将以编程复杂性增加为代价；我们特别需要了解**线程**、**块**和**网格**及其在核心中的作用，以及如何**同步**我们的核心正在执行的线程，以及如何在线程之间交换数据。
- en: Let's start simple and try to re-create some of the element-wise operations
    we saw in the last chapter, but this time without using the `ElementwiseKernel`
    function; we'll now be using the `SourceModule` function. This is a very powerful
    function in PyCUDA that allows us to build a kernel from scratch, so as usual
    it's best to start simple.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单开始，尝试重新创建我们在上一章中看到的一些逐元素操作，但这次不使用`ElementwiseKernel`函数；我们现在将使用`SourceModule`函数。这是PyCUDA中非常强大的函数，允许我们从头构建一个内核，所以通常最好从简单开始。
- en: The PyCUDA SourceModule function
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyCUDA SourceModule函数
- en: We'll use the `SourceModule` function from PyCUDA to compile raw inline CUDA
    C code into usable kernels that we can launch from Python. We should note that
    `SourceModule` actually compiles code into a **CUDA module**, this is like a Python
    module or Windows DLL, only it contains a collection of compiled CUDA code. This
    means we'll have to "pull out" a reference to the kernel we want to use with PyCUDA's
    `get_function`, before we can actually launch it. Let's start with a basic example
    of how to use a CUDA kernel with `SourceModule`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用PyCUDA的`SourceModule`函数将原始内联CUDA C代码编译为可用的内核，我们可以从Python中启动。我们应该注意，`SourceModule`实际上将代码编译为**CUDA模块**，这类似于Python模块或Windows
    DLL，只是它包含一组编译的CUDA代码。这意味着我们必须使用PyCUDA的`get_function`“提取”我们想要使用的内核的引用，然后才能实际启动它。让我们从如何使用`SourceModule`的基本示例开始。
- en: 'As before, we''ll start with making one of the most simple kernel functions
    possible—one that multiplies a vector by a scalar. We''ll start with the imports:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，我们将从制作最简单的内核函数之一开始，即将向量乘以标量。我们将从导入开始：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can immediately dive into writing our kernel:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以立即开始编写我们的内核：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So, let''s stop and contrast this with how it was done in `ElementwiseKernel`.
    First, when we declare a kernel function in CUDA C proper, we precede it with
    the `__global__` keyword. This will distinguish the function as a kernel to the
    compiler. We''ll always just declare this as a `void` function, because we''ll
    always get our output values by passing a pointer to some empty chunk of memory
    that we pass in as a parameter. We can declare the parameters as we would with
    any standard C function: first we have `outvec`, which will be our output scaled
    vector, which is of course a floating-point array pointer. Next, we have `scalar`,
    which is represented with a mere `float`; notice that this is not a pointer! If
    we wish to pass simple singleton input values to our kernel, we can always do
    so without using pointers. Finally, we have our input vector, `vec`, which is
    of course another floating-point array pointer.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们停下来，对比一下在`ElementwiseKernel`中是如何完成的。首先，在CUDA C中声明内核函数时，我们要在前面加上`__global__`关键字。这将使编译器将该函数标识为内核。我们总是将其声明为`void`函数，因为我们总是通过传递指向一些空内存块的指针来获得输出值。我们可以像声明任何标准C函数一样声明参数：首先是`outvec`，这将是我们的输出缩放向量，当然是浮点数组指针。接下来是`scalar`，用一个简单的`float`表示；注意这不是一个指针！如果我们希望将简单的单例输入值传递给我们的内核，我们总是可以在不使用指针的情况下这样做。最后，我们有我们的输入向量`vec`，当然是另一个浮点数组指针。
- en: Singleton input parameters to a kernel function can be passed in directly from
    the host without using pointers or allocated device memory.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 单例输入参数可以直接从主机传递给内核函数，而无需使用指针或分配的设备内存。
- en: Let's peer into the kernel before we continue with testing it. We recall that
    `ElementwiseKernel` automatically parallelized over multiple GPU threads by a
    value, `i`, which was set for us by PyCUDA; the identification of each individual
    thread is given by the `threadIdx` value, which we retrieve as follows: `int i
    = threadIdx.x;`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在继续测试之前先深入了解内核。我们记得`ElementwiseKernel`会自动并行化多个GPU线程，通过PyCUDA为我们设置的值`i`；每个单独线程的标识由`threadIdx`值给出，我们可以通过以下方式检索：`int
    i = threadIdx.x;`。
- en: '`threadIdx` is used to tell each individual thread its identity. This is usually
    used to determine an index for what values should be processed on the input and
    output data arrays. (This can also be used for assigning particular threads different
    tasks than others with standard C control flow statements such as `if` or `switch`.)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`threadIdx`用于告诉每个单独的线程其身份。这通常用于确定应在输入和输出数据数组上处理哪些值的索引。（这也可以用于使用标准C控制流语句（如`if`或`switch`）为特定线程分配不同的任务。）'
- en: Now, we are ready to perform our scalar multiplication in parallel as before: `outvec[i]
    = scalar*vec[i];`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备像以前一样并行执行标量乘法：`outvec[i] = scalar*vec[i];`。
- en: 'Now, let''s test this code: we first must *pull out* a reference to our compiled
    kernel function from the CUDA module we just compiled with `SourceModule`. We
    can get this kernel reference with Python''s `get_function` as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们测试这段代码：我们首先必须从我们刚刚使用`SourceModule`编译的CUDA模块中*提取*编译的内核函数的引用。我们可以使用Python的`get_function`来获取这个内核引用，如下所示：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we have to put some data on the GPU to actually test our kernel. Let''s
    set up a floating-point array of 512 random values, and then copy these into an
    array in the GPU''s global memory using the `gpuarray.to_gpu` function. (We''re
    going to multiply this random vector by a scalar both on the GPU and CPU, and
    see if the output matches.) We''ll also allocate a chunk of empty memory to the
    GPU''s global memory using the `gpuarray.empty_like` function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须在GPU上放一些数据来实际测试我们的内核。让我们设置一个包含512个随机值的浮点数组，然后使用`gpuarray.to_gpu`函数将这些值复制到GPU的全局内存中的数组中。（我们将在GPU和CPU上将这个随机向量乘以一个标量，并查看输出是否匹配。）我们还将使用`gpuarray.empty_like`函数在GPU的全局内存中分配一块空内存块：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now prepared to launch our kernel. We''ll set the scalar value as `2`.
    (Again, since the scalar is a singleton, we don''t have to copy this value to
    the GPU—we should be careful that we typecast it properly, however.) Here we''ll
    have to specifically set the number of threads to `512` with the `block` and `grid`
    parameters. We are now ready to launch:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备启动我们的内核。我们将标量值设置为`2`。（再次，由于标量是单例，我们不必将该值复制到GPU，但是我们必须小心确保正确地进行类型转换。）在这里，我们必须使用`block`和`grid`参数明确设置线程数为`512`。我们现在准备启动：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now check whether the output matches with the expected output by using
    the `get` function in our `gpuarray` output object and comparing this to the correct
    output with NumPy''s `allclose` function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`gpuarray`输出对象中的`get`函数来检查输出是否与预期输出匹配，并将其与NumPy的`allclose`函数进行比较：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: (The code to this example is available as the `simple_scalar_multiply_kernel.py` file,
    under `4` in the repository.)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例的代码可在存储库中的`simple_scalar_multiply_kernel.py`文件中的`4`下找到。）
- en: Now we are starting to remove the training wheels of the PyCUDA kernel templates
    we learned in the previous chapter—we can now directly write a kernel in pure
    CUDA C and launch it to use a specific number of threads on our GPU. However,
    we'll have to learn a bit more about how CUDA structures threads into collections
    of abstract units known as **blocks** and **grids** before we can continue with
    kernels.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始去掉前一章中学到的PyCUDA内核模板的训练轮——我们现在可以直接用纯CUDA C编写内核，并启动它在GPU上使用特定数量的线程。但是，在继续使用内核之前，我们必须更多地了解CUDA如何将线程结构化为抽象单位**块**和**网格**。
- en: Threads, blocks, and grids
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程、块和网格
- en: So far in this book, we have been taking the term **thread** for granted. Let's
    step back for a moment and see exactly what this means—a thread is a sequence
    of instructions that is executed on a single core of the GPU—*cores *and *threads* should
    not be thought of as synonymous! In fact, it is possible to launch kernels that
    use many more threads than there are cores on the GPU. This is because, similar
    to how an Intel chip may only have four cores and yet be running hundreds of processes
    and thousands of threads within Linux or Windows, the operating system's scheduler
    can switch between these tasks rapidly, giving the appearance that they are running
    simultaneously. The GPU handles threads in a similar way, allowing for seamless
    computation over tens of thousands of threads.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们一直认为**线程**这个术语是理所当然的。让我们退后一步，看看这究竟意味着——线程是在GPU的单个核心上执行的一系列指令—*核心*和*线程*不应被视为同义词！事实上，可以启动使用的线程数量比GPU上的核心数量多得多。这是因为，类似于英特尔芯片可能只有四个核心，但在Linux或Windows中运行数百个进程和数千个线程，操作系统的调度程序可以快速在这些任务之间切换，使它们看起来是同时运行的。GPU以类似的方式处理线程，允许在成千上万的线程上进行无缝计算。
- en: Multiple threads are executed on the GPU in abstract units known as **blocks**.
    You should recall how we got the thread ID from `threadIdx.x` in our scalar multiplication
    kernel; there is an `x` at the end because there is also `threadIdx.y` and `threadIdx.z`.
    This is because you can index blocks over three dimensions, rather than just one
    dimension. Why do we do this? Let's recall the example regarding the computation
    of the Mandelbrot set from [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why
    GPU Programming?* and [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml),
    *Getting Started with PyCUDA*. This is calculated point-by-point over a two-dimensional
    plane. It may therefore make more sense for us to index the threads over two dimensions
    for algorithms like this. Similarly, it may make sense to use three dimensions
    in some cases—in a physics simulation, we may have to calculate the positions
    of moving particles within a 3D grid.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 多个线程在GPU上以抽象单位**块**执行。您应该回忆一下我们如何从标量乘法内核中的`threadIdx.x`获得线程ID；末尾有一个`x`，因为还有`threadIdx.y`和`threadIdx.z`。这是因为您可以在三个维度上对块进行索引，而不仅仅是一个维度。为什么我们要这样做？让我们回忆一下有关从[第1章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)中计算Mandelbrot集的示例，*为什么使用GPU编程？*和[第3章](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)，*使用PyCUDA入门*。这是在二维平面上逐点计算的。因此，对于这样的算法，我们可能更倾向于在两个维度上对线程进行索引。同样，在某些情况下，使用三个维度可能是有意义的——在物理模拟中，我们可能需要在3D网格内计算移动粒子的位置。
- en: Blocks are further executed in abstract batches known as **grids**, which are
    best thought of as *blocks of blocks.* As with threads in a block, we can index
    each block in the grid in up to three dimensions with the constant values that
    are given by `blockIdx.x` , `blockIdx.y`, and `blockIdx.z`. Let's look at an example
    to help us make sense of these concepts; we'll only use two dimensions here for
    simplicity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 块进一步以称为**网格**的抽象批次执行，最好将其视为*块的块*。与块中的线程一样，我们可以使用`blockIdx.x`、`blockIdx.y`和`blockIdx.z`给出的常量值在网格中的最多三个维度上对每个块进行索引。让我们看一个示例来帮助我们理解这些概念；为了简单起见，我们这里只使用两个维度。
- en: Conway's game of life
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 康威的生命游戏
- en: '*The Game of Life* (often called *LIFE* for short) is a cellular automata simulation
    that was invented by the British mathematician John Conway back in 1970\. This
    sounds complex, but it''s really quite simple—LIFE is a zero-player *game* that
    consists of a two-dimensional binary lattice of *cells* that are either considered
    *live* or *dead*. The lattice is iteratively updated by the following set of rules:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 《生命游戏》（通常简称为LIFE）是一种细胞自动机模拟，由英国数学家约翰·康威于1970年发明。听起来很复杂，但实际上非常简单——LIFE是一个零玩家的“游戏”，由一个二维二进制格子组成，其中的“细胞”被认为是“活着的”或“死了的”。这个格子通过以下一组规则进行迭代更新：
- en: Any live cell with fewer than two live neighbors dies
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何活细胞周围少于两个活邻居的细胞会死亡
- en: Any live cell with two or three neighbors lives
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何活细胞周围有两个或三个邻居的细胞会存活
- en: Any live cell with more than three neighbors dies
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何活细胞周围有三个以上的邻居的细胞会死亡
- en: Any dead cell with exactly three neighbors comes to life
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何死细胞周围恰好有三个邻居的细胞会复活
- en: These four simple rules give rise to a complex simulation with interesting mathematical
    properties that is also aesthetically quite pleasing to watch when animated. However,
    with a large number of cells in the lattice, it can run quite slowly, and usually
    results in *choppy* animation when programmed in pure serial Python. However,
    this is parallelizable, as it is clear that each cell in the lattice can be managed
    by a single CUDA thread.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这四条简单的规则产生了一个复杂的模拟，具有有趣的数学特性，而且在动画时也非常美观。然而，在晶格中有大量的细胞时，它可能运行得很慢，并且通常在纯串行Python中编程时会导致动画不流畅。然而，这是可以并行化的，因为很明显晶格中的每个细胞可以由一个单独的CUDA线程管理。
- en: We'll now implement LIFE as a CUDA kernel and animate it as using the `matplotlib.animation`
    module. This will be interesting to us right now because namely we'll be able
    to apply our new knowledge of blocks and grids here.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将 LIFE 实现为一个CUDA核函数，并使用 `matplotlib.animation` 模块来进行动画。这对我们来说现在很有趣，因为我们将能够在这里应用我们对块和网格的新知识。
- en: 'We''ll start by including the appropriate modules as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先包括适当的模块，如下所示：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s dive into writing our kernel via `SourceModule` . We''re going
    to start by using the C language''s `#define` directive to set up some constants
    and macros that we''ll use throughout our kernel. Let''s look at the first two
    we''ll set up, `_X` and `_Y`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过 `SourceModule` 来编写我们的核函数。我们将首先使用C语言的 `#define` 指令来设置一些我们将在整个核函数中使用的常量和宏。让我们看看我们将设置的前两个，`_X`
    和 `_Y`：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let's first remember how `#define` works here—it will literally replace any
    text of `_X` or `_Y` with the defined values (in the parentheses here) at compilation
    time—that is, it creates macros for us. (As a matter of personal style, I usually
    precede all of my C macros with an underscore.)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们记住这里 `#define` 的工作原理——它会在编译时用定义的值（在括号中）直接替换 `_X` 或 `_Y` 的任何文本，也就是说，它为我们创建了宏。（作为个人风格的问题，我通常会在所有的C宏之前加上下划线。）
- en: In C and C++, `#define` is used for creating **macros**. This means that `#define`
    doesn't create any function or set up a proper constant variables—it just allows
    us to write things shorthand in our code by swapping text out right before compilation
    time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在C和C++中，`#define` 用于创建**宏**。这意味着 `#define` 不会创建任何函数或设置正确的常量变量——它只允许我们在编译之前通过交换文本来在我们的代码中以简写方式编写东西。
- en: 'Now, let''s talk about what `_X` and `_Y` mean specifically—these will be the
    Cartesian *x* and *y* values of a single CUDA thread''s cell on the two-dimensional
    lattice we are using for LIFE. We''ll launch the kernel over a two-dimensional
    grid consisting of two-dimensional blocks that will correspond to the entire cell
    lattice. We''ll have to use both thread and block constants to find the Cartesian
    point on the lattice. Let''s look at some diagrams to make the point. A thread
    residing in a two-dimensional CUDA block can be visualized as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们具体讨论一下 `_X` 和 `_Y` 的含义——这将是单个CUDA线程在我们用于LIFE的二维晶格上的笛卡尔 *x* 和 *y* 值。我们将在一个二维网格上启动核函数，由二维块组成，这些块将对应整个细胞晶格。我们将使用线程和块常量来找到晶格上的笛卡尔点。让我们看一些图表来说明这一点。驻留在二维CUDA块中的线程可以被可视化如下：
- en: '![](assets/b18aaa4b-b830-44f1-9371-80c57b0ab285.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b18aaa4b-b830-44f1-9371-80c57b0ab285.png)'
- en: At this point, you may be wondering why we don't launch our kernel over a single
    block, so we can just set `_X` as `threadIdx.x` and `_Y` as `threadIdx.y` and
    be done with it. This is due to a limitation on block size imposed on us by CUDA—currently,
    only blocks consisting of at most 1,024 threads are supported. This means that
    we can only make our cell lattice of dimensions 32 x 32 at most, which would make
    for a rather boring simulation that might be better done on a CPU, so we'll have
    to launch multiple blocks over a grid. (The dimensions of our current block will
    be given by `blockDim.x` and `blockDim.y`, which will help us determine the objective
    *x* and *y* coordinates, as we'll see.)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可能会想知道为什么我们不在一个单独的块上启动我们的核函数，这样我们就可以将 `_X` 设置为 `threadIdx.x`，将 `_Y` 设置为
    `threadIdx.y`，然后就完成了。这是由于CUDA对我们施加了块大小的限制——目前只支持由最多1024个线程组成的块。这意味着我们只能将我们的细胞晶格的尺寸最大设为32
    x 32，这将导致一个相当无聊的模拟，最好在CPU上完成，所以我们将在网格上启动多个块。（我们当前块的尺寸将由 `blockDim.x` 和 `blockDim.y`
    给出，这将帮助我们确定目标 *x* 和 *y* 坐标，正如我们将看到的。）
- en: 'Similarly, as before, we can determine which block we are in within a two-dimensional
    grid with `blockIdx.x` and `blockIdx.y`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，和之前一样，我们可以确定我们在二维网格中的块是哪个，使用 `blockIdx.x` 和 `blockIdx.y`：
- en: '![](assets/45159b32-9f17-4a39-a50f-0e4bfb47b1f2.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/45159b32-9f17-4a39-a50f-0e4bfb47b1f2.png)'
- en: 'After we think of the math a little bit, it should be clear that `_X` should
    be defined as `(threadIdx.x + blockIdx.x * blockDim.x)` and `_Y` should be defined
    as `( threadIdx.y + blockIdx.y * blockDim.y )`. (The parentheses are added so
    as not to interfere with the order of operations when the macros are inserted
    in the code.) Now, let''s continue defining the remaining macros:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们稍微思考一下数学之后，应该很清楚 `_X` 应该被定义为 `(threadIdx.x + blockIdx.x * blockDim.x)`，而
    `_Y` 应该被定义为 `(threadIdx.y + blockIdx.y * blockDim.y)`。（添加括号是为了不干扰宏插入代码时的运算顺序。）现在，让我们继续定义剩下的宏：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `_WIDTH` and `_HEIGHT` macros will give us the width and height of our cell
    lattice, respectively, which should be clear from the diagrams. Let's discuss
    the `_XM` and `_YM` macros. In our implementation of LIFE, we'll have the endpoints
    "wrap around" to the other side of the lattice—for example, we'll consider the
    *x*-value of `-1` to be `_WIDTH - 1`, and a *y*-value of `-1` to be `_HEIGHT -
    1`, and we'll likewise consider an *x*-value of `_WIDTH` to be `0` and a *y*-value
    of `_HEIGHT` to be `0`. Why do we need this? When we calculate the number of living
    neighbors of a given cell, we might be at some edge and the neighbors might be
    external points—defining these macros to modulate our points will cover this for
    us automatically. Notice that we have to add the width or height before we use
    C's modulus operator—this is because, unlike Python, the modulus operator in C
    can return negative values for integers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`_WIDTH`和`_HEIGHT`宏将分别给出我们单元格格子的宽度和高度，这应该从图表中清楚地看出。让我们讨论`_XM`和`_YM`宏。在我们的LIFE实现中，我们将端点“环绕”到格子的另一侧
    - 例如，我们将考虑`-1`的*x*值为`_WIDTH - 1`，*y*值为`-1`为`_HEIGHT - 1`，同样地，我们将考虑`_WIDTH`的*x*值为`0`，*y*值为`_HEIGHT`为`0`。我们为什么需要这个？当我们计算给定单元格的存活邻居数时，我们可能处于某个边缘，邻居可能是外部点
    - 定义这些宏来调制我们的点将自动为我们覆盖这一点。请注意，在使用C的模运算符之前，我们必须添加宽度或高度 - 这是因为，与Python不同，C中的模运算符对于整数可以返回负值。'
- en: 'We now have one final macro to define. We recall that PyCUDA passes two-dimensional
    arrays into CUDA C as one-dimensional pointers; two-dimensional arrays are passed
    in **row-wise** from Python into one dimensional C pointers. This means that we''ll
    have to translate a given Cartesian (*x*,*y*) point for a given cell on the lattice
    into a one dimensional point within the pointer corresponding to the lattice.
    Here, we can do so as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个最终的宏要定义。我们记得PyCUDA将二维数组作为一维指针传递到CUDA C中；二维数组从Python以**按行**的方式传递到一维C指针中。这意味着我们必须将格子上给定的笛卡尔（*x*，*y*）点转换为指向格子对应的指针中的一维点。在这里，我们可以这样做：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since our cell lattice is stored row-wise, we have to multiply the *y*-value
    by the width to offset to the point corresponding to the appropriate row. We can
    now finally begin with our implementation of LIFE. Let''s start with the most
    important part of LIFE—counting the number of living neighbors a given cell has.
    We''ll implement this using a CUDA **device function**, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的单元格格子是按行存储的，我们必须将*y*值乘以宽度以偏移到对应行的点。我们现在终于可以开始我们的LIFE实现了。让我们从LIFE最重要的部分开始
    - 计算给定单元格的存活邻居数。我们将使用CUDA **设备函数**来实现这一点，如下所示：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A device function is a C function written in serial, which is called by an individual
    CUDA thread in kernel. That is to say, this little function will be called in
    parallel by multiple threads from our kernel. We'll represent our cell lattice
    as a collection of 32-bit integers (1 will represent a living cell and 0 will
    represent a dead one), so this will work for our purposes; we just have to add
    the values of the neighbors around our current cell.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 设备函数是以串行方式编写的C函数，由内核中的单个CUDA线程调用。也就是说，这个小函数将由我们内核中的多个线程并行调用。我们将把我们的单元格格子表示为32位整数的集合（1表示活细胞，0表示死细胞），所以这对我们的目的是有效的；我们只需要添加周围当前单元格的值。
- en: A CUDA **device function** is a serial C function that is called by an individual
    CUDA thread from within a kernel. While these functions are serial in themselves,
    they can be run in parallel by multiple GPU threads. Device functions cannot by
    themselves by launched by a host computer onto a GPU, only kernels.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA **设备函数**是由内核中的单个CUDA线程调用的串行C函数。虽然这些函数本身是串行的，但它们可以由多个GPU线程并行运行。设备函数本身不能由主机计算机启动到GPU上，只能由内核启动。
- en: 'We are now prepared to write our kernel implementation of LIFE. Actually, we''ve
    done most of the hard work already—we check the number of neighbors of the current
    thread''s cell, check whether the current cell is living or dead, and then use
    the appropriate switch-case statements to determine its status for the next iteration
    according to the rules of LIFE. We''ll use two integer pointer arrays for this
    kernel—one will be in reference to the last iteration as input (`lattice`) and
    the other in reference to the iteration that we''ll calculate as output (`lattice_out`):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备编写LIFE的内核实现。实际上，我们已经完成了大部分的艰苦工作 - 我们检查当前线程单元格的邻居数量，检查当前单元格是生还是死，然后使用适当的switch-case语句来根据LIFE的规则确定下一次迭代的状态。我们将使用两个整数指针数组作为内核
    - 一个将用作输入参考上一次迭代（`lattice`），另一个将用作输出我们将计算的迭代（`lattice_out`）的参考。
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We remember to close off the inline CUDA C segment with the triple-parentheses,
    and then get a reference to our CUDA C kernel with `get_function`. Since the kernel
    will only update the lattice once, we''ll set up a short function in Python that
    will cover for all of the overhead of updating the lattice for the animation:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们记得用三重括号关闭内联CUDA C段落，然后用`get_function`获取对我们的CUDA C内核的引用。由于内核只会一次更新格子，我们将在Python中设置一个简短的函数，它将涵盖更新格子的所有开销以用于动画：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `frameNum` parameter is just a value that is required by Matplotlib's animation
    module for update functions that we can ignore, while `img` will be the representative
    image of our cell lattice that is required by the module that will be iteratively
    displayed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`frameNum`参数只是Matplotlib动画模块对于我们可以忽略的更新函数所需的一个值，而`img`将是我们单元格格子的代表图像，这是动画模块所需的，将被迭代显示。'
- en: Let's focus on the other three remaining parameters—`newLattice_gpu` and `lattice_gpu`
    will be PyCUDA arrays that we'll keep persistent, as we want to avoid re-allocating
    chunks of memory on the GPU when we can. `lattice_gpu` will be the current generation
    of the cell array that will correspond to the `lattice` parameter in the kernel,
    while `newLattice_gpu` will be the next generation of the lattice. `N` will indicate
    the the height and width of the lattice (in other words, we'll be working with
    an *N x N* lattice).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注另外三个参数—`newLattice_gpu`和`lattice_gpu`将是我们保持持久的PyCUDA数组，因为我们希望避免在GPU上重新分配内存块。`lattice_gpu`将是细胞数组的当前一代，对应于内核中的`lattice`参数，而`newLattice_gpu`将是下一代晶格。`N`将指示晶格的高度和宽度（换句话说，我们将使用*N
    x N*晶格）。
- en: 'We launch the kernel with the appropriate parameters and set the block and
    grid sizes as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用适当的参数启动内核，并设置块和网格大小如下：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We'll set the block sizes as 32 x 32 with `(32, 32, 1)`; since we are only using
    two dimensions for our cell lattice, we can just set the *z*-dimension as one.
    Remember that blocks are limited to 1,024 threads—*32 x 32 = 1024*, so this will
    work. (Keep in mind that there is nothing special here about 32 x 32; we could
    use values such as 16 x 64 or 10 x 10 if we wanted to, as long as the total number
    of threads does not exceed 1,024.)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将块大小设置为32 x 32，使用`(32, 32, 1)`；因为我们只使用两个维度来表示我们的细胞晶格，所以我们可以将*z*维度设置为1。请记住，块的线程数限制为1,024个线程—*32
    x 32 = 1024*，所以这样可以工作。（请记住，32 x 32没有什么特别之处；如果需要，我们可以使用16 x 64或10 x 10等值，只要总线程数不超过1,024。）
- en: The number of threads in a CUDA block is limited to a maximum of 1,024.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA块中的线程数最多为1,024。
- en: We now look at grid value—here, since we are working with dimensions of 32,
    it should be clear that *N* (in this case) should be divisible by 32\. That means
    that in this case, we are limited to lattices such as 64 x 64, 96 x 96, 128 x
    128, and 1024 x 1024\. Again, if we want to use lattices of a different size,
    then we'll have to alter the dimensions of the blocks. (If this doesn't make sense,
    then please look at the previous diagrams and review how we defined the width
    and height macros in our kernel.)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下网格值—在这里，因为我们使用32的维度，很明显*N*（在这种情况下）应该是32的倍数。这意味着在这种情况下，我们只能使用64 x 64、96
    x 96、128 x 128和1024 x 1024等晶格。同样，如果我们想使用不同大小的晶格，那么我们将不得不改变块的维度。（如果这不太清楚，请查看之前的图表，并回顾一下我们如何在内核中定义宽度和高度宏。）
- en: 'We can now set up the image data for our animation after grabbing the latest
    generated lattice from the GPU''s memory with the `get()` function. We finally
    copy the new lattice data into the current data using the PyCUDA slice operator, `[:]`,
    which will copy over the previously allocated memory on the GPU so that we don''t
    have to re-allocate:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`get()`函数从GPU的内存中获取最新生成的晶格，并为我们的动画设置图像数据。最后，我们使用PyCUDA切片操作`[:]`将新的晶格数据复制到当前数据中，这将复制GPU上先前分配的内存，这样我们就不必重新分配了：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s set up a lattice of size 256 x 256\. We now will set up an initial state
    for our lattice using the choice function from the `numpy.random` module. We''ll
    populate a *N* x *N* graph of integers randomly with ones and zeros; generally,
    if around 25% of the points are ones and the rest zeros, we can generate some
    interesting lattice animations, so we''ll go with that:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置一个大小为256 x 256的晶格。现在我们将使用`numpy.random`模块中的choice函数为我们的晶格设置初始状态。我们将随机用1和0填充一个*N
    x N*的整数图表；通常，如果大约25%的点是1，其余的是0，我们可以生成一些有趣的晶格动画，所以我们就这样做吧：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we can set up the lattices on the GPU with the appropriate `gpuarray`
    functions and set up the Matplotlib animation accordingly, as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用适当的`gpuarray`函数在GPU上设置晶格，并相应地设置Matplotlib动画，如下所示：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can now run our program and enjoy the show (the code is also available as the `conway_gpu.py` file under
    the `4` directory in the GitHub repository):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行我们的程序并享受展示（代码也可以在GitHub存储库的`4`目录下的`conway_gpu.py`文件中找到）：
- en: '![](assets/bb012845-31d4-4511-a697-9eef0e2772b2.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/bb012845-31d4-4511-a697-9eef0e2772b2.png)'
- en: Thread synchronization and intercommunication
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程同步和互通
- en: We'll now discuss two important concepts in GPU programming—**thread synchronization**
    and **thread intercommunication**. Sometimes, we need to ensure that every single
    thread has reached the same exact line in the code before we continue with any
    further computation; we call this thread synchronization. Synchronization works
    hand-in-hand with thread intercommunication, that is, different threads passing
    and reading input from each other; in this case, we'll usually want to make sure
    that all of the threads are aligned at the same step in computation before any
    data is passed around. We'll start here by learning about the CUDA `__syncthreads`
    device function, which is used for synchronizing a single block in a kernel.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论GPU编程中的两个重要概念—**线程同步**和**线程互通**。有时，我们需要确保每个线程在继续任何进一步的计算之前都已经到达了代码中完全相同的行；我们称之为线程同步。同步与线程互通相辅相成，也就是说，不同的线程之间传递和读取输入；在这种情况下，我们通常希望确保所有线程在传递数据之前都处于计算的相同步骤。我们将从学习CUDA
    `__syncthreads`设备函数开始，该函数用于同步内核中的单个块。
- en: Using the __syncthreads() device function
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用__syncthreads()设备函数
- en: In our prior example of Conway's *Game of Life*, our kernel only updated the
    lattice once for every time it was launched by the host. There are no issues with
    synchronizing all of the threads among the launched kernel in this case, since
    we only had to work with the lattice's previous iteration that was readily available.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的康威生命游戏的示例中，我们的内核每次由主机启动时只更新了晶格一次。在这种情况下，同步所有在启动的内核中的线程没有问题，因为我们只需要处理已经准备好的晶格的上一个迭代。
- en: Now let's suppose that we want to do something slightly different—we want to
    re-write our kernel so that it performs a certain number of iterations on a given
    cell lattice without being re-launched over and over by the host. This may initially
    seem trivial—a naive solution would be to just put an integer parameter to indicate
    the number of iterations and a `for` loop in the inline `conway_ker` kernel, make
    some additional trivial changes, and be done with it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想做一些稍微不同的事情——我们想重新编写我们的内核，以便在给定的细胞点阵上执行一定数量的迭代，而不是由主机一遍又一遍地重新启动。这一开始可能看起来很琐碎——一个天真的解决方案将是只需在内联`conway_ker`内核中放置一个整数参数来指示迭代次数和一个`for`循环，进行一些额外的琐碎更改，然后就完成了。
- en: However, this raises the issue of **race conditions**; this is the issue of
    multiple threads reading and writing to the same memory address and the problems
    that may arise from that. Our old `conway_ker` kernel avoids this issue by using
    two arrays of memory, one that is strictly read from, and one that is strictly
    written to for each iteration. Furthermore, since the kernel only performs a single
    iteration, we are effectively using the host for the synchronization of the threads.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这引发了**竞争条件**的问题；这是多个线程读取和写入相同内存地址以及由此可能产生的问题。我们的旧`conway_ker`内核通过使用两个内存数组来避免这个问题，一个严格用于读取，一个严格用于每次迭代写入。此外，由于内核只执行单次迭代，我们实际上是在使用主机来同步线程。
- en: We want to do multiple iterations of LIFE on the GPU that are fully synchronized;
    we also will want to use a single array of memory for the lattice. We can avoid
    race conditions by using a CUDA device function called `__syncthreads()`. This
    function is a **block level synchronization barrier**—this means that every thread
    that is executing within a block will stop when it reaches a `__syncthreads()`
    instance and wait until each and every other thread within the same block reaches
    that same invocation of `__syncthreads()` before the the threads continue to execute
    the subsequent lines of code.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在GPU上进行多次完全同步的LIFE迭代；我们还希望使用单个内存数组来存储点阵。我们可以通过使用CUDA设备函数`__syncthreads()`来避免竞争条件。这个函数是一个**块级同步屏障**——这意味着在一个块内执行的每个线程在到达`__syncthreads()`实例时都会停止，并等待直到同一块内的每个其他线程都到达`__syncthreads()`的同一调用，然后线程才会继续执行后续的代码行。
- en: '` __syncthreads()` can only synchronize threads within a single CUDA block,
    not all threads within a CUDA grid!'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`__syncthreads()`只能同步单个CUDA块内的线程，而不能同步CUDA网格内的所有线程！'
- en: 'Let''s now create our new kernel; this will be a modification of the prior
    LIFE kernel that will perform a certain number of iterations and then stop. This
    means we''ll not represent this as an animation, just as a static image, so we''ll
    load the appropriate Python modules in the beginning. (This code is also available
    in the `conway_gpu_syncthreads.py` file, in the GitHub repository):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在创建我们的新内核；这将是之前LIFE内核的修改，它将执行一定数量的迭代然后停止。这意味着我们不会将其表示为动画，而是作为静态图像，因此我们将在开始时加载适当的Python模块。（此代码也可在GitHub存储库的`conway_gpu_syncthreads.py`文件中找到）：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let''s again set up our kernel that will compute LIFE:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次设置计算LIFE的内核：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Of course, our CUDA C code will go here, which will be largely the same as
    before. We''ll have to only make some changes to our kernel. Of course, we can
    preserve the device function, `nbrs`. In our declaration, we''ll use only one
    array to represent the cell lattice. We can do this since we''ll be using proper
    thread synchronization. We''ll also have to indicate the number of iterations
    with an integer. We set the parameters as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的CUDA C代码将放在这里，这将大致与以前相同。我们只需要对内核进行一些更改。当然，我们可以保留设备函数`nbrs`。在我们的声明中，我们将只使用一个数组来表示细胞点阵。我们可以这样做，因为我们将使用适当的线程同步。我们还必须用一个整数表示迭代次数。我们设置参数如下：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We''ll continue similarly as before, only iterating with a `for` loop:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续与以前类似，只是使用`for`循环进行迭代：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s recall that previously, we directly set the new cell lattice value directly
    within the array. Here, we''ll hold the value in the `cell_value` variable until
    all of the threads in the block are synchronized. We proceed similarly as before,
    blocking execution with `__syncthreads` until all of the new cell values are determined
    for the current iteration, and only then setting the values within the lattice
    array:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回想一下以前，我们直接在数组中设置新的细胞点阵值。在这里，我们将在`cell_value`变量中保存值，直到块内的所有线程都同步。我们以前也是类似地进行，使用`__syncthreads`阻止执行，直到确定了当前迭代的所有新细胞值，然后才在点阵数组中设置值：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We''ll now launch the kernel as before and display the output, iterating over
    the lattice 1,000,000 times. Note that we are using only a single block in our
    grid, which is of a size of 32 x 32, due to the limit of 1,024 threads per block.
    (Again, it should be emphasized that `__syncthreads` only works over all threads
    in a block, rather than over all threads in a grid, which is why we are limiting
    ourselves to a single block here):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将像以前一样启动内核并显示输出，迭代点阵100万次。请注意，由于每个块的线程限制为1,024个，我们在网格中只使用一个块，大小为32 x 32。（再次强调，`__syncthreads`仅在块中的所有线程上工作，而不是在网格中的所有线程上工作，这就是为什么我们在这里限制自己使用单个块的原因）：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When we run the program, we''ll get the desired output as follows (this is
    what a random LIFE lattice will converge to after one million iterations!):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行程序时，我们将得到以下所需的输出（这是随机LIFE点阵在一百万次迭代后会收敛到的结果！）：
- en: '![](assets/38be0537-84a4-447c-a25b-0f60c15726b1.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/38be0537-84a4-447c-a25b-0f60c15726b1.png)'
- en: Using shared memory
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用共享内存
- en: We can see from the prior example that the threads in the kernel can intercommunicate
    using arrays within the GPU's global memory; while it is possible to use global
    memory for most operations, we can speed things up by using **shared memory**.
    This is a type of memory meant specifically for intercommunication of threads
    within a single CUDA block; the advantage of using this over global memory is
    that it is much faster for pure inter-thread communication. In contrast to global
    memory, though, memory stored in shared memory cannot directly be accessed by
    the host—shared memory must be copied back into global memory by the kernel itself
    first.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从先前的例子中，我们可以看到内核中的线程可以使用GPU全局内存中的数组进行相互通信；虽然可以使用全局内存进行大多数操作，但使用**共享内存**可以加快速度。这是一种专门用于单个CUDA块内线程相互通信的内存类型；与全局内存相比，使用共享内存的优势在于纯线程间通信速度更快。不过，与全局内存相反，存储在共享内存中的内存不能直接被主机访问——共享内存必须首先由内核自己复制回全局内存。
- en: 'Let''s first step back for a moment before we continue and think about what
    we mean. Let''s look at some of the variables that are declared in our iterative
    LIFE kernel that we just saw. Let''s first look at `x` and `y`, two integers that
    hold the Cartesian coordinates of a particular thread''s cell. Remember that we
    are setting their values with the `_X` and `_Y` macros. (Compiler optimizations
    notwithstanding, we want to store these values in variables to reduce computation
    because directly using `_X` and `_Y` will recompute the `x` and `y` values every
    time these macros are referenced in our code):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们先退一步思考一下我们的意思。让我们看看我们刚刚看到的迭代LIFE内核中声明的一些变量。首先看看`x`和`y`，这两个整数保存着特定线程单元的笛卡尔坐标。请记住，我们正在使用`_X`和`_Y`宏设置它们的值。（尽管编译器优化，我们希望将这些值存储在变量中以减少计算，因为直接使用`_X`和`_Y`将在我们的代码中引用这些宏时每次重新计算`x`和`y`的值）：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We note that, for every single thread, there will be a unique Cartesian point
    in the lattice that will correspond to `x` and `y`. Similarly, we use a variable, `n`,
    which is declared with `int n = nbrs(x, y, lattice);`, to indicate the number
    of living neighbors around a particular cell. This is because, when we normally
    declare variables in CUDA, they are by default local to each individual thread.
    Note that, even if we declare an array within a thread such as `int a[10];`, there
    will be an array of size 10 that is local to each thread.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，对于每个单个线程，点阵中将对应于`x`和`y`的唯一笛卡尔点。同样，我们使用一个变量`n`，它声明为`int n = nbrs(x, y,
    lattice);`，来表示特定单元周围的存活邻居的数量。这是因为，当我们通常在CUDA中声明变量时，它们默认是每个单独线程的本地变量。请注意，即使我们在线程内部声明数组如`int
    a[10];`，也将有一个大小为10的数组，它是每个线程的本地数组。
- en: Local thread arrays (for example, a declaration of `int a[10];` within the kernel)
    and pointers to global GPU memory (for example, a value passed as a kernel parameter
    of the form `int * b`) may look and act similarly, but are very different. For
    every thread in the kernel, there will be a separate `a` array that the other
    threads cannot read, yet there is a single `b` that will hold the same values
    and be equally accessible for all of the threads.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本地线程数组（例如，在内核内部声明`int a[10];`）和指向全局GPU内存的指针（例如，以`int * b`形式作为内核参数传递的值）可能看起来和行为类似，但实际上非常不同。对于内核中的每个线程，将有一个单独的`a`数组，其他线程无法读取，但将有一个单独的`b`，它将保存相同的值，并且对所有线程都是同样可访问的。
- en: We are prepared to use shared memory. This allows us to declare variables and
    arrays that are shared among the threads within a single CUDA block. This memory
    is much faster than using global memory pointers (as we have been using till now),
    as well as reduces the overhead of allocating memory in the case of pointers.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备使用共享内存。这使我们能够声明在单个CUDA块内的线程之间共享的变量和数组。这种内存比使用全局内存指针（我们到目前为止一直在使用的）要快得多，同时减少了指针分配内存的开销。
- en: Let's say we want a shared integer array of size 10\. We declare it as follows—`__shared__
    int a[10] `. Note that we don't have to limit ourselves to arrays; we can make
    shared singleton variables as follows: `__shared__ int x`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要一个大小为10的共享整数数组。我们声明如下——`__shared__ int a[10] `。请注意，我们不必局限于数组；我们可以按如下方式创建共享的单例变量：`__shared__
    int x`。
- en: 'Let''s rewrite a few lines of iterative version of LIFE that we saw in the
    last sub-section to make use of shared memory. First, let''s just rename the input
    pointer to `p_lattice`, so we can instead use this variable name on our shared
    array, and lazily preserve all of the references to " lattice" in our code. Since
    we''ll be sticking with a 32 x 32 cell lattice here, we set up the new shared
    `lattice` array as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新编写LIFE的迭代版本的一些行，以利用共享内存。首先，让我们将输入指针重命名为`p_lattice`，这样我们可以在我们的共享数组上使用这个变量名，并在我们的代码中懒惰地保留所有对“lattice”的引用。由于我们将坚持使用32
    x 32个单元的点阵，我们设置新的共享`lattice`数组如下：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We''ll now have to copy all values from the global memory `p_lattice` array into
    `lattice`. We''ll index our shared array exactly in the same way, so we can just
    use our old `_INDEX` macro here. Note that we make sure to put `__syncthreads()`
    after we copy, to ensure that all of the memory accesses to lattice are entirely
    completed before we proceed with the LIFE algorithm:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须将全局内存`p_lattice`数组中的所有值复制到`lattice`中。我们将以完全相同的方式索引我们的共享数组，因此我们可以在这里使用我们旧的`_INDEX`宏。请注意，在复制后我们确保在我们继续LIFE算法之前放置`__syncthreads()`，以确保所有对lattice的内存访问完全完成：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The rest of the kernel is exactly as before, only we have to copy from the
    shared lattice back into the GPU array. We do so as follows and then close off
    the inline code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 内核的其余部分与以前完全相同，只是我们必须将共享的点阵复制回GPU数组。我们这样做，然后关闭内联代码：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can now run this as before, with the same exact test code. (This example
    can be seen in `conway_gpu_syncthreads_shared.py` in the GitHub repository.)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以像以前一样运行，使用完全相同的测试代码。（此示例可以在GitHub存储库中的`conway_gpu_syncthreads_shared.py`中找到。）
- en: The parallel prefix algorithm
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行前缀算法
- en: We'll now be using our new knowledge of CUDA kernels to implement the **parallel
    prefix algorithm**, also known as the **scan design pattern**. We have already
    seen simple examples of this in the form of PyCUDA's `InclusiveScanKernel` and
    `ReductionKernel` functions in the previous chapter. We'll now look into this
    idea in a little more detail.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将利用我们对CUDA核心的新知识来实现**并行前缀算法**，也称为**扫描设计模式**。我们已经在上一章中以PyCUDA的`InclusiveScanKernel`和`ReductionKernel`函数的形式看到了这种简单的例子。现在让我们更详细地了解这个想法。
- en: The central motivation of this design pattern is that we have a binary operator ![](assets/9388a619-6713-4ea7-93d8-a85fc2fd8094.png) ,
    that is to say a function that acts on two input values and gives one output value
    (such as—+, ![](assets/362dcb88-5323-4213-8ea4-03a9785d4984.png), ![](assets/2abe6459-7144-4bdf-9569-b0a70726e422.png) (maximum), ![](assets/380e1f66-b930-42d9-b7d0-a565b74b858f.png) (minimum)),
    and collection of elements, ![](assets/d10897a0-55d1-4a8f-9862-fd43a6f729ea.png),
    and from these we wish to compute ![](assets/d1dace09-f460-4cee-abb6-81691be4dcf6.png) efficiently.
    Furthermore, we make the assumption that our binary operator ![](assets/2c1163a9-8a0f-480d-be93-f5cbaa606034.png) is
    **associative**—this means that, for any three elements, *x*, *y*, and *z*, we
    always have:![](assets/7f9f94dd-6751-4a0e-abcc-32333a71812d.png) .
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设计模式的核心动机是，我们有一个二元运算符![](assets/9388a619-6713-4ea7-93d8-a85fc2fd8094.png)，也就是说，一个作用于两个输入值并给出一个输出值的函数（比如—+，![](assets/362dcb88-5323-4213-8ea4-03a9785d4984.png)，![](assets/2abe6459-7144-4bdf-9569-b0a70726e422.png)（最大值），![](assets/380e1f66-b930-42d9-b7d0-a565b74b858f.png)（最小值）），和元素的集合![](assets/d10897a0-55d1-4a8f-9862-fd43a6f729ea.png)，我们希望从中高效地计算![](assets/d1dace09-f460-4cee-abb6-81691be4dcf6.png)。此外，我们假设我们的二元运算符![](assets/2c1163a9-8a0f-480d-be93-f5cbaa606034.png)是**可结合的**—这意味着，对于任意三个元素*x*、*y*和*z*，我们总是有:![](assets/7f9f94dd-6751-4a0e-abcc-32333a71812d.png)。
- en: We wish to retain the partial results, that is the *n - 1* sub-computations—![](assets/cadd1c5c-4dfa-45f8-bca5-e3e810c6187b.png).
    The aim of the parallel prefix algorithm is to produce this collection of *n*
    sums efficiently. It normally takes *O(n)* time to produce these *n* sums in a
    serial operation, and we wish to reduce the time complexity.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望保留部分结果，也就是*n-1*个子计算—![](assets/cadd1c5c-4dfa-45f8-bca5-e3e810c6187b.png)。并行前缀算法的目的是高效地产生这些*n*个和。在串行操作中，通常需要*O(n)*的时间来产生这些*n*个和，我们希望降低时间复杂度。
- en: When the terms "parallel prefix" or "scan" are used, it usually means an algorithm
    that produces all of these *n* results, while "reduce"/"reduction" usually means
    an algorithm that only yields the single final result, ![](assets/864d06dd-ccd8-48c3-8a0a-fd437cd6436e.png).
    (This is the case with PyCUDA.)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用术语“并行前缀”或“扫描”时，通常意味着一个产生所有这些*n*个结果的算法，而“减少”/“归约”通常意味着只产生单个最终结果，![](assets/864d06dd-ccd8-48c3-8a0a-fd437cd6436e.png)。（这是PyCUDA的情况。）
- en: There are actually several variations of the parallel prefix algorithm, and
    we'll first start with the simplest (and oldest) version first, which is called
    the naive parallel prefix algorithm.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，并行前缀算法有几种变体，我们将首先从最简单（也是最古老的）版本开始，这就是所谓的天真并行前缀算法。
- en: The naive parallel prefix algorithm
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天真并行前缀算法
- en: 'The **naive parallel prefix algorithm** is the original version of this algorithm;
    this algorithm is "naive" because it makes an assumption that given *n* input
    elements, ![](assets/2a2cf114-8fcf-41ec-83de-66803d5c7345.png), with the further
    assumption that *n* is *dyadic* (that is, ![](assets/f23d2bc1-c7c4-44fc-a947-ac7571697516.png) for
    some positive integer, *k*), and we can run the algorithm in parallel over *n*
    processors (or *n* threads). Obviously, this will impose strong limits on the
    cardinality *n* of sets that we may process. However, given these conditions are
    satisfied, we have a nice result in that its computational time complexity is
    only *O(log n)*. We can see this from the pseudocode of the algorithm. Here, we''ll
    indicate the input values with ![](assets/79ff2d6e-25d9-478d-8a3c-5fe79faa77cf.png) and
    the output values as ![](assets/be8218cd-e2fd-4453-87fd-50f3cf71308c.png):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法是原始版本的**天真并行前缀算法**；这个算法是“天真的”，因为它假设给定*n*个输入元素，![](assets/2a2cf114-8fcf-41ec-83de-66803d5c7345.png)，进一步假设*n*是*二进制*的（也就是说，![](assets/f23d2bc1-c7c4-44fc-a947-ac7571697516.png)对于某个正整数*k*），我们可以在*n*个处理器（或*n*个线程）上并行运行算法。显然，这将对我们可以处理的集合的基数*n*施加严格的限制。然而，只要满足这些条件，我们就有一个很好的结果，即其计算时间复杂度仅为*O(log
    n)*。我们可以从算法的伪代码中看到这一点。在这里，我们将用![](assets/79ff2d6e-25d9-478d-8a3c-5fe79faa77cf.png)表示输入值，用![](assets/be8218cd-e2fd-4453-87fd-50f3cf71308c.png)表示输出值：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now, we can clearly see that this will take *O(log n)* asymptotic time, as the
    outer loop is parallelized over the `parfor` and the inner loop takes *log[2](n)*.
    It should be easy to see after a few minutes of thought that the *y[i]* values
    will yield our desired output.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以清楚地看到这将花费*O(log n)*的渐近时间，因为外部循环是在`parfor`上并行化的，内部循环需要*log[2](n)*。经过几分钟的思考，很容易看出*y[i]*值将产生我们期望的输出。
- en: Now let's begin our implementation; here, our binary operator will simply be
    addition. Since this example is illustrative, this kernel will be strictly over
    1,024 threads.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始我们的实现；在这里，我们的二元运算符将简单地是加法。由于这个例子是说明性的，这个核心代码将严格地针对1,024个线程。
- en: 'Let''s just set up the header and dive right into writing our kernel:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先设置好头文件，然后开始编写我们的核心代码：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'So, let''s look at what we have: we represent our input elements as a GPU array
    of doubles, that is `double *vec`, and represent the output values with `double
    *out`. We declare a shared memory `sum_buf` array that we''ll use for the calculation
    of our output. Now, let''s look at the implementation of the algorithm itself:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看我们有什么：我们将我们的输入元素表示为双精度GPU数组，即`double *vec`，并用`double *out`表示输出值。我们声明一个共享内存`sum_buf`数组，我们将用它来计算我们的输出。现在，让我们看看算法本身的实现：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Of course, there is no `parfor,` which is implicit over the `tid` variable, which
    indicates the thread number. We are also able to omit the use of *log[2]* and
    *2^i* by starting with a variable that is initialized to 1, and then iteratively
    multiplying by 2 every iteration of i. (Note that if we want to be even more technical,
    we can do this with the bitwise shift operators .) We bound the iterations of `i`
    by 10, since *2^(10) = 1024*. Now we''ll close off our new kernel as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这里没有`parfor`，它是隐式的，通过`tid`变量表示线程编号。我们还可以通过从初始化为1的变量开始，然后在每次i的迭代中乘以2来省略*log[2]*和*2^i*的使用。（请注意，如果我们想更加技术化，我们可以使用位移运算符来实现这一点。）我们将`i`的迭代限制在10次，因为*2^(10)
    = 1024*。现在我们将结束我们的新内核如下：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s now look at the test code following the kernel:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下内核后面的测试代码：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We're only going to concern ourselves with the final sum in the output, which
    we retrieve with `outvec_gpu[-1].get()`, recalling that the "-1" index gives the
    last member of an array in Python. This will be the sum of every element in `vec`;
    the partial sums are in the prior values of `outvec_gpu`. (This example can be
    seen in the `naive_prefix.py` file in the GitHub repository.)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关心输出中的最终总和，我们使用`outvec_gpu[-1].get()`来检索它，需要回想一下，"-1"索引给出了Python数组中的最后一个成员。这将是`vec`中每个元素的总和；部分总和在`outvec_gpu`的先前值中。（此示例可以在GitHub存储库中的`naive_prefix.py`文件中看到。）
- en: By its nature, the parallel prefix algorithm has to run over *n* threads, corresponding
    to a size-n array, where *n* is dyadic (again, this means that *n* is some power
    of 2). However, we can extend this algorithm to an arbitrary non-dyadic size assuming
    that our operator has a **identity element** (or equivalently, **neutral element**)—that
    is to say, that there is some value *e* so that for any *x* value, we have—![](assets/9a41546d-6056-4ca9-bfdd-0e9ec13ae195.png). 
    In the case that our operator is + , the identity element is 0; in the case that
    it is ![](assets/04590efc-4d27-477f-a7bb-96da2f050011.png), it is 1; all we do
    then is just pad the elements ![](assets/f1e4ffaf-26a6-4f45-b9f4-20340837e38c.png) with
    a series of *e* values so that we have the a dyadic cardinality of the new set
    ![](assets/8c22f00e-f896-44d2-99f2-7d5ecc3b35eb.png).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其性质，并行前缀算法必须在*n*个线程上运行，对应于一个大小为n的数组，其中*n*是二进制的（这意味着*n*是2的某个幂）。然而，我们可以将这个算法扩展到任意非二进制大小，假设我们的运算符具有**单位元素**（或等效地，**中性元素**）——也就是说，存在某个值*e*，使得对于任何*x*值，我们有 ![](assets/9a41546d-6056-4ca9-bfdd-0e9ec13ae195.png)。在运算符为+的情况下，单位元素是0；在运算符为 ![](assets/04590efc-4d27-477f-a7bb-96da2f050011.png) 的情况下，它是1；然后我们只需用一系列*e*值填充 ![](assets/f1e4ffaf-26a6-4f45-b9f4-20340837e38c.png) 的元素，以便我们有新集合的二进制基数 ![](assets/8c22f00e-f896-44d2-99f2-7d5ecc3b35eb.png)。
- en: Inclusive versus exclusive prefix
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 包含与排他前缀
- en: Let's stop for a moment and make a very subtle, but very important distinction.
    So far, we have been concerned with taking inputs of the form ![](assets/cb300a9a-6dff-4df9-b962-91cb62ccd143.png) ,
    and as output producing an array of sums of the form ![](assets/244e529c-37f3-46ba-a492-b34765482abb.png).
    Prefix algorithms that produce output as such are called **inclusive**; in the
    case of an **inclusive prefix algorithm**, the corresponding element at each index
    is included in the summation in the same index of the output array. This is in
    contrast to prefix algorithms that are **exclusive**. An **exclusive prefix algorithm**
    differs in that it similarly takes *n* input values of the form ![](assets/1bba2db0-de4c-42d3-a676-d482b67584c2.png) and
    produces the length-*n* output array ![](assets/c789f125-403d-4264-b4f4-a57d73c11977.png).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停下来，做一个非常微妙但非常重要的区分。到目前为止，我们一直关心接受形式为 ![](assets/cb300a9a-6dff-4df9-b962-91cb62ccd143.png) 的输入，并产生形式为 ![](assets/244e529c-37f3-46ba-a492-b34765482abb.png) 的总和数组作为输出。产生这种输出的前缀算法称为**包含**；在**包含前缀算法**的情况下，每个索引处的相应元素包含在输出数组的相同索引处的总和中。这与**排他前缀算法**形成对比。**排他前缀算法**不同之处在于，它同样接受形式为 ![](assets/1bba2db0-de4c-42d3-a676-d482b67584c2.png) 的*n*个输入值，并产生长度为*n*的输出数组 ![](assets/c789f125-403d-4264-b4f4-a57d73c11977.png)。
- en: This is important because some efficient variations of the prefix algorithm
    are exclusive by their nature. We'll see an example of one in the next sub-section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为一些高效的前缀算法的变体天生就是排他的。我们将在下一个小节中看到一个例子。
- en: Note that the exclusive algorithm yields nearly the same output as the inclusive
    algorithm, only it is right-shifted and omits the final value. We can therefore
    trivially obtain the equivalent output from either algorithm, provided we keep
    a copy of ![](assets/179c86ba-3a68-4b15-98bf-24d9e930e963.png).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，排他算法产生的输出与包含算法几乎相同，只是右移并省略了最后一个值。因此，我们可以从任一算法中轻松获得等效输出，只要我们保留 ![](assets/179c86ba-3a68-4b15-98bf-24d9e930e963.png) 的副本。
- en: A work-efficient parallel prefix algorithm
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个高效的并行前缀算法
- en: Before we continue with our new algorithm, we'll look at the naive algorithm
    from two perspectives. In an ideal case, the computational time complexity is
    *O(log n)*, but this is only when we have a sufficient number of processors for
    our data set; when the cardinality (number of elements) of our dataset, *n*, is
    much larger than the number of processors, we have that this becomes an *O(n log
    n)* time algorithm.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续使用新算法之前，我们将从两个角度看待朴素算法。在理想情况下，计算时间复杂度为*O(log n)*，但这仅当我们有足够数量的处理器用于我们的数据集时成立；当数据集的基数（元素数量）*n*远大于处理器数量时，这将成为*O(n
    log n)*时间复杂度的算法。
- en: Let's define a new concept with relation to our binary operator ![](assets/b89f2449-e123-451e-a958-6a782b932821.png)—the
    **work** performed by a parallel algorithm here is the number of invocations of
    this operator across all threads for the duration of the execution. Similarly,
    the **span** is the number of invocations a thread makes in the duration of execution
    of the kernel; while the **span** of the whole algorithm is the same as the longest
    span among each individual thread, which will tell us the total execution time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个与我们的二进制运算符 ![](assets/b89f2449-e123-451e-a958-6a782b932821.png)相关的新概念——这里并行算法的**工作**是执行期间所有线程对此运算符的调用次数。同样，**跨度**是线程在内核执行期间进行调用的次数；而整个算法的**跨度**与每个单独线程的最长跨度相同，这将告诉我们总执行时间。
- en: We seek to specifically reduce the amount of work performed by the algorithm
    across all threads, rather than focus merely span. In the case of the naive prefix,
    the additional work that is required costs a more time when the number of available
    processors falls short; this extra work will just spill over into the limited
    number of processors available.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求特别减少算法在所有线程中执行的工作量，而不仅仅是关注跨度。在简单前缀的情况下，所需的额外工作在可用处理器数量不足时会花费更多时间；这些额外工作将溢出到有限数量的可用处理器中。
- en: We'll present a new algorithm that is **work efficient**, and hence more suitable
    for a limited number of processors. This consists of two separate two distinct
    parts—the **up-sweep (or reduce) phase** and the **down-sweep phase**. We should
    also note the algorithm we'll see is an exclusive prefix algorithm.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍一种新的算法，它是**工作高效**的，因此更适合有限数量的处理器。这包括两个独立的部分——**向上扫描（或减少）阶段**和**向下扫描阶段**。我们还应该注意，我们将看到的算法是一种独占前缀算法。
- en: The **up-sweep phase** is similar to a single reduce operation to produce the
    value that is given by the reduce algorithm, that is ![](assets/f380e618-5246-4853-b1eb-976537a28ab6.png) ;
    in this case we retain the partial sums (![](assets/b739e65c-64e0-4a07-ae84-740401252763.png))
    that are required the achieve the end result. The down-sweep phase will then operate
    on these partial sums and give us the final result. Let's look at some pseudocode,
    starting with the up-sweep phase. (The next subsection will then dive into the
    implementation from the pseudocode immediately.)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**向上扫描阶段**类似于单个减少操作，以产生由减少算法给出的值，即 ![](assets/f380e618-5246-4853-b1eb-976537a28ab6.png) ；在这种情况下，我们保留所需的部分和（![](assets/b739e65c-64e0-4a07-ae84-740401252763.png)）以实现最终结果。然后，向下扫描阶段将对这些部分和进行操作，并给出最终结果。让我们看一些伪代码，从向上扫描阶段开始。（接下来的小节将立即深入到伪代码的实现中。）'
- en: Work-efficient parallel prefix (up-sweep phase)
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作高效的并行前缀（向上扫描阶段）
- en: 'This is the pseudocode for the up-sweep. (Notice the `parfor` over the `j` variable, which
    means that this block of code can be parallelized over threads indexed by `j`):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是向上扫描的伪代码。（注意`parfor`覆盖`j`变量，这意味着此代码块可以并行化，由`j`索引的线程）：
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Work-efficient parallel prefix (down-sweep phase)
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作高效的并行前缀（向下扫描阶段）
- en: 'Now let''s continue with the down-sweep, which will operate on the output of
    the up-sweep:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续向下扫描，它将操作向上扫描的输出：
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Work-efficient parallel prefix — implementation
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作高效的并行前缀 — 实现
- en: As a capstone for this chapter, we'll write an implementation of this algorithm
    that can operate on arrays of arbitrarily large size over 1,024\. This will mean
    that this will operate over grids as well as blocks; that being such, we'll have
    to use the host for synchronization; furthermore, this will require that we implement
    two separate kernels for up-sweep and down-sweep phases that will act as the `parfor` loops
    in both phases, as well as Python functions that will act as the outer `for` loop
    for the up- and down-sweeps.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '作为本章的压轴，我们将编写该算法的实现，该算法可以在大于1,024的任意大小的数组上操作。这意味着这将在网格和块上操作；因此，我们将不得不使用主机进行同步；此外，这将要求我们为向上扫描和向下扫描阶段实现两个单独的内核，这将作为两个阶段的`parfor`循环，以及作为向上和向下扫描的外部`for`循环的Python函数。 '
- en: 'Let''s begin with an up-sweep kernel. Since we''ll be iteratively re-launching
    this kernel from the host, we''ll also need a parameter that indicates current
    iteration (`k`). We''ll use two arrays for the computation to avoid race conditions—`x` (for
    the current iteration) and `x_old` (for the prior iteration). We declare the kernel
    as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从向上扫描内核开始。由于我们将从主机迭代重新启动此内核，我们还需要一个指示当前迭代（`k`）的参数。我们将使用两个数组进行计算，以避免竞争条件——`x`（用于当前迭代）和`x_old`（用于之前的迭代）。我们声明内核如下：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now let''s set the `tid` variable, which will be the current thread''s identification
    among *all* threads in *all* *blocks* in the grid. We use the same trick as in
    our original grid-level implementation of Conway''s *Game of Life* that we saw
    earlier:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们设置`tid`变量，它将是网格中*所有*块中*所有*线程中当前线程的标识。我们使用与我们之前看到的康威生命游戏的原始网格级实现相同的技巧：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We''ll now use C bit-wise shift operators to generate 2^k and 2^(k+1 )directly
    from `k`. We now set `j` to be `tid` times `_2k1`—this will enable us to remove
    the "if `j` is divisible by 2^(k+1)", as in the pseudocode, enabling us to only
    launch as many threads as we''ll need:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用C位移运算符直接从`k`生成2^k和2^(k+1 )。我们现在将`j`设置为`tid`乘以`_2k1`—这将使我们能够删除“如果`j`可被2^(k+1)整除”的部分，如伪代码中所示，从而使我们只启动所需数量的线程：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We can easily generate dyadic (power-of-2) integers in CUDA C with the left
    bit-wise shift operator (`<<`). Recall that the integer 1 (that is 2⁰) is represented
    as 0001, 2 (2¹) is represented as 0010, 4 (2² ) is represented as 0100, and so
    on. We can therefore compute 2^k with the `1 << k` operation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用CUDA C中的左位移运算符（`<<`）轻松生成二进制（2的幂）整数。请记住，整数1（即2⁰）表示为0001，2（2¹）表示为0010，4（2²）表示为0100，依此类推。因此，我们可以使用`1
    << k`操作计算2^k。
- en: 'We can now run the up-sweep phase with a single line, noting that `j` is indeed
    divisible by 2^(k+1) by its construction:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行上扫描阶段的单行代码，注意`j`确实可以被2^(k+1)整除，因为它的构造方式是这样的：
- en: '[PRE37]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We're done writing our kernel! But this is not a full implementation of the
    up-sweep, of course. We have to do the rest in Python. Let's get our kernel and
    begin the implementation. This mostly speaks for itself as it follows the pseudocode
    exactly; we should recall that we are updating `x_old_gpu` by copying from `x_gpu`
    using `[:]`, which will preserve the memory allocation and merely copy the new
    data over rather than re-allocate. Also note how we set our block and grid sizes
    depending on how many threads we have to launch—we try to keep our block sizes
    as multiples of size 32 (which is our rule-of-thumb in this text, we go into the
    details why we use 32 specifically in [Chapter 11](e853faad-3ee4-4df7-9cdb-98f74e435527.xhtml),
    *Performance Optimization in CUDA*). We should put `from __future__ import division` at
    the beginning of our file, since we'll use Python 3-style division in calculating
    our block and kernel sizes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经编写完我们的核心代码了！但这当然不是完整的上扫描实现。我们还需要在Python中完成其余部分。让我们拿到我们的核心代码并开始实现。这基本上是按照伪代码进行的，我们应该记住，我们通过使用`[:]`从`x_gpu`复制到`x_old_gpu`来更新`x_old_gpu`，这将保留内存分配，并仅复制新数据而不是重新分配。还要注意，我们根据要启动的线程数量设置我们的块和网格大小
    - 我们尝试保持我们的块大小为32的倍数（这是本文中的经验法则，我们在[第11章](e853faad-3ee4-4df7-9cdb-98f74e435527.xhtml)中详细介绍为什么我们特别使用32，*CUDA性能优化*）。我们应该在文件开头加上`from
    __future__ import division`，因为我们将使用Python 3风格的除法来计算我们的块和核心大小。
- en: 'One issue to mention is that we are assuming that `x` is of dyadic length 32
    or greater—this can be modified trivially if you wish to have this operate on
    arrays of other sizes by padding our arrays with zeros, however:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有一点需要提到的是，我们假设`x`的长度是二进制长度32或更大 - 如果您希望将其操作在其他大小的数组上，可以通过用零填充我们的数组来轻松修改，然而：
- en: '[PRE38]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now we''ll embark on writing the down-sweep. Again, let''s start with the kernel,
    which will have the functionality of the inner `parfor` loop of the pseudocode.
    It follows similarly as before—again, we''ll use two arrays, so using a `temp` variable
    as in the pseudocode is unnecessary here, and again we use bit-shift operators
    to obtain the values of 2^k and 2^(k+1). We calculate `j` similarly to before:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将开始编写下扫描。同样，让我们从核心开始，它将具有伪代码中内部`parfor`循环的功能。它与之前类似 - 再次使用两个数组，因此在这里使用`temp`变量与伪代码中是不必要的，我们再次使用位移运算符来获得2^k和2^(k+1)的值。我们计算`j`与之前类似：
- en: '[PRE39]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We now can write our Python function that will iteratively launch the kernel,
    which corresponds to the outer `for` loop of the down-sweep phase. This is similar
    to the Python function for the up-sweep phase. One important distinction from
    looking at the pseudocode is that we have to iterate from the largest value in
    the outer `for` loop to the smallest; we can just use Python''s `reversed` function
    to do this. Now we can implement the down-sweep phase:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写一个Python函数，该函数将迭代地启动核心，这对应于下扫描阶段的外部`for`循环。这类似于上扫描阶段的Python函数。从伪代码中看到的一个重要区别是，我们必须从外部`for`循环中的最大值迭代到最小值；我们可以使用Python的`reversed`函数来做到这一点。现在我们可以实现下扫描阶段：
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Having implemented both the up-sweep and down-sweep phases, our last task is
    trivial to complete:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现了上扫描和下扫描阶段之后，我们的最后任务是轻而易举地完成：
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We have now fully implemented a host-synchronized version of the work-efficient
    parallel prefix algorithm! (This implementation is available in the `work-efficient_prefix.py`
    file in the repository, along with some test code.)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完全实现了一个与主机同步的高效并行前缀算法！（这个实现可以在存储库中的`work-efficient_prefix.py`文件中找到，还有一些测试代码。）
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started with an implementation of Conway's *Game of Life*, which gave us
    an idea of how the many threads of a CUDA kernel are organized in a block-grid
    tensor-type structure. We then delved into block-level synchronization by way
    of the CUDA function, `__syncthreads()`, as well as block-level thread intercommunication
    by using shared memory; we also saw that single blocks have a limited number of
    threads that we can operate over, so we'll have to be careful in using these features
    when we create kernels that will use more than one block across a larger grid.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从康威的*生命游戏*实现开始，这给了我们一个关于CUDA核心的许多线程是如何在块-网格张量类型结构中组织的想法。然后，我们通过CUDA函数`__syncthreads()`深入研究了块级同步，以及通过使用共享内存进行块级线程互通；我们还看到单个块有一定数量的线程，我们可以操作，所以在创建将使用多个块跨越更大网格的核心时，我们必须小心使用这些功能。
- en: We gave an overview of the theory of parallel prefix algorithms, and we ended
    by implementing a naive parallel prefix algorithm as a single kernel that could
    operate on arrays limited by a size of 1,024 (which was synchronized with `___syncthreads`
    and performed both the `for` and `parfor` loops internally), and with a work-efficient
    parallel prefix algorithm that was implemented across two kernels and three Python
    functions could operate on arrays of arbitrary size, with the kernels acting as
    the inner `parfor` loops of the algorithm, and with the Python functions effectively
    operating as the outer `for` loops and synchronizing the kernel launches.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们概述了并行前缀算法的理论，并最后实现了一个天真的并行前缀算法，作为一个单个核心，可以操作大小受限的数组，该数组与`___syncthreads`同步，并在内部执行`for`和`parfor`循环，并且实现了一个高效的并行前缀算法，该算法跨两个核心和三个Python函数实现，可以操作任意大小的数组，核心充当算法的内部`parfor`循环，而Python函数有效地充当外部`for`循环并同步核心启动。
- en: Questions
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Change the random vector in `simple_scalar_multiply_kernel.py` so that it is
    of a length of 10,000, and modify the `i` index in the definition of the kernel
    so that it can be used over multiple blocks in the form of a grid. See if you
    can now launch this kernel over 10,000 threads by setting block and grid parameters
    to something like `block=(100,1,1)` and `grid=(100,1,1)`.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改`simple_scalar_multiply_kernel.py`中的随机向量，使其长度为10,000，并修改内核定义中的`i`索引，以便它可以在网格形式的多个块中使用。看看现在是否可以通过将块和网格参数设置为`block=(100,1,1)`和`grid=(100,1,1)`来启动这个内核超过10,000个线程。
- en: In the previous question, we launched a kernel that makes use of 10,000 threads
    simultaneously; as of 2018, there is no NVIDIA GPU with more than 5,000 cores.
    Why does this still work and give the expected results?
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一个问题中，我们启动了一个同时使用10,000个线程的内核；截至2018年，没有NVIDIA GPU具有超过5,000个核心。为什么这仍然有效并产生预期的结果？
- en: The naive parallel prefix algorithm has time complexity O(*log n*) given that
    we have *n* or more processors for a dataset of size *n*. Suppose that we use
    a naive parallel prefix algorithm on a GTX 1050 GPU with 640 cores. What does
    the asymptotic time complexity become in the case that `n >> 640`?
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 天真的并行前缀算法在数据集大小为*n*的情况下，时间复杂度为O(*log n*)，假设我们有*n*个或更多处理器。假设我们在具有640个核心的GTX 1050
    GPU上使用天真的并行前缀算法。在`n >> 640`的情况下，渐近时间复杂度会变成什么？
- en: Modify `naive_prefix.py` to operate on arrays of arbitrary size (possibly non-dyadic),
    only bounded by 1,024.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`naive_prefix.py`以在大小为1,024的数组上运行（可能是非二进制的）。
- en: The `__syncthreads()` CUDA device function only synchronizes threads across
    a single block. How can we synchronize across all threads in all blocks across
    a grid?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`__syncthreads()` CUDA设备函数只在单个块中同步线程。我们如何在整个网格的所有块中同步所有线程？'
- en: You can convince yourself that the second prefix sum algorithm really is more
    work-efficient than the naive prefix sum algorithm with this exercise. Suppose
    that we have a dataset of size 32\. What is the exact number of "addition" operations
    required by the first and second algorithm in this case?
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过这个练习说服自己，第二个前缀和算法确实比天真的前缀和算法更有效。假设我们有一个大小为32的数据集。在这种情况下，第一个和第二个算法所需的“加法”操作的确切数量是多少？
- en: In the implementation of the work-efficient parallel prefix we use a Python
    function to iterate our kernels and synchronize the results. Why can't we just
    put a `for` loop inside the kernels with careful use of `__syncthreads()` instead?
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在高效并行前缀的实现中，我们使用Python函数来迭代我们的内核并同步结果。为什么我们不能在内核中使用`for`循环，同时小心地使用`__syncthreads()`？
- en: Why does it make more sense to implement the naive parallel prefix within a
    single kernel that handles its own synchronization within CUDA C, than it makes
    more sense to implement the work-efficient parallel prefix using both kernels
    and Python functions and have the host handle the synchronization?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在CUDA C中实现天真的并行前缀在一个单独的内核中处理自己的同步比使用两个内核和Python函数实现高效并行前缀更有意义，并且让主机处理同步更有意义？
