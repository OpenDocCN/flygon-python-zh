- en: Concurrency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下食谱：
- en: ThreadPools—running tasks concurrently through a pool of threads
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程池-通过线程池并发运行任务
- en: Coroutines—interleaving the execution of code through coroutines
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协程-通过协程交错执行代码
- en: Processes—dispatching work to multiple subprocesses
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程-将工作分派给多个子进程
- en: Futures—futures represent a task that will complete in the future
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期货-期货代表将来会完成的任务
- en: Scheduled tasks—setting a task to run at a given time, or every few seconds
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划任务-设置在特定时间运行的任务，或每隔几秒运行一次
- en: Sharing data between processes—managing variables that are accessible across
    multiple processes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进程之间共享数据-管理可在多个进程中访问的变量
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Concurrency is the ability to run two or more tasks in the same time span,
    whether they are parallel or not. Python provides many tools to implement concurrency
    and asynchronous behaviors: threads, coroutines, and processes. While some of
    them don''t allow real parallelism due to their design (coroutines), or due to
    a Global Interpreter Lock (threads), they are very easy to use and can be leveraged
    to perform parallel I/O operations or to interleave functions with minimum effort.
    When real parallelism is required, multiprocessing is easy enough in Python to
    be a viable solution for any kind of software.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 并发是在相同的时间段内运行两个或多个任务的能力，无论它们是并行的还是不并行的。Python提供了许多工具来实现并发和异步行为：线程、协程和进程。虽然其中一些由于设计（协程）或全局解释器锁（线程）的原因不允许真正的并行，但它们非常易于使用，并且可以用于执行并行I/O操作或以最小的工作量交错函数。当需要真正的并行时，Python中的多进程足够容易，可以成为任何类型软件的可行解决方案。
- en: This chapter will cover the most common ways to achieve concurrency in Python,
    will show you how to perform asynchronous tasks that will wait in the background
    for certain conditions, and how to share data between processes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍在Python中实现并发的最常见方法，将向您展示如何执行异步任务，这些任务将在后台等待特定条件，并且如何在进程之间共享数据。
- en: ThreadPools
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程池
- en: Threads have been, historically, the most common way to achieve concurrency
    within software.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 线程在软件中实现并发的历史上一直是最常见的方式。
- en: In theory, when the system allows, these threads can achieve real parallelism,
    but in Python, the **Global Interpreter Lock** (**GLI**) doesn't allow threads
    actually to leverage multicore systems, as the lock will allow a single Python
    operation to proceed at any given time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，当系统允许时，这些线程可以实现真正的并行，但在Python中，全局解释器锁（GIL）不允许线程实际上利用多核系统，因为锁将允许单个Python操作在任何给定时间进行。
- en: For this reason, threads are frequently undervalued in Python, but in fact,
    even when the GIL is involved, they can be a very convenient solution to run I/O
    operations concurrently.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，线程在Python中经常被低估，但实际上，即使涉及GIL，它们也可以是运行I/O操作的非常方便的解决方案。
- en: While using coroutines, we would need a `run` loop and some custom code to ensure
    that the I/O operation proceeds in parallel. Using threads, we can run any kind
    of function within a thread and, if that function does some kind of I/O, such
    as reading from a socket or from a disk, the other threads will proceed in the
    meantime.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用协程时，我们需要一个`run`循环和一些自定义代码来确保I/O操作可以并行进行。使用线程，我们可以在线程中运行任何类型的函数，如果该函数进行某种I/O操作，例如从套接字或磁盘中读取，其他线程将同时进行。
- en: 'One of the major drawbacks of threads is the cost of spawning them. That''s
    frequently stated as one of the reasons why coroutines can be a better solution,
    but there is a way to avoid paying that cost whenever you need a thread: `ThreadPool`.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 线程的一个主要缺点是产生它们的成本。这经常被认为是协程可能是更好的解决方案的原因之一，但是有一种方法可以避免在需要线程时支付成本：`ThreadPool`。
- en: A `ThreadPool` is a set of threads that is usually started when your application
    starts and sits there doing nothing until you actually have some work to dispatch.
    This way, when we have a task that we want to run into a separate thread, we just
    have to send it to `ThreadPool`, and `ThreadPool` will assign it to the first
    available thread out of all the threads that it owns. As those threads are already
    there and running, we don't have to pay the cost to spawn a thread every time
    we have work to do.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`ThreadPool`是一组线程，通常在应用程序启动时启动，并且一直保持空闲，直到您实际上有一些工作要分派。这样，当我们有一个任务想要在单独的线程中运行时，我们只需将其发送到`ThreadPool`，`ThreadPool`将把它分配给它拥有的所有线程中的第一个可用线程。由于这些线程已经在那里运行，我们不必每次有工作要做时都支付产生线程的成本。'
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps for this recipe are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此食谱的步骤如下：
- en: 'To showcase how `ThreadPool` works, we will need two operations that we want
    to run concurrently. One will fetch a URL from the web, which might take some
    time:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了展示`ThreadPool`的工作原理，我们需要两个我们想要同时运行的操作。一个将从网络中获取一个URL，这可能需要一些时间：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The other will just wait for a given condition to be true, looping over and
    over until it''s done:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个将只是等待给定条件为真，一遍又一遍地循环，直到完成：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then we will just fire the download for `https://httpbin.org/delay/3`, which
    will take 3 seconds, and concurrently wait for the download to complete.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将只下载`https://httpbin.org/delay/3`，这将需要3秒，并且同时等待下载完成。
- en: 'To do so, we will run the two tasks in a `ThreadPool` (of four threads), and
    we will wait for both of them to complete:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，我们将在一个`ThreadPool`（四个线程）中运行这两个任务，并等待它们都完成：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works...
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`ThreadPool` is made of two major components: a bunch of threads and a bunch
    of queues. When the pool is created, a few orchestration threads are started together
    with as many worker threads as you specified at pool initialization.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`ThreadPool`由两个主要组件组成：一堆线程和一堆队列。在创建池时，一些协调线程与您在池初始化时指定的工作线程一起启动。'
- en: The worker threads will be in charge of actually running the tasks you dispatch
    to them, while the orchestration threads will be in charge of managing the worker
    threads, doing things such as telling them to quit when the pool is closed, or
    restarting them when they crash.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 工作线程将负责实际运行分派给它们的任务，而编排线程将负责管理工作线程，例如在池关闭时告诉它们退出，或在它们崩溃时重新启动它们。
- en: If no number of worker threads is provided, `TaskPool` will just start as many
    threads as the amount of cores on your system as returned by `os.cpu_count()`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有提供工作线程的数量，`TaskPool`将会启动与系统核心数量相同的线程，由`os.cpu_count()`返回。
- en: Once the threads are started, they will just sit there waiting to consume something
    from the queue containing the work that is to be done. As soon as the queue has
    an entry, the worker thread will wake up and consume it, starting the work.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦线程启动，它们将等待从包含要完成的工作的队列中获取内容。一旦队列有条目，工作线程将唤醒并消耗它，开始工作。
- en: Once the work is done, the job and its result are put back into the results
    queue so that whoever was waiting for them can fetch them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 工作完成后，工作及其结果将放回结果队列，以便等待它们的人可以获取它们。
- en: 'So, when we created `TaskPool`, we actually started four workers that began
    waiting for anything to do from the tasks queue:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们创建`TaskPool`时，实际上启动了四个工作线程，这些线程开始等待从任务队列中获取工作：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, once we provided work for the `TaskPool`, we actually queued up two functions
    into the tasks queue, and as soon as a worker became available, it fetched one
    of them and started running it:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦我们为`TaskPool`提供了工作，实际上我们将两个函数排入任务队列，一旦有工作线程可用，它就会获取其中一个并开始运行：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Meanwhile, `TaskPool` returns an `AsyncResult` object, which has two interesting
    methods: `AsyncResult.ready()`, which tells us whether the result is ready (the
    task finished), and `AsyncResult.get()`, which returns the result once it''s available.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，`TaskPool`返回一个`AsyncResult`对象，该对象有两个有趣的方法：`AsyncResult.ready()`告诉我们结果是否准备好（任务完成），`AsyncResult.get()`在结果可用时返回结果。
- en: 'The second function we queued up was the one that would wait for a specific
    predicate to be `True`, and in this case, we provided `t1.ready`, which is the
    ready method of the previous `AsyncResult`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们排队的第二个函数是等待特定谓词为`True`的函数，在这种情况下，我们提供了` t1.ready`，这是先前`AsyncResult`的就绪方法：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This means that the second task will complete once the first one completes,
    as it will wait until `t1.ready() == True`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着第二个任务将在第一个任务完成后完成，因为它将等待直到`t1.ready() == True`。
- en: 'Once both of the tasks are running, we tell `pool` that we have nothing more
    to do, so that it can quit once it''s finished what it''s doing:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这两个任务都在运行，我们告诉`pool`我们没有更多事情要做，这样它就可以在完成任务后退出：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And we wait for `pool` to quit:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们等待`pool`退出：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This way, we will wait for both tasks to complete and then we will quit all
    the threads started by `pool`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们将等待两个任务都完成，然后退出`pool`启动的所有线程。
- en: 'Once we know that all tasks are completed (because `pool.join()` returned),
    we can grab the results and print them:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道所有任务都已完成（因为`pool.join()`返回），我们可以获取结果并打印它们：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If we had more work to do, we would avoid running the `pool.close()` and `pool.join()`
    methods, so that we could send more work to `TaskPool`, which would get done as
    soon as there was a thread free.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有更多工作要做，我们将避免运行`pool.close()`和`pool.join()`方法，这样我们就可以将更多工作发送给`TaskPool`，一旦有空闲线程，工作就会完成。
- en: There's more...
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '`ThreadPool` is particularly convenient when you have multiple entries to which
    you need to apply the same operation over and over. Suppose you have a list of
    four URLs that you need to download:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当您有多个条目需要反复应用相同操作时，`ThreadPool`特别方便。假设您有一个包含四个URL的列表需要下载：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Fetching them in a single thread would take a lot of time:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个线程中获取它们将需要很长时间：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can test the time by running the function through the `timeit` module:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`timeit`模块运行函数来测试时间：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If we could do so using a separate thread for each function, it would only take
    the time of the slowest one to fetch all the provided URLs, as the download would
    proceed concurrently for all of them.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以使用单独的线程来执行每个函数，那么获取所有提供的URL只需要最慢的一个的时间，因为下载将同时进行。
- en: '`ThreadPool` actually provides us with the `map` method that does exactly that:
    it applies a function to a list of arguments:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`ThreadPool`实际上为我们提供了`map`方法，该方法正是这样做的：它将一个函数应用于一系列参数：'
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result will be a list containing the results returned by each call and
    we can easily test that this will be much faster than our original example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一个包含每次调用返回结果的列表，我们可以轻松测试这将比我们原始示例快得多：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Coroutines
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协程
- en: Threads are the most common way to implement concurrency in most languages and
    use cases, but they are expensive in terms of cost, and while `ThreadPool` can
    be a good solution for cases when thousands of threads are involved, it's usually
    unreasonable to involve thousands of threads. Especially when long-lived I/O is
    involved, you might easily reach thousands of operations running concurrently
    (think of the amount of concurrent HTTP requests an HTTP server might have to
    handle) and most of those tasks will be sitting doing nothing, just waiting for
    data from the network or from the disk most of the time.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 线程是大多数语言和用例中实现并发的最常见方式，但它们在成本方面很昂贵，而且虽然`ThreadPool`在涉及数千个线程的情况下可能是一个很好的解决方案，但通常不合理涉及数千个线程。特别是在涉及长期I/O时，您可能会轻松地达到数千个并发运行的操作（考虑一下HTTP服务器可能需要处理的并发HTTP请求数量），其中大多数任务将无所事事，只是大部分时间等待来自网络或磁盘的数据。
- en: In those cases, asynchronous I/O is the preferred approach. Compared to synchronous
    blocking I/O where your code is sitting there waiting for the read or write operation
    to complete, asynchronous I/O allows a task that needs data to initiate the read
    operation, switch to doing something else, and once the data is available, go
    back to what it was doing.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，异步I/O是首选的方法。与同步阻塞I/O相比，你的代码坐在那里等待读取或写入操作完成，异步I/O允许需要数据的任务启动读取操作，切换到做其他事情，一旦数据可用，就返回到原来的工作。
- en: In some cases, the notification of available data might come in the form of
    a signal, which would interrupt the concurrently running code, but, more commonly,
    asynchronous I/O is implemented through the usage of a selector (such as `select`,
    `poll`, or `epoll`) and an event loop that will resume the function waiting for
    the data as soon as the selector is notified that the data is available.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，可用数据的通知可能以信号的形式到来，这将中断并发运行的代码，但更常见的是，异步I/O是通过使用选择器（如`select`、`poll`或`epoll`）和一个事件循环来实现的，该事件循环将在选择器通知数据可用时立即恢复等待数据的函数。
- en: This actually leads to interleaving functions that are able to run for a while,
    reach a point where they need some I/O, and pass control to another function that
    will give it back as soon as it needs to perform some I/O too. Functions whose
    execution can be interleaved by suspending and resuming them are called **coroutines**,
    as they run cooperatively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上导致了交错运行的功能，能够运行一段时间，达到需要一些I/O的时候，将控制权传递给另一个函数，只要它需要执行一些I/O，就会立即返回。通过暂停和恢复它们的执行来交错执行的函数称为**协程**，因为它们是协作运行的。
- en: How to do it...
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: In Python, coroutines are implemented through the `async def` syntax and are
    executed through an `asyncio` event loop.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，协程是通过`async def`语法实现的，并通过`asyncio`事件循环执行。
- en: 'For example, we might write a function that runs two coroutines that count
    down from a given number of seconds, printing their progress. That would easily
    allow us to see that the two coroutines are running concurrently, as we would
    see output from one interleaved with output from the other:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以编写一个函数，运行两个协程，从给定的秒数开始倒计时，并打印它们的进度。这将很容易让我们看到这两个协程是同时运行的，因为我们会看到一个协程的输出与另一个协程的输出交错出现：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once an event loop is created and we run `main` within it, we will see the
    two functions running:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了一个事件循环，并在其中运行`main`，我们将看到这两个函数在运行：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the execution has completed, we can close the event loop as we won''t
    need it anymore:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行完成，我们可以关闭事件循环，因为我们不再需要它：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The core of our coroutines world is the **event loop**. It''s not possible
    to run coroutines (or, at least, it gets very complicated) without an event loop,
    so the first thing our code does is create an event loop:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们协程世界的核心是**事件循环**。没有事件循环，就不可能运行协程（或者说，会变得非常复杂），所以我们代码的第一件事就是创建一个事件循环：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then we ask the event loop to wait until a provided coroutine is completed:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们要求事件循环等待直到提供的协程完成：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `main` coroutine only starts two `countdown` coroutines and waits for their
    completion. That''s done by using `await` and, in that, the `asyncio.wait` function
    is in charge of waiting for a bunch of coroutines:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`main`协程只启动两个`countdown`协程并等待它们完成。这是通过使用`await`来完成的，而`asyncio.wait`函数负责等待一堆协程：'
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`await` is important here, because we are talking about coroutines, so unless
    they are explicitly awaited, our code would immediately move forward, and thus,
    even though we called `asyncio.wait`, we would not be waiting.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`await`在这里很重要，因为我们在谈论协程，所以除非它们被明确等待，否则我们的代码会立即向前移动，因此，即使我们调用了`asyncio.wait`，我们也不会等待。'
- en: In this case, we are waiting for the two countdowns to complete. The first countdown
    will start from `2` and will be identified by the character `A`, while the second
    countdown will start from `3` and will be identified by `B`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们正在等待两个倒计时完成。第一个倒计时将从`2`开始，并由字符`A`标识，而第二个倒计时将从`3`开始，并由`B`标识。
- en: The `countdown` function by itself is very simple. It's just a function that
    loops forever and prints how much there is left to wait.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`countdown`函数本身非常简单。它只是一个永远循环并打印剩下多少时间要等待的函数。'
- en: 'Between each loop it waits one second, so that it waits the expected number
    of seconds:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个循环之间等待一秒钟，这样就等待了预期的秒数：
- en: '[PRE20]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You might be wondering why we are using `asyncio.sleep` instead of `time.sleep`,
    and the reason is that, when working with coroutines, you must ensure that every
    other function that will block is a coroutine too. That way, you know that while
    your function is blocked, you would let the other coroutines move forward.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我们使用`asyncio.sleep`而不是`time.sleep`，原因是，当使用协程时，你必须确保每个其他会阻塞的函数也是一个协程。这样，你就知道在你的函数被阻塞时，你会让其他协程继续向前移动。
- en: By using `asyncio.sleep`, we let the event loop move the other `countdown` function
    forward while the first one is waiting and, thus, we properly interleave the execution
    of the two functions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`asyncio.sleep`，我们让事件循环在第一个协程等待时推进另一个`countdown`函数，因此，我们正确地交错执行了这两个函数。
- en: 'This can be verified by checking the output. When `asyncio.sleep` is used,
    the output will be interleaved between the two functions:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过检查输出来验证。当使用`asyncio.sleep`时，输出将在两个函数之间交错出现：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When `time.sleep` is used, the first coroutine will have to complete fully
    before the second one can move forward:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`time.sleep`时，第一个协程必须完全完成，然后第二个协程才能继续向前移动：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: So, a general rule when working with coroutines is that whenever you are going
    to call something that will block, make sure that it's a coroutine too, or you
    will lose the concurrency property of coroutines.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用协程时的一个一般规则是，每当要调用会阻塞的东西时，确保它也是一个协程，否则你将失去协程的并发属性。
- en: There's more...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We already know that the most important benefit of coroutines is that the event
    loop is able to pause their execution while they are waiting for I/O operations
    to let other coroutines proceed. While there is currently no built-in implementation
    of HTTP protocol with support for coroutines, it's easy enough to roll out a back
    version to reproduce our example of downloading a website concurrently to track
    how long it's taking.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道协程最重要的好处是事件循环能够在它们等待I/O操作时暂停它们的执行，以便让其他协程继续。虽然目前没有支持协程的HTTP协议的内置实现，但很容易推出一个后备版本来重现我们同时下载网站的示例以跟踪它花费了多长时间。
- en: 'As for the `ThreadPool` example, we will need the `wait_until` function that
    will wait for any given predicate to be true:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 至于`ThreadPool`示例，我们将需要`wait_until`函数，它将等待任何给定的谓词为真：
- en: '[PRE23]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will also need a `fetch_url` function to download the content of the URL.
    As we want this to run as a coroutine, we can''t rely on `urllib`, or it would
    block forever instead of passing control back to the event loop. So, we will have
    to read the data using `asyncio.open_connection`, which works at pure TCP level
    and thus will require us to implement HTTP support ourselves:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个`fetch_url`函数来下载URL的内容。由于我们希望这个函数作为协程运行，所以我们不能依赖`urllib`，否则它会永远阻塞而不是将控制权传递回事件循环。因此，我们将不得不使用`asyncio.open_connection`来读取数据，这将在纯TCP级别工作，因此需要我们自己实现HTTP支持：
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'At this point, it''s possible to interleave the two coroutines and see that
    the download proceeds concurrently with the waiting, and that it completes in
    the expected time:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，可以交错两个协程，看到下载与等待同时进行，并且在预期时间内完成：
- en: '[PRE25]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Processes
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进程
- en: Threads and coroutines are concurrency models that coexist with the Python GIL
    and the leverage execution time left available by I/O operations to allow other
    tasks to continue. With modern multicore systems, it's great to be able to use
    the full power that the system provides by involving real parallelism and distributing
    the work across all the cores that are available.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 线程和协程是与Python GIL并存的并发模型，并利用I/O操作留下的执行时间来允许其他任务继续。在现代多核系统中，能够利用系统提供的全部性能并涉及真正的并行性并将工作分配到所有可用的核心上是非常好的。
- en: The Python standard library provides very refined tools to work with multiprocessing,
    which is a great solution to leverage parallelism on Python. As multiprocessing
    will lead to multiple separate interpreters, the GIL won't get in the way, and
    compared to threads and coroutines, it might even be easier to reason with them
    as totally isolated processes that need to cooperate, rather than to think of
    multiple threads/coroutines within same system sharing the underlying memory state.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Python标准库提供了非常精细的工具来处理多进程，这是在Python上利用并行性的一个很好的解决方案。由于多进程将导致多个独立的解释器，因此GIL不会成为障碍，并且与线程和协程相比，甚至可能更容易理解它们作为完全隔离的进程，需要合作，而不是考虑在同一系统中共享底层内存状态的多个线程/协程。
- en: The major cost in managing processes is usually the spawn cost and the complexity
    of having to ensure you don't fork subprocesses in any odd condition, leading
    to unwanted data in memory being copied or file descriptors being reused.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 管理进程的主要成本通常是生成成本和确保您不会在任何奇怪的情况下分叉子进程的复杂性，从而导致在内存中复制不需要的数据或重用文件描述符。
- en: '`multiprocessing.ProcessPool` can be a very good solution to all these problems,
    as starting one at the beginning of our software will ensure that we don''t have
    to pay any particular cost when we have a task to submit to a subprocess. Furthermore,
    by creating the processes only once at the beginning, we can guarantee a predictable
    (and mostly empty) state of the software being copied to create the subprocesses.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing.ProcessPool`可以是解决所有这些问题的一个很好的解决方案，因为在软件开始时启动它将确保当我们有任务要提交给子进程时不必支付任何特定的成本。此外，通过在开始时仅创建进程，我们可以保证软件的状态可预测（并且大部分为空），被复制以创建子进程。'
- en: How to do it...
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Pretty much like in the *ThreadPool* recipe, we will need two functions that
    will act as our tasks running concurrently in the processes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在*ThreadPool*示例中一样，我们将需要两个函数，它们将作为我们在进程中并行运行的任务。
- en: 'In the case of processes, we don''t need to perform I/O actually to run concurrently,
    so our tasks could be doing anything. What I''m going to use is the computing
    of the Fibonacci series while printing out progress, so that we can see how the
    output of the two processes will interleave:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在进程的情况下，我们实际上不需要执行I/O来实现并发运行，因此我们的任务可以做任何事情。我将使用计算斐波那契数列并打印出进度，以便我们可以看到两个进程的输出是如何交错的：
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'So, now we need to create the multiprocessing `Pool` that will run the `fib`
    function and spawn computation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们需要创建运行`fib`函数并生成计算的多进程`Pool`：
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You can see how the process IDs of the two processes interleave, and once the
    job is completed, it's possible to get the results of both of them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到两个进程的进程ID是如何交错的，一旦作业完成，就可以获得它们两者的结果。
- en: How it works...
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'When `multiprocessing.Pool` is created, a number of processes equal to the
    number of cores on the system (as stated by `os.cpu_count()`) is created through
    `os.fork` or by spawning a new Python interpreter, depending on what''s supported
    by the underlying system:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`multiprocessing.Pool`时，将通过`os.fork`或生成一个新的Python解释器创建与系统上的核心数量相等的进程（由`os.cpu_count()`指定），具体取决于底层系统支持的情况：
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once the new processes are started, they will all do the same thing: execute
    the `worker` function that loops forever consuming from the queue of jobs that
    were sent to `Pool` and running them one by one.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动了新进程，它们将都执行相同的操作：执行`worker`函数，该函数循环消耗发送到`Pool`的作业队列，并逐个运行它们。
- en: This means that if we create a `Pool` of two processes, we will have two workers.
    As soon as we ask `Pool` to perform something (through `Pool.apply_async`, `Pool.map`,
    or any other method), the jobs (functions and its arguments) are placed in `multiprocessing.SimpleQueue`
    from which the worker will fetch it.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果我们创建了两个进程的`Pool`，我们将有两个工作进程。一旦我们要求`Pool`执行某些操作（通过`Pool.apply_async`，`Pool.map`或任何其他方法），作业（函数及其参数）将被放置在`multiprocessing.SimpleQueue`中，工作进程将从中获取。
- en: Once `worker` fetches the task from the queue, it will run it. If multiple `worker`
    instances are running, each one of them will pick a task from the queue and run
    it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`worker`从队列中获取任务，它将运行它。如果有多个`worker`实例在运行，每个实例都会从队列中选择一个任务并运行它。
- en: Once the task has completed, the result of the function that was executed is
    pushed back into a results queue (together with the job itself to identify which
    task the result refers to), from which `Pool` will be able to consume the results
    and provide them back to the code that originally fired the tasks.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 任务完成后，执行的函数结果将被推送回结果队列（与任务本身一起，以标识结果所属的任务），`Pool`将能够消耗结果并将其提供给最初启动任务的代码。
- en: All this communication happens across multiple processes, so it can't happen
    in memory. Instead `multiprocessing.SimpleQueue`, which is underlying, uses `pipe`,
    each producer will write into `pipe`, and each consumer will read from `pipe`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些通信都发生在多个进程之间，因此它不能在内存中发生。相反，`multiprocessing.SimpleQueue`使用`pipe`，每个生产者将写入`pipe`，每个消费者将从`pipe`中读取。
- en: As `pipe` is only able to read and write bytes, the arguments we submit to `pool`
    and the results of the functions executed by `pool` are converted to bytes through
    the `pickle` protocol. That is able to marshal/unmarshal Python objects in as
    far as the same modules are available on both sides (sender and receiver).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`pipe`只能读取和写入字节，我们提交给`pool`的参数以及由`pool`执行的函数的结果通过`pickle`协议转换为字节。只要发送方和接收方都有相同的模块可用，它就能够在Python对象之间进行编组/解组。
- en: 'So, we submit our requests to `Pool`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们向`Pool`提交我们的请求：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `fib` function, `20`, and the empty set all get pickled and sent into the
    queue for one of the `Pool` workers to consume.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`fib`函数，`20`和空集都被pickled并发送到队列中，供`Pool`的一个工作进程消耗。'
- en: 'Meanwhile, while workers are picking up data and running the Fibonacci function,
    we join the pool, so that our primary process will block until all the processes
    on the pool have completed:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，当工作进程正在获取数据并运行斐波那契函数时，我们加入池，以便我们的主进程将阻塞，直到池中的所有进程都完成：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In theory, a process of the pool never completes (it runs forever, continuously
    looking for things to do in the queue). Before calling `join`, we `close` the
    pool. Closing the pool tells the pool to *exit all its processes once they finish
    what they are doing right now*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，池中的进程永远不会完成（它将永远运行，不断地查找队列中的任务）。在调用`join`之前，我们关闭池。关闭池告诉池一旦它们完成当前正在做的事情，就*退出所有进程*。
- en: Then, by immediately joining after `close`, we wait until the pool finishes
    what it's doing right now, which is serving our two requests.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在`close`之后立即加入，我们等待直到池完成它现在正在做的事情，即为我们提供服务的两个请求。
- en: 'As with threads, `multiprocessing.Pool` returns `AsyncResult` objects, which
    means we can check their completion through the `AsyncResult.ready()` method and
    we can grab the returned value, once it''s ready, through `AsyncResult.get()`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与线程一样，`multiprocessing.Pool`返回`AsyncResult`对象，这意味着我们可以通过`AsyncResult.ready()`方法检查它们的完成情况，并且一旦准备就绪，我们可以通过`AsyncResult.get()`获取返回的值：
- en: '[PRE31]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: There's more...
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '`multiprocessing.Pool` works in nearly the same way as `multiprocessing.pool.ThreadPool`.
    In fact, they share a lot of their implementation as one is a subclass of the
    other.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing.Pool`的工作方式与`multiprocessing.pool.ThreadPool`几乎相同。实际上，它们共享很多实现，因为其中一个是另一个的子类。'
- en: But there are some major differences that are caused by the underlying technology
    used. One is based on threads and the other on subprocesses.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于使用的底层技术不同，这会导致一些主要差异。一个基于线程，另一个基于子进程。
- en: The major benefit of using processes is that the Python interpreter lock won't
    limit their parallelism, and they will be able to actually run in parallel with
    each other.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用进程的主要好处是Python解释器锁不会限制它们的并行性，它们将能够实际并行运行。
- en: On the other side, there is a cost for that. Using processes is both more expensive
    in startup time (forking a process is usually slower than spawning a thread),
    and more expensive in terms of memory used, as each process will need to have
    its own state of memory. While a lot of this cost is reduced heavily on most systems
    through techniques such as copy on write, threads usually end up being a lot cheaper
    than processes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，这是有成本的。使用进程在启动时间上更昂贵（fork一个进程通常比生成一个线程慢），而且在内存使用方面更昂贵，因为每个进程都需要有自己的内存状态。虽然大部分系统通过写时复制等技术大大降低了这些成本，但线程通常比进程便宜得多。
- en: For this reason, it's usually a good idea to start the process `pool` only at
    the beginning of your application, so that the additional cost of spawning processes
    is only paid once.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常最好在应用程序开始时只启动进程`pool`，这样生成进程的额外成本只需支付一次。
- en: Processes are not only more expensive to start, but by contrast with threads,
    they don't share the state of the program; each process has its own state and
    memory. So it's not possible to share the data between `Pool` and the workers
    that will perform the tasks. All the data needs to be encoded through `pickle` and
    sent through `pipe` for the other end to consume. This has a huge cost compared
    to threads that can rely on a shared queue, especially when the data that has
    to be sent is big.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 进程不仅更昂贵，而且与线程相比，它们不共享程序的状态；每个进程都有自己的状态和内存。因此，无法在`Pool`和执行任务的工作进程之间共享数据。所有数据都需要通过`pickle`编码并通过`pipe`发送到另一端进行消耗。与可以依赖共享队列的线程相比，这将产生巨大的成本，特别是当需要发送的数据很大时。
- en: For this reason, it's usually a good idea to avoid involving processes when
    big files or data are involved in arguments or return values, as that data will
    have to be copied multiple times to reach its final destination. In that case,
    it's better to save the data on disk and pass around the path of the file.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常最好避免在参数或返回值中涉及大文件或数据时涉及进程，因为该数据将不得不多次复制才能到达最终目的地。在这种情况下，最好将数据保存在磁盘上，并传递文件的路径。
- en: Futures
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来
- en: When a background task is spawned, it might be running concurrently with your
    main flow forever and never complete its own job (such as the worker threads of
    a `ThreadPool`), or it might be something that will return a result to you sooner
    or later and you might be waiting for that result (such as a thread that downloads
    the content of a URL in the background).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动后台任务时，它可能会与您的主流程并发运行，永远不会完成自己的工作（例如`ThreadPool`的工作线程），或者它可能是某种迟早会向您返回结果并且您可能正在等待该结果的东西（例如在后台下载URL内容的线程）。
- en: 'These second types of task all share a common behavior: their result will be
    available in `_future_`. So, a result that will be available in the future is
    commonly referred to as `Future`. Programming languages don''t all share the same
    exact definition of futures, and on Python `Future` is any function that will
    be completed in the future, typically returning a result.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些第二种类型的任务都共享一个共同的行为：它们的结果将在`_future_`中可用。因此，通常将将来可用的结果称为`Future`。编程语言并不都具有完全相同的`futures`定义，而在Python中，`Future`是指将来会完成的任何函数，通常返回一个结果。
- en: '`Future` is the callable itself, so it''s unrelated to the technology that
    will be used actually to run the callable. You will need a way to let the execution
    of the callable proceed, and in Python, that''s provided by `Executor`.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`Future`是可调用本身，因此与实际用于运行可调用的技术无关。您需要一种让可调用的执行继续进行的方法，在Python中，这由`Executor`提供。'
- en: There are executors that can run the futures into threads, processes, or coroutines
    (in the case of coroutines, the loop itself is the executor).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些执行器可以将futures运行到线程、进程或协程中（在协程的情况下，循环本身就是执行器）。
- en: How to do it...
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To run a future, we will need an executor (either `ThreadPoolExecutor`, `ProcessPoolExecutor`)
    and the futures we actually want to run. For the sake of our example, we will
    use a function that returns the time it takes to load a web page so we can benchmarks
    multiple websites to see which one is the fastest:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行一个future，我们将需要一个执行器（`ThreadPoolExecutor`、`ProcessPoolExecutor`）和我们实际想要运行的futures。为了举例说明，我们将使用一个返回加载网页所需时间的函数，以便对多个网站进行基准测试，以查看哪个网站速度最快：
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then we can create any kind of executor and have our `UrlsBenchmarker` run
    its futures within it:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以创建任何类型的执行器，并让`UrlsBenchmarker`在其中运行其`futures`：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`UrlsBenchmarker` will fire a future for each URL through `UrlsBenchmarker._benchmark_urls`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`UrlsBenchmarker`将通过`UrlsBenchmarker._benchmark_urls`为每个URL触发一个future：'
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Each future will perform `benchmark_url`, which downloads the content of the
    given URL and returns the time it took to download it, along with the URL itself:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每个future将执行`benchmark_url`，该函数下载给定URL的内容并返回下载所用的时间，以及URL本身：
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Returning the URL itself is necessary, as `future` can know its return value,
    but not its arguments. So once we `submit` the function, we have lost which URL
    it is related to and by returning it together with the timing, we will always
    have the URL available whenever the timing is present.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 返回URL本身是必要的，因为`future`可以知道其返回值，但无法知道其参数。因此，一旦我们`submit`函数，我们就失去了它与哪个URL相关，并通过将其与时间一起返回，每当时间存在时我们将始终有URL可用。
- en: 'Then for each `future`, a callback is added through `future.add_done_callback`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对于每个`future`，通过`future.add_done_callback`添加一个回调：
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As soon as the future completes, it will call `UrlsBenchmarker._print_timing`,
    which prints the time it took to run the URL. This informs the user that the benchmark
    is proceeding and that it completed one of the URLs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦future完成，它将调用`UrlsBenchmarker._print_timing`，该函数打印运行URL所用的时间。这通知用户基准测试正在进行，并且已完成其中一个URL。
- en: '`UrlsBenchmarker._benchmark_urls` will then return `futures` for all the URLs
    that we had to benchmark in a list.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`UrlsBenchmarker._benchmark_urls` 然后会返回一个包含所有需要在列表中进行基准测试的URL的`futures`。'
- en: 'That list is then passed to `concurrent.futures.as_completed`. This will create
    an iterator that will return all `futures` in the order they completed and only
    when they are completed. So, we know that by iterating over it, we will only fetch
    `futures` that are already completed and we will block waiting for the completion
    of a new future as soon as the consumed all `futures` that already completed:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将该列表传递给`concurrent.futures.as_completed`。这将创建一个迭代器，按照完成的顺序返回所有`futures`，并且只有在它们完成时才返回。因此，我们知道通过迭代它，我们只会获取已经完成的`futures`，并且一旦消耗了所有已完成的`futures`，我们将阻塞等待新的future完成：
- en: '[PRE37]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So, the loop will only finish when all `futures` are complete.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，只有当所有`futures`都完成时，循环才会结束。
- en: The list of completed `futures` is consumed by a `list` comprehension that will
    create a list containing the results of those `futures`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 已完成`futures`的列表被`list`推导式所消耗，它将创建一个包含这些`futures`结果的列表。
- en: As the results are all in the (`time`, `url`) form, we can use `min` to grab
    the result with the minimum time, which is the URL that took less time to download.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于结果都是以（时间，URL）形式存在，我们可以使用`min`来获取具有最短时间的结果，即下载时间最短的URL。
- en: 'This works because comparing two tuples compares the elements in order:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为比较两个元组会按顺序比较元素：
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'So, calling `min` on a list of tuples will grab the entry with the minimum
    value in the first element of the tuple:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在元组列表上调用`min`将抓取元组中第一个元素的最小值的条目：
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The second element is only looked at when there are two first elements with
    the same value:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当有两个第一个元素具有相同值时，才会查看第二个元素：
- en: '[PRE40]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'So, we grab the URL with the shortest timing (as the timing was the first of
    the entries in the tuple returned by the future) and print it as the fastest:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们获取具有最短时间的 URL（因为时间是由未来返回的元组中的第一个条目）并将其打印为最快的：
- en: '[PRE41]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: There's more...
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The futures executors are very similar to the worker pools provided by `multiprocessing.pool`,
    but they have some differences that might push you toward one direction or another.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 未来执行器与 `multiprocessing.pool` 提供的工作进程池非常相似，但它们有一些差异可能会推动您朝一个方向或另一个方向。
- en: The major difference is probably the way the workers are started. The pools
    start a fixed number of workers that are created and started all at the same time
    when the pool is created. So, creating the pool early moves the cost of spawning
    the workers at the beginning of the application. This means that the application
    can be quite slow to start because it might have to fork many processes according
    to the number of workers you requested or the number of cores your system has.
    Instead, the executor creates workers only when they are needed, and it's meant
    to evolve in the future to avoid making new workers when there are available ones.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别可能是工作进程的启动方式。池会启动固定数量的工作进程，在创建池时同时创建和启动它们。因此，早期创建池会将生成工作进程的成本移到应用程序的开始。这意味着应用程序可能启动相当慢，因为它可能需要根据您请求的工作进程数量或系统核心数量来分叉许多进程。相反，执行器仅在需要时创建工作进程，并且它旨在在将来避免在有可用工作进程时创建新的工作进程。
- en: So, executors are generally faster to start up at the expense of a bit more
    delay the first time a future is sent to it, while pools focus most of their cost
    on startup time. For this reason, if you have cases where you frequently need
    to create and destroy a pool of worker processes, the `futures` executor can be
    more efficient to work with.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，执行器通常更快地启动，但第一次将未来发送到执行器时会有更多的延迟，而池则将大部分成本集中在启动时间上。因此，如果您经常需要创建和销毁一组工作进程池的情况下，使用
    `futures` 执行器可能更有效。
- en: Scheduled tasks
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度的任务
- en: A common kind of background task is an action that should run by itself in the
    background at any given time. Typically, those are managed through a cron daemon
    or similar system tools by configuring the daemon to run a given Python script
    at the provided time.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的后台任务是应该在任何给定时间自行在后台运行的操作。通常，这些通过 cron 守护程序或类似的系统工具进行管理，通过配置守护程序在提供的时间运行给定的
    Python 脚本。
- en: 'When you have a primary application that needs to perform tasks cyclically
    (such as expiring caches, resetting password links, flushing a queue of emails
    to send, or similar tasks), it''s not really viable to do so through a cron job
    as you would need to dump the data somewhere accessible to the other process:
    on disk, on a database, or any similarly shared storage.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当您有一个主要应用程序需要周期性执行任务（例如过期缓存、重置密码链接、刷新待发送的电子邮件队列或类似任务）时，通过 cron 作业进行操作并不是可行的，因为您需要将数据转储到其他进程可以访问的地方：磁盘上、数据库上，或者任何类似的共享存储。
- en: Luckily, the Python standard library has an easy way to schedule tasks that
    are to be executed at any given time and joined with threads. It can be a very
    simple and effective solution for scheduled background tasks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Python 标准库有一种简单的方法来安排在任何给定时间执行并与线程一起加入的任务。这可以是一个非常简单和有效的定时后台任务的解决方案。
- en: How to do it...
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The `sched` module provides a fully functioning scheduled tasks executor that
    we can mix with threads to create a background scheduler:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`sched` 模块提供了一个完全功能的调度任务执行器，我们可以将其与线程混合使用，创建一个后台调度器：'
- en: '[PRE42]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`BackgroundScheduler` can be started and jobs can be added to it to start their
    execution at fixed times:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`BackgroundScheduler` 可以启动，并且可以向其中添加作业，以便在固定时间开始执行它们：'
- en: '[PRE43]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works...
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: '`BackgroundScheduler` subclasses `threading.Thread` so that it runs in the
    background while our application is doing something else. Registered tasks will
    fire and perform in a secondary thread without getting in the way of the primary
    code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`BackgroundScheduler` 是 `threading.Thread` 的子类，因此它在我们的应用程序在做其他事情时在后台运行。注册的任务将在辅助线程中触发和执行，而不会妨碍主要代码：'
- en: '[PRE44]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Whenever `BackgroundScheduler` is created, the thread for it is started too,
    so it becomes immediately available. The thread will run in `daemon` mode, which
    means that it won't block the program from exiting if it's still running at the
    time the program ends.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 每当创建 `BackgroundScheduler` 时，它的线程也会启动，因此它立即可用。该线程将以 `daemon` 模式运行，这意味着如果程序在结束时仍在运行，它不会阻止程序退出。
- en: Usually Python waits for all threads when exiting the application, so setting
    a thread as a `daemon` one makes it possible to quit without having to wait for
    them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通常 Python 在退出应用程序时会等待所有线程，因此将线程设置为 `daemon` 可以使其在无需等待它们的情况下退出。
- en: '`threading.Thread` executes the `run` method as the thread code. In our case,
    it''s a method that runs the tasks registered in the scheduler over and over:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`threading.Thread` 作为线程代码执行 `run` 方法。在我们的情况下，这是一个重复运行调度器中注册的任务的方法：'
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`_scheduler.run(blocking=False)` means to pick one task to run from the scheduled
    ones and run it. Then, it returns the time that it still has to be waited for
    before running the next task. If no time is returned, it means there are no tasks
    to run.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`_scheduler.run(blocking=False)` 表示从计划的任务中选择一个任务并运行它。然后，它返回在运行下一个任务之前仍需等待的时间。如果没有返回时间，这意味着没有要运行的任务。'
- en: Through `_scheduler.delayfunc(min(delta, 0.5))`, we wait for the time it takes
    before the next task needs to run, which is most half a second at most.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `_scheduler.delayfunc(min(delta, 0.5))`，我们等待下一个任务需要运行的时间，最多为半秒钟。
- en: We wait at most half a second, because while we are waiting, the scheduled tasks
    might change. A new task might get registered and we want to ensure it won't have
    to wait more than half a second for the scheduler to catch it.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最多等待半秒钟，因为当我们等待时，调度的任务可能会发生变化。可能会注册一个新任务，我们希望确保它不必等待超过半秒钟才能被调度器捕捉到。
- en: If we waited exactly the time that was pending before the next task, we might
    do a run, get back that the next task was in 60 seconds, and start waiting 60
    seconds. But what if, while we were waiting, the user registered a new task that
    had to run in 5 seconds? We would run it in 60 seconds anyway, because we were
    already waiting. By waiting at most 0.5 seconds, we know that it will take half
    a second to pick up the next task and that it will run properly in 5 seconds.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们等待的时间正好是下一个任务挂起的时间，我们可能会运行，得到下一个任务在60秒内，然后开始等待60秒。但是，如果我们在等待时，用户注册了一个必须在5秒内运行的新任务，我们无论如何都会在60秒内运行它，因为我们已经在等待。通过等待最多0.5秒，我们知道需要半秒钟才能接收下一个任务，并且它将在5秒内正确运行。
- en: Waiting less than the time that is pending before the next task won't make the
    tasks run any faster, because the scheduler won't run any tasks that don't already
    surpass its scheduled time. So, if there are no tasks to run, the scheduler would
    continuously tell us, *you have to wait*, and we would be waiting half a second
    for as many times as it was needed to reach the scheduled time of the next scheduled
    task.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 等待少于下一个任务挂起的时间不会使任务运行得更快，因为调度程序不会运行任何已经超过其计划时间的任务。因此，如果没有要运行的任务，调度程序将不断告诉我们*你必须等待*，我们将等待半秒钟，直到达到下一个计划任务的计划时间为止。
- en: The `run_at`, `run_after`, and `run_every` methods are the ones actually involved
    in registering functions for execution at specific times.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_at`，`run_after`和`run_every`方法实际上是注册在特定时间执行函数的方法。'
- en: '`run_at` and `run_after` simply wrap the `enterabs` and `enter` methods of
    the scheduler, which allow us to register a task to run at a specific time or
    after *n* seconds.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_at`和`run_after`只是包装调度程序的`enterabs`和`enter`方法，这些方法允许我们在特定时间或*n*秒后注册任务运行。'
- en: 'The most interesting function is probably `run_every`, which runs a task over
    and over every *n* seconds:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最有趣的函数可能是`run_every`，它每*n*秒运行一次任务：
- en: '[PRE46]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The method takes the callable that has to be run and wraps it into a decorator
    that actually does run the function, but once it completes, it schedules the function
    back for re-execution. This way, it will run over and over until the scheduler
    is stopped, and whenever it completes, it's scheduled again.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接受必须运行的可调用对象，并将其包装成实际运行该函数的装饰器，但是一旦完成，它会将函数重新安排为再次执行。这样，它将一直运行，直到调度程序停止，并且每当它完成时，它都会再次安排。
- en: Sharing data between processes
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在进程之间共享数据
- en: When working with threads or coroutines, data is shared across them by virtue
    of the fact that they share the same memory space. So, you can access any object
    from any thread, as long as attention is paid to avoiding race conditions and
    providing proper locking.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用线程或协程时，数据是通过它们共享相同的内存空间而共享的。因此，只要注意避免竞争条件并提供适当的锁定，您就可以从任何线程访问任何对象。
- en: With processes, instead, things get far more complicated and no data is shared
    across them. So when using `ProcessPool` or `ProcessPoolExecutor`, we need to
    find a way to pass data across the processes and make them able to share a common
    state.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，使用进程时，情况变得更加复杂，数据不会在它们之间共享。因此，在使用`ProcessPool`或`ProcessPoolExecutor`时，我们需要找到一种方法来在进程之间传递数据，并使它们能够共享一个公共状态。
- en: 'The Python standard library provides many tools to create a communication channel
    between processes: `multiprocessing.Queues`, `multiprocessing.Pipe`, `multiprocessing.Value`,
    and `multiprocessing.Array` can be used to create queues that one process can
    feed and the other consume, or simply values shared between multiple processes
    in a shared memory.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Python标准库提供了许多工具来创建进程之间的通信渠道：`multiprocessing.Queues`，`multiprocessing.Pipe`，`multiprocessing.Value`和`multiprocessing.Array`可用于创建一个进程可以提供并且另一个进程可以消费的队列，或者在共享内存中共享的多个进程之间的值。
- en: 'While all these are viable solutions, they have some limits: you must create
    all shared values before creating any process, so they are not viable if the amount
    of shared values is variable and they are limited in terms of types they can store.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所有这些都是可行的解决方案，但它们有一些限制：您必须在创建任何进程之前创建所有共享值，因此如果共享值的数量是可变的并且在存储类型方面受到限制，则它们就不可行。
- en: '`multiprocessing.Manager`, instead, allows us to store any number of shared
    values through a shared `Namespace`.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，`multiprocessing.Manager`允许我们通过共享的`Namespace`存储任意数量的共享值。
- en: How to do it...
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Here are the steps for this recipe:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此配方的步骤：
- en: '`manager` should be created at the beginning of your application, then all
    processes will be able to set and read values from it:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`管理器`应该在应用程序开始时创建，然后所有进程都能够从中设置和读取值：'
- en: '[PRE47]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Once we have our `namespace`, any process will be able to set values to it:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了我们的`namespace`，任何进程都能够向其设置值：
- en: '[PRE48]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Any process will be able to access them all:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任何进程都能够访问它们：
- en: '[PRE49]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Without the need to create the variables early on or from the main process,
    all processes will be able to read or set any variable as far as they have access
    to `Namespace`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 无需提前创建变量或从主进程创建，只要进程能够访问`Namespace`，所有进程都能够读取或设置任何变量。
- en: How it works...
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `multiprocessing.Manager` class acts as a server that is able to store values
    accessible by any process that has a reference to `Manager` and to the values
    it wants to access.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing.Manager`类充当服务器，能够存储任何进程都能够访问的值，只要它具有对`Manager`和它想要访问的值的引用。'
- en: '`Manager` itself is accessible by knowing the address of the socket or pipe
    where it is listening, and each process that has a reference to the `Manager`
    instance knows those:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过知道它正在侦听的套接字或管道的地址，可以访问`Manager`本身，每个具有对`Manager`实例的引用的进程都知道这些：
- en: '[PRE50]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Then, once you know how to contact the manager itself, you need to be able to
    tell the manager which object you want to access out of all that the manager is
    managing.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦您知道如何联系管理器本身，您需要能够告诉管理器要访问的对象。
- en: 'That can be done by having `Token` that represents and pinpoints that object:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过拥有代表并确定该对象的`Token`来完成：
- en: '[PRE51]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Particularly, `Namespace` is a kind of object that allows us to store any variable
    within it. So, it makes anything stored within `Namespace` accessible by using
    just the `namespace` token.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，`Namespace`是一种允许我们在其中存储任何变量的对象。因此，通过仅使用`namespace`令牌就可以访问`Namespace`中存储的任何内容。
- en: All processes, as they were copied from the same original process, that had
    the token of the namespace and the address of the manager are able to access `namespace`
    and thus set or read values from it.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 所有进程，因为它们是从同一个原始进程复制出来的，都具有`namespace`的令牌和管理器的地址，因此能够访问`namespace`，并因此设置或读取其中的值。
- en: There's more...
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '`multiprocessing.Manager` is not constrained to work with processes that originated
    from the same process.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing.Manager` 不受限于与源自同一进程的进程一起工作。'
- en: 'It''s possible to create a `Manager` that will listen on a network so that
    any process that is able to connect to it might be able to access its content:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 可以创建一个在网络上监听的`Manager`，以便任何能够连接到它的进程可能能够访问其内容：
- en: '[PRE52]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Then, once the server is started:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦服务器启动：
- en: '[PRE53]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The other processes will be able to connect to it by creating a `manager2`
    instance with the exact same arguments of the manager they want to connect to,
    and then explicitly connect:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其他进程将能够通过使用与他们想要连接的管理器完全相同的参数创建一个`manager2`实例，然后显式连接：
- en: '[PRE54]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s create a `namespace` in manager and set a value into it:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在管理器中创建一个`namespace`并将一个值设置到其中：
- en: '[PRE55]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Knowing the token value of `namespace`, it''s possible to create a proxy object
    to access `namespace` from `manager2` through the network:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 知道`namespace`的令牌值后，可以创建一个代理对象通过网络从`manager2`访问`namespace`：
- en: '[PRE56]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
