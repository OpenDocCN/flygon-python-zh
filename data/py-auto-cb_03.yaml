- en: Building Your First Web Scraping Application
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建您的第一个Web抓取应用程序
- en: 'In this chapter, we''ll cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Downloading web pages
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载网页
- en: Parsing HTML
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析HTML
- en: Crawling the web
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬取网络
- en: Subscribing to feeds
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订阅源
- en: Accessing web APIs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问Web API
- en: Interacting with forms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与表单交互
- en: Using Selenium for advanced interaction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Selenium进行高级交互
- en: Accessing password-protected pages
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问受密码保护的页面
- en: Speeding up web scraping
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速网络抓取
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The internet, and the **WWW** (**World Wide Web**), is probably the most prominent
    source of information today. Most of that information is retrievable through the
    HTTP protocol. **HTTP** was invented originally to share pages of hypertext (hence
    the name **HyperText Transfer Protocol**), which started the WWW.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网和**WWW**（**万维网**）可能是当今最重要的信息来源。大部分信息可以通过HTTP协议检索。**HTTP**最初是为了共享超文本页面而发明的（因此称为**超文本传输协议**），这开创了WWW。
- en: This operation is very familiar, as it is what happens in any web browser. But
    we can also perform those operations programmatically to automatically retrieve
    and process information. Python has included in the standard library an HTTP client,
    but the fantastic `requests` module makes it very easy. In this chapter, we will
    see how.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作非常熟悉，因为它是任何网络浏览器中发生的事情。但我们也可以以编程方式执行这些操作，自动检索和处理信息。Python在标准库中包含了一个HTTP客户端，但是fantastic
    `requests`模块使它变得非常容易。在本章中，我们将看到如何做到这一点。
- en: Downloading web pages
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载网页
- en: 'The basic ability to download a web page involves making an HTTP `GET` request
    against a URL. This is the basic operation of any web browser. Let''s quickly
    recap the different parts of this operation:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下载网页的基本能力涉及对URL发出HTTP `GET`请求。这是任何网络浏览器的基本操作。让我们快速回顾一下这个操作的不同部分：
- en: Using the HTTP protocol.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用HTTP协议。
- en: Using the `GET` method, which is the most common HTTP method. We'll see more
    in the *Accessing web APIs* recipe.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最常见的HTTP方法`GET`。我们将在*访问Web API*配方中看到更多。
- en: URL describing the full address of the page, including the server and the path.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: URL描述页面的完整地址，包括服务器和路径。
- en: That request will be processed by the server and a response will be sent back.
    This response will contain a **status code**, typically 200 if everything went
    fine, and a body with the result, which will normally be text with an HTML page.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该请求将由服务器处理，并发送回一个响应。这个响应将包含一个**状态码**，通常是200，如果一切顺利的话，以及一个包含结果的body，通常是一个包含HTML页面的文本。
- en: Most of this is handled automatically by the HTTP client used to perform the
    request. We'll see in this recipe how to make a simple request to obtain a web
    page.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分由用于执行请求的HTTP客户端自动处理。在这个配方中，我们将看到如何发出简单的请求以获取网页。
- en: HTTP requests and responses can also contain headers. Headers contain extra
    information, such as the total size of the request, the format of the content,
    the date of the request, and what browser or server is used.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP请求和响应也可以包含头部。头部包含额外的信息，如请求的总大小，内容的格式，请求的日期以及使用的浏览器或服务器。
- en: Getting ready
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Using the fantastic `requests` module, getting web pages is super simple. Install
    the module:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用fantastic `requests`模块，获取网页非常简单。安装模块：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We'll download the page at [http://www.columbia.edu/~fdc/sample.html](http://www.columbia.edu/~fdc/sample.html) because
    it is a straightforward HTML page that is easy to read in text mode.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将下载页面在[http://www.columbia.edu/~fdc/sample.html](http://www.columbia.edu/~fdc/sample.html)，因为它是一个简单的HTML页面，很容易在文本模式下阅读。
- en: How to do it...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Import the `requests` module:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`requests`模块：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Make a request to the URL, which will take a second or two:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对URL发出请求，这将花费一两秒钟：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Check the returned object status code:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查返回的对象状态码：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Check the content of the result:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查结果的内容：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Check the ongoing and returned headers:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查进行中和返回的头部：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The operation of `requests` is very simple; perform the operation, `GET` in
    this case, over the URL. This returns a `result` object that can be analyzed.
    The main elements are the `status_code` and the body content, which can be presented
    as `text`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`requests`的操作非常简单；在URL上执行操作，这种情况下是`GET`，返回一个可以分析的`result`对象。主要元素是`status_code`和body内容，可以呈现为`text`。'
- en: 'The full request can be checked in the `request` field:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在`request`字段中检查完整的请求：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The full request's documentation can be found here: [http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/).
    Over the course of the chapter, we'll be showing more features.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的请求文档可以在这里找到：[http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/)。在本章中，我们将展示更多功能。
- en: There's more...
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: All HTTP status codes can be checked on this web page: [https://httpstatuses.com/](https://httpstatuses.com/).
    They are also described in the `httplib` module with convenient constant names,
    such as `OK`, `NOT_FOUND`, or `FORBIDDEN`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有HTTP状态码可以在这个网页上检查：[https://httpstatuses.com/](https://httpstatuses.com/)。它们也在`httplib`模块中以方便的常量名称进行描述，如`OK`，`NOT_FOUND`或`FORBIDDEN`。
- en: The most famous error status code is arguably 404, which happens when a URL
    is not found. Try it out by doing `requests.get('http://www.columbia.edu/invalid')`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的错误状态码可能是404，当URL未找到时会发生。通过执行`requests.get('http://www.columbia.edu/invalid')`来尝试。
- en: A request can use the **HTTPS** protocol (**secure HTTP**). It is equivalent,
    but ensures that the contents of the request and response are private. `requests`
    handles it transparently.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请求可以使用**HTTPS**协议（**安全HTTP**）。它是等效的，但确保请求和响应的内容是私有的。`requests`会自动处理它。
- en: Any website that handles any private information will use HTTPS to ensure that
    the information has not leaked out. HTTP is vulnerable to someone eavesdropping.
    Use HTTPS where available.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 任何处理任何私人信息的网站都将使用HTTPS来确保信息没有泄漏。HTTP容易受到窃听。尽可能使用HTTPS。
- en: See also
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Installing third-party packages* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml), *Let
    Us Begin Our Automation Journey*
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第1章](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml)的*让我们开始自动化之旅*中的*安装第三方包*配方中
- en: The *Parsing HTML* recipe
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解析HTML*配方'
- en: Parsing HTML
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析HTML
- en: Downloading raw text or a binary file is a good starting point, but the main
    language of the web is HTML.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下载原始文本或二进制文件是一个很好的起点，但是网页的主要语言是HTML。
- en: HTML is a structured language, defining different parts of a document such as
    headers and paragraphs. HTML is also hierarchical, defining sub-elements. The
    ability to parse raw text into a structured document is basically to be able to
    extract information automatically from a web page. For example, some text can
    be relevant if enclosed in a particular `class div` or after a header `h3` tag.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: HTML是一种结构化语言，定义文档的不同部分，如标题和段落。HTML也是分层的，定义了子元素。将原始文本解析为结构化文档的能力基本上是能够从网页中自动提取信息的能力。例如，如果在特定的`class
    div`中或在标题`h3`标签后面包含一些文本，则该文本可能是相关的。
- en: Getting ready
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We''ll use the excellent Beautiful Soup module to parse the HTML text into
    a memory object that can be analyzed. We need to use the `beautifulsoup4` package to
    use the latest Python 3 version that is available. Add the package to your `requirements.txt`
    and install the dependencies in the virtual environment:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用优秀的Beautiful Soup模块将HTML文本解析为可以分析的内存对象。我们需要使用`beautifulsoup4`包来使用可用的最新Python
    3版本。将包添加到您的`requirements.txt`并在虚拟环境中安装依赖项：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How to do it...
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Import `BeautifulSoup` and `requests`:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`BeautifulSoup`和`requests`：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Set up the URL of the page to download and retrieve it:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置要下载并检索的页面的URL：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Parse the downloaded page:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析下载的页面：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Obtain the title of the page. See that it is the same as what''s displayed
    in the browser:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取页面的标题。注意它与浏览器中显示的内容相同：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Find all the `h3` elements in the page, to determine the existing sections:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在页面中查找所有的`h3`元素，以确定现有的部分：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Extract the text on the section links. Stop when you reach the next `<h3>`
    tag:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取部分链接上的文本。当达到下一个`<h3>`标签时停止：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice that there are no HTML tags; it's all raw text.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意没有HTML标记；这都是原始文本。
- en: How it works...
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The first step is to download the page. Then, the raw text can be parsed, as
    in step 3\. The resulting `page` object contains the parsed information.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是下载页面。然后，可以像第3步那样解析原始文本。生成的`page`对象包含解析的信息。
- en: The `html.parser` parser is the default one, but for specific operations it
    can have problems. For example, for big pages it can be slow, or has issue rendering
    highly dynamic web pages. You can use other parsers, such as,  `lxml`, which is
    much faster, or `html5lib`, which will be closer to how a browser operates, including
    dynamic changes produced by HTML5\. They are external modules that will need to
    be added to the `requirements.txt` file.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`html.parser`解析器是默认的，但是对于特定操作可能会出现问题。例如，对于大页面，它可能会很慢，或者在渲染高度动态的网页时可能会出现问题。您可以使用其他解析器，例如`lxml`，它速度更快，或者`html5lib`，它将更接近浏览器的操作，包括HTML5产生的动态更改。它们是外部模块，需要添加到`requirements.txt`文件中。'
- en: '`BeautifulSoup` allows us to search for HTML elements. It can search for the
    first one with `.find()` or return a list with `.find_all()`. In step 5, it searched
    for a specific tag `<a>` that had a particular attribute, `name=link`. After that,
    it kept iterating on `.next_elements` until it finds the next `h3` tag, which
    marks the end of the section.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`BeautifulSoup`允许我们搜索HTML元素。它可以使用`.find()`搜索第一个元素，或者使用`.find_all()`返回一个列表。在第5步中，它搜索了一个具有特定属性`name=link`的特定标签`<a>`。之后，它继续在`.next_elements`上迭代，直到找到下一个`h3`标签，标志着该部分的结束。'
- en: The text of each element is extracted and finally composed into a single text.
    Note the `or` that avoids storing `None`, returned when an element has no text.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 提取每个元素的文本，最后组合成单个文本。注意`or`，它避免存储`None`，当元素没有文本时返回。
- en: HTML is highly versatile, and can have multiple structures. The case presented
    in this recipe is typical, but other options on dividing sections can be grouping
    related sections inside a big `<div>` tag or other elements, or even raw text.
    Some experimentation will be required until you find the specific process to extract
    the juicy bits on a web page. Don't be afraid to try!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: HTML非常灵活，可以有多种结构。本配方中介绍的情况是典型的，但是在划分部分方面的其他选项可能是将相关部分组合在一个大的`<div>`标签或其他元素内，甚至是原始文本。需要进行一些实验，直到找到从网页中提取重要部分的特定过程。不要害怕尝试！
- en: There's more...
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Regexes can be used as well as input in the `.find()` and `.find_all()` methods.
    For example, this search uses the `h2` and `h3` tags:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式也可以用作`.find()`和`.find_all()`方法的输入。例如，此搜索使用`h2`和`h3`标签：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Another useful find parameter is including the CSS class with the `class_` parameter.
    This will be shown later in the book.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的find参数是包含`class_`参数的CSS类。这将在本书的后面显示。
- en: The full Beautiful Soup documentation can be found here: [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的Beautiful Soup文档可以在这里找到：[https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)。
- en: See also
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Installing third-party packages* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml), *Let
    Us Begin Our Automation Journey*
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第1章](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml)的*让我们开始自动化之旅*中的*安装第三方包*配方
- en: The *Introducing regular expressions* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml), *Let
    Us Begin Our Automation Journey*
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第1章](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml)的*让我们开始自动化之旅*中的*介绍正则表达式*配方
- en: The *Downloading web pages* recipe
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下载网页*配方'
- en: Crawling the web
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取网页
- en: Given the nature of hyperlink pages, starting from a known place and following
    links to other pages is a very important tool in the arsenal when scraping the
    web.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到超链接页面的性质，从已知位置开始并跟随链接到其他页面是在抓取网页时的重要工具。
- en: To do so, we crawl a page looking for a small phrase, and will print any paragraph
    that contains it. We will search only in pages that belong to the same site. I.e. only
    URLs starting with www.somesite.com. We won't follow links to external sites.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们爬取页面寻找一个小短语，并打印包含它的任何段落。我们只会在属于同一网站的页面中搜索。即只有以www.somesite.com开头的URL。我们不会跟踪外部网站的链接。
- en: Getting ready
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe builds on the introduced concepts, so it will download and parse
    the pages to search for links and continue downloading.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱是基于介绍的概念构建的，因此它将下载和解析页面以搜索链接并继续下载。
- en: When crawling the web, remember to set limits when downloading. It's very easy
    to crawl over too many pages. As anyone checking Wikipedia can confirm, the internet
    is potentially limitless.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬取网页时，记得在下载时设置限制。很容易爬取太多页面。任何查看维基百科的人都可以证实，互联网是潜在无限的。
- en: 'We''ll use as an example a prepared example, available in the GitHub repo:
    [https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site](https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site).
    Download the whole site and run the included script.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个准备好的示例，该示例可在GitHub存储库中找到：[https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site](https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site)。下载整个站点并运行包含的脚本。
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This serves the site in the URL `http://localhost:8000`. You can check it on
    a browser. It's a simple blog with three entries. Most of it is uninteresting,
    but we added a couple of paragraphs that contain the keyword `python`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 它在URL`http://localhost:8000`中提供站点。您可以在浏览器上查看它。这是一个简单的博客，有三篇文章。大部分内容都不那么有趣，但我们添加了一些包含关键字`python`的段落。
- en: '![](assets/28ceb1d7-d5a3-47b8-b776-e6a0d1bf8bcb.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/28ceb1d7-d5a3-47b8-b776-e6a0d1bf8bcb.png)'
- en: How to do it...
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'The full script, `crawling_web_step1.py`, is available in GitHub at the following
    link: [https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/crawling_web_step1.py](https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/crawling_web_step1.py).
    The most relevant bits are displayed here:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完整的脚本`crawling_web_step1.py`可以在GitHub的以下链接找到：[https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/crawling_web_step1.py](https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/crawling_web_step1.py)。最相关的部分显示在这里：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Search for references to `python`, to return a list with URLs that contain
    it and the paragraph. Notice there are a couple of errors because of broken links:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索对`python`的引用，返回包含它和段落的URL列表。请注意，由于损坏的链接，有一些错误：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Another good search term is `crocodile`. Try it out:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个很好的搜索词是`crocodile`。试一下：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How it works...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Let''s see each of the components of the script:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看脚本的每个组件：
- en: 'A loop that goes through all the found links, in the `main` function:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个循环，遍历`main`函数中找到的所有链接：
- en: Note that there's a retrieval limit of 10 pages, and it's checking that any
    new link to add is not added already.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有10页的检索限制，并且正在检查是否已经添加了要添加的任何新链接。
- en: Note these two things are limits. We won't download the same link twice and
    we'll stop at some point.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意这两件事是有限制的。我们不会下载相同的链接两次，我们会在某个时候停止。
- en: 'Downloading and parsing the link, in the `process_link` function:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`process_link`函数中下载和解析链接：
- en: It downloads the file, and checks that the status is correct to skip errors
    such as broken links. It also checks that the type (as described in `Content-Type`)
    is a HTML page to skip PDFs and other formats. And finally, it parses the raw
    HTML into a `BeautifulSoup` object.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它下载文件，并检查状态是否正确，以跳过诸如损坏链接之类的错误。它还检查类型（如`Content-Type`中描述的）是否为HTML页面，以跳过PDF和其他格式。最后，它将原始HTML解析为`BeautifulSoup`对象。
- en: 'It also parses the source link using `urlparse`, so later, in step 4, it can
    skip all the references to external sources. `urlparse` divides a URL into its
    composing elements:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 它还使用`urlparse`解析源链接，以便在步骤4中跳过所有对外部来源的引用。`urlparse`将URL分解为其组成元素：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It finds the text to search, in the `search_text` function:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`search_text`函数中找到要搜索的文本：
- en: 'It searches the parsed object for the specified text. Note the search is done
    as a `regex` and only in the text. It prints the resulting matches, including
    `source_link`, referencing the URL where the match was found:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它在解析的对象中搜索指定的文本。请注意，搜索是作为`regex`进行的，仅在文本中进行。它打印出结果的匹配项，包括`source_link`，引用找到匹配项的URL：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The  **`get_links`** function retrieves all links on a page:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`get_links`**函数检索页面上的所有链接：'
- en: It searches in the parsed page all `<a>` elements, and retrieves the `href`
    elements, but only elements that have such `href` elements and that are a fully
    qualified URL (starting with `http`). This removes links that are not a URL, such
    as a `'#'` link, or that are internal to the page.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 它在解析页面中搜索所有`<a>`元素，并检索`href`元素，但只有具有这些`href`元素并且是完全合格的URL（以`http`开头）的元素。这将删除不是URL的链接，例如`'#'`链接，或者是页面内部的链接。
- en: An extra check is done to check they have the same source as the original link,
    then they are registered as valid links. The `netloc` attribute allows to detect
    that the link comes from the same URL domain than the parsed URL generated in
    step 2.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 还进行了额外的检查，以检查它们是否与原始链接具有相同的来源，然后将它们注册为有效链接。`netloc`属性允许检测链接是否来自与步骤2中生成的解析URL相同的URL域。
- en: We won't follow links that point to a different address (for example, a [http://www.google.com](http://www.google.com) one).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会跟踪指向不同地址的链接（例如[http://www.google.com](http://www.google.com)）。
- en: Finally, the links are returned, where they'll be added to the loop described
    in step 1.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，链接被返回，它们将被添加到步骤1中描述的循环中。
- en: There's more...
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Further filters could be enforced, for example, discarding all links that end
    in `.pdf`, meaning they are PDF files:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以进一步强制执行其他过滤器，例如丢弃所有以`.pdf`结尾的链接，这意味着它们是PDF文件：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The use of `Content-Type` can also be determined to parse the returned object
    in different ways. A PDF result (`Content-Type: application/pdf`) won''t have
    a valid `response.text` object to be parsed, but it can be parsed in other ways.
    The same is valid for other types, such as a CSV file (`Content-Type: text/csv`)
    or a ZIP file that may need to be decompressed (`Content-Type: application/zip`).
    We''ll see how to deal with those later.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '还可以使用`Content-Type`来确定以不同方式解析返回的对象。例如，PDF结果（`Content-Type: application/pdf`）将没有有效的`response.text`对象进行解析，但可以用其他方式解析。其他类型也是如此，例如CSV文件（`Content-Type:
    text/csv`）或可能需要解压缩的ZIP文件（`Content-Type: application/zip`）。我们将在后面看到如何处理这些。'
- en: See also
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Downloading web pages* recipe
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下载网页*食谱'
- en: The *Parsing HTML* recipe
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解析HTML*食谱'
- en: Subscribing to feeds
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 订阅Feed
- en: RSS is probably the biggest *secret* of the internet. While its moment of glory
    seemed to be during the 2000s, and now it's not in the spotlight anymore, it allows
    easy subscription to websites. It is present in lots of places, and it's incredibly
    useful.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: RSS可能是互联网上最大的“秘密”。虽然它的辉煌时刻似乎是在2000年代，现在它不再处于聚光灯下，但它可以轻松订阅网站。它存在于许多地方，非常有用。
- en: At its core, RSS is a way of presenting a succession of ordered references (typically
    articles, but also other elements such as podcast episodes or YouTube publications)
    and a publishing time. This makes for a very natural way of knowing what articles
    are new since the last check, as well as presenting some structured data about
    them, such as the title and a summary.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，RSS是一种呈现有序引用（通常是文章，但也包括其他元素，如播客剧集或YouTube出版物）和发布时间的方式。这使得很自然地知道自上次检查以来有哪些新文章，以及呈现一些关于它们的结构化数据，如标题和摘要。
- en: In this recipe, we will present the `feedparser` module, and determine how to
    obtain data from an RSS feed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将介绍`feedparser`模块，并确定如何从RSS Feed中获取数据。
- en: '**RSS** is not the only available feed format. There''s also a format called
    **Atom**, but both are very much equivalent. `feedparser` is also capable of parsing
    it, so both can be used indistinctly.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**RSS**不是唯一可用的Feed格式。还有一种称为**Atom**的格式，但两者几乎是等效的。`feedparser`也能够解析它，因此两者可以不加区分地使用。'
- en: Getting ready
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We need to add the `feedparser` dependency to our `requirements.txt` file and
    reinstall it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将`feedparser`依赖项添加到我们的`requirements.txt`文件中并重新安装它：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Feed URLs can be found on almost all pages that deal with publications, including
    blogs, news, podcasts, and so on. Sometimes they are very easy to find, but sometimes
    they are a little bit hidden. Search by `feed` or `RSS`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有涉及出版物的页面上都可以找到Feed URL，包括博客、新闻、播客等。有时很容易找到它们，但有时它们会隐藏得有点深。可以通过“feed”或“RSS”进行搜索。
- en: Most newspapers and news agencies has their RSS feeds divided by themes. We'll
    use as example to parse **The New York Times** main page feed, [http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml](http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml).
    There are more feeds available in the main feed page: [https://archive.nytimes.com/www.nytimes.com/services/xml/rss/index.html](https://archive.nytimes.com/www.nytimes.com/services/xml/rss/index.html?mcubz=0).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数报纸和新闻机构都将它们的RSS Feed按主题划分。我们将使用**纽约时报**主页Feed作为示例，[http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml](http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)。主要Feed页面上还有更多可用的Feed：[https://archive.nytimes.com/www.nytimes.com/services/xml/rss/index.html](https://archive.nytimes.com/www.nytimes.com/services/xml/rss/index.html?mcubz=0)。
- en: Please note the feeds may be subjected to terms and conditions of use. In the
    New York Times case, they are described at the end of the main feed page.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Feed可能受到使用条款和条件的约束。在纽约时报的情况下，它们在主要Feed页面的末尾有描述。
- en: Please note that this feed changes quite often, meaning that the linked entries
    will change from the examples in this book.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此Feed经常更改，这意味着链接的条目将与本书中的示例不同。
- en: How to do it...
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Import the `feedparser` module, as well as `datetime`, `delorean`, and `requests`:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`feedparser`模块，以及`datetime`、`delorean`和`requests`：
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Parse the feed (it will be downloaded automatically) and check when it was
    last updated. Feed information, like the title of the feed, can be obtained in
    the `feed` attribute:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析Feed（它将自动下载）并检查其上次更新时间。Feed信息，如Feed的标题，可以在`feed`属性中获取：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Get the entries that are newer than six hours:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取新于六小时的条目：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'There will be fewer entries than the total ones, because some of the returned
    entries will be older than six hours:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 条目将比总条目少，因为返回的条目中有些条目的时间已经超过六个小时：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Retrieve information about the entries, such as the `title`. The full entry
    URL is available as `link`. Explore the available information in this particular
    feed:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索条目的信息，如`title`。完整的条目URL可作为`link`获取。探索此特定Feed中的可用信息：
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works...
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The parsed `feed` object contains the information of the entries, as well as
    general information about the feed itself, such as when it was updated. The `feed`
    information can be found in the `feed` attribute:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 解析的`feed`对象包含条目的信息，以及有关Feed本身的一般信息，例如更新时间。`feed`信息可以在`feed`属性中找到：
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Each of the entries work as a dictionary, so the fields are easy to retrieve.
    They can also be accessed as attributes, but treating them as keys allows us to
    get all the available fields:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 每个条目都像一个字典，因此很容易检索字段。它们也可以作为属性访问，但将它们视为键可以获取所有可用的字段：
- en: '[PRE31]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The basic strategy when dealing with feeds is to parse them and go through the
    entries, performing a quick check on whether they are interesting or not, for
    example, by checking the *description* or *summary*. If they are download the
    whole page using the `link` field. Then, to avoid rechecking entires, store the
    latest publication date and next time, only check newer entries.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 处理Feed的基本策略是解析它们并浏览条目，快速检查它们是否有趣，例如通过检查*描述*或*摘要*。如果它们有趣，就使用“链接”字段下载整个页面。然后，为了避免重新检查条目，存储最新的发布日期，下次只检查更新的条目。
- en: There's more...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The full `feedparser` documentation can be found here: [https://pythonhosted.org/feedparser/](https://pythonhosted.org/feedparser/).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的`feedparser`文档可以在这里找到：[https://pythonhosted.org/feedparser/](https://pythonhosted.org/feedparser/)。
- en: The information available can differ from feed to feed. In the New York Times
    example, there's a `tag` field with tag information, but this is not standard.
    As a minimum, entries will have a title, a description, and a link.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的信息可能因源而异。在纽约时报的例子中，有一个带有标签信息的`tag`字段，但这不是标准的。至少，条目将有一个标题，一个描述和一个链接。
- en: RSS feeds are also a great way of curating your own selection of news sources.
    There are great feed readers for that.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: RSS订阅也是筛选自己的新闻来源的好方法。有很好的订阅阅读器。
- en: See also
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Installing third-party packages* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml), *Let
    Us Begin Our Automation Journey*
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第1章](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml)中的*安装第三方软件包*配方，*让我们开始我们的自动化之旅*'
- en: The *Downloading web pages* recipe
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下载网页*的配方'
- en: Accessing web APIs
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问Web API
- en: Rich interfaces can be created through the web, allowing powerful interactions
    through HTTP. The most common interface is through RESTful APIs using JSON. These
    text-based interfaces are easy to understand and to program, and use common technologies
    that are **language agnostic**, meaning they can be accessed in any programming
    language that has an HTTP `client` module, including, of course, Python.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Web可以创建丰富的接口，通过HTTP进行强大的交互。最常见的接口是使用JSON的RESTful API。这些基于文本的接口易于理解和编程，并使用通用技术，**与语言无关**，这意味着它们可以在任何具有HTTP`client`模块的编程语言中访问，当然包括Python。
- en: Formats other than JSON are used, such as XML, but JSON is a very simple and
    readable format that translates very well into Python dictionaries (and other
    language equivalents). JSON is, by far, the most common format in RESTful APIs
    at the moment. Learn more about JSON here: [https://www.json.org/](https://www.json.org/).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 除了JSON之外，还使用了其他格式，例如XML，但JSON是一种非常简单和可读的格式，非常适合转换为Python字典（以及其他语言的等价物）。JSON目前是RESTful
    API中最常见的格式。在这里了解更多关于JSON的信息：[https://www.json.org/](https://www.json.org/)。
- en: The strict definition of RESTful requires some characteristics, but a more informal
    definition could be accessing resources through URLs. This means a URL represents
    a particular resource, such as an article in a newspaper or a property on a real
    estate site. Resources can then be manipulated through HTTP methods (`GET` to
    view, `POST` to create, `PUT`/`PATCH` to edit, and `DELETE` to delete) to manipulate
    them.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: RESTful的严格定义需要一些特征，但更非正式的定义可能是通过URL访问资源。这意味着URL代表特定的资源，例如报纸上的文章或房地产网站上的属性。然后可以通过HTTP方法（`GET`查看，`POST`创建，`PUT`/`PATCH`编辑和`DELETE`删除）来操作资源。
- en: Proper RESTful interfaces need to have certain characteristics, and are a way
    of creating interfaces that is not strictly restricted to HTTP interfaces. You
    can read more about it here: [https://codewords.recurse.com/issues/five/what-restful-actually-means](https://codewords.recurse.com/issues/five/what-restful-actually-means).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的RESTful接口需要具有某些特征，并且是创建接口的一种方式，不严格限于HTTP接口。您可以在这里阅读更多信息：[https://codewords.recurse.com/issues/five/what-restful-actually-means](https://codewords.recurse.com/issues/five/what-restful-actually-means)。
- en: Using `requests` is very easy with them, as it includes native JSON support.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`requests`与它们非常容易，因为它包含本机JSON支持。
- en: Getting ready
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To demonstrate how to operate RESTful APIs, we''ll use the example site [https://jsonplaceholder.typicode.com/](https://jsonplaceholder.typicode.com/).
    It simulates a common case with posts, comments, and other common resources. We
    will use posts and comments. The URLs to use will be as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示如何操作RESTful API，我们将使用示例站点[https://jsonplaceholder.typicode.com/](https://jsonplaceholder.typicode.com/)。它模拟了帖子，评论和其他常见资源的常见情况。我们将使用帖子和评论。要使用的URL如下：
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The site returns the adequate result to each of them. Pretty handy!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 网站为它们中的每一个返回了适当的结果。非常方便！
- en: Because it is a test site, data won't be created, but the site will return all
    the correct responses.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个测试站点，数据不会被创建，但站点将返回所有正确的响应。
- en: How to do it...
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Import `requests`:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`requests`：
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Get the list of all posts and display the latest post:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取所有帖子的列表并显示最新帖子：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create a new post. See the URL of the new created resource. The call also returns
    the resource:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新帖子。查看新创建资源的URL。调用还返回资源：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Notice that the `POST` request to create the resource returns 201, which is
    the proper status for created.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，创建资源的`POST`请求返回201，这是创建的适当状态。
- en: 'Fetch an existing post with `GET`:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`GET`获取现有帖子：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Use `PATCH` to update its values. Check the returned resource:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`PATCH`更新其值。检查返回的资源：
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How it works...
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Two kinds of resources are typically accessed. Single resources (`https://jsonplaceholder.typicode.com/posts/X`)
    and collections (`https://jsonplaceholder.typicode.com/posts`):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通常访问两种资源。单个资源（`https://jsonplaceholder.typicode.com/posts/X`）和集合（`https://jsonplaceholder.typicode.com/posts`）：
- en: Collections accept `GET` to retrieve them all and `POST` to create a new resource
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合接受`GET`以检索它们所有，并接受`POST`以创建新资源
- en: Single elements accept `GET` to get the element, `PUT` and `PATCH` to edit,
    and `DELETE` to remove them
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个元素接受`GET`以获取元素，`PUT`和`PATCH`以编辑，`DELETE`以删除它们
- en: All the available HTTP methods can be called in `requests`. In the previous
    recipes, we used `.get()`, but `.post()`, `.patch()`, `.put()`, and `.delete()`
    are available.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可用的HTTP方法都可以在`requests`中调用。在以前的配方中，我们使用了`.get()`，但`.post()`，`.patch()`，`.put()`和`.delete()`也可用。
- en: The returned response object has a `.json()` method that decodes the result
    from JSON.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的响应对象具有`.json()`方法，用于解码JSON结果。
- en: Equally, to send information, a `json` argument is available. This encodes a
    dictionary into JSON and sends it to the server. The data needs to follow the
    format of the resource or an error may be raised.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，发送信息时，有一个`json`参数可用。这将字典编码为JSON并将其发送到服务器。数据需要遵循资源的格式，否则可能会引发错误。
- en: '`GET` and `DELETE` don''t require data, while `PATCH`, `PUT`, and `POST` do
    require data.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`GET`和`DELETE`不需要数据，而`PATCH`、`PUT`和`POST`需要数据。'
- en: The referred resource will be returned, and its URL is available in the header
    location. This is useful when creating a new resource, where its URL is not known
    beforehand.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 将返回所引用的资源，其URL在标头位置可用。这在创建新资源时非常有用，因为其URL事先是未知的。
- en: The difference between `PATCH` and `PUT` is that the latter replaces the whole
    resource, while the first does a partial update.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`PATCH`和`PUT`之间的区别在于后者替换整个资源，而前者进行部分更新。'
- en: There's more...
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: RESTful APIs are very powerful, but also have huge variability. Please check
    the documentation of the specific API to learn about its details.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: RESTful API非常强大，但也具有巨大的可变性。请查看特定API的文档，了解其详细信息。
- en: See also
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Downloading web pages* recipe
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下载网页*配方'
- en: The *Installing third-party packages* recipe in [Chapter 1](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml),
    *Let Us Begin Our Automation Journey*
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第1章](e139aa50-5631-4b75-9257-d4eb2e12ef90.xhtml)中的*安装第三方软件包*配方，*让我们开始我们的自动化之旅*'
- en: Interacting with forms
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与表单交互
- en: A common element present in web pages is forms. Forms are a way of sending values
    into a web page, for example, to create a new comment on a blog post, or to submit
    a purchase.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 网页中常见的一个元素是表单。表单是将值发送到网页的一种方式，例如，在博客文章上创建新评论或提交购买。
- en: Browsers present the forms so you can input values and send them in a single
    action after pressing the submit or equivalent button. We'll see how to create
    this action programatically in this recipe.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览器呈现表单，以便您可以输入值并在按下提交或等效按钮后以单个操作发送它们。我们将在此配方中看到如何以编程方式创建此操作。
- en: Be aware that sending data to a site is normally more sensible than receiving
    data from it. For example, sending automatic comments to a website is very much
    the definition of **spam**. This means that it can be more difficult to automate
    and include security measures. Double-check that what you're trying to achieve
    is a valid, ethical use case.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，向网站发送数据通常比从网站接收数据更明智。例如，向网站发送自动评论非常符合**垃圾邮件**的定义。这意味着自动化和包含安全措施可能更加困难。请仔细检查您尝试实现的是否是有效的、符合道德的用例。
- en: Getting ready
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We'll work against the test server, [https://httpbin.org/forms/post](https://httpbin.org/forms/post),
    which allows us to send a test form and sends back the submitted information.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将针对测试服务器[https://httpbin.org/forms/post](https://httpbin.org/forms/post)进行操作，该服务器允许我们发送测试表单并返回提交的信息。
- en: 'The following is an example form to order a pizza:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个订购比萨的示例表单：
- en: '![](assets/3fe93fc3-a5a2-450e-ba88-003566953c79.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3fe93fc3-a5a2-450e-ba88-003566953c79.png)'
- en: You can fill the form manually and see it return the information in JSON format,
    including extra information such as the browser use.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以手动填写表单并查看它以JSON格式返回信息，包括浏览器使用等额外信息。
- en: 'The following is the frontend of the web form generated:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成的Web表单的前端：
- en: '![](assets/32f55d90-7967-4733-9b79-a3a6eca11557.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/32f55d90-7967-4733-9b79-a3a6eca11557.png)'
- en: 'The following image is the back end of the web form generated:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像是生成的Web表单的后端：
- en: '![](assets/1f516fd0-d5de-484c-8875-9369cbe29b1b.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1f516fd0-d5de-484c-8875-9369cbe29b1b.png)'
- en: 'We need to analyze the HTML to see the accepted data for the form. Checking
    the source code, it shows this:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要分析HTML以查看表单的接受数据。检查源代码，显示如下：
- en: '![](assets/a2f505fc-424b-47f5-a94f-93541d72e0b8.png)Source code'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/a2f505fc-424b-47f5-a94f-93541d72e0b8.png)源代码'
- en: Check the name of the inputs, `custname`, `custtel`, `custemail`, `size` (a
    radio option), `topping` (a multiselection checkbox), `delivery` (time), and `comments`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 检查输入的名称，`custname`、`custtel`、`custemail`、`size`（单选按钮选项）、`topping`（多选复选框）、`delivery`（时间）和`comments`。
- en: How to do it...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Import `requests`, `BeautifulSoup`, and `re` modules:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`requests`、`BeautifulSoup`和`re`模块：
- en: '[PRE38]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Retrieve the form page, parse it, and print the input fields. Check that the
    posting URL is `/post` (not `/forms/post`):'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索表单页面，解析它，并打印输入字段。检查发布URL是否为`/post`（而不是`/forms/post`）：
- en: '[PRE39]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note `textarea` is a valid input, as well as defined in the HTML format.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`textarea`是有效输入，也是在HTML格式中定义的。
- en: 'Prepare the data to be posted as a dictionary. Check the values are the same
    as defined in the form:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备要发布的数据作为字典。检查值是否与表单中定义的相同：
- en: '[PRE40]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Post the values and check that the response is the same as returned in the
    browser:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发布值并检查响应是否与浏览器中返回的相同：
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: How it works...
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`requests` directly accepts to send data in the proper way. By default, it
    sends the `POST` data in the `application/x-www-form-urlencoded` format.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`requests`直接接受以正确方式发送数据。默认情况下，它以`application/x-www-form-urlencoded`格式发送`POST`数据。'
- en: Compare that with the *Accessing web APIs* recipe, where the data is explicitly
    sent in JSON format using the argument `json`. This makes the `Content-Type` be
    `application/json` instead of `application/x-www-form-urlencoded`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与*访问Web API*配方进行比较，其中数据是使用参数`json`以JSON格式明确发送的。这使得`Content-Type`为`application/json`而不是`application/x-www-form-urlencoded`。
- en: The key aspect here is to respect the format of the form and the possible values
    that can return an error if incorrect, typically a 400 error.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键是尊重表单的格式和可能返回错误的可能值，通常是400错误。
- en: There's more...
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Other than following the format of forms and inputting valid values, the main
    problem when working with forms is the multiple ways of preventing spam and abusive
    behavior.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 除了遵循表单的格式和输入有效值之外，在处理表单时的主要问题是防止垃圾邮件和滥用行为的多种方式。
- en: A very common limitation is to ensure that you downloaded the form before submitting
    it, to avoid submitting multiple forms or **Cross-Site Request Forgery** (**CSRF**).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 非常常见的限制是确保在提交表单之前下载了表单，以避免提交多个表单或**跨站点请求伪造**（**CSRF**）。
- en: CSRF, which means producing a malicious call from a page to another taking advantage
    that  your browser is authenticated, is a serious problem. For example, entering
    in a puppies site that take advantage of you being logged into your bank page
    to perform operations "on your behalf". Here is a good description of it: [https://stackoverflow.com/a/33829607](https://stackoverflow.com/a/33829607).
    New techniques in browsers help with these issues by default.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: CSRF，意味着从一个页面对另一个页面发出恶意调用，利用您的浏览器已经经过身份验证，这是一个严重的问题。例如，进入一个利用您已经登录到银行页面执行操作的小狗网站。这是一个很好的描述：[https://stackoverflow.com/a/33829607](https://stackoverflow.com/a/33829607)。浏览器中的新技术默认情况下有助于解决这些问题。
- en: 'To obtain the specific token, you need to first download the form, as shown
    in the recipe, obtain the value of the CSRF token, and resubmit it. Note that
    the token can have different names; this is just an example:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取特定令牌，您需要首先下载表单，如配方中所示，获取CSRF令牌的值，并重新提交。请注意，令牌可以有不同的名称；这只是一个例子：
- en: '[PRE43]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: See also
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Downloading web pages* recipe
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下载网页*的配方'
- en: The *Parsing HTML* recipe
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解析HTML*的配方'
- en: Using Selenium for advanced interaction
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Selenium进行高级交互
- en: Sometimes, nothing short of the real thing will work. Selenium is a project
    to achieve automation in web browsers. It's conceived as a way of automatic testing,
    but it also can be used to automate interactions with a site.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，除了真实的东西外，什么都行不通。 Selenium是一个实现Web浏览器自动化的项目。它被构想为一种自动测试的方式，但也可以用于自动化与网站的交互。
- en: Selenium can control Safari, Chrome, Firefox, Internet Explorer, or Microsoft
    Edge, though it requires installing a specific driver for each case. We'll use
    Chrome.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Selenium可以控制Safari、Chrome、Firefox、Internet Explorer或Microsoft Edge，尽管它需要为每种情况安装特定的驱动程序。我们将使用Chrome。
- en: Getting ready
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We need to install the right driver for Chrome, called `chromedriver`. It is
    available here: [https://sites.google.com/a/chromium.org/chromedriver/](https://sites.google.com/a/chromium.org/chromedriver/).
    It is available for most platforms. It also requires that you have Chrome installed: [https://www.google.com/chrome/](https://www.google.com/chrome/).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为Chrome安装正确的驱动程序，称为`chromedriver`。它在这里可用：[https://sites.google.com/a/chromium.org/chromedriver/](https://sites.google.com/a/chromium.org/chromedriver/)。它适用于大多数平台。它还要求您已安装Chrome：[https://www.google.com/chrome/](https://www.google.com/chrome/)。
- en: 'Add the `selenium` module to `requirements.txt` and install it:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 将`selenium`模块添加到`requirements.txt`并安装它：
- en: '[PRE44]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: How to do it...
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Import Selenium, start a browser, and load the form page. A page will open
    reflecting the operations:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Selenium，启动浏览器，并加载表单页面。将打开一个反映操作的页面：
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note the banner with Chrome is being controlled by automated test software.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Chrome中的横幅由自动化测试软件控制。
- en: 'Add a value in the Customer name field. Remember that it is called `custname`:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“客户名称”字段中添加一个值。请记住它被称为`custname`：
- en: '[PRE46]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The form will update:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表单将更新：
- en: '![](assets/13d8d74f-9f2e-4ef9-87e1-29deb1c70b95.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/13d8d74f-9f2e-4ef9-87e1-29deb1c70b95.png)'
- en: 'Select the pizza size as `medium`:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择披萨大小为`medium`：
- en: '[PRE47]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This will change the pizza size ratio box.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这将改变披萨大小比例框。
- en: 'Add `bacon` and `cheese`:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加`bacon`和`cheese`：
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, the checkboxes will appear as marked:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，复选框将显示为已标记：
- en: '![](assets/789403c5-3c8f-4c14-b9c5-6318286ae9c1.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/789403c5-3c8f-4c14-b9c5-6318286ae9c1.png)'
- en: 'Submit the form. The page will submit and the result will be displayed:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提交表单。页面将提交，结果将显示：
- en: '[PRE49]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The form will be submitted and the result from the server will be displayed:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 表单将被提交，服务器的结果将显示：
- en: '![](assets/b9063025-2521-4820-a446-90e9d89b4dbd.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b9063025-2521-4820-a446-90e9d89b4dbd.png)'
- en: 'Close the browser:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭浏览器：
- en: '[PRE50]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: How it works...
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Step 1 in the *How to do it…* section shows how to create a Selenium page and
    go to a particular URL.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '*如何做...*部分的第1步显示了如何创建一个Selenium页面并转到特定的URL。'
- en: Selenium works in a similar way to Beautiful Soup. Select the adequate element,
    and then manipulate it. The selectors in Selenium work in a similar way to those
    in Beautiful Soup, with the most common ones being `find_element_by_id`, `find_element_by_class_name`,
    `find_element_by_name`, `find_element_by_tag_name`, and `find_element_by_css_selector`.
    There are equivalent `find_elements_by_X`  that return a list instead of the first
    found element ( `find_elements_by_tag_name`, `find_elements_by_name`, and more).
    This is also useful when checking whether the element is there or not. If there's
    no elements, `find_element` will raise an error while `find_elements` will return
    an empty list.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Selenium的工作方式与Beautiful Soup类似。选择适当的元素，然后操纵它。 Selenium中的选择器的工作方式与Beautiful Soup中的选择器的工作方式类似，最常见的选择器是`find_element_by_id`、`find_element_by_class_name`、`find_element_by_name`、`find_element_by_tag_name`和`find_element_by_css_selector`。还有等效的`find_elements_by_X`，它们返回一个列表而不是第一个找到的元素（`find_elements_by_tag_name`、`find_elements_by_name`等）。当检查元素是否存在时，这也很有用。如果没有元素，`find_element`将引发错误，而`find_elements`将返回一个空列表。
- en: The data on the elements can be obtained through `.get_attribute()` for HTML
    attributes (such as the values on the form elements) or `.text`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过`.get_attribute()`获取元素上的数据，用于HTML属性（例如表单元素上的值）或`.text`。
- en: The elements can be manipulated by simulating sending keystrokes to input text,
    with the method `.send_keys()`, clicked with `.click()` or submitted with `.submit()`
    if they accept that. `.submit()` will search on a form for the proper submission,
    and `.click()` will select/deselect in the same way that a click of the mouse
    will do.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过模拟发送按键输入文本来操作元素，方法是`.send_keys()`，点击是`.click()`，如果它们接受，可以使用`.submit()`进行提交。`.submit()`将在表单上搜索适当的提交，`.click()`将以与鼠标点击相同的方式选择/取消选择。
- en: Finally, step 6 closes the browser.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第6步关闭浏览器。
- en: There's more...
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Here is the full Selenium documentation: [http://selenium-python.readthedocs.io/](http://selenium-python.readthedocs.io/).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的Selenium文档：[http://selenium-python.readthedocs.io/](http://selenium-python.readthedocs.io/)。
- en: For each of the elements, there's extra information that can be extracted, such
    as `.is_displayed()` or `.is_selected()`. Text can be searched using `.find_element_by_link_text()`
    and `.find_element_by_partial_link_text()`.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个元素，都可以提取额外的信息，例如`.is_displayed()`或`.is_selected()`。可以使用`.find_element_by_link_text()`和`.find_element_by_partial_link_text()`来搜索文本。
- en: 'Sometimes, opening a browser can be inconvenient. An alternative is to start
    the browser in headless mode and manipulate it from there, like this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，打开浏览器可能会不方便。另一种选择是以无头模式启动浏览器，并从那里操纵它，就像这样：
- en: '[PRE51]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The page won''t be displayed. But a screenshot can be saved anyway with the
    following line:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 页面不会显示。但是可以使用以下命令保存截图：
- en: '[PRE52]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: See also
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Parsing HTML* recipe
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解析HTML*配方'
- en: The *Interact with forms* recipe
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与表单交互*配方'
- en: Accessing password-protected pages
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问受密码保护的页面
- en: Sometimes a web page is not open to the public, but protected in some way. The
    most basic aspect is to use basic HTTP authentication, which is integrated into
    virtually every web server, and it's a user/password schema.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，网页对公众不开放，而是以某种方式受到保护。最基本的方面是使用基本的HTTP身份验证，它集成到几乎每个Web服务器中，并且是用户/密码模式。
- en: Getting ready
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We can test this kind of authentication in [https://httpbin.org](https://httpbin.org).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[https://httpbin.org](https://httpbin.org)中测试这种身份验证。
- en: It has a path, `/basic-auth/{user}/{password}`, which forces authentication,
    with the user and password stated. This is very handy for understanding how authentication
    works.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 它有一个路径，“/basic-auth/{user}/{password}”，强制进行身份验证，用户和密码已声明。这对于理解身份验证的工作原理非常方便。
- en: How to do it...
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Import `requests`:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`requests`：
- en: '[PRE53]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Make a `GET` request to the URL with the wrong credentials. Notice that we
    set the credentials on the URL to be `user` and `psswd`:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用错误的凭据对URL进行`GET`请求。注意，我们在URL上设置了凭据为`user`和`psswd`：
- en: '[PRE54]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Use the wrong credentials to return a 401 status code (Unauthorized):'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用错误的凭据返回401状态码（未经授权）：
- en: '[PRE55]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The credentials can be also passed directly in the URL, separated by a colon
    and an `@` symbol before the server, like this:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 凭据也可以直接通过URL传递，在服务器之前用冒号和`@`符号分隔，就像这样：
- en: '[PRE56]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: How it works...
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: As HTTP basic authentication is supported everywhere, support from `requests`
    is very easy.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 由于HTTP基本身份验证在各处都受支持，因此从“请求”获得支持非常容易。
- en: Steps 2 and 4 in the *How to do it…* section show how to provide the proper
    password. Step 3 shows what happens when the password is the wrong one.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '*如何做...*部分的第2步和第4步显示了如何提供正确的密码。第3步显示了密码错误时会发生什么。'
- en: Remember to always use HTTPS to ensure that the sending of the password is kept
    secret. If you use HTTP, the password will be sent in the open over the web.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住始终使用HTTPS，以确保密码的发送保密。如果使用HTTP，密码将在网络上以明文发送。
- en: There's more...
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Adding the user and password to the URL works on the browser as well. Try to
    access the page directly to see a box displayed asking for the username and password:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 将用户和密码添加到URL中也适用于浏览器。尝试直接访问页面，看到一个框显示要求输入用户名和密码：
- en: '![](assets/b0d91d6e-fa37-4084-82f1-d825d15441bc.png)User credentials page'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/b0d91d6e-fa37-4084-82f1-d825d15441bc.png)用户凭据页面'
- en: When using the URL containing the user and password, `https://user:psswd@httpbin.org/basic-auth/user/psswd`,
    the dialog does not appear and it authenticates automatically.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用包含用户和密码的URL时，`https://user:psswd@httpbin.org/basic-auth/user/psswd`，对话框不会出现，它会自动进行身份验证。
- en: 'If you need to access multiple pages, you can create a session in `requests`
    and set the authentication parameters to avoid having to input them everywhere:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要访问多个页面，可以在“请求”中创建一个会话，并将身份验证参数设置为避免在各处输入它们：
- en: '[PRE57]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: See also
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Downloading web pages* recipe
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下载网页*配方'
- en: The *Accessing Web APIs* recipe
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*访问Web API*配方'
- en: Speeding up web scraping
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加快网页抓取速度
- en: Most of the time spent downloading information from web pages is usually spent
    waiting. A request goes from our computer to whatever server will process it,
    and until the response is composed and comes back to our computer, we cannot do
    much about it.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 从网页下载信息所花费的大部分时间通常是在等待。请求从我们的计算机发送到将处理它的任何服务器，直到响应被组成并返回到我们的计算机，我们无法做太多事情。
- en: During the execution of the recipes in the book, you'll notice there's a wait
    involved in `requests` calls, normally of around one or two seconds. But computers
    can do other stuff while waiting, including making more requests at the same time.
    In this recipe, we will see how to download a list of pages in parallel and wait
    until they are all ready. We will use an intentionally slow server to show the
    point.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在书中执行配方时，您会注意到`requests`调用通常需要等待大约一两秒。但是计算机可以在等待时做其他事情，包括同时发出更多请求。在这个配方中，我们将看到如何并行下载页面列表并等待它们全部准备就绪。我们将使用一个故意缓慢的服务器来说明这一点。
- en: Getting ready
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We'll get code to crawl and search for keywords, making use of the `futures`
    capabilities of Python 3 to download multiple pages at the same time.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得代码来爬取和搜索关键字，利用Python 3的`futures`功能同时下载多个页面。
- en: A `future` is an object that represents the promise of a value. This means that
    you immediately receive an object while the code is being executed in the background.
    Only, when specifically requesting for its `.result()` the code blocks until getting
    it.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`future`是表示值承诺的对象。这意味着在代码在后台执行时，您立即收到一个对象。只有在明确请求其`.result()`时，代码才会阻塞，直到获取它。'
- en: To generate a `future`, you need a background engine, called **executor**. Once
    created, `submit` a function and parameters to it to retrieve a `future`.  The
    retrieval of the result can be delayed as long as necessary, allowing the generation
    of several `futures` in a row, and waiting until all are finished, executing them
    in parallel, instead of creating one, wait until it finishes, creating another,
    and so on.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成一个`future`，你需要一个后台引擎，称为**executor**。一旦创建，`submit`一个函数和参数给它以检索一个`future`。结果的检索可以被延迟，直到需要，允许连续生成多个`futures`，并等待直到所有都完成，以并行执行它们，而不是创建一个，等待它完成，再创建另一个，依此类推。
- en: There are several ways to create an executor; in this recipe, we'll use `ThreadPoolExecutor`,
    which will use threads.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以创建一个executor；在这个示例中，我们将使用`ThreadPoolExecutor`，它将使用线程。
- en: We'll use as an example a prepared example, available in the GitHub repo: [https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site](https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site).
    Download the whole site and run the included script
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以一个准备好的示例为例，该示例可在GitHub存储库中找到：[https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site](https://github.com/PacktPublishing/Python-Automation-Cookbook/tree/master/Chapter03/test_site)。下载整个站点并运行包含的脚本。
- en: '[PRE58]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This serves the site in the URL `http://localhost:8000`. You can check it on
    a browser. It's s simple blog with three entries. Most of it is uninteresting,
    but we added a couple of paragraphs that contain the keyword `python`. The parameter
    `-d 2` makes the server intentionally slow, simulating a bad connection.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这是URL为`http://localhost:8000`的站点。你可以在浏览器上查看它。这是一个简单的博客，有三篇文章。大部分内容都不那么有趣，但我们添加了几段包含关键字`python`的段落。参数`-d
    2`使服务器故意变慢，模拟一个糟糕的连接。
- en: How to do it...
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Write the following script, `speed_up_step1.py`. The full code is available
    in GitHub under the  `Chapter03`: [https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/speed_up_step1.py ](https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/speed_up_step1.py)directory.
    Here are only the most relevant parts. It is based on `crawling_web_step1.py`:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写以下脚本`speed_up_step1.py`。完整的代码可以在GitHub的`Chapter03`目录下找到：[https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/speed_up_step1.py](https://github.com/PacktPublishing/Python-Automation-Cookbook/blob/master/Chapter03/speed_up_step1.py)。这里只列出了最相关的部分。它基于`crawling_web_step1.py`。
- en: '[PRE59]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Notice the differences in the `main` function. Also, there's an extra parameter
    added (number of concurrent workers), and the function `process_link` now returns
    the source link.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意`main`函数中的差异。还添加了一个额外的参数（并发工作人员的数量），并且`process_link`函数现在返回源链接。
- en: 'Run the `crawling_web_step1.py` script to get a time baseline. Notice the output
    has been removed here for clarity:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`crawling_web_step1.py`脚本以获取时间基准。注意这里已经删除了输出以保持清晰：
- en: '[PRE60]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Run the new script with one worker, which is slower than the original one:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用比原始版本慢的一个工作人员运行新脚本：
- en: '[PRE61]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Increase the number of workers:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加工作人员的数量：
- en: '[PRE62]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Adding more workers decreases the time:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加更多的工作人员会减少时间。
- en: '[PRE63]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: How it works...
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The main engine to create the concurrent requests is the main function. Notice
    that the rest of the code is basically untouched (other than returning the source
    link in the `process_link` function).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 创建并发请求的主要引擎是主函数。请注意，代码的其余部分基本上没有改动（除了在`process_link`函数中返回源链接）。
- en: This change is actually quite common when adapting for concurrency. Concurrent
    tasks need to return all the relevant data, as they cannot rely on an ordered
    context.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变化在适应并发时实际上是相当常见的。并发任务需要返回所有相关的数据，因为它们不能依赖于有序的上下文。
- en: 'This is the relevant part of the code that handles the concurrent engine:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这是处理并发引擎的代码的相关部分：
- en: '[PRE64]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The `with` context creates a pool of workers, specifying its number. Inside,
    a list of futures containing all the URLs to retrieve is created. The `.as_completed()`
    function returns the futures that are finished, and then there's some work dealing
    with obtaining newly found links and checking whether they need to be added to
    be retrieved or not. This process is similar to the one presented in the *Crawling
    the web *recipe.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '`with`上下文创建了一个指定数量的工作人员池。在内部，创建了一个包含所有要检索的URL的`futures`列表。`.as_completed()`函数返回已完成的`futures`，然后进行一些工作，处理获取到的新链接，并检查它们是否需要被添加到检索中。这个过程类似于*Crawling
    the web*示例中呈现的过程。'
- en: The process starts again until enough links have been retrieved or there are
    no links to retrieve. Note that the links are retrieved in batches; the first
    time, the base link is processed and all links are retrieved. In the second iteration,
    all those links will be requested. Once they are all downloaded, a new batch will
    be processed.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程会再次开始，直到检索到足够的链接或没有链接可检索为止。请注意，链接是批量检索的；第一次，处理基本链接并检索所有链接。在第二次迭代中，将请求所有这些链接。一旦它们都被下载，将处理一个新的批次。
- en: When dealing with concurrent requests, keep in mind that they can change order
    between two executions. If a request takes a little more or a little less time,
    that can affect the ordering of the retrieved information. Because we stop after
    downloading 10 pages, that also means that the 10 pages could be different.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 处理并发请求时，请记住它们可以在两次执行之间改变顺序。如果一个请求花费的时间稍微多一点或少一点，那可能会影响检索信息的顺序。因为我们在下载10页后停止，这也意味着这10页可能是不同的。
- en: There's more...
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The full `futures` documentation in Python can be found here: [https://docs.python.org/3/library/concurrent.futures.html](https://docs.python.org/3/library/concurrent.futures.html).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Python的完整`futures`文档可以在这里找到：[https://docs.python.org/3/library/concurrent.futures.html](https://docs.python.org/3/library/concurrent.futures.html)。
- en: As you can see in steps 4 and 5 in the *How to do it…* section, properly determining
    the number of workers can require some tests. Some numbers can make the process
    slower, due the increase in management. Do not be afraid to experiment!
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*如何做…*部分的第4和第5步中所看到的，正确确定工作人员的数量可能需要一些测试。一些数字可能会使过程变慢，因为管理增加了。不要害怕尝试！
- en: In the Python world, there are other ways to make concurrent HTTP requests.
    There's a native request module that allows us to work with `futures`, called
    `requests-futures`. It can be found here: [https://github.com/ross/requests-futures](https://github.com/ross/requests-futures).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python世界中，还有其他方法可以进行并发的HTTP请求。有一个原生请求模块，允许我们使用`futures`，名为`requests-futures`。它可以在这里找到：[https://github.com/ross/requests-futures](https://github.com/ross/requests-futures)。
- en: Another alternative is to use asynchronous programming. This way of working
    has recently gotten a lot of attention, as it can be very efficient in situations
    when dealing with many concurrent calls, but the resulting way of coding is different
    from the traditional way and requires some time to get used to. Python includes
    the `asyncio` module to work that way, and there's a good module called `aiohttp`
    to work with HTTP requests. You can find more information about `aiohttp` here: [https://aiohttp.readthedocs.io/en/stable/client_quickstart.html](https://aiohttp.readthedocs.io/en/stable/client_quickstart.html).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用异步编程。最近，这种工作方式引起了很多关注，因为在处理许多并发调用时可以非常高效，但编码的方式与传统方式不同，需要一些时间来适应。Python包括`asyncio`模块来进行这种工作，还有一个名为`aiohttp`的好模块来处理HTTP请求。您可以在这里找到有关`aiohttp`的更多信息：[https://aiohttp.readthedocs.io/en/stable/client_quickstart.html](https://aiohttp.readthedocs.io/en/stable/client_quickstart.html)。
- en: A good introduction to asynchronous programming can be found in this article: [https://djangostars.com/blog/asynchronous-programming-in-python-asyncio/](https://djangostars.com/blog/asynchronous-programming-in-python-asyncio/).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 关于异步编程的良好介绍可以在这篇文章中找到：[https://djangostars.com/blog/asynchronous-programming-in-python-asyncio/](https://djangostars.com/blog/asynchronous-programming-in-python-asyncio/)。
- en: See also
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The *Crawling the web* recipe
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*爬取网页*配方'
- en: The *Downloading web pages* recipe
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下载网页*配方'
