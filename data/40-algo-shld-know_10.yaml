- en: Neural Network Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络算法
- en: A combination of various factors has made **Artificial Neural Networks** (**ANNs**)
    one of the most important machine learning techniques available today. These factors
    include the need to solve increasingly complex problems, the explosion of data,
    and the emergence of technologies, such as readily available cheap clusters, that
    provide the computing power necessary to design very complex algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 各种因素的结合使得人工神经网络（ANNs）成为当今最重要的机器学习技术之一。这些因素包括解决日益复杂的问题的需求，数据的爆炸以及诸如可用的廉价集群等技术的出现，这些技术提供了设计非常复杂算法所需的计算能力。
- en: In fact, this is the research area that is rapidly evolving and is responsible
    for most of the major advances claimed by leading-edge tech fields such as robotics,
    natural language processing, and self-driving cars.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这是一个迅速发展的研究领域，负责实现机器人技术、自然语言处理和自动驾驶汽车等领先技术领域所宣称的大部分重大进展。
- en: Looking into the structure of an ANN, its basic unit is a neuron. The real strength
    of the ANN lies in its ability to use the power of multiple neurons by organizing
    them in a layered architecture. An ANN creates a layered architecture by chaining
    neurons together in various layers. A signal passes through these layers and is
    processed in different ways in each of the layers until the final required output
    is generated. As we will see in this chapter, the hidden layers used by ANNs act
    as layers of abstraction, enabling deep learning, which is extensively used in
    realizing powerful applications such as Amazon's Alexa, Google's image search,
    and Google Photos.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 观察ANN的结构，其基本单元是神经元。ANN的真正优势在于其能够通过将它们组织成分层架构来利用多个神经元的力量。ANN通过在不同层中将神经元链接在一起来创建分层架构。信号通过这些层传递，并在每个层中以不同的方式进行处理，直到生成最终所需的输出。正如我们将在本章中看到的，ANN使用的隐藏层充当抽象层，实现了深度学习，这在实现强大的应用程序中被广泛使用，如亚马逊的Alexa，谷歌的图像搜索和谷歌相册。
- en: This chapter first introduces the main concepts and components of a typical
    neural network. Then, it presents the various types of neural networks and explains
    the different kinds of activation functions used in these neural networks. Then,
    the backpropagation algorithm is discussed in detail, which is the most widely
    used algorithm for training a neural network. Next, the transfer learning technique
    is explained, which can be used to greatly simplify and partially automate the
    training of models. Finally, how to use deep learning to flag fraudulent documents
    is looked at by way of a real-world example application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先介绍了典型神经网络的主要概念和组件。然后，它介绍了各种类型的神经网络，并解释了这些神经网络中使用的不同类型的激活函数。接下来，详细讨论了反向传播算法，这是训练神经网络最广泛使用的算法。然后，解释了迁移学习技术，可以用来大大简化和部分自动化模型的训练。最后，通过一个真实世界的应用程序案例，探讨了如何使用深度学习来标记欺诈文件。
- en: 'The following are the main concepts discussed in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的主要概念如下：
- en: Understanding ANNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解人工神经网络
- en: The evolution of ANNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ANN的演变
- en: Training a neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: Tools and frameworks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具和框架
- en: Transfer learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: 'Case study: using deep learning for fraud detection'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究：使用深度学习进行欺诈检测
- en: Let's start by looking at the basics of ANNs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从人工神经网络的基础知识开始。
- en: Understanding ANNs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解人工神经网络
- en: Inspired by the working of neurons in the human brain, the concept of neural
    networks was proposed by Frank Rosenblatt in 1957\. To understand the architecture
    fully, it is helpful to briefly look at the layered structure of neurons in the
    human brain. (Refer to the following diagram to get an idea of how the neurons
    in the human brain are chained together.)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 受人脑神经元工作的启发，神经网络的概念是由Frank Rosenblatt在1957年提出的。要充分理解其架构，有助于简要了解人脑中神经元的分层结构。（参考以下图表，了解人脑中的神经元是如何连接在一起的。）
- en: 'In the human brain, **dendrites**act as sensors that detect a signal. The signal
    is then passed on to an **a****xon**, which is a long, slender projection of a
    nerve cell. The function of the axon is to transmit this signal to muscles, glands,
    and other neurons. As shown in the following diagram, the signal travels through
    interconnecting tissue called a **synapse** before being passed on to other neurons.  Note
    that through this organic pipeline, the signal keeps traveling until it reaches
    the target muscle or gland, where it causes the required action. It typically
    takes seven to eight milliseconds for the signal to pass through the chain of
    neurons and reach its destination:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在人脑中，树突充当检测信号的传感器。然后，信号传递给轴突，轴突是神经细胞的一种长而细的突出部分。轴突的功能是将这个信号传递给肌肉、腺体和其他神经元。如下图所示，信号通过称为突触的相互连接的组织传递，然后传递给其他神经元。请注意，通过这种有机管道，信号一直传递，直到它到达目标肌肉或腺体，引起所需的动作。信号通常需要七到八毫秒才能通过神经元链传递并到达目标位置：
- en: '![](assets/0c34e87b-05f8-4248-a1d3-bfecc9d58c0b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/0c34e87b-05f8-4248-a1d3-bfecc9d58c0b.png)'
- en: 'Inspired by this natural architectural masterpiece of signal processing, Frank
    Rosenblatt devised a technique that would mean digital information could be processed
    in layers to solve a complex mathematical problem. His initial attempt at designing
    a neural network was quite simple and looked similar to a linear regression model.
    This simple neural network did not have any hidden layers and was named a *perceptron*.
    The following diagram illustrates it:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这种自然的信号处理的建筑杰作的启发，Frank Rosenblatt设计了一种意味着数字信息可以在层中处理以解决复杂数学问题的技术。他最初设计的神经网络非常简单，看起来类似于线性回归模型。这种简单的神经网络没有任何隐藏层，被命名为感知器。以下图表对其进行了说明：
- en: '![](assets/88d873d7-e3fb-4de6-a442-85e1cd5539c9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/88d873d7-e3fb-4de6-a442-85e1cd5539c9.png)'
- en: 'Let''s try to develop the mathematical representation of this perceptron. In
    the preceding diagram, the input signals are shown on the left-hand side. It is
    a weighted sum of inputs because each of the inputs *(x[1], x[2]..x[n])* gets
    multiplied by a corresponding weight *(w[1],w[2]… w[n])* and then summed up:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试开发这个感知器的数学表示。在前面的图表中，输入信号显示在左侧。这是输入的加权和，因为每个输入*(x[1]，x[2]..x[n])*都会乘以相应的权重*(w[1]，w[2]…
    w[n])*，然后求和：
- en: '![](assets/3f4de7d2-6950-4b64-a0c5-3f96e3b1f24e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3f4de7d2-6950-4b64-a0c5-3f96e3b1f24e.png)'
- en: Note that it is a binary classifier because the final output from this perceptron
    is true or false depending on the output of the aggregator (shown as **∑** in
    the diagram).  The aggregator will produce a true signal if it can detect a valid
    signal from at least one of the inputs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个二元分类器，因为这个感知器的最终输出取决于聚合器的输出是真还是假（在图表中显示为**∑**）。如果聚合器能够从至少一个输入中检测到有效信号，它将产生一个真实的信号。
- en: Let's now look into how neural networks have evolved over time.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看神经网络是如何随着时间的推移而发展的。
- en: The Evolution of ANNs
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络的演变
- en: In the preceding section, we looked into a simple neural network without any
    layers called a perceptron. The perceptron was found to have serious limitations,
    and in 1969, Marvin Minsky and Seymour Papert worked on research that led to the
    conclusion that a perceptron is incapable of learning any complex logic.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们研究了一个没有任何层的简单神经网络，称为感知器。发现感知器有严重的局限性，并且在1969年，马文·明斯基和西摩·帕帕特进行了研究，得出结论感知器无法学习任何复杂的逻辑。
- en: In fact, they showed that it would be a struggle to learn even logical functions
    as simple as XOR. That led to a decrease in interest in machine learning in general,
    and neural networks in particular, and started an era that is now known as the
    **AI winter**. Researchers around the world would not take AI seriously, thinking
    that it was incapable of solving any complex problems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，他们表明即使学习像异或这样简单的逻辑函数也是一种挑战。这导致了对机器学习和神经网络的兴趣下降，开始了一个现在被称为**AI寒冬**的时代。世界各地的研究人员不会认真对待人工智能，认为它无法解决任何复杂的问题。
- en: One of the primary reasons for the so-called AI winter was the limitation of
    the hardware capabilities available at that time. Either the necessary computing
    power was not available or it was prohibitively expensive. Toward the end of the
    1990s, advances in distributed computing provided easily available and affordable
    infrastructure, which resulted in the thaw of the AI winter. The thaw reinvigorated
    research in AI. This eventually resulted in turning the current era into an era
    that can be called the **AI spring**, where there is so much interest in AI in
    general and neural networks in particular.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓的AI寒冬的主要原因之一是当时可用的硬件能力的限制。要么是必要的计算能力不可用，要么是价格昂贵。到了20世纪90年代末，分布式计算的进步提供了易于获得和负担得起的基础设施，这导致了AI寒冬的融化。这种融化重新激发了对人工智能的研究。最终导致了将当前时代变成一个可以称为**AI春天**的时代，人们对人工智能和神经网络特别感兴趣。
- en: 'For more complex problems, researchers have developed a multilayer neural network
    called a **multilayer perceptron**. A multilayer neural network has a few different
    layers, as shown in the following diagram. These layers are as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的问题，研究人员开发了一个称为**多层感知器**的多层神经网络。多层神经网络有几个不同的层，如下图所示。这些层包括：
- en: Input layer
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Hidden layer(s)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: 'Output layer:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层：
- en: A deep neural network is a neural network with one or more hidden layers. Deep
    learning is the process of training an ANN.![](assets/4ec6b21a-e4bb-4c00-b243-d7b438d0f3d9.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络是一个具有一个或多个隐藏层的神经网络。深度学习是训练人工神经网络的过程。![](assets/4ec6b21a-e4bb-4c00-b243-d7b438d0f3d9.png)
- en: An important thing to note is that the neuron is the basic unit of this network,
    and  each neuron of a layer is connected to all neurons of the next layer. For
    complex networks, the number of these interconnections explodes, and we will explore
    different ways of reducing these interconnections without sacrificing too much
    quality.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的事情要注意的是神经元是这个网络的基本单元，每个层的神经元都连接到下一层的所有神经元。对于复杂的网络，这些互连的数量激增，我们将探索不牺牲太多质量的不同减少这些互连的方法。
- en: First, let's try to formulate the problem we are trying to solve.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们尝试阐明我们要解决的问题。
- en: The input is a feature vector, *x*, of dimensions *n*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个特征向量*x*，维度为*n*。
- en: We want the neural network to predict values. The predicted values are represented
    by *ý*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望神经网络能够预测值。预测值由*ý*表示。
- en: 'Mathematically, we want to determine, given a particular input, the probability
    that a transaction is fraudulent. In other words, given a particular value of
    *x*, what is the probability that *y* = 1? Mathematically, we can represent this
    as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们想要确定在给定特定输入的情况下，交易是欺诈的概率。换句话说，给定特定的*x*值，*y*=1的概率是多少？从数学上讲，我们可以表示如下：
- en: '![](assets/a68e1e70-b03b-423e-8eb2-b6e57787c0d5.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/a68e1e70-b03b-423e-8eb2-b6e57787c0d5.png)'
- en: Note that x is an *n[x]*-dimensional vector, where *n*[*x*] is the number of
    input variables.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*x*是一个*n[x]*维向量，其中*n[x]*是输入变量的数量。
- en: This neural network has four layers. The layers between the input and the output
    are the hidden layers. The number of neurons in the first  hidden layer is denoted
    by ![](assets/11fb6e2b-3c01-4cb3-b314-0d3f70d33ea8.png). The links between various
    nodes are multiplied by parameters called *weights*. Training a neural network
    is all about finding the right values for the weights.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络有四层。输入和输出之间的层是隐藏层。第一个隐藏层中的神经元数量用![](assets/11fb6e2b-3c01-4cb3-b314-0d3f70d33ea8.png)表示。各个节点之间的连接由称为*权重*的参数相乘。训练神经网络就是找到权重的正确值。
- en: Let's see how we can train a neural network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何训练神经网络。
- en: Training a Neural Network
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: The process of building a neural network using a given dataset is called training
    a neural network*.* Let's look into the anatomy of a typical neural network. When
    we talk about training a neural network, we are talking about calculating the
    best values for the weights. The training is done iteratively by using a set of
    examples in the form of training data. The examples in the training data have
    the expected values of the output for different combinations of input values.
    The training process for neural networks is different from the way traditional
    models are trained (which were discussed in [Chapter 7](e3df232d-9571-4514-a5f1-2789965492e1.xhtml),
    *Traditional Supervised Learning Algorithms*).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用给定数据集构建神经网络的过程称为训练神经网络。让我们来看一下典型神经网络的解剖结构。当我们谈论训练神经网络时，我们谈论计算权重的最佳值。训练是通过使用一组示例（训练数据的形式）进行迭代完成的。训练数据中的示例具有不同输入值组合的输出的预期值。神经网络的训练过程与传统模型的训练方式不同（这些在[第7章](e3df232d-9571-4514-a5f1-2789965492e1.xhtml)中讨论过，*传统监督学习算法*）。
- en: Understanding the Anatomy of a Neural Network
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络的解剖结构
- en: 'Let''s see what a neural network consists of:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看神经网络包括什么：
- en: '**Layers:** Layers are the core building blocks of a neural network. Each layer
    is a data-processing module that acts as a filter. It takes one or more inputs,
    processes it in a certain way, and then produces one or more outputs. Each time
    data passes through a layer, it goes through a processing phase and shows patterns
    that are relevant to the business question we are trying to answer.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层：** 层是神经网络的核心构建模块。每一层都是一个数据处理模块，充当过滤器。它接受一个或多个输入，以某种方式处理它，然后产生一个或多个输出。每当数据通过一层时，它都会经过一个处理阶段，并显示与我们试图回答的业务问题相关的模式。'
- en: '**Loss function:** A loss function provides the feedback signal that is used
    in the various iterations of the learning process. The loss function provides
    the deviation for a single example.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数：** 损失函数提供了在学习过程的各个迭代中使用的反馈信号。损失函数提供了单个示例的偏差。'
- en: '**Cost function:** The cost function is the loss function on a complete set
    of examples.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本函数：** 成本函数是完整示例集上的损失函数。'
- en: '**Optimizer:**  An optimizer determines how the feedback signal provided by
    the loss function will be interpreted.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器：** 优化器确定损失函数提供的反馈信号将如何被解释。'
- en: '**Input data:**  Input data is the data that is used to train the neural network.
    It specifies the target variable.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入数据：** 输入数据是用于训练神经网络的数据。它指定目标变量。'
- en: '**Weights:** The weights are calculated by training the network. Weights roughly
    correspond to the importance of each of the inputs. For example, if a particular
    input is more important than other inputs, after training, it is given a greater
    weight value, acting as a multiplier. Even a weak signal for that important input
    will gather strength from the large weight value (that acts as a multiplier).
    Thus weight ends up turning each of the inputs according to their importance.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重：** 权重是通过训练网络计算的。权重大致对应于每个输入的重要性。例如，如果特定输入比其他输入更重要，在训练后，它将被赋予更大的权重值，充当乘数。即使对于该重要输入的弱信号也将从大的权重值（充当乘数）中获得力量。因此，权重最终会根据它们的重要性转换每个输入。'
- en: '**Activation function:** The values are multiplied by different weights and
    then aggregated. Exactly how they will be aggregated and how their value will
    be interpreted will be determined by the type of the chosen activation function.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数：** 值将由不同的权重相乘然后聚合。它们将如何被聚合以及它们的值将如何被解释将由所选择的激活函数的类型确定。'
- en: Let's now have a look at a very important aspect of neural network training.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看神经网络训练的一个非常重要的方面。
- en: While training neural networks, we take each of the examples one by one. For
    each of the examples, we generate the output using our under-training model. We
    calculate the difference between the expected output and the predicted output.
    For each individual example, this difference is called the **loss**. Collectively,
    the loss across the complete training dataset is called the **cost**. As we keep
    on training the model, we aim to find the right values of weights that will result
    in the smallest loss value. Throughout the training, we keep on adjusting the
    values of the weights until we find the set of values for the weights that results
    in the minimum possible overall cost. Once we reach the minimum cost, we mark
    the model as trained.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，我们逐个示例地进行。对于每个示例，我们使用正在训练的模型生成输出。我们计算期望输出和预测输出之间的差异。对于每个单独的示例，这种差异称为**损失**。在整个训练数据集中，损失被称为**成本**。随着我们不断训练模型，我们的目标是找到导致最小损失值的权重值。在整个训练过程中，我们不断调整权重的值，直到找到导致最小可能总成本的权重值集。一旦达到最小成本，我们标记模型已经训练完成。
- en: Defining Gradient Descent
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义梯度下降
- en: The purpose of training a neural network model is to find the right values for
    weights. We start training a neural network with random or default values for
    the weights. Then, we iteratively use an optimizer algorithm, such as gradient
    descent, to change the weights in such a way that our predictions improve.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络模型的目的是找到权重的正确值。我们开始训练神经网络时，权重使用随机或默认值。然后，我们迭代使用优化算法，例如梯度下降，以改变权重，使我们的预测改善。
- en: The starting point of a gradient descent algorithm is the random values of weights
    that need to be optimized as we iterate through the algorithm. In each of the
    subsequent iterations, the algorithm proceeds by changing the values of the weights
    in such a way that the cost is minimized.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法的起点是需要通过算法迭代优化的随机权重值。在随后的每次迭代中，算法通过以最小化成本的方式改变权重值来进行。
- en: 'The following diagram explains the logic of the gradient descent algorithm:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图解释了梯度下降算法的逻辑：
- en: '![](assets/47b9db15-d8f5-4fa7-b88f-044a27d2cb3c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/47b9db15-d8f5-4fa7-b88f-044a27d2cb3c.png)'
- en: In the preceding diagram, the input is the feature vector **X**. The actual
    value of the target variable is **Y** and the predicted value of the target variable
    is **Y'**. We determine the deviation of the actual value from the predicted values.
    We update the weights and repeat the steps until the cost is minimized.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，输入是特征向量**X**。目标变量的实际值是**Y**，目标变量的预测值是**Y'**。我们确定实际值与预测值的偏差。我们更新权重并重复这些步骤，直到成本最小化。
- en: 'How to vary the weight in each iteration of the algorithm will depend on the
    following two factors:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法的每次迭代中如何改变权重将取决于以下两个因素：
- en: '**Direction:** Which direction to go in to get the minimum of the loss function'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方向：** 要走的方向以获得损失函数的最小值'
- en: '**Learning Rate:**  How big the change should be in the direction we have chosen'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率：** 我们选择的方向中变化应该有多大'
- en: 'A simple iterative process is shown in the following diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了一个简单的迭代过程：
- en: '![](assets/116bf2a0-a40d-45f4-aa58-cd0c7ddec8ba.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/116bf2a0-a40d-45f4-aa58-cd0c7ddec8ba.png)'
- en: The diagram shows how, by varying the weights, gradient descent tries to find
    the minimum cost. The learning rate and chosen direction will determine the next
    point on the graph to explore.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了通过改变权重，梯度下降试图找到最小成本。学习率和选择的方向将决定要探索的图表上的下一个点。
- en: Selecting the right value for the learning rate is important. If the learning
    rate is too small, the problem may take a lot of time to converge. If the learning
    rate is too high, the problem will not converge. In the preceding diagram, the
    dot representing our current solution will keep oscillating between the two opposite
    lines of the graph.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的学习率很重要。如果学习率太小，问题可能需要很长时间才能收敛。如果学习率太高，问题将无法收敛。在前面的图中，代表当前解决方案的点将在图表的两条相反线之间不断振荡。
- en: 'Now, let''s see how to minimize a gradient. Consider only two variables, *x*
    and *y*. The gradient of *x* and *y* is calculated as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何最小化梯度。只考虑两个变量，*x*和*y*。*x*和*y*的梯度计算如下：
- en: '![](assets/9c045c39-cac5-4e9d-976d-7e77da01c018.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9c045c39-cac5-4e9d-976d-7e77da01c018.png)'
- en: 'To minimize the gradient, the following approach can be used:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化梯度，可以使用以下方法：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This algorithm can also be used to find the optimal or near-optimal values of
    weights for a neural network.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法也可以用于找到神经网络的最优或接近最优的权重值。
- en: Note that the calculation of gradient descent proceeds backward throughout the
    network. We start by calculating the gradient of the final layer first, and then
    the second-to-last one, and then the one before that, until we reach the first
    layer. This is called backpropagation, which was introduced by Hinton, Williams,
    and Rumelhart in 1985\.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，梯度下降的计算是在整个网络中向后进行的。我们首先计算最后一层的梯度，然后是倒数第二层，然后是倒数第二层之前的层，直到达到第一层。这被称为反向传播，由Hinton、Williams和Rumelhart于1985年引入。
- en: Next, let's look into activation functions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看激活函数。
- en: Activation Functions
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: An activation function formulates how the inputs to a particular neuron will
    be processed to generate an output.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数规定了特定神经元的输入如何被处理以生成输出。
- en: 'As shown in the following diagram, each of the neurons in a neural network
    has an activation function that determines how inputs will be processed:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，神经网络中的每个神经元都有一个激活函数，确定输入将如何被处理：
- en: '![](assets/536fd811-bb85-40d2-9ac6-28e8cf18bc20.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/536fd811-bb85-40d2-9ac6-28e8cf18bc20.png)'
- en: In the preceding diagram, we can see that the results generated by an activation
    function are passed on to the output. The activation function sets the criteria
    that how the values of the inputs are supposed to be interpreted to generate an
    output.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到激活函数生成的结果传递到输出。激活函数设置了如何解释输入值以生成输出的标准。
- en: For exactly the same input values, different activation functions will produce
    different outputs. Understanding how to select the right activation function is
    important when using neural networks to solve problems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完全相同的输入值，不同的激活函数将产生不同的输出。在使用神经网络解决问题时，了解如何选择正确的激活函数是很重要的。
- en: Let's now look into these activation functions one by one.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们逐个查看这些激活函数。
- en: Threshold Function
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阈值函数
- en: 'The simplest possible activation function is the threshold function. The output
    of the threshold function is binary: 0 or 1\. It will generate 1 as the output
    if any of the input is greater than 1\. This can be explained in the following
    diagram:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的激活函数是阈值函数。阈值函数的输出是二进制的：0或1。如果任何输入大于1，它将生成1作为输出。这可以在下图中解释：
- en: '![](assets/3b653d1d-c86d-45e7-8d19-366b8fb9dd19.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3b653d1d-c86d-45e7-8d19-366b8fb9dd19.png)'
- en: Note that as soon as there are any signs of life detected in the weighted sums
    of inputs, the output (*y*) becomes 1\. This makes the threshold activation function
    very sensitive. It is quite vulnerable to being wrongly triggered by the slightest
    signal in the input due to a glitch or some noise.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一旦检测到输入加权和中有任何生命迹象，输出（*y*）就变为1。这使得阈值激活函数非常敏感。它很容易被输入中的最轻微的信号或噪音错误触发。
- en: Sigmoid
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'The sigmoid function can be thought of as an improvement of the threshold function.
    Here, we have control over the sensitivity of the activation function:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数可以被认为是阈值函数的改进。在这里，我们可以控制激活函数的灵敏度。
- en: '![](assets/581c93c0-8b11-4abf-8e29-2923349c1a40.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/581c93c0-8b11-4abf-8e29-2923349c1a40.png)'
- en: 'The sigmoid function, *y*, is defined as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数*y*定义如下：
- en: '![](assets/cf23e5ef-72a7-42de-a05e-71c0595f48bb.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/cf23e5ef-72a7-42de-a05e-71c0595f48bb.png)'
- en: 'It can be implemented in Python as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在Python中实现如下：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that by reducing the sensitivity of the activation function, we make glitches
    in the input  less disruptive. Note that the output of the sigmoid activation
    function is still binary, that is, 0 or 1.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过降低激活函数的灵敏度，我们可以减少输入中的故障的影响。请注意，Sigmoid激活函数的输出仍然是二进制的，即0或1。
- en: Rectified linear unit (ReLU)
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）
- en: The output for the first two activation functions presented in this chapter
    was binary. That means that they will take a set of input variables and convert
    them into binary outputs. ReLU is an activation function that takes a set of input
    variables as input and converts them into a single continuous output. In neural
    networks, ReLU is the most popular activation function and is usually used in
    the hidden layers, where we do not want to convert continuous  variables into
    category variables.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的前两个激活函数的输出是二进制的。这意味着它们将取一组输入变量并将它们转换为二进制输出。ReLU是一个激活函数，它将一组输入变量作为输入，并将它们转换为单一的连续输出。在神经网络中，ReLU是最流行的激活函数，通常用于隐藏层，我们不希望将连续变量转换为类别变量。
- en: 'The following diagram summarizes the ReLU activation function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表总结了ReLU激活函数：
- en: '![](assets/f99737ce-4ece-44ae-afc6-35e51f95a0e6.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f99737ce-4ece-44ae-afc6-35e51f95a0e6.png)'
- en: 'Note that when *x≤ 0*, that means *y = 0*. This means that any signal from
    the input that is zero or less than zero is translated into a zero output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当*x≤ 0*时，*y = 0*。这意味着输入中的任何信号为零或小于零都被转换为零输出：
- en: '![](assets/c36cfe68-1975-4d9b-98f9-1e404b75399e.png) for ![](assets/9ab31cd7-0b6a-4412-b32f-72ec6a44227f.png)![](assets/ecea8529-3f94-4351-84bb-3841d873a580.png)
    for ![](assets/0b063a5e-03ee-44f1-8a19-664c6536e069.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/c36cfe68-1975-4d9b-98f9-1e404b75399e.png) for ![](assets/9ab31cd7-0b6a-4412-b32f-72ec6a44227f.png)![](assets/ecea8529-3f94-4351-84bb-3841d873a580.png)
    for ![](assets/0b063a5e-03ee-44f1-8a19-664c6536e069.png)'
- en: As soon as *x* becomes more than zero, it is *x*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦*x*大于零，它就是*x*。
- en: 'The ReLU function is one of the most used activation functions in neural networks.
    It can be implemented in Python as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数是神经网络中最常用的激活函数之一。它可以在Python中实现如下：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now let's look into Leaky ReLU, which is based on ReLU.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看Leaky ReLU，它是基于ReLU的。
- en: Leaky ReLU
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: 'In ReLU, a negative value for *x* results in a zero value for *y*. It means
    that some information is lost in the process, which makes training cycles longer,
    especially at the start of training. The Leaky ReLU activation function resolves
    this issue. The following applies for Leaky ReLu:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在ReLU中，*x*的负值导致*y*的值为零。这意味着在这个过程中丢失了一些信息，这使得训练周期变得更长，特别是在训练开始时。Leaky ReLU激活函数解决了这个问题。对于Leaky
    ReLu，以下内容适用：
- en: '![](assets/10f3d67b-45d7-44b6-8455-576d8bae3079.png) ; for ![](assets/2fd2b88a-3be6-49d4-ae6b-fbb781c0c34e.png)![](assets/6aab91fb-9ecf-489a-a7d1-f74144c4dcb0.png)
    for![](assets/01238af1-5c97-438f-9c4d-6eeb770fc177.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/10f3d67b-45d7-44b6-8455-576d8bae3079.png) ; for ![](assets/2fd2b88a-3be6-49d4-ae6b-fbb781c0c34e.png)![](assets/6aab91fb-9ecf-489a-a7d1-f74144c4dcb0.png)
    for![](assets/01238af1-5c97-438f-9c4d-6eeb770fc177.png)'
- en: 'This is shown in the following diagram:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了这一点：
- en: '![](assets/75ed9e52-7fbb-4b34-9e63-3bd0fff166ef.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/75ed9e52-7fbb-4b34-9e63-3bd0fff166ef.png)'
- en: Here, *ß* is a parameter with a value less than one.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，ß是一个小于一的参数。
- en: 'It can be implemented in Python as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以在Python中实现如下：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There are three ways of specifying the value for ß:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种指定ß值的方法：
- en: We can specify a default value of ß.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以指定一个默认值为ß。
- en: We can make ß a parameter in our neural network and we can let the neural network
    decide the value (this is called **parametric ReLU**).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在我们的神经网络中将ß设为一个参数，并让神经网络决定该值（这称为**参数化ReLU**）。
- en: We can make ß a random value (this is called **randomized ReLU**).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将ß设为一个随机值（这称为**随机化ReLU**）。
- en: Hyperbolic tangent (tanh)
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双曲正切（tanh）
- en: 'The tanh function is similar to the sigmoid function, but it has the ability
    to give a negative signal as well. The following diagram illustrates this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: tanh函数类似于Sigmoid函数，但它也能给出负信号。下图说明了这一点：
- en: '![](assets/f43bf042-f802-41dd-8990-e192757ad7c5.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f43bf042-f802-41dd-8990-e192757ad7c5.png)'
- en: 'The *y* function is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*函数如下：'
- en: '![](assets/88736b33-6ea8-4e3e-ab82-887e5f487fe8.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/88736b33-6ea8-4e3e-ab82-887e5f487fe8.png)'
- en: 'It can be implemented by the following Python code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下Python代码实现：
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now let's look at the softmax function.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看softmax函数。
- en: Softmax
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax
- en: 'Sometimes we need more than two levels for the output of the activation function.
    Softmax is an activation function that provides us with more than two levels for
    the output. It is best suited to multiclass classification problems.  Let''s assume
    that we have *n* classes. We have input values. The input values map the classes
    as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要激活函数的输出不止两个级别。Softmax是一个可以为输出提供不止两个级别的激活函数。它最适用于多类分类问题。假设我们有*n*个类。我们有输入值。输入值将类映射如下：
- en: '*x = {x^((1)),x^((2)),....x^((n))}*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*x = {x^((1)),x^((2)),....x^((n))}*'
- en: 'Softmax operates on probability theory. The output probability of the *e*^(th)
    class of the softmax is calculated as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax是基于概率理论的。Softmax的第*e*类的输出概率计算如下：
- en: '![](assets/bb2a0fea-2795-46ca-8749-aafbd462c707.png)For binary classifiers,
    the activation function in the final layer will be sigmoid, and for multiclass
    classifiers it will be softmax.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/bb2a0fea-2795-46ca-8749-aafbd462c707.png)对于二元分类器，最终层的激活函数将是Sigmoid，对于多类分类器，它将是Softmax。'
- en: Tools and Frameworks
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具和框架
- en: In this section, we will look at the frameworks and tools available for implementing
    neural networks in detail.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细了解用于实现神经网络的框架和工具。
- en: Over time, many different frameworks have been developed to implement neural
    networks. Different frameworks have their own strengths and weaknesses. In this
    section, we will focus on Keras with a TensorFlow backend.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，许多不同的框架已经被开发出来来实现神经网络。不同的框架有各自的优势和劣势。在本节中，我们将重点关注具有TensorFlow后端的Keras。
- en: Keras
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras
- en: Keras is one of the most popular and easy-to-use neural network libraries and
    is written in Python. It was written with ease of use in mind and provides the
    fastest way to implement deep learning. Keras only provides high-level blocks
    and is considered at the model level.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是最受欢迎和易于使用的神经网络库之一，用Python编写。它考虑到易用性，并提供了实现深度学习的最快方式。Keras只提供高级模块，并被认为是在模型级别上。
- en: Backend Engines of Keras
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras的后端引擎
- en: 'Keras needs a lower-level deep learning library to perform tensor-level manipulations.
    This lower level deep-learning library is called the *backend engine*. Possible
    backend engines for Keras include the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Keras需要一个低级别的深度学习库来执行张量级别的操作。这个低级别的深度学习库称为*后端引擎*。Keras的可能后端引擎包括以下内容：
- en: '**TensorFlow** ([www.tensorflow.org](https://www.tensorflow.org/)):This is
    the most popular framework of its kind and is open sourced by Google.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow** ([www.tensorflow.org](https://www.tensorflow.org/))：这是同类框架中最受欢迎的框架，由谷歌开源。'
- en: '**Theona**  ([deeplearning.net/software/theano](http://deeplearning.net/software/theano)):
    This was developed at the MILA lab at Université de Montréal.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Theona** ([deeplearning.net/software/theano](http://deeplearning.net/software/theano))：这是在蒙特利尔大学MILA实验室开发的。'
- en: '**Microsoft Cognitive Toolkit** (**CNTK**): This was developed by Microsoft.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Microsoft Cognitive Toolkit**（**CNTK**）：这是由微软开发的。'
- en: 'The format of this modular deep learning technology stack is shown in the following
    diagram:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模块化深度学习技术堆栈的格式如下图所示：
- en: '![](assets/d57a079b-0263-474a-9c6f-a419ed2ecb88.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d57a079b-0263-474a-9c6f-a419ed2ecb88.png)'
- en: The advantage of this modular deep learning architecture is that the backend
    of Keras can be changed without rewriting any code. For example, if we find TensorFlow
    better than Theona for a particular task, we can simply change the backend to
    TensorFlow without rewriting any code.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模块化的深度学习架构的优势在于，Keras的后端可以在不重写任何代码的情况下进行更改。例如，如果我们发现对于特定任务，TensorFlow比Theona更好，我们可以简单地将后端更改为TensorFlow，而无需重写任何代码。
- en: Low-level layers of the deep learning stack
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习堆栈的低级层
- en: The three backend engines we just mentioned can all run both on CPUs and GPUs
    using the low-level layers of the stack. For CPUs, a low-level library of tensor
    operations called **Eigen** is used. For GPUs, TensorFlow uses NVIDIA's **CUDA
    Deep Neural Network** (**cuDNN**) library.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚提到的三种后端引擎都可以在CPU和GPU上运行，使用堆栈的低级层。对于CPU，使用了一个名为**Eigen**的张量操作低级库。对于GPU，TensorFlow使用了NVIDIA的**CUDA深度神经网络**（**cuDNN**）库。
- en: Defining hyperparameters
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义超参数
- en: 'As discussed in [Chapter 6](ce14ecc1-8ad5-406e-88d0-7f3acb3e4569.xhtml), *Unsupervised
    Machine Learning Algorithms*, a hyperparameter is a parameter whose value is chosen
    before the learning process starts. We start with common-sense values and then
    try to optimize them later. For neural networks, the important hyperparameters
    are these:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第6章](ce14ecc1-8ad5-406e-88d0-7f3acb3e4569.xhtml)中所讨论的*无监督机器学习算法*，超参数是在学习过程开始之前选择其值的参数。我们从常识值开始，然后尝试稍后优化它们。对于神经网络，重要的超参数包括：
- en: The activation function
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: The learning rate
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: The number of hidden layers
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的数量
- en: The number of neurons in each hidden layer
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层中的神经元数量
- en: Let's look into how we can define a model using Keras.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Keras定义模型。
- en: Defining a Keras model
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义Keras模型
- en: 'There are three steps involved in defining a complete Keras model:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 定义完整的Keras模型涉及三个步骤：
- en: '**Define the layers**.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义层**。'
- en: 'We can build a model using Keras in two possible ways:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用两种可能的方式使用Keras构建模型：
- en: '**The** **Sequential API:**  This allows us to architect models for a linear
    stack of layers. It is used for relatively simple models and is the usual choice
    for building models:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sequential API：**这允许我们为一系列层构建模型。它用于相对简单的模型，并且通常是构建模型的常规选择：'
- en: '![](assets/d836d917-46b6-4525-b030-bd434e2ee4cd.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d836d917-46b6-4525-b030-bd434e2ee4cd.png)'
- en: Note that, here, we have created three layers – the first two layers have the
    ReLU activation function and the third layer has softmax as the activation function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这里，我们创建了三层 - 前两层具有ReLU激活函数，第三层具有softmax作为激活函数。
- en: '**The Functional API:** This allows us to architect models for acyclic graphs
    of layers. More complex models can be created using the Functional API.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Functional API：**这允许我们为层的非循环图形构建模型。使用Functional API可以创建更复杂的模型。'
- en: '![](assets/7caf80b6-fee4-4173-be0d-5a6efd584932.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/7caf80b6-fee4-4173-be0d-5a6efd584932.png)'
- en: Note that we can define the same neural network using both the Sequential and
    Functional APIs. From the point of view of performance, it does not make any difference
    which approach you take to define the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以使用顺序和功能API来定义相同的神经网络。从性能的角度来看，使用哪种方法来定义模型并没有任何区别。
- en: '**Define the learning process**.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义学习过程**。'
- en: 'In this step, we define three things:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们定义了三件事：
- en: The optimizer
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器
- en: The loss function
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The metrics that will quantify the quality of the model:'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将量化模型质量的指标：
- en: '![](assets/c540f046-60b4-4740-8d6a-f164581aed1e.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/c540f046-60b4-4740-8d6a-f164581aed1e.png)'
- en: Note that we use the `model.compile` function to define the optimizer, loss
    function, and metrics.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`model.compile`函数来定义优化器、损失函数和指标。
- en: '**Train the model**.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练模型**。'
- en: 'Once the architecture is defined, it is time to train the model:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了架构，就是训练模型的时候了：
- en: '![](assets/96bff123-20b0-49ed-9739-c22f3e8ddff5.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/96bff123-20b0-49ed-9739-c22f3e8ddff5.png)'
- en: Note that parameters such as `batch_size` and `epochs` are configurable parameters,
    making them hyperparameters.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`batch_size`和`epochs`等参数是可配置的参数，使它们成为超参数。
- en: Choosing sequential or functional model
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择顺序或功能模型
- en: The sequential model creates the ANN as a simple stack of layer. The sequential
    model is easy and straightforward to understand and implement, but its simplistic
    architecture also has a major restriction. Each layer is connected to exactly
    one input and output tensor. This means that if our model has multiple inputs
    or multiple outputs either at the input or output or at any of the hidden layers,
    then we cannot use a sequential model. In this case, we will have to use the functional
    model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序模型将 ANN 创建为简单的层堆叠。顺序模型易于理解和实现，但其简单的架构也有一个主要限制。每一层只连接到一个输入和输出张量。这意味着如果我们的模型在任何隐藏层的输入或输出处有多个输入或多个输出，那么我们不能使用顺序模型。在这种情况下，我们将不得不使用功能模型。
- en: Understanding TensorFlow
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 TensorFlow
- en: TensorFlow is one of the most popular libraries for working with neural networks.
    In the preceding section, we saw how we can use it as the backend engine of Keras.
    It is an open source, high-performance library that can actually be used for any
    numerical computation. If we look at the stack, we can see that we can write TensorFlow
    code in a high-level language such as Python or C++, which gets interpreted by
    the TensorFlow distributed execution engine. This makes it quite useful for and
    popular with developers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是处理神经网络的最流行的库之一。在前面的部分中，我们看到了如何将其用作 Keras 的后端引擎。它是一个开源的高性能库，实际上可以用于任何数值计算。如果我们看一下堆栈，我们可以看到我们可以用高级语言（如
    Python 或 C++）编写 TensorFlow 代码，然后由 TensorFlow 分布式执行引擎解释。这使得它对开发人员非常有用和受欢迎。
- en: The way TensorFlow works is that you create a **Directed Graph** (**DG**) to
    represent your computation.  Connecting the nodes are the edges, the input, and
    the output of the mathematical operations. Also, they represent arrays of data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的工作方式是创建一个**有向图**（**DG**）来表示您的计算。连接节点的是边，数学运算的输入和输出。它们也代表数据的数组。
- en: Presenting TensorFlow's Basic Concepts
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 TensorFlow 的基本概念
- en: 'Let''s take a brief look at TensorFlow concepts such as scalars, vectors, and
    matrices. We know that a simple number, such as three or five, is called a **scalar**
    in traditional mathematics. Moreover, in physics, a **vector** is something with
    magnitude and direction. In terms of TensorFlow, we use a vector to mean one-dimensional
    arrays. Extending this concept, a two-dimensional array is a **matrix**. For a
    three-dimensional array, we use the term **3D tensor**. We use the term **rank**
    to capture the dimensionality of a data structure. As such, a **scalar** is a
    **rank 0** data structure, a **vector** is a **rank** **1** data structure, and
    a **matrix** is a **rank** **2** data structure. These multi-dimensional structures
    are known as **tensors** and are shown in the following diagram:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要了解一下 TensorFlow 的概念，比如标量、向量和矩阵。我们知道，在传统数学中，简单的数字，比如三或五，被称为**标量**。此外，在物理学中，**向量**是具有大小和方向的东西。在
    TensorFlow 中，我们使用向量来表示一维数组。延伸这个概念，二维数组被称为**矩阵**。对于三维数组，我们使用术语**3D张量**。我们使用术语**等级**来捕捉数据结构的维度。因此，**标量**是一个**等级0**的数据结构，**向量**是一个**等级1**的数据结构，**矩阵**是一个**等级2**的数据结构。这些多维结构被称为**张量**，并在下图中显示：
- en: '![](assets/1eceea7d-8244-4156-90ee-d4b739fe0fe1.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1eceea7d-8244-4156-90ee-d4b739fe0fe1.png)'
- en: As we can see in the preceding diagram, the rank defines the dimensionality
    of a tensor.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们可以看到等级定义了张量的维度。
- en: 'Let''s now look at another parameter, `shape`. `shape` is a tuple of integers
    specifying the length of an array in each dimension. The following diagram explains
    the concept of `shape`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看另一个参数，`shape`。`shape`是一个整数元组，指定每个维度中数组的长度。下图解释了`shape`的概念：
- en: '![](assets/909e6fb6-e3be-42fb-aa53-48010ffe319a.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/909e6fb6-e3be-42fb-aa53-48010ffe319a.png)'
- en: Using `shape` and rank, we can specify the details of tensors.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`shape`和等级，我们可以指定张量的详细信息。
- en: Understanding Tensor Mathematics
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解张量数学
- en: 'Let''s now look at different mathematical computations using tensors:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看使用张量进行不同的数学计算：
- en: 'Let''s define two scalars and try to add and multiply them using TensorFlow:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们定义两个标量，并尝试使用 TensorFlow 进行加法和乘法：
- en: '![](assets/5a659402-0713-4684-9f2a-82b06e36c24c.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/5a659402-0713-4684-9f2a-82b06e36c24c.png)'
- en: 'We can add and multiply them and display the results:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将它们相加和相乘，并显示结果：
- en: '![](assets/ced54688-183d-4fb6-b225-0363341a3a76.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ced54688-183d-4fb6-b225-0363341a3a76.png)'
- en: 'We can also create a new scalar tensor by adding the two tensors:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以通过将两个张量相加来创建一个新的标量张量：
- en: '![](assets/fcdc1e92-3893-494a-ba31-d64e09f19618.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/fcdc1e92-3893-494a-ba31-d64e09f19618.png)'
- en: 'We can also perform complex tensor functions:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以执行复杂的张量函数：
- en: '![](assets/d8ad73e8-25c0-4028-b542-2da93252e716.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d8ad73e8-25c0-4028-b542-2da93252e716.png)'
- en: Understanding the Types of Neural Networks
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络的类型
- en: There is more than one way that neural networks can be built. If every neuron
    in each layer is connected to each of the neurons in another layer, then we call
    it a dense or fully connected neural network. Let's look at some other forms of
    neural networks.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以有多种构建方式。如果每一层中的每个神经元都连接到另一层中的每个神经元，那么我们称之为密集或全连接神经网络。让我们看看一些其他形式的神经网络。
- en: Convolutional Neural Networks
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '**Convolution Neural Networks** (**CNNs**) are typically used to analyze multimedia
    data. In order to learn more about how a CNN is used to analyze image-based data,
    we need to have a grasp of the following processes:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）通常用于分析多媒体数据。为了更多地了解 CNN 如何用于分析基于图像的数据，我们需要掌握以下过程：'
- en: Convolution
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积
- en: Pooling
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化
- en: Let's explore them one by one.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一探索它们。
- en: Convolution
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: The process of convolution emphasizes a pattern of interest in a particular
    image by processing it with another smaller image called a **filter** (also called
    a **kernel**). For example, if we want to find the edges of objects in an image,
    we can convolve the image with a particular filter to get them. Edge detection
    can help us in object detection, object classification, and other applications.
    So, the process of convolution is about finding characteristics and features in
    an image.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的过程通过使用另一个较小的图像（也称为**过滤器**或**核**）来处理特定图像中感兴趣的模式。例如，如果我们想要在图像中找到物体的边缘，我们可以使用特定的过滤器对图像进行卷积来得到它们。边缘检测可以帮助我们进行物体检测、物体分类和其他应用。因此，卷积的过程是关于在图像中找到特征和特点。
- en: The approach to finding patterns is based on finding patterns that can be reused
    on different data. The reusable patterns are called filters or kernels.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找模式的方法是基于寻找可以在不同数据上重复使用的模式。可重复使用的模式称为过滤器或核。
- en: Pooling
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化
- en: 'An important part of processing multimedia data for the purpose of machine
    learning is downsampling it. This provides two benefits:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行机器学习的多媒体数据处理的重要部分是对其进行下采样。这提供了两个好处：
- en: It reduces the overall dimensionality of the problem, decreasing the time needed
    to train the model in a major way.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了问题的整体维度，大大减少了训练模型所需的时间。
- en: Through aggregation, we abstract the unnecessary details in the multimedia data,
    making it more generic and more representative of similar problems.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过聚合，我们可以提取多媒体数据中不必要的细节，使其更通用并更具代表性。
- en: 'Downsampling is performed as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样的执行如下：
- en: '![](assets/e0c416f9-1cf3-4056-9e3d-26d427784c53.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e0c416f9-1cf3-4056-9e3d-26d427784c53.png)'
- en: Note that we have replaced every block of four pixels with one pixel, choosing
    the highest value of the four pixels to be the value of that one pixel. This means
    that we have downsampled by a factor of four. As we have chosen the maximum value
    in each block, this process is called **max** **pooling**. We could have chosen
    the average value; in that case, it would be average pooling.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经用一个像素替换了每个四个像素的块，选择了四个像素中的最高值作为该像素的值。这意味着我们已经按四分之一的比例进行了下采样。由于我们选择了每个块中的最大值，这个过程被称为**最大池化**。我们也可以选择平均值；在那种情况下，它将是平均池化。
- en: Recurrent Neural Networks
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: '**Recurrent Neural Networks** (**RNNs**) are a special type of neural network
    that are based on looped architecture. This is why they are called *recurrent*.
    The important thing to note is that RNNs have memory. This means that they have
    the ability to store information from recent iterations. They are used in areas
    such as analyzing sentence structures to predict the next word in a sentence.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）是一种特殊类型的神经网络，它们基于循环架构。这就是为什么它们被称为*循环*。需要注意的重要事情是RNNs具有记忆。这意味着它们有能力存储最近迭代的信息。它们被用于分析句子结构以预测句子中的下一个单词等领域。'
- en: Generative Adversarial Networks
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: '**Generative Adversarial Networks** (**GANs**) are a type of neural network
    that generate synthetic data. They were created in 2014 by Ian Goodfellow and
    his colleagues. They can be used to generate photographs of people that have never
    existed. More importantly, they are used to generate synthetic data to augment
    training datasets.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）是一种生成合成数据的神经网络类型。它们是由Ian Goodfellow及其同事于2014年创建的。它们可以用来生成从未存在过的人的照片。更重要的是，它们用于生成合成数据以增加训练数据集。'
- en: In the upcoming section, we will see what transfer learning is.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看到什么是迁移学习。
- en: Transfer Learning
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: 'Over the years, many organizations, research groups, and individuals within
    the open source community have perfected some complex models trained using gigantic
    amounts of data for generic use cases. In some cases, they have invested years
    of effort in optimizing these models. Some of these open source models can be
    used for the following applications:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，许多组织、研究团体和开源社区内的个人已经完善了一些使用大量数据进行训练的复杂模型，以供通用用途。在某些情况下，他们已经投入了多年的努力来优化这些模型。一些这些开源模型可以用于以下应用：
- en: Object detection in video
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频中的物体检测
- en: Object detection in images
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中的物体检测
- en: Transcription for audio
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频的转录
- en: Sentiment analysis for text
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本的情感分析
- en: 'Whenever we start working on training a new machine learning model, the question
    to ask ourselves is this: instead of starting from scratch, can we simply customize
    a well-established pre-trained model for our purposes? In other words, can we
    transfer the learning of existing models to our custom model so that we can answer
    our business question? If we can do that, it will provide three benefits:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们开始训练一个新的机器学习模型时，我们要问自己的问题是：我们是否可以简单地定制一个经过充分验证的预训练模型，而不是从头开始？换句话说，我们是否可以将现有模型的学习迁移到我们的自定义模型，以便回答我们的业务问题？如果我们能做到这一点，它将提供三个好处：
- en: Our model training efforts will be given a jump-start.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的模型训练工作将得到一个快速启动。
- en: By using a well-tested and well-established model, the overall quality of our
    model is likely to improve.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用经过充分测试和建立的模型，我们的模型整体质量可能会得到提高。
- en: If we do not have enough data for the problem we are working on, using a pre-trained
    model through transfer learning may help.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们没有足够的数据来解决我们正在处理的问题，使用通过迁移学习的预训练模型可能会有所帮助。
- en: 'Let''s look into two actual examples where this would be useful:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看两个实际例子，这将是有用的：
- en: When training a robot, we could first use a simulation game to train a neural
    network model. In that simulation, we could create all those rare events that
    are very hard to find in the real world. Once trained, we could use transfer learning
    ;to train the model for the real world.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练机器人时，我们可以首先使用模拟游戏来训练神经网络模型。在那个模拟中，我们可以创建所有那些在现实世界中很难找到的罕见事件。一旦训练完成，我们可以使用迁移学习来训练模型适用于真实世界。
- en: Let's assume that we want to train a model that can classify Apple And Windows
    laptops from a video feed. There are already well-established object detection
    models available as open source that can accurately classify various objects in
    a video feed. We can use these models as a starting point and identify objects
    as laptops. Once we have identified the objects as laptops, we can further train
    the model to differentiate between Apple and Windows laptops.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们想要训练一个模型，可以从视频源中分类苹果和Windows笔记本电脑。已经有成熟的开源目标检测模型可以准确分类视频源中的各种物体。我们可以使用这些模型作为起点，识别笔记本电脑。一旦我们识别出物体是笔记本电脑，我们可以进一步训练模型区分苹果和Windows笔记本电脑。
- en: In the next section, we will apply the concepts that we have covered in this
    chapter to building a fraudulent document classifying neural network.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将应用本章涵盖的概念来构建一个欺诈文档分类神经网络。
- en: Case study – using deep learning for fraud detection
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究-使用深度学习进行欺诈检测
- en: Using **Machine Learning** (**ML**) techniques to identify fraudulent documents
    is an active and challenging field of research. Researchers are investigating
    to what extent the pattern recognition power of neural networks can be exploited
    for this purpose. Instead of manual attribute extractors, raw pixels can be used
    for several deep learning architectural structures.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**机器学习**（**ML**）技术识别欺诈文档是一个活跃且具有挑战性的研究领域。研究人员正在调查神经网络的模式识别能力在多大程度上可以用于这个目的。可以使用原始像素而不是手动属性提取器，用于几种深度学习架构结构。
- en: Methodology
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论
- en: 'The technique presented in this section uses a type of neural network architecture
    called **Siamese neural networks**, which features two branches that share identical
    architectures and parameters. The use of Siamese neural networks to flag fraudulent
    documents is shown in the following diagram:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍的技术使用了一种称为**Siamese神经网络**的神经网络架构，它具有两个共享相同架构和参数的分支。使用Siamese神经网络来标记欺诈文档如下图所示：
- en: '![](assets/46b8db83-bcc9-4c6a-9bbd-bb19de3a84ac.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/46b8db83-bcc9-4c6a-9bbd-bb19de3a84ac.png)'
- en: When a particular document needs to be verified for authenticity, we first classify
    the document based on its layout and type, and then we compare it against its
    expected template and pattern. If it deviates beyond a certain threshold, it is
    flagged as a fake document; otherwise, it is considered an authentic or true document.
    For critical use cases, we can add a manual process for borderline cases where
    the algorithm cannot conclusively classify a document as authentic or fake.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要验证特定文档的真实性时，我们首先基于其布局和类型对文档进行分类，然后将其与预期的模板和模式进行比较。如果偏离超过一定阈值，它被标记为伪造文档；否则，它被视为真实文档。对于关键用例，我们可以添加一个手动流程，用于边界情况，算法无法确定地将文档分类为真实或伪造。
- en: To compare a document against its expected template, we use two identical CNNs
    in our Siamese architecture. CNNs have the advantage of learning optimal shift-invariant
    local feature detectors and can build representations that are robust to geometric
    distortions of the input image. This is well suited to our problem since we aim
    to pass authentic and test documents through a single network, and then compare
    their outcomes for similarity. To achieve this goal, we implement the following
    steps.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较文档与其预期模板，我们在Siamese架构中使用两个相同的CNN。CNN具有学习最佳的平移不变局部特征检测器和可以构建对输入图像的几何失真具有鲁棒性的表示的优势。这非常适合我们的问题，因为我们的目标是通过单个网络传递真实和测试文档，然后比较它们的相似性。为了实现这个目标，我们实施以下步骤。
- en: 'Let''s assume that we want to test a document. For each class of document,
    we perform the following steps:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要测试一个文档。对于每类文档，我们执行以下步骤：
- en: Get the stored image of the authentic document. We call it the **true document**.
    The test document should look like the true document.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取真实文档的存储图像。我们称之为**真实文档**。测试文档应该看起来像真实文档。
- en: The true document is passed through the neural network layers to create a feature
    vector, which is the mathematical representation of the patterns of the true document.
    We call it F**eature Vector 1**, as shown in the preceding diagram.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真实文档通过神经网络层，创建一个特征向量，这是真实文档模式的数学表示。我们称之为**特征向量1**，如前图所示。
- en: The document that needs to be tested is called the **test document**. We pass
    this document through a neural network similar to the one that was used to create
    the feature vector for the true document. The feature vector of the test document
    is called F**eature Vector 2**.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要测试的文档称为**测试文档**。我们通过一个类似于用于创建真实文档特征向量的网络来传递这个文档。测试文档的特征向量称为**特征向量2**。
- en: We use the Euclidean distance between feature vector 1 and feature vector 2
    to calculate the similarity score between the true document and the test document.
    This similarity score is called the **Measure Of Similarity** (**MOS**). The MOS
    is a number between 0 and 1\. A higher number represents a lower distance between
    the documents and a greater likelihood that the documents are similar.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用特征向量1和特征向量2之间的欧氏距离来计算真实文档和测试文档之间的相似度分数。这个相似度分数被称为**相似度测量**（**MOS**）。MOS是0到1之间的数字。较高的数字代表文档之间的距离较小，文档相似的可能性较大。
- en: If the similarity score calculated by the neural network is below a pre-defined
    threshold, we flag the document as fraudulent.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果神经网络计算的相似度分数低于预定义的阈值，我们将文档标记为欺诈。
- en: 'Let''s see  how we can implement Siamese neural networks using Python:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Python实现Siamese神经网络：
- en: 'First, let''s import the Python packages that are required:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入所需的Python包：
- en: '[PRE5]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we will define the neural network that will be used to process each of
    the branches of the Siamese network:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义将用于处理Siamese网络各个分支的神经网络：
- en: '[PRE6]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that, in order to reduce overfitting, we have also specified  a dropout
    rate of `0.15`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了减少过拟合，我们还指定了`0.15`的丢失率。
- en: 'To implement Siamese networks, we will use MNIST images. MNIST images are ideal
    for testing the effectiveness of our approach. Our approach entails preparing
    the data in such a way that each sample will have two images and a binary similarity
    flag. This flag is an indicator that they are from the same class. Let''s now
    implement the function named  `prepareData`, which can prepare the data for us:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了实现Siamese网络，我们将使用MNIST图像。MNIST图像非常适合测试我们的方法的有效性。我们的方法包括以每个样本包含两个图像和一个二进制相似度标志的方式准备数据。这个标志是它们来自相同类别的指示器。现在让我们实现名为`prepareData`的函数，它可以为我们准备数据：
- en: '[PRE7]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that  `prepareData()`  will result in an equal number of samples across
    all digits.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`prepareData()`将导致所有数字的样本数量相等。
- en: 'We will now prepare the training and testing datasets:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将准备训练和测试数据集：
- en: '[PRE8]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s create the two halves of the Siamese system:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建Siamese系统的两个部分：
- en: '[PRE9]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we will implement the  **measure of similarity**  (**MOS**), which will
    quantify the distance between two documents that we want to compare:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将实现**相似度度量**（MOS），它将量化我们想要比较的两个文档之间的距离：
- en: '[PRE10]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s train the model. We will use 10 epochs to train this model:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练模型。我们将使用10个epochs来训练这个模型：
- en: '![](assets/df9a4124-9b34-4f4e-86ad-464cdf028948.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/df9a4124-9b34-4f4e-86ad-464cdf028948.png)'
- en: Note that we reached an accuracy of 97.49% using 10 epochs. Increasing the number
    of epochs will further improve the level of accuracy.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用10个epochs达到了97.49%的准确率。增加epochs的数量将进一步提高准确度水平。
- en: Summary
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we first looked at the details of neural networks. We started
    by looking at how neural networks have evolved over the years. We studied different
    types of neural networks. Then, we looked at the various building blocks of neural
    networks. We studied in depth the gradient descent algorithm, which is used to
    train neural networks. We discussed various activation functions and studied the
    applications of activation functions in a neural network. We also looked at the
    concept of transfer learning. Finally, we looked at a practical example of how
    a neural network can be used to train a machine learning model that can be deployed
    to flag forged or fraudulent documents.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先看了神经网络的细节。我们首先看了神经网络多年来的发展。我们研究了不同类型的神经网络。然后，我们看了神经网络的各种构建模块。我们深入研究了用于训练神经网络的梯度下降算法。我们讨论了各种激活函数，并研究了激活函数在神经网络中的应用。我们还看了迁移学习的概念。最后，我们看了一个实际例子，说明了神经网络如何用于训练可以部署到标记伪造或欺诈文件的机器学习模型。
- en: Looking ahead, in the next chapter, we will look into how we can use such algorithms
    for natural language processing. We will also introduce the concept of web embedding
    and will look into the use of recurrent networks for natural language processing.
    Finally, we will also look into how to implement sentiment analysis.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，在下一章中，我们将探讨如何将这样的算法用于自然语言处理。我们还将介绍网络嵌入的概念，并将研究循环网络在自然语言处理中的应用。最后，我们还将研究如何实现情感分析。
