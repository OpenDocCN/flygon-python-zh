- en: Deep Learning for Finance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金融领域的深度学习
- en: Deep learning represents the very cutting edge of **Artificial Intelligence**
    (**AI**). Unlike machine learning, deep learning takes a different approach in
    making predictions by using a neural network. An artificial neural network is
    modeled on the human nervous system, consisting of an input layer and an output
    layer, with one or more hidden layers in between. Each layer consists of artificial
    neurons working in parallel and passing outputs to the next layer as inputs. The
    word *deep* in deep learning comes from the notion that as data passes through
    more hidden layers in an artificial neural network, more complex features can
    be extracted.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习代表着**人工智能**（**AI**）的最前沿。与机器学习不同，深度学习通过使用神经网络来进行预测。人工神经网络是模仿人类神经系统的，包括一个输入层和一个输出层，中间有一个或多个隐藏层。每一层都由并行工作的人工神经元组成，并将输出传递给下一层作为输入。深度学习中的*深度*一词来源于这样一个观念，即当数据通过人工神经网络中的更多隐藏层时，可以提取出更复杂的特征。
- en: '**TensorFlow** is an open source, powerful machine learning and deep learning
    framework developed by Google. In this chapter, we will take a hands-on approach
    to learning TensorFlow by building a deep learning model with four hidden layers
    to predict the prices of a security. Deep learning models are trained by passing
    the entire dataset forward and backward through the network, with each iteration
    known as an **epoch**. Because the input data can be too big to be fed, training
    can be done in batches, and this process is known as **mini-batch training**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow**是由谷歌开发的开源、强大的机器学习和深度学习框架。在本章中，我们将采用实践方法来学习TensorFlow，通过构建一个具有四个隐藏层的深度学习模型来预测某项证券的价格。深度学习模型是通过将整个数据集前向和后向地通过网络进行训练的，每次迭代称为一个**时代**。由于输入数据可能太大而无法被馈送，训练可以分批进行，这个过程称为**小批量训练**。'
- en: Another popular deep learning library is Keras, which utilizes TensorFlow as
    the backend. We will also take a hands-on approach to learning Keras and see how
    easy it is to build a deep learning model to predict credit card payment defaults.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的深度学习库是Keras，它利用TensorFlow作为后端。我们还将采用实践方法来学习Keras，并看看构建一个用于预测信用卡支付违约的深度学习模型有多容易。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: An introduction to neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: Neurons, activation functions, loss functions, and optimizers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元、激活函数、损失函数和优化器
- en: Different types of neural network architectures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的神经网络架构
- en: How to build security price prediction deep learning model using TensorFlow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用TensorFlow构建安全价格预测深度学习模型
- en: Keras, a user-friendly deep learning framework
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras，一个用户友好的深度学习框架
- en: How to build credit card payment default prediction deep learning model using
    Keras
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Keras构建信用卡支付违约预测深度学习模型
- en: How to display recorded events in a Keras history
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在Keras历史记录中显示记录的事件
- en: A brief introduction to deep learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的简要介绍
- en: The theory behind deep learning began as early as the 1940s. However, its popularity
    has soared in recent years thanks in part to improvements in computing hardware
    technology, smarter algorithms, and the adoption of deep learning frameworks.
    There is much to cover beyond this book. This section serves as a quick guide
    to gain a working knowledge for following the examples that we will cover in later
    parts of this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的理论早在20世纪40年代就开始了。然而，由于计算硬件技术的改进、更智能的算法和深度学习框架的采用，它近年来的流行度飙升。这本书之外还有很多内容要涵盖。本节作为一个快速指南，旨在为后面本章将涵盖的示例提供一个工作知识。
- en: What is deep learning ?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: In [Chapter 10](28d7845a-ecc1-46b1-99a0-0e6dea8fd2be.xhtml), *Machine Learning
    for Finance*, we learned how machine learning is useful for making predictions.
    Supervised learning uses error-minimization techniques to fit a model with training
    data, and can be regression based or classification based.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](28d7845a-ecc1-46b1-99a0-0e6dea8fd2be.xhtml)中，*金融领域的机器学习*，我们了解了机器学习如何用于进行预测。监督学习使用误差最小化技术来拟合训练数据的模型，可以是基于回归或分类的。
- en: 'Deep learning takes a different approach in making predictions by using a neural
    network. Modeled on the human brain and the nervous system, an artificial neural
    network consists of a hierarchy of layers, with each layer made up of many simple
    units known as neurons, working in parallel and transforming the input data into
    abstract representations as the output data, which are fed to the next layer as
    input. The following diagram illustrates an artificial neural network:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通过使用神经网络来进行预测采用了一种不同的方法。人工神经网络是模仿人脑和神经系统的，由一系列层组成，每一层由许多称为神经元的简单单元并行工作，并将输入数据转换为抽象表示作为输出数据，然后将其作为输入馈送到下一层。以下图示说明了一个人工神经网络：
- en: '![](Images/58845918-179c-4b97-adf0-fa1c1a52d550.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/58845918-179c-4b97-adf0-fa1c1a52d550.png)'
- en: Artificial neural networks consist of three types of layers. The first layer
    that accepts input is known as the **input layer**. The last layer where output
    is collected is known as the **output layer**. The layers between the input and
    output layers are known as **hidden layers**, since they are hidden from the interface
    of the network. There can be many combinations of hidden layers performing different
    activation functions. Naturally, more complex computations lead to a rise in demand
    for more powerful machines, such as the GPUs required to compute them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络由三种类型的层组成。接受输入的第一层称为**输入层**。收集输出的最后一层称为**输出层**。位于输入和输出层之间的层称为**隐藏层**，因为它们对网络的接口是隐藏的。隐藏层可以有许多组合，执行不同的激活函数。自然地，更复杂的计算导致对更强大的机器的需求增加，例如计算它们所需的GPU。
- en: The artificial neuron
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元
- en: 'An artificial neuron receives one or more input and are multiplied by values
    known as **weights**, summed up and passed to an activation function. The final
    values computed by the activation function makes up the neuron''s output. A bias
    value may be included in the summation term to help fit the data. The following
    diagram illustrates an artificial neuron:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元接收一个或多个输入，并由称为**权重**的值相乘，然后求和并传递给激活函数。激活函数计算的最终值构成了神经元的输出。偏置值可以包含在求和项中以帮助拟合数据。以下图示了一个人工神经元：
- en: '![](Images/4cac940f-299f-438f-b10b-7931a0bd1ecf.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/4cac940f-299f-438f-b10b-7931a0bd1ecf.png)'
- en: The summation term can be written as a linear equation such that *Z=x[1]w[1]+x[2]w[2]+...+b.*
    The neuron uses a nonlinear activation function *f* to transform the input to
    become the output ![](Images/36c3c2e2-b1af-4e63-996e-a9bee2122a52.png), and can
    be written as ![](Images/07dfbda6-8a1f-4a27-9880-d57a3a8ffd35.png).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 求和项可以写成线性方程，如 *Z=x[1]w[1]+x[2]w[2]+...+b.* 神经元使用非线性激活函数 *f* 将输入转换为输出 ![](Images/36c3c2e2-b1af-4e63-996e-a9bee2122a52.png)，可以写成
    ![](Images/07dfbda6-8a1f-4a27-9880-d57a3a8ffd35.png)。
- en: Activation function
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'An activation function is part of an artificial neuron that transforms the
    sum of weighted inputs into another value for the next layer. Usually, the range
    of this output value is -1 or 0 to 1\. An artificial neuron is said to be activated
    when it passes a non-zero value to another neuron. There are several types of
    activation functions, mainly:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是人工神经元的一部分，它将加权输入的总和转换为下一层的另一个值。通常，此输出值的范围为-1或0到1。当人工神经元向另一个神经元传递非零值时，它被激活。主要有几种类型的激活函数，包括：
- en: Linear
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性
- en: Sigmoid
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Tanh
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双曲正切
- en: Hard tanh
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬双曲正切
- en: Rectified linear unit
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修正线性单元
- en: Leaky ReLU
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: Softplus
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softplus
- en: 'For example, a **rectified linear unit** (**ReLU**) function is written as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，**修正线性单元**（ReLU）函数可以写成：
- en: '![](Images/82e59b35-40fa-4400-9942-6b51fed40fa5.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/82e59b35-40fa-4400-9942-6b51fed40fa5.png)'
- en: The ReLU activates a node with the same input value only when the input is above
    zero. Researchers prefer to use ReLU as it trains better than sigmoid activation
    functions. We will be using ReLU in later parts of this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 仅在输入大于零时激活节点的输入值相同。研究人员更喜欢使用 ReLU，因为它比 Sigmoid 激活函数训练效果更好。我们将在本章的后面部分使用
    ReLU。
- en: 'In another example, the leaky ReLU is written as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个例子中，leaky ReLU 可以写成：
- en: '![](Images/6e095e7b-4ee0-485e-8f49-d5425cd952b1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/6e095e7b-4ee0-485e-8f49-d5425cd952b1.png)'
- en: The leaky ReLU addresses the issue of a dead ReLU when ![](Images/c809e2c3-a19f-4989-b587-6cdb3167d31c.png)
    by having a small negative slope around 0.01 when *x* is zero and below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 解决了当 ![](Images/c809e2c3-a19f-4989-b587-6cdb3167d31c.png) 时死亡 ReLU
    的问题，当 *x* 为零或更小时，它具有约 0.01 的小负斜率。
- en: Loss functions
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The loss function computes the error between the predicted value of a model
    and the actual value. The smaller the error value, the better the model is in
    prediction. Some loss functions used in regression-based models are:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数计算模型的预测值与实际值之间的误差。误差值越小，模型的预测就越好。一些用于基于回归的模型的损失函数包括：
- en: '**Mean squared error** (**MSE**) loss'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（MSE）损失'
- en: '**Mean absolute error** (**MAE**) loss'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（MAE）损失'
- en: Huber loss
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huber 损失
- en: Quantile loss
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分位数损失
- en: 'Some loss functions used in classification-based models are:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一些用于基于分类的模型的损失函数包括：
- en: Focal loss
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 焦点损失
- en: Hinge loss
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 铰链损失
- en: Logistic loss
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑损失
- en: Exponential loss
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数损失
- en: Optimizers
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器
- en: 'Optimizers help to tweak the model weights optimally in minimizing the loss
    function. There are several types of optimizers that you may come across in deep
    learning:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器有助于在最小化损失函数时最佳地调整模型权重。在深度学习中可能会遇到几种类型的优化器：
- en: '**AdaGrad** (**adaptive gradient**)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应梯度**（AdaGrad）'
- en: '**Adam** (**adaptive moment estimation**)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应矩估计**（Adam）'
- en: '**LBFGS** (**limited-memory Broyden-Fletcher-Goldfarb-Shannon**)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限内存 Broyden-Fletcher-Goldfarb-Shannon**（LBFGS）'
- en: '**Rprop** (**resilient backpropagation**)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒反向传播**（Rprop）'
- en: '**RMSprop** (**root mean square propagation**)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**根均方传播**（RMSprop）'
- en: '**SGD** (**stochastic gradient descent**)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（SGD）'
- en: Adam is a popular choice of optimizer, and is seen as a combination of RMSprop
    and SGD with momentum. It is an adaptive learning rate optimization algorithm,
    computing individual learning rates for different parameters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 是一种流行的优化器选择，被视为 RMSprop 和带动量的 SGD 的组合。它是一种自适应学习率优化算法，为不同参数计算单独的学习率。
- en: Network architecture
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络架构
- en: 'The network architecture of a neural network defines its behavior. There are
    many forms of network architecture available; some are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的网络架构定义了其行为。有许多形式的网络架构可用；其中一些是：
- en: '**Perceptron** (**P**)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**感知器**（P）'
- en: '**Feed forward** (**FF**)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈**（FF）'
- en: '**Deep feed forward** (**DFF**)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度前馈**（DFF）'
- en: '**Radial basis function network** (**RBF**)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**径向基函数网络**（RBF）'
- en: '**Recurrent neural network** (**RNN**)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（RNN）'
- en: '**Long/short-term memory** (**LSTM**)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长/短期记忆**（LSTM）'
- en: '**Autoencoder** (**AE**)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器**（AE）'
- en: '**Hopfield network** (**HN**)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hopfield 网络**（HN）'
- en: '**Boltzmann machine** (**BM**)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玻尔兹曼机**（BM）'
- en: '**Generative adversarial network** (**GAN**)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（GAN）'
- en: The most well-known and easy-to-understand neural network is the feed forward
    multilayer neural network. It can represent any function using an input layer,
    one or more hidden layers, and a single output layer. A list of neural networks
    can be found at [http://www.asimovinstitute.org/neural-network-zoo/](http://www.asimovinstitute.org/neural-network-zoo/).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名且易于理解的神经网络是前馈多层神经网络。它可以使用输入层、一个或多个隐藏层和一个输出层来表示任何函数。可以在[http://www.asimovinstitute.org/neural-network-zoo/](http://www.asimovinstitute.org/neural-network-zoo/)找到神经网络列表。
- en: TensorFlow and other deep learning frameworks
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 和其他深度学习框架
- en: TensorFlow is a free and open source library from Google, available in Python,
    C++, Java, Rust, and Go. It contains various neural networks for training deep
    learning models. TensorFlow can be applied to various scenarios, such as image
    classification, malware detection, and speech recognition. The official page for
    TensorFlow is [https://www.tensorflow.org](https://www.tensorflow.org).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是来自谷歌的免费开源库，可用于Python、C++、Java、Rust和Go。它包含各种神经网络，用于训练深度学习模型。TensorFlow可应用于各种场景，如图像分类、恶意软件检测和语音识别。TensorFlow的官方页面是[https://www.tensorflow.org](https://www.tensorflow.org)。
- en: Other popular deep learning frameworks used in the industry are Theano, PyTorch,
    CNTK (Microsoft Cognitive Toolkit), Apache MXNet, and Keras.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在行业中使用的其他流行的深度学习框架包括Theano、PyTorch、CNTK（Microsoft Cognitive Toolkit）、Apache
    MXNet和Keras。
- en: What is a tensor ?
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量是什么？
- en: The *Tensor* in TensorFlow indicates that the frameworks define and run computations
    involving tensors. A tensor is nothing more than a type of *n*-dimensional vector
    with certain transformative properties. A non-dimensional tensor is a scalar or
    number. A one-dimensional tensor is a vector. A two-dimensional tensor is a matrix.
    Tensors offer more natural representations of data, for example in images in the
    field of computer vision.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中的“Tensor”表示这些框架定义和运行涉及张量的计算。张量只不过是具有特定变换属性的一种*n*维向量类型。非维度张量是标量或数字。一维张量是向量。二维张量是矩阵。张量提供了数据的更自然表示，例如在计算机视觉领域的图像中。
- en: The basic properties of vector spaces and the elementary mathematical properties
    of tensors make them particularly useful in physics and engineering.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 向量空间的基本属性和张量的基本数学属性使它们在物理学和工程学中特别有用。
- en: A deep learning price prediction model with TensorFlow
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow的深度学习价格预测模型
- en: In this section, we will learn how to use TensorFlow as a deep learning framework
    in building a price prediction model. Five years of pricing data, from 2013 to
    2017, will be used for training our deep learning model. We will attempt to predict
    the prices of Apple (AAPL) in the following year of 2018.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用TensorFlow作为深度学习框架来构建价格预测模型。我们将使用2013年至2017年的五年定价数据来训练我们的深度学习模型。我们将尝试预测2018年苹果（AAPL）的价格。
- en: Feature engineering our model
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程我们的模型
- en: 'The daily adjusted closing prices of our data make up the target variables.
    The independent variables defining the features of our model are made up of these
    technical indicators:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据的每日调整收盘价构成了目标变量。定义我们模型特征的自变量由这些技术指标组成：
- en: '**Relative strength index** (**RSI**)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相对强弱指数**（**RSI**）'
- en: '**Williams %R** (**WR**)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**威廉指标**（**WR**）'
- en: '**Awesome oscillator** (**AO**)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**令人敬畏的振荡器**（**AO**）'
- en: '**Volume-weighted average price** (**VWAP**)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成交量加权平均价格**（**VWAP**）'
- en: '**Average daily trading volume** (**ADTV**)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均每日交易量**（**ADTV**）'
- en: 5-day **moving average** (**MA**)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5天**移动平均**（**MA**）
- en: 15-day moving average
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 15天移动平均
- en: 30-day moving average
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 30天移动平均
- en: This gives us eight features for our model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们的模型提供了八个特征。
- en: Requirements
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要求
- en: You should have NumPy, pandas, Jupyter, and scikit-learn libraries installed,
    as mentioned in previous chapters. The following sections highlight additional
    important requirements for building our deep learning model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，您应该已安装了NumPy、pandas、Jupyter和scikit-learn库。以下部分重点介绍了构建我们的深度学习模型所需的其他重要要求。
- en: Intrinio as our data provider
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Intrinio作为我们的数据提供商
- en: Intrinio ([https://intrinio.com/](https://intrinio.com/)) is a premium API-based
    financial data provider. We will be using the US Fundamentals and Stock Prices
    subscription, which gives us access to US historical stock prices and well-calculated
    technical indicator values. After registering for an account, your API keys can
    be found in your account settings, which we will use later.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Intrinio（[https://intrinio.com/](https://intrinio.com/)）是一个高级API金融数据提供商。我们将使用美国基本面和股价订阅，这使我们可以访问美国历史股价和精心计算的技术指标值。注册账户后，您的API密钥可以在您的账户设置中找到，稍后我们将使用它们。
- en: Compatible Python environment for TensorFlow
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow的兼容Python环境
- en: At the time of writing, the latest stable version of TensorFlow is r1.13\. This
    version is compatible with Python 2.7, 3.4, 3.5, and 3.6\. As the preceding chapters
    in this book use Python 3.7, we need to set up a separate Python 3.6 environment
    for running the examples in this chapter. The virtualenv tool ([https://virtualenv.pypa.io/](https://virtualenv.pypa.io/))
    is recommended to isolate Python environments.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，TensorFlow的最新稳定版本是r1.13。该版本兼容Python 2.7、3.4、3.5和3.6。由于本书前面的章节使用Python
    3.7，我们需要为本章的示例设置一个单独的Python 3.6环境。建议使用virtualenv工具（[https://virtualenv.pypa.io/](https://virtualenv.pypa.io/)）来隔离Python环境。
- en: The requests library
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: requests库
- en: 'The `requests` Python library is required to help us make HTTP calls to Intrinio
    APIs. The official web page for `requests` is [http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/).
    Install `requests` by running this command in your terminal: `pip install requests`.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 需要`requests` Python库来帮助我们调用Intrinio的API。`requests`的官方网页是[http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/)。在终端中运行以下命令来安装`requests`：`pip
    install requests`。
- en: The TensorFlow library
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow库
- en: 'There are a number of variants of TensorFlow available for installation. You
    may choose between CPU-only or GPU support versions, alpha versions, and nightly
    versions. More installation instructions are available at [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip).
    At a minimum, the following terminal command installs the latest CPU-only stable
    version of TensorFlow: `pip install tensorflow`.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多TensorFlow的变体可供安装。您可以选择仅CPU或GPU支持版本、alpha版本和nightly版本。更多安装说明请参阅[https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)。至少，以下终端命令将安装最新的CPU-only稳定版本的TensorFlow：`pip
    install tensorflow`。
- en: Downloading the dataset
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载数据集
- en: 'This section describes the steps for downloading our required prices and technical
    indicator values from Intrinio. Comprehensive documentation on the API calls can
    be found at [https://docs.intrinio.com/documentation/api_v2](https://docs.intrinio.com/documentation/api_v2).
    If you decide to use another data provider, go ahead and skip this section:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了从Intrinio下载所需价格和技术指标值的步骤。API调用的全面文档可以在[https://docs.intrinio.com/documentation/api_v2](https://docs.intrinio.com/documentation/api_v2)找到。如果决定使用另一个数据提供商，请继续并跳过本节：
- en: 'Write a `query_intrinio()` function that will make an API call to Intrinio,
    with the following codes:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个`query_intrinio()`函数，该函数将调用Intrinio的API，具有以下代码：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This function accepts the `path` and `kwargs` parameters. The `path` parameter
    refers to the specific Intrinio API context path. The `kwargs` keyword argument is
    a dictionary that gets passed along to a HTTP GET request call as request parameters.
    The API key is inserted into this dictionary on every API call to identify the
    user account. Any API responses are expected to be in JSON format with a HTTP
    status code of 200; otherwise, an exception will be thrown.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受`path`和`kwargs`参数。`path`参数是指特定的Intrinio API上下文路径。`kwargs`关键字参数是一个字典，作为请求参数传递给HTTP
    GET请求调用。API密钥被插入到这个字典中，以便在每次API调用时识别用户帐户。预期任何API响应都以JSON格式呈现，HTTP状态码为200；否则，将抛出异常。
- en: 'Write a `get_technicals()` function to download technical indicator values
    from Intrinio, with the following codes:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个`get_technicals()`函数，使用以下代码从Intrinio下载技术指标值：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `ticker` and `indicator` parameters make up the API context path for downloading
    the specific indicator of a security. The response is expected to be in JSON format,
    with a key named `technicals` containing the list of technical indicator values.
    The `json_normalize()` function of pandas helps to convert these values into a
    flat table DataFrame object. Extra formatting is required to set date and time
    values as the index under the `date` name.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`ticker`和`indicator`参数构成了下载特定安全性指标的API上下文路径。预期响应以JSON格式呈现，其中包含一个名为`technicals`的键，其中包含技术指标值列表。pandas的`json_normalize()`函数有助于将这些值转换为平面表DataFrame对象。需要额外的格式设置以将日期和时间值设置为`date`名称下的索引。'
- en: 'Define the values for the request parameters:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义请求参数的值：
- en: '[PRE2]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We shall be querying data for the security `AAPL`, from 2013 to 2018, inclusive.
    The big `page_size` value gives us sufficient space to request six years of data
    in a single query.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查询2013年至2018年（含）期间的安全性`AAPL`的数据。大的`page_size`值为我们提供了足够的空间，以便在单个查询中请求六年的数据。
- en: 'Run the following commands at one-minute intervals each to download the technical
    indicator data:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以一分钟的间隔运行以下命令来下载技术指标数据：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Beware of paging limits when performing Intrinio API queries! API requests with
    a `page_size` greater than 100 are subjected to a per-minute request limit. If
    a call fails with status code 429, try again in one minute. Information on Intrinio's
    limits can be found at [https://docs.intrinio.com/documentation/api_v2/limits](https://docs.intrinio.com/documentation/api_v2/limits).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行Intrinio API查询时要注意分页限制！`page_size`大于100的API请求受到每分钟请求限制。如果调用失败并显示状态码429，请在一分钟后重试。有关Intrinio限制的信息可以在[https://docs.intrinio.com/documentation/api_v2/limits](https://docs.intrinio.com/documentation/api_v2/limits)找到。
- en: This gives us eight variables, each containing the DataFrame object of the respective
    technical indicator values. The MA data columns are renamed to avoid a naming
    conflict when joining the data later.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们了八个变量，每个变量都包含各自技术指标值的DataFrame对象。稍后加入数据时，MA数据列被重命名以避免命名冲突。
- en: 'Write a `get_prices()` function to download the historical prices of a security:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个`get_prices()`函数，使用以下代码下载安全性的历史价格：
- en: '[PRE4]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `tag` parameter specifies the data tag of the security to download. The
    JSON response is expected to contain a key named `historical_data` containing
    the list of values. The column containing the prices in the DataFrame object is
    renamed from `value` to its data tag.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`tag`参数指定要下载的安全性的数据标签。预期JSON响应包含一个名为`historical_data`的键，其中包含值列表。DataFrame对象中包含价格的列从`value`重命名为其数据标签。'
- en: The Intrinio data tags are used to download specific values from the system.
    The list of data tags available with explanations can be found at [https://data.intrinio.com/data-tags/all](https://data.intrinio.com/data-tags/all).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Intrinio数据标签用于从系统中下载特定值。可在[https://data.intrinio.com/data-tags/all](https://data.intrinio.com/data-tags/all)找到带有解释的数据标签列表。
- en: 'Using the `get_prices()` function, download the adjusted closing prices of
    AAPL:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`get_prices()`函数，下载AAPL的调整收盘价：
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As the features are used to predict the next day''s closing prices, we need
    to shift the prices backwards by one day to align this mapping. Create the target
    variables:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于特征用于预测第二天的收盘价，我们需要将价格向后移动一天以对齐这种映射。创建目标变量：
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, combine all the DataFrame objects together with the `join()` command
    and drop the empty values:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`join()`命令将所有DataFrame对象组合在一起，并删除空值：
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Our dataset is now ready, contained in the `df` DataFrame. We can proceed to
    split the data for training.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集现在已经准备好，包含在`df`DataFrame中。我们可以继续拆分训练数据。
- en: Scaling and splitting the data
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放和拆分数据
- en: 'We are interested in using the earliest five years of pricing data for training
    our model, and the most recent year of 2018 for testing our predictions. Run the
    following codes to split our `df` dataset:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有兴趣使用最早的五年定价数据来训练我们的模型，并使用2018年的最近一年来测试我们的预测。运行以下代码来拆分我们的`df`数据集：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `df_train` and `df_test` variables contain our training and testing data
    respectively.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`df_train`和`df_test`变量分别包含我们的训练和测试数据。'
- en: An important step in data preprocessing is to normalize the dataset. This will
    transform input feature values to a mean of zero and a variance of one. Normalization
    helps to avoid biases during training due to the different scales of input features.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理中的一个重要步骤是对数据集进行归一化。这将使输入特征值转换为零的平均值和一个的方差。归一化有助于避免由于输入特征的不同尺度而导致训练中的偏差。
- en: 'The `MinMaxScaler` function of the `sklearn` module helps to transform each
    feature into a range between -1 and 0, with the following codes:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`模块的`MinMaxScaler`函数有助于将每个特征转换为-1到0之间的范围，使用以下代码：'
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `fit_transform()` function computes the parameters for scaling and transforms
    the data, while the `transform()` function only transforms the data by reusing
    the computed parameters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit_transform()`函数计算用于缩放和转换数据的参数，而`transform()`函数仅通过重用计算的参数来转换数据。'
- en: 'Next, split the scaled training dataset into independent and target variables.
    The target values are on the last column, with the remaining columns as the features:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将缩放的训练数据集分成独立的和目标变量。目标值在最后一列，其余列为特征：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Do the same on our testing data for features only:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的测试数据上只针对特征执行相同的操作：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With our training and testing dataset prepared, let's begin to build an artificial
    neural network with TensorFlow.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好我们的训练和测试数据集后，让我们开始使用TensorFlow构建一个人工神经网络。
- en: Building an artificial neural network with TensorFlow
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow构建人工神经网络
- en: This section walks you through the process of setting up an artificial neural
    network for deep learning with four hidden layers. There are two phases involved;
    first in assembling the graph, and next in training the model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将指导您完成设置具有四个隐藏层的深度学习人工神经网络的过程。涉及两个阶段；首先是组装图形，然后是训练模型。
- en: Phase 1 – assembling the graph
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一阶段 - 组装图形
- en: 'The following steps describe the process of setting up a TensorFlow graph:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了设置TensorFlow图的过程：
- en: 'Create placeholders for inputs and labels with the following codes:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码为输入和标签创建占位符：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A TensorFlow operation starts with placeholders. Here, we defined two placeholders
    `x` and `y` for containing the network inputs and outputs, respectively. The `shape`
    parameter defines the shape of the tensor to be fed, with `None` meaning that
    the number of observations is unknown at this point. The second dimension of `x`
    is the number of features that we have, reflected in the `num_features` variable.
    Later, as we shall see, placeholder values are fed using the `feed_dict` command.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow操作始于占位符。在这里，我们定义了两个占位符`x`和`y`，分别用于包含网络输入和输出。`shape`参数定义了要提供的张量的形状，其中`None`表示此时观察数量是未知的。`x`的第二个维度是我们拥有的特征数量，反映在`num_features`变量中。稍后，我们将看到，占位符值是使用`feed_dict`命令提供的。
- en: Create weight and bias initializers for hidden layers. Our model will consist
    of four hidden layers. The first layer contains 512 neurons, about three times
    the size of the input. The second, third, and fourth layers contain 256, 128,
    and 64 neurons, respectively. The reduction of the number of neurons in subsequent
    layers compresses the information in the network.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为隐藏层创建权重和偏差初始化器。我们的模型将包括四个隐藏层。第一层包含512个神经元，大约是输入大小的三倍。第二、第三和第四层分别包含256、128和64个神经元。在后续层中减少神经元的数量会压缩网络中的信息。
- en: 'Initializers are used to initialize the network variables before training.
    It is important to use proper initialization at the start of the optimization
    problem to produce good solutions to the underlying problem. The use of a variance
    scaling initializer and a zeros initializer is demonstrated with the following
    code:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化器用于在训练之前初始化网络变量。在优化问题开始时使用适当的初始化非常重要，以产生潜在问题的良好解决方案。以下代码演示了使用方差缩放初始化器和零初始化器：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Besides placeholders, variables within TensorFlow are updated during graph execution.
    Here, the variables are the weights and bias that will change during training.
    The  `variance_scaling_initializer()` command returns an initializer that generates
    tensors for our weights without scaling variance. The `FAN_AVG` mode indicates
    to the initializer to use the average number of input and output connections,
    with the `uniform` parameter as `True` to use uniform random initialization and
    a scale factor of 1\. This is akin to training DFF neural networks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 除了占位符，TensorFlow中的变量在图执行期间会被更新。在这里，变量是在训练期间会发生变化的权重和偏差。`variance_scaling_initializer()`命令返回一个初始化器，用于生成我们的权重张量而不缩放方差。`FAN_AVG`模式指示初始化器使用输入和输出连接的平均数量，`uniform`参数为`True`表示使用均匀随机初始化和缩放因子为1。这类似于训练DFF神经网络。
- en: In **multilayer perceptrons** (**MLP**) such as our model, the first dimension
    of the weights layer is the same as the second dimension of the previous weights
    layer. The bias dimensions correspond to the number of neurons in the current
    layer. The neuron of the last layer is expected to have only one output.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在**多层感知器**（**MLP**）中，例如我们的模型，权重层的第一个维度与上一个权重层的第二个维度相同。偏差维度对应于当前层中的神经元数量。预期最后一层的神经元只有一个输出。
- en: 'Now is the time to combine our placeholder inputs with weights and bias for
    the four hidden layers using the following code:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候使用以下代码将我们的占位符输入与权重和偏差结合起来，用于四个隐藏层。
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `tf.matmul` command multiplies the input and weight matrices, adding the
    bias values using the `tf.add` command. Each hidden layer of the neural network
    is transformed by an activation function. In this model, we are using ReLU as
    the activation function for all layers using the `tf.nn.relu` command. The output
    of each hidden layer is fed to the input of the next hidden layer. The last layer,
    which is the output layer with a single vector output, must be transposed with
    the `tf.transpose` command.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.matmul`命令将输入和权重矩阵相乘，使用`tf.add`命令添加偏差值。神经网络的每个隐藏层都通过激活函数进行转换。在这个模型中，我们使用`tf.nn.relu`命令将ReLU作为所有层的激活函数。每个隐藏层的输出被馈送到下一个隐藏层的输入。最后一层是输出层，具有单个向量输出，必须使用`tf.transpose`命令进行转置。'
- en: 'Specify the loss function of the network to measure the error between predicted
    and actual values during training. For regression-based models such as ours, the
    MSE is commonly used:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定网络的损失函数，用于在训练期间测量预测值和实际值之间的误差。对于像我们这样的基于回归的模型，通常使用MSE：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `tf.squared_difference` command is defined to return the squared errors
    between the predicted and actual values, and the `tf.reduce_mean` command is the
    loss function for minimizing the mean during training.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.squared_difference`命令被定义为返回预测值和实际值之间的平方误差，`tf.reduce_mean`命令是用于在训练期间最小化均值的损失函数。'
- en: 'Create the optimizer with the following code:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码创建优化器：
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In minimizing the loss function, an optimizer helps to compute the network weight
    and bias during training. Here, we are using the Adam algorithm with default values.
    With this important step completed, we may now embark on phase two in training
    our model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在最小化损失函数时，优化器在训练期间帮助计算网络的权重和偏差。在这里，我们使用默认值的Adam算法。完成了这一重要步骤后，我们现在可以开始进行模型训练的第二阶段。
- en: Phase 2 – training our model
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二阶段 - 训练我们的模型
- en: 'The following steps describe the process of training our model:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了训练我们的模型的过程：
- en: 'Create a TensorFlow `Session` object to encapsulate the environment in which
    a neural network model operates:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个TensorFlow `Session`对象来封装神经网络模型运行的环境：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we are specifying a session for use in an interactive context, in this
    case a Jupyter notebook. A regular `tf.Session` is non-interactive and requires
    an explicit `Session` object to be passed using the `with` keyword when running
    operations. `InteractiveSession` removes this need and is more convenient as it
    reuses the `session` variable.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在指定一个会话以在交互式环境中使用，即Jupyter笔记本。常规的`tf.Session`是非交互式的，需要在运行操作时使用`with`关键字传递一个显式的`Session`对象。`InteractiveSession`消除了这种需要，更方便，因为它重用了`session`变量。
- en: 'TensorFlow requires that all global variables are to be initialized before
    training. Do this using the `session.run` command:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow要求在训练之前初始化所有全局变量。使用`session.run`命令进行初始化。
- en: '[PRE18]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Run the following codes to train our model using mini-batch training:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码使用小批量训练来训练我们的模型：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: An epoch is a single iteration of the entire dataset being passed forward and
    backward through the network. Usually several epochs are performed on different
    permutations of the training data for the network to learn its behavior. There
    is no fixed number of epochs for a good model, as it depends on how diverse the
    data is. Because the dataset can be too big to be fed into the model in one epoch,
    mini-batch training divides the dataset into parts and feeds it into the `session.run`
    command for learning. The first parameter specifies the optimization algorithm
    instance. The `feed_dict` parameter is given a dictionary containing our `x` and
    `y` placeholders mapped to batches of our independent and target values respectively.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一个时期是整个数据集通过网络前向和后向传递的单次迭代。通常对训练数据的不同排列执行几个时期，以便网络学习其行为。对于一个好的模型，没有固定的时期数量，因为它取决于数据的多样性。因为数据集可能太大而无法在一个时期内输入模型，小批量训练将数据集分成部分，并将其馈送到`session.run`命令进行学习。第一个参数指定了优化算法实例。`feed_dict`参数接收一个包含我们的`x`和`y`占位符的字典，分别映射到我们的独立值和目标值的批次。
- en: 'After our model is fully trained, use it for prediction with our testing data
    containing the features:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的模型完全训练后，使用它对包含特征的测试数据进行预测：
- en: '[PRE20]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `session.run` command is called with the first parameter as the output layer
    transformation function. The `feed_dict` parameter is fed with our testing data.
    The first item in the output list is read as the final output predicted values.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`session.run`命令，第一个参数是输出层的转换函数。`feed_dict`参数用我们的测试数据进行馈送。输出列表中的第一项被读取为最终输出的预测值。
- en: 'As the predicted values are also normalized, we need to scale them back to
    the original values:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于预测值也被标准化，我们需要将它们缩放回原始值：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Create a copy of our initial training data with the `copy()` command onto the
    new `predicted_scaled_data `variable. The last column will be replaced with our
    predicted values. Next, the `inverse_transform()` command scales our data back
    to the original size, giving us the predicted values for comparison with actual
    observed values.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`copy()`命令创建我们初始训练数据的副本到新的`predicted_scaled_data`变量。最后一列将被替换为我们的预测值。接下来，`inverse_transform()`命令将我们的数据缩放回原始大小，给出我们的预测值，以便与实际观察值进行比较。
- en: Plotting predicted and actual values
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制预测值和实际值
- en: 'Let''s plot the predicted and actual values onto a graph to visualize the performance
    of our deep learning model. Run the following codes to extract our values of interest:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将预测值和实际值绘制到图表上，以可视化我们深度学习模型的性能。运行以下代码提取我们感兴趣的值：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The rescaled `predicted_values` dataset is a NumPy `ndarray` object with predicted
    values on the last column. These values and the actual adjusted closing prices
    of 2018 are extracted to the `predictions` and `actual` variables respectively.
    Since the format of the original dataset is in descending order of time, we reverse
    them in ascending order for plotting on a graph. Run the following codes to generate
    a graph:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 重新缩放的`predicted_values`数据集是一个带有预测值的NumPy `ndarray`对象，这些值和2018年的实际调整收盘价分别提取到`predictions`和`actual`变量中。由于原始数据集的格式是按时间降序排列的，我们将它们反转为升序以绘制图表。运行以下代码生成图表：
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following output is produced:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 生成以下输出：
- en: '![](Images/a894ecce-9919-4d3e-aff9-3655883f5ff5.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/a894ecce-9919-4d3e-aff9-3655883f5ff5.png)'
- en: The solid line shows the actual adjusted closing prices, while the dotted lines
    show the predicted prices. Notice how our predictions follow a general trend with
    actual prices even though the model did not have any knowledge of the actual prices
    in 2018\. Still, there is plenty of room for improvements in our deep learning
    prediction model, such as in the design of the neuron network architecture, hidden
    layers, activation functions, and initialization schemes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 实线显示了实际调整后的收盘价，而虚线显示了预测价格。请注意，尽管模型没有任何关于2018年实际价格的知识，我们的预测仍然遵循实际价格的一般趋势。然而，我们的深度学习预测模型还有很多改进空间，比如神经元网络架构、隐藏层、激活函数和初始化方案的设计。
- en: Credit card payment default prediction with Keras
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras进行信用卡支付违约预测
- en: Another popular deep learning Python library is Keras. In this section, we will
    use Keras to build a credit card payment default prediction model, and see how
    easy it is to construct an artificial neural network with five hidden layers,
    apply activation functions, and train this model as compared to TensorFlow.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的深度学习Python库是Keras。在本节中，我们将使用Keras构建一个信用卡支付违约预测模型，并看看相对于TensorFlow，构建一个具有五个隐藏层的人工神经网络、应用激活函数并训练该模型有多容易。
- en: Introduction to Keras
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras简介
- en: Keras is an open source deep learning library in Python, designed to be high
    level, user friendly, modular, and extensible. Keras was conceived to be an interface
    rather than a standalone machine learning framework, running on top of TensorFlow,
    CNTK, and Theano. Its huge community base with over 200,000 users makes it one
    of the most popular deep learning libraries.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个开源的Python深度学习库，旨在高层次、用户友好、模块化和可扩展。Keras被设计为一个接口，而不是一个独立的机器学习框架，运行在TensorFlow、CNTK和Theano之上。其拥有超过20万用户的庞大社区使其成为最受欢迎的深度学习库之一。
- en: Installing Keras
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Keras
- en: 'The official documentation page for Keras is at [https://keras.io](https://keras.io).
    The easiest way to install Keras is running this command in your terminal: `pip
    install keras`. By default, Keras will use TensorFlow as its tensor manipulation
    library, though it is also possible to configure another backend implementation.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的官方文档页面位于[https://keras.io](https://keras.io)。安装Keras的最简单方法是在终端中运行以下命令：`pip
    install keras`。默认情况下，Keras将使用TensorFlow作为其张量操作库，但也可以配置其他后端实现。
- en: Obtaining the dataset
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据集
- en: 'We will use the Default of Credit Card Clients dataset downloaded from the
    UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)
    ). Source: Yeh, I. C., and Lien, C. H.(2009).* The comparisons of data mining
    techniques for the predictive accuracy of probability of default of credit card
    clients. Expert Systems with Applications, 36(2), 2473-2480.*'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用从UCI机器学习库下载的信用卡客户违约数据集（[https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)）。来源：Yeh,
    I. C., and Lien, C. H.(2009).* The comparisons of data mining techniques for the
    predictive accuracy of probability of default of credit card clients. Expert Systems
    with Applications, 36(2), 2473-2480.*
- en: This dataset contains customer default payments in Taiwan. Refer to the section
    Attribute Information on the web page for the naming conventions used for the
    columns in the dataset. As the original dataset is in Microsoft Excel Spreadsheet
    XLS format, additional data processing is required. Open the file and remove the
    first row and first column containing supplementary attribute information, and
    save it as a CSV file. A copy of this file is found in `files\chapter11\default_cc_clients.csv`
    of the source code repository.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含台湾客户的违约支付。请参考网页上的属性信息部分，了解数据集中列的命名约定。由于原始数据集是以Microsoft Excel电子表格XLS格式存在的，需要进行额外的数据处理。打开文件并删除包含附加属性信息的第一行和第一列，然后将其保存为CSV文件。源代码存储库的`files\chapter11\default_cc_clients.csv`中可以找到此文件的副本。
- en: 'Read this dataset as a `pandas` DataFrame object to a new variable named `df`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 将此数据集读取为一个名为`df`的`pandas` DataFrame对象：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Inspect this DataFrame with the `info()` command:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`info()`命令检查这个DataFrame：
- en: '[PRE25]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output is truncated, but the summary shows that we have 30,000 rows of credit
    default data available with 23 features. The target variable is the last column
    named `default payment next month`. A value of 1 indicates a default has occurred,
    and 0 otherwise.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输出被截断，但总结显示我们有30,000行信用违约数据，共23个特征。目标变量是名为`default payment next month`的最后一列。值为1表示发生了违约，值为0表示没有。
- en: Should you get a chance to open the CSV file, you will notice that all values
    in the dataset are in numeric format, and values such as gender, education, and
    marital status are already converted to the integer equivalent, saving the need
    for additional data preprocessing steps. Should you have datasets containing string
    or Boolean values, remember to perform label encoding and convert them to dummy
    or indicator values.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有机会打开CSV文件，您会注意到数据集中的所有值都是数字格式，而诸如性别、教育和婚姻状况等值已经转换为整数等效值，省去了额外的数据预处理步骤。如果您的数据集包含字符串或布尔值，请记得执行标签编码并将它们转换为虚拟或指示器值。
- en: Splitting and scaling the data
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拆分和缩放数据
- en: 'Before feeding the dataset into our model, we have to prepare it in a proper
    format. The following steps guide you through the process:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据集输入模型之前，我们必须以适当的格式准备它。以下步骤将指导您完成这个过程：
- en: 'Split the dataset into independent and target variables:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集拆分为独立变量和目标变量：
- en: '[PRE26]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our target values in the last column of the dataset are assigned to the `target` variable,
    while remaining values are feature values and are assigned to the `features` variable.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中最后一列中的目标值被分配给 `target` 变量，而剩余的值是特征值，并被分配给 `features` 变量。
- en: 'Split the dataset into training data and testing data:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集拆分为训练数据和测试数据：
- en: '[PRE27]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `train_test_split()` command of `sklearn` helps to split arrays or matrices
    into random train and test subsets. Every non-keyword argument supplied provides
    a pair of train-test splits of input. Here, we will obtain two such pairs for
    input and output data. The `test_size` parameter indicates we will be including
    20 percent of the input in the test split. The `random_state` parameter sets the
    random number generator to zero.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn` 的 `train_test_split()` 命令有助于将数组或矩阵拆分为随机的训练和测试子集。提供的每个非关键字参数都提供了一对输入的训练-测试拆分。在这里，我们将为输入和输出数据获得两个这样的拆分对。`test_size`
    参数表示我们将在测试拆分中包含 20% 的输入。`random_state` 参数将随机数生成器设置为零。'
- en: 'Convert the split data into NumPy array objects:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将拆分的数据转换为 NumPy 数组对象：
- en: '[PRE28]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, standardize the dataset by scaling the features with `MinMaxScaler()`
    of the `sklearn` module:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过使用 `sklearn` 模块的 `MinMaxScaler()` 来对特征进行缩放，标准化数据集：
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As in the previous section, the `fit_transform()` and `transform()` commands
    are applied. However, this time the default scaling range is 0 to 1\. With our
    dataset prepared, we can start to design a neural network using Keras.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一节一样，应用了 `fit_transform()` 和 `transform()` 命令。但是，这次默认的缩放范围是 0 到 1。准备好我们的数据集后，我们可以开始使用
    Keras 设计神经网络。
- en: Designing a deep neural network with five hidden layers using Keras
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 设计一个具有五个隐藏层的深度神经网络
- en: 'Keras uses the concept of layers when working with models. There are two ways
    to do so. The simplest way is by using a sequential model for a linear stack of
    layers. The other is the functional API for building complex models such as multi-output
    models, directed acyclic graphs, or models with shared layers. This means that the
    tensor output from a layer can be used to define a model, or a model itself can
    become a layer:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 在处理模型时使用层的概念。有两种方法可以做到这一点。最简单的方法是使用顺序模型来构建层的线性堆叠。另一种是使用功能 API 来构建复杂的模型，如多输出模型、有向无环图或具有共享层的模型。这意味着可以使用来自层的张量输出来定义模型，或者模型本身可以成为一个层：
- en: 'Let''s use the Keras library and create a `Sequential` model:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 Keras 库并创建一个 `Sequential` 模型：
- en: '[PRE30]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `add()` method simply adds layers to our model. The first and last layers
    are input and output layers respectively. Each `Dense()` command creates a regular
    layer of densely connected neurons. In between them, a dropout layer is used to
    randomly set input units to zero, helping to prevent overfitting. Here, we specified
    the dropout rate as 20%, though 20% to 50% is usually used.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`add()` 方法简单地向我们的模型添加层。第一层和最后一层分别是输入层和输出层。每个 `Dense()` 命令创建一个密集连接神经元的常规层。它们之间，使用了一个
    dropout 层来随机将输入单元设置为零，有助于防止过拟合。在这里，我们将 dropout 率指定为 20%，尽管通常使用 20% 到 50%。'
- en: The first `Dense()` command parameter with a value of 80 refers to the dimensionality
    of the output space. The optional `input_dim` parameter refers to the number of
    features for the input layer only. The ReLU activation function is specified for
    all except the output layer. Right before the output layer, a batch normalization
    layer transforms the activation mean to zero and standard deviation close to one.
    Together with a sigmoid activation function at the final output layer, output
    values can be rounded off to the nearest 0 or 1, satisfying our binary classification
    solution.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 具有值为 80 的第一个 `Dense()` 命令参数指的是输出空间的维度。可选的 `input_dim` 参数仅适用于输入层的特征数量。ReLU 激活函数被指定为除输出层外的所有层。在输出层之前，批量归一化层将激活均值转换为零，标准差接近于一。与最终输出层的
    sigmoid 激活函数一起，输出值可以四舍五入到最近的 0 或 1，满足我们的二元分类解决方案。
- en: 'The `summary()` command prints a summary of the model:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`summary()` 命令打印模型的摘要：'
- en: '[PRE31]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can see the output shape and weights in each layer. The number of parameters
    for a dense layer is calculated as the total number of weights matrix plus the
    number of elements in the bias matrix. For example, the first hidden layer, `dense_17`,
    will have 23×80+80=1920 parameters.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每一层的输出形状和权重。密集层的参数数量计算为权重矩阵的总数加上偏置矩阵中的元素数量。例如，第一个隐藏层 `dense_17` 将有 23×80+80=1920
    个参数。
- en: The list of activations available in Keras can be found at [https://keras.io/activations/](https://keras.io/activations/).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供的激活函数列表可以在 [https://keras.io/activations/](https://keras.io/activations/)
    找到。
- en: 'Configure this model for training with the `compile()` command:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `compile()` 命令为训练配置此模型：
- en: '[PRE32]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `optimizer` parameter specifies the optimizer for training the model. Keras
    provides some optimizers, but we can choose a custom optimizer instance instead,
    using the Adam optimizer in TensorFlow from the previous section. The binary cross-entropy
    calculation is chosen as the loss function as it is suitable for our binary classification
    problem. The `metrics` parameter specifies a list of metrics to be produced during
    training and testing. Here, the accuracy will be produced for retrieval after
    fitting the model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`optimizer` 参数指定了用于训练模型的优化器。Keras 提供了一些优化器，但我们可以选择使用自定义优化器实例，例如在前面的 TensorFlow
    中使用 Adam 优化器。选择二元交叉熵计算作为损失函数，因为它适用于我们的二元分类问题。`metrics` 参数指定在训练和测试期间要生成的指标列表。在这里，准确度将在拟合模型后生成。'
- en: A list of optimizers available in Keras can be found at [https://keras.io/optimizers/](https://keras.io/optimizers/).
    A list of loss functions available in Keras can be found at [https://keras.io/losses/](https://keras.io/losses/).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中可以找到一系列可用的优化器列表，网址为[https://keras.io/optimizers/](https://keras.io/optimizers/)。在Keras中可以找到一系列可用的损失函数列表，网址为[https://keras.io/losses/](https://keras.io/losses/)。
- en: 'Now is the time to train our model using the `fit()` command with 100 epochs:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是使用`fit()`命令进行100个时期的模型训练的时候了：
- en: '[PRE33]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The preceding output is truncated as the model produces detailed training updates
    for every epoch. A `History()` object is created and fed into the model's callback
    for recording events during training. The `fit()` command allows the number of
    epochs and batch size to be specified. The `validation_split` parameter is set
    such that 20% of the training data will be set aside as validation data, evaluating
    the loss and model metrics at the end of each epoch.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型为每个时期生成详细的训练更新，因此上述输出被截断。创建一个`History()`对象并将其馈送到模型的回调中以记录训练期间的事件。`fit()`命令允许指定时期数和批量大小。设置`validation_split`参数，使得20%的训练数据将被保留为验证数据，在每个时期结束时评估损失和模型指标。
- en: 'Instead of training the data all at once, you can also train your data in batches.
    Call the `fit()` command with an `epochs` and `batch_size` parameter, like this:
    `model.fit(x_train, y_train, epochs=5, batch_size=32)`. You can also train batches
    manually using the `train_on_batch()` command, like this: `model.train_on_batch(x_batch,
    y_batch)`.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以分批训练数据，而不是一次性训练数据。使用`fit()`命令和`epochs`和`batch_size`参数，如下所示：`model.fit(x_train,
    y_train, epochs=5, batch_size=32)`。您也可以使用`train_on_batch()`命令手动训练批次，如下所示：`model.train_on_batch(x_batch,
    y_batch)`。
- en: Measuring the performance of our model
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量我们模型的性能
- en: 'Using our test data, we can compute the loss and accuracy of our model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的测试数据，我们可以计算模型的损失和准确率：
- en: '[PRE34]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Our model has 82% prediction accuracy.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型有82%的预测准确率。
- en: Running risk metrics
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行风险指标
- en: In [Chapter 10](28d7845a-ecc1-46b1-99a0-0e6dea8fd2be.xhtml), *Machine Learning
    for Finance*, we discussed the confusion matrix, accuracy score, precision score,
    recall score, and F1 score in measuring classification-based predictions. We can
    reuse those metrics on our model as well.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](28d7845a-ecc1-46b1-99a0-0e6dea8fd2be.xhtml)中，*金融机器学习*，我们讨论了混淆矩阵、准确率、精确度分数、召回率和F1分数在测量基于分类的预测时的应用。我们也可以在我们的模型上重复使用这些指标。
- en: 'Since the model output is in the normalized decimal format between 0 and 1,
    we round it up to the nearest 0 or 1 integer to obtain the predicted binary classification
    labels:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型输出以0到1之间的标准化小数格式为基础，我们将其四舍五入到最接近的0或1整数，以获得预测的二元分类标签：
- en: '[PRE35]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `ravel()` command presents the result as a single list stored in the `pred_values` variable.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`ravel()`命令将结果呈现为存储在`pred_values`变量中的单个列表。'
- en: 'Compute and display the confusion matrix:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 计算并显示混淆矩阵：
- en: '[PRE36]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This produces the following output:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '![](Images/3df17841-5419-4db4-beda-87adfab81b81.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/3df17841-5419-4db4-beda-87adfab81b81.png)'
- en: 'Print the accuracy, precision score, recall score, and F1 score using the `sklearn`
    module:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`模块打印准确率、精确度分数、召回率和F1分数：
- en: '[PRE37]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The low recall score and the slightly below-average F1 score hint that our model
    is not sufficiently competitive. Perhaps we can visit historical metrics in the
    next section to find out more.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 低召回率和略低于平均水平的F1分数暗示我们的模型不够竞争力。也许我们可以在下一节中查看历史指标以了解更多信息。
- en: Displaying recorded events in Keras history
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras历史记录中显示记录的事件
- en: 'Let''s review the `callback_history` variable, which is the `History` object
    populated during the `fit()` command. The `History.history` attribute is a dictionary
    containing four keys, storing the accuracy and loss values during training and
    validation. These are represented as a list of values saved after every epoch.
    Extract this information into separate variables:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下`callback_history`变量，这是在`fit()`命令期间填充的`History`对象。`History.history`属性是一个包含四个键的字典，存储训练和验证期间的准确率和损失值。这些值被保存在每个时期之后。将这些信息提取到单独的变量中：
- en: '[PRE38]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Plot the training and validation loss with the following codes:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码绘制训练和验证损失：
- en: '[PRE39]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This produces the following graph of losses:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下损失图：
- en: '![](Images/09db1e3c-fd8d-4c82-99b5-29c17b5bba05.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/09db1e3c-fd8d-4c82-99b5-29c17b5bba05.png)'
- en: The solid line shows the path of the training loss decreasing as the number
    of epochs increases, meaning that our model is learning the training data better
    over time. The dashed line shows the validation loss increasing as the number
    of epochs increases, meaning that our model is not generalizing well enough on
    the validation set. These trends suggest that our model is prone to overfitting.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 实线显示了随着时期数的增加，训练损失在减少，这意味着我们的模型随着时间更好地学习训练数据。虚线显示了随着时期数的增加，验证损失在增加，这意味着我们的模型在验证集上的泛化能力不够好。这些趋势表明我们的模型容易过拟合。
- en: 'Plot the training and validation accuracy with the following code:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码绘制训练和验证准确率：
- en: '[PRE40]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This produces the following graph:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图形：
- en: '![](Images/ec25ea42-9a97-419f-8b9e-2418e1b09b75.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ec25ea42-9a97-419f-8b9e-2418e1b09b75.png)'
- en: The solid line shows the path of the training accuracy increasing as the number
    of epochs increases, while the dashed line shows the validation accuracy decreasing.
    These two graphs strongly suggest that our model is overfitting the training data.
    Looks like more work needs to be done! To prevent overfitting, you can use more
    training data, reduce the capacity of the network, add weight regularization,
    and/or use a dropout layer. In reality, deep learning modeling requires understanding
    the underlying problem, finding a suitable neural network architecture, and investigating
    the effects of activation functions at each layer in order to produce good results.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 实线显示了随着时期数量的增加，训练准确性增加的路径，而虚线显示了验证准确性的下降。这两个图表强烈暗示我们的模型正在过度拟合训练数据。看起来还需要做更多的工作！为了防止过度拟合，可以使用更多的训练数据，减少网络的容量，添加权重正则化，和/或使用一个丢失层。实际上，深度学习建模需要理解潜在问题，找到合适的神经网络架构，并调查每一层激活函数的影响，以产生良好的结果。
- en: Summary
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have been introduced to deep learning and the use of neural
    networks. An artificial neutral network consists of an input layer and an output
    layer, with one or more hidden layers in between. Each layer consists of artificial
    neurons, and each artificial neuron receives weighted inputs that are  summed
    together with a bias. An activation function transforms these inputs into an output,
    and feeds it as input to another neuron.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了深度学习和神经网络的使用。人工神经网络由输入层和输出层组成，在中间有一个或多个隐藏层。每一层都由人工神经元组成，每个人工神经元接收加权输入，这些输入与偏差相加。激活函数将这些输入转换为输出，并将其作为输入馈送到另一个神经元。
- en: Using the TensorFlow Python library, we built a deep learning model with four
    hidden layers to predict the prices of a security. The dataset is preprocessed
    by scaling and split into training and testing data. Designing an artificial neuron
    network involves two phases. The first phase is to assemble the graph, and the
    second phase is to train the model. A TensorFlow session object provides an execution
    environment, where training is done over several epochs, and each epoch uses mini-batch
    training. As the model output includes normalized values, we scale the data back
    to its original representation to return predicted prices.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow Python库，我们构建了一个具有四个隐藏层的深度学习模型，用于预测证券的价格。数据集经过缩放预处理，并分为训练和测试数据。设计人工神经网络涉及两个阶段。第一阶段是组装图形，第二阶段是训练模型。TensorFlow会话对象提供了一个执行环境，在那里训练在多个时期内进行，每个时期使用小批量训练。由于模型输出包括归一化值，我们将数据缩放回其原始表示以返回预测价格。
- en: Another popular deep learning library is Keras, utilizing TensorFlow as the
    backend. We built another deep learning model to predict credit card payment defaults
    with five hidden layers. Keras uses the concept of layers when working with models,
    and we saw how easy it was to add layers, configure the model, train it, and evaluate
    its performance. The `History` object of Keras records the loss and accuracy of
    training and validation data for successive epochs.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是另一个流行的深度学习库，利用TensorFlow作为后端。我们构建了另一个深度学习模型，用于预测信用卡支付违约，其中包括五个隐藏层。Keras在处理模型时使用层的概念，我们看到添加层、配置模型、训练和评估性能是多么容易。Keras的`History`对象记录了连续时期的训练和验证数据的损失和准确性。
- en: In reality, a good deep learning model requires effort and understanding the
    underlying problem in order to produce good results.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，一个良好的深度学习模型需要努力和理解潜在问题，以产生良好的结果。
