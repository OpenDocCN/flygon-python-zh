- en: Unsupervised Machine Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督机器学习算法
- en: 'This chapter is about unsupervised machine learning algorithms. The chapter
    starts with an introduction to unsupervised learning techniques. Then, we will
    learn about two clustering algorithms: k-means clustering and hierarchical clustering
    algorithms. The next section looks at a dimensionality reduction algorithm, which
    may be effective when we have a large number of input variables. The following
    section shows how unsupervised learning can be used for anomaly detection. Finally,
    we will look at one of the most powerful unsupervised learning techniques, association
    rules mining. This section also explains how patterns discovered from association
    rules mining represent interesting relationships between the various data elements
    across transactions that can help us in our data-driven decision making.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是关于无监督机器学习算法的。本章以介绍无监督学习技术开始。然后，我们将学习两种聚类算法：k均值聚类和层次聚类算法。接下来的部分将介绍一种降维算法，当我们有大量输入变量时可能会很有效。接下来的部分展示了无监督学习如何用于异常检测。最后，我们将看看最强大的无监督学习技术之一，关联规则挖掘。本节还解释了从关联规则挖掘中发现的模式如何代表跨交易中各种数据元素之间的有趣关系，这可以帮助我们进行基于数据的决策。
- en: By the end of this chapter, the reader should be able to understand how unsupervised
    learning can be used to solve some real-world problems. The reader will understand
    the basic algorithms and methodologies that are currently being used for unsupervised
    learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，读者应该能够理解无监督学习如何用于解决一些现实世界的问题。读者将了解目前用于无监督学习的基本算法和方法论。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Unsupervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Clustering algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类算法
- en: Dimensionality reduction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维
- en: Anomaly detection algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测算法
- en: Association rules mining
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联规则挖掘
- en: Introducing unsupervised learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍无监督学习
- en: 'The simplest definition of unsupervised learning is that it is the process
    of providing some sort of structure to unstructured data by discovering and utilizing
    the inherent patterns of the data.  If data is not produced by some random process,
    it will have some patterns between its data elements in its multidimensional problem
    space. Unsupervised learning algorithms work by discovering these patterns and
    using them to provide some structure to the dataset.  This concept is shown in
    the following diagram:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的最简单定义是，它是通过发现和利用数据的固有模式来为非结构化数据提供某种结构的过程。如果数据不是由某种随机过程产生的，它在其多维问题空间中的数据元素之间将具有一些模式。无监督学习算法通过发现这些模式并利用它们来为数据集提供一些结构。这个概念在下图中显示：
- en: '![](assets/32f60651-b8ff-4ec0-96fa-7b7145366467.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/32f60651-b8ff-4ec0-96fa-7b7145366467.png)'
- en: Note that unsupervised learning adds structure by discovering new features from
    the existing patterns.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，无监督学习通过发现现有模式的新特征来添加结构。
- en: Unsupervised learning in the data-mining life cycle
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据挖掘生命周期中的无监督学习
- en: 'To understand the role of unsupervised learning, it is important to first look
    at the overall life cycle of the data-mining process. There are different methodologies
    that divide the life cycle of the data-mining process into different independent
    stages, called  **phases**. Currently, there are two popular ways to represent
    the data-mining life cycle:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 理解无监督学习的作用，首先要看数据挖掘过程的整体生命周期。有不同的方法论将数据挖掘过程的生命周期划分为不同的独立阶段，称为**阶段**。目前，有两种流行的表示数据挖掘生命周期的方式：
- en: '**CRISP-DM**  (**Cross-Industry Standard Process for Data Mining**) life cycle'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CRISP-DM**（**跨行业标准数据挖掘过程**）生命周期'
- en: '**SEMMA**  (**Sample, Explore, Modify, Model, Access**) data-mining process'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SEMMA**（**样本、探索、修改、建模、访问**）数据挖掘过程'
- en: CRISP-DM was developed by a consortium of data miners who belonged to various
    companies, including Chrysler and  **SPSS**  (**Statistical Package for Social
    Science**). SEMMA was proposed by  **SAS** (**Statistical Analysis System**).
    Let's look at one of these two representations of the data-mining life cycle,
    CRISP-DM, and try to understand the place of unsupervised learning in the data-mining
    life cycle. Note that SEMMA has somewhat similar phases within its life cycle.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CRISP-DM是由一些数据挖掘者联合开发的，他们来自包括克莱斯勒和**SPSS**（**社会科学统计软件包**）在内的各种公司。SEMMA是由**SAS**（**统计分析系统**）提出的。让我们看看这两种数据挖掘生命周期的表示之一，CRISP-DM，并尝试理解无监督学习在数据挖掘生命周期中的位置。请注意，SEMMA在其生命周期内有一些类似的阶段。
- en: 'If we look at the CRISP-DM life cycle, we can see that it consists of six distinct
    phases, which are shown in the following figure:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看CRISP-DM生命周期，可以看到它包括六个不同的阶段，如下图所示：
- en: '![](assets/ff5d5601-03c9-465c-92dd-b830b77cd130.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ff5d5601-03c9-465c-92dd-b830b77cd130.png)'
- en: 'Let''s understand each phase one by one:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个了解每个阶段：
- en: '**Phase 1: Business Understanding**: This is about gathering the requirements
    and involves trying to fully understand the problem in depth from a business point
    of view. Defining the scope of the problem and properly rephrasing it according
    to  **machine learning**  (**ML**) is an important part of this phase—for example,
    for a binary classification problem, sometimes it is helpful to phrase the requirements
    in terms of a hypothesis that can be proved or rejected. This phase is also about
    documenting the expectations for the machine learning model that will be trained
    downstream in Phase 4—for example, for a classification problem, we need to document
    the minimum acceptable accuracy of the model that can be deployed in production.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阶段1：业务理解**：这是收集需求的阶段，涉及从业务角度深入全面地理解问题。根据**机器学习**（**ML**）的要求定义问题的范围，并适当地重新表述它是这个阶段的重要部分。例如，对于二元分类问题，有时将需求用可以证明或拒绝的假设来表述是有帮助的。本阶段还涉及记录将在下游阶段4中训练的机器学习模型的期望。例如，对于分类问题，我们需要记录可以部署到生产中的模型的最低可接受准确性。'
- en: It is important to note that Phase 1 of the CRISP-DM life cycle is about business
    understanding. It focuses on what needs to be done, not on how it will be done.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: CRISP-DM生命周期的第一阶段是业务理解，重点是需要做什么，而不是如何做。
- en: '**Phase 2: Data Understanding**:This is about understanding the data that is
    available for data mining. In this phase, we will find out whether the right datasets
    are available for the problem we are trying to solve. After identifying the datasets,
    we need to understand the quality of the data and its structure. We need to find
    out what patterns can be extracted out of the data that can potentially lead us
    toward important insights. We will also try to find the right feature that can
    be used as the label (or the target variable) according to the requirements gathered
    in Phase 1\. Unsupervised learning algorithms can play a powerful role in achieving
    the objectives of Phase 2\. Unsupervised algorithms can be used for the following
    purposes:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二阶段：数据理解**：这是关于理解可用于数据挖掘的数据。在这个阶段，我们将找出是否有适合解决问题的正确数据集。在确定数据集之后，我们需要了解数据的质量和结构。我们需要找出可以从数据中提取的模式，这些模式可能会引导我们获得重要的见解。我们还将尝试找到可以根据第一阶段收集的要求用作标签（或目标变量）的正确特征。无监督学习算法可以在实现第二阶段目标方面发挥强大作用。无监督算法可以用于以下目的：'
- en: To discover patterns in the dataset
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据集中发现模式
- en: To understand the structure of the dataset by analyzing the discovered patterns
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分析发现的模式来理解数据集的结构
- en: To identify or derive the target variable
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别或推导目标变量
- en: '**Phase 3: Data Preparation**:This is about preparing the data for the ML model
    that we will train in Phase 4\. The available labeled data is divided into two
    unequal parts. The larger portion is called the  **training data**  and is used
    for training the model downstream in Phase 4\. The smaller portion is called the  **testing
    data**  and is used in Phase 5 for model evaluation. In this phase, the unsupervised
    machine learning algorithms can be used as a tool to prepare the data—for example,
    they can be used to convert unstructured data into structured data, providing
    additional dimensions that can be helpful in training the model.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三阶段：数据准备**：这是为我们将在第四阶段训练的ML模型准备数据的阶段。可用的标记数据被分成两个不相等的部分。较大的部分称为**训练数据**，用于在第四阶段训练模型。较小的部分称为**测试数据**，在第五阶段用于模型评估。在这个阶段，无监督机器学习算法可以用作准备数据的工具，例如，它们可以用于将非结构化数据转换为结构化数据，提供可以帮助训练模型的额外维度。'
- en: '**Phase 4: Modeling**:  This is the phase where we use supervised learning
    to formulate the patterns that we have discovered. We are expected to successfully
    prepare the data according to the requirements of our chosen supervised learning
    algorithm. This is also the phase in which the particular feature that will be
    used as the label will be identified. In Phase 3, we divided the data into testing
    and training sets. In this phase, we form mathematical formulations to represent
    the relationships in our patterns of interest. This is done by training the model
    using the training data that was created in Phase 3\. As mentioned before, the
    resulting mathematical formulation will depend on our choice of algorithm.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第四阶段：建模**：这是我们使用监督学习来制定已发现模式的阶段。我们需要根据所选的监督学习算法的要求成功准备数据。这也是确定将用作标签的特定特征的阶段。在第三阶段，我们将数据分为测试集和训练集。在这个阶段，我们形成数学公式来表示我们感兴趣的模式中的关系。这是通过使用第三阶段创建的训练数据来训练模型完成的。如前所述，最终的数学公式将取决于我们选择的算法。'
- en: '**Phase 5: Evaluation**:This phase is about testing the newly trained model
    using the test data from Phase 3\. If the evaluation matches the expectations
    set in Phase 1, then we need iterate through all the preceding phases again, starting
    with Phase 1\. This is illustrated in the preceding image.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第五阶段：评估**：这个阶段是关于使用第三阶段的测试数据测试新训练的模型。如果评估符合第一阶段设定的期望，那么我们需要再次迭代所有前面的阶段，从第一阶段开始。这在前面的图像中有所说明。'
- en: '**Phase 6: Deployment**:If the evaluation meets or exceeds the expectations
    described in Phase 5, then the trained model is deployed in production and starts
    generating a solution to the problem we defined in Phase 1\.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第六阶段：部署**：如果评估符合或超过第五阶段描述的期望，那么训练好的模型将被部署到生产环境中，并开始为我们在第一阶段定义的问题提供解决方案。'
- en: Phase 2 (Data Understanding) and Phase 3 (Data Preparation) of the CRISP-DM
    life cycle are all about understanding the data and preparing it for training
    the model. These phases involve data processing. Some organizations employ specialists
    for this data engineering phase.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: CRISP-DM生命周期的第二阶段（数据理解）和第三阶段（数据准备）都是关于理解数据并为训练模型做准备。这些阶段涉及数据处理。一些组织为这个数据工程阶段雇佣专家。
- en: It is obvious that the process of suggesting a solution to a problem is fully
    data driven. A combination of supervised and unsupervised machine learning is
    used to formulate a workable solution. This chapter focuses on the unsupervised
    learning part of the solution.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，提出问题的解决方案的过程完全是数据驱动的。结合监督和无监督机器学习用于制定可行的解决方案。本章重点介绍解决方案的无监督学习部分。
- en: Data engineering comprises Phase 2 and Phase 3, and is the most time-consuming
    part of machine learning. It can take as much as 70% of the time and resources
    of a typical ML project. The unsupervised learning algorithms can play an important
    role in data engineering.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程包括第二阶段和第三阶段，是机器学习中最耗时的部分。它可能占据典型ML项目时间和资源的70%。无监督学习算法在数据工程中可以发挥重要作用。
- en: The following sections provide more details regarding unsupervised algorithms.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节提供了有关无监督算法的更多细节。
- en: Current research trends in unsupervised learning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习的当前研究趋势
- en: For years, research into machine learning algorithms was more focused on supervised
    learning techniques. As supervised learning techniques can be directly used for
    inference, their benefits in terms of time, cost, and accuracy are relatively
    easily measurable. The power of unsupervised machine learning algorithms has been
    recognized more  recently. As unsupervised learning is not guided, it is less
    dependent on assumptions and can potentially converge the solution in any dimension.
    Although it is more difficult to control the scope and processing requirements
    of unsupervised learning algorithms, they have more potential to unearth the hidden
    patterns. Researchers are also working to combine unsupervised machine learning
    techniques with supervised learning techniques in order to design new powerful
    algorithms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，对机器学习算法的研究更多地集中在监督学习技术上。由于监督学习技术可以直接用于推断，因此它们在时间、成本和准确性方面的优势相对容易衡量。无监督机器学习算法的潜力最近才被认识到。由于无监督学习不受指导，因此它不太依赖假设，并且可能在任何维度上收敛解决方案。尽管更难控制无监督学习算法的范围和处理要求，但它们有更多潜力发现隐藏的模式。研究人员还在努力将无监督机器学习技术与监督学习技术相结合，以设计新的强大算法。
- en: Practical examples
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际例子
- en: Currently, unsupervised learning is used to get a better sense of the data and
    provide it with more structure—for example, it is used in marketing segmentation,
    fraud detection, and market basket analysis (which is discussed later in this
    chapter). Let's look at a couple of examples.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，无监督学习用于更好地理解数据并为其提供更多结构，例如，它用于市场细分、欺诈检测和市场篮分析（稍后在本章中讨论）。让我们看几个例子。
- en: Voice categorization
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语音分类
- en: Unsupervised learning can be used to classify individual voices in a voice file.
    It uses the fact that each individual's voice has distinct characteristics, creating
    potentially separable audio patterns. These patterns can then be used for voice
    recognition—for example, Google uses this technique in their Google Home devices
    to train them to differentiate between different people's voices. Once trained,
    Google Home can personalize the response for each user individually.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可以用于对语音文件中的个别声音进行分类。它利用了每个人的声音具有独特的特征这一事实，从而创建可能可分离的音频模式。这些模式可以用于语音识别，例如，谷歌在其Google
    Home设备中使用这种技术来训练它们区分不同人的声音。一旦训练完成，Google Home可以个性化地为每个用户提供响应。
- en: 'For example, let''s assume that we have a recorded conversation of three people
    talking to each other for half an hour. Using unsupervised learning algorithms,
    we can identify the voices of distinct people in this dataset. Note that through
    unsupervised learning, we are adding structure to the given set of unstructured
    data. This structure gives us additional useful dimensions in our problem space
    that can be used to gain insights and to prepare data for our chosen machine learning
    algorithm. The following diagram shows how unsupervised learning is used for voice
    recognition:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一段录制的三个人互相交谈半个小时的对话。使用无监督学习算法，我们可以识别数据集中不同人的声音。请注意，通过无监督学习，我们为给定的非结构化数据集添加了结构。这种结构为我们的问题空间提供了额外有用的维度，可以用于获取见解并为我们选择的机器学习算法准备数据。以下图表显示了无监督学习用于语音识别的情况：
- en: '![](assets/71ed2127-a466-4865-b70e-6511b810bb98.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/71ed2127-a466-4865-b70e-6511b810bb98.png)'
- en: Note that, in this case, unsupervised learning suggests that we add a new feature
    with three distinct levels.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，无监督学习建议我们添加一个具有三个不同级别的新特征。
- en: Document categorization
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档分类
- en: 'Unsupervised machine learning algorithms can also be applied to a repository
    of unstructured textual data—for example, if we have a dataset of PDF documents,
    then unsupervised learning can be used to do the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习算法也可以应用于非结构化文本数据的存储库，例如，如果我们有一组PDF文档的数据集，那么无监督学习可以用于以下目的：
- en: Discover various topics in the dataset
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现数据集中的各种主题
- en: Associate each PDF document to one of the discovered topics
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个PDF文档与发现的主题之一关联起来
- en: 'This use of unsupervised learning for document classification is shown in the
    following figure. This is another example in which we are adding more structure
    to unstructured data:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习用于文档分类的情况如下图所示。这是另一个例子，我们在非结构化数据中添加了更多的结构：
- en: '![](assets/bafdb93e-edf1-4f72-a99e-dfd51fe936b8.png)Figure 6.4: Using unsupervised
    learning for document classification'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/bafdb93e-edf1-4f72-a99e-dfd51fe936b8.png)图6.4：使用无监督学习进行文档分类'
- en: Note that, in this case, unsupervised learning suggests that we add a new feature
    with five distinct levels.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，无监督学习建议我们添加一个具有五个不同级别的新特征。
- en: Understanding clustering algorithms
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类算法
- en: One of the simplest and most powerful techniques used in unsupervised learning
    is based on grouping similar patterns together through clustering algorithms.
    It is used to understand a particular aspect of the data that is related to the
    problem we are trying to solve. Clustering algorithms look for natural grouping
    in data items. As the group is not based on any target or assumptions, it is classified
    as an unsupervised learning technique.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中使用的最简单和最强大的技术之一是基于通过聚类算法将相似模式分组在一起。它用于理解与我们试图解决的问题相关的数据的特定方面。聚类算法寻找数据项中的自然分组。由于该组不是基于任何目标或假设，因此被归类为无监督学习技术。
- en: Groupings created by various clustering algorithms are based on finding the
    similarities between various data points in the problem space. The best way to
    determine the similarities between data points will vary from problem to problem
    and will depend on the nature of the problem we are dealing with. Let's look at
    the various methods that can be used to calculate the similarities between various
    data points.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 各种聚类算法创建的分组是基于在问题空间中找到各种数据点之间的相似性。确定数据点之间的相似性的最佳方法将因问题而异，并且将取决于我们正在处理的问题的性质。让我们看看可以用来计算各种数据点之间相似性的各种方法。
- en: Quantifying similarities
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化相似性
- en: 'The reliability of the grouping created by clustering algorithms is based on
    the assumption that we can accurately quantify the similarities or closeness between
    various data points in the problem space. This is done by using various distance  measures.
    The following are three of the most popular methods that are used to quantify
    similarities:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法创建的分组的可靠性是基于这样一个假设：我们能够准确量化问题空间中各种数据点之间的相似性或接近程度。这是通过使用各种距离度量来实现的。以下是用于量化相似性的三种最流行的方法：
- en: Euclidean distance measure
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离度量
- en: Manhattan distance measure
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曼哈顿距离度量
- en: Cosine distance measure
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦距离度量
- en: Let's look at these distance measures in more detail.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些距离度量。
- en: Euclidean distance
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'The distance between different points can quantify the similarity between two
    data points and is extensively used in unsupervised machine learning techniques,
    such as clustering. Euclidean distance is the most common and simple distance
    measure used. It is calculated by measuring the shortest distance between two
    data points in multidimensional space. For example, let''s consider two points,
    **A(1,1)** and **B(4,4)**, in a two -dimensional space, as shown in the following
    plot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 不同点之间的距离可以量化两个数据点之间的相似性，并且广泛用于无监督机器学习技术，如聚类。欧几里得距离是最常见和简单的距离度量。它通过测量多维空间中两个数据点之间的最短距离来计算。例如，让我们考虑二维空间中的两点**A(1,1)**和**B(4,4)**，如下图所示：
- en: '![](assets/7a13e022-8b59-44d6-a2de-f009d99da231.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/7a13e022-8b59-44d6-a2de-f009d99da231.png)'
- en: 'To calculate the distance between  **A**  and  **B**—that is *d(A,B)*, we can
    use the following Pythagorean formula:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算**A**和**B**之间的距离——即*d(A,B)*，我们可以使用以下毕达哥拉斯公式：
- en: '![](assets/b816650c-9590-4145-850e-b428499fbb7c.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b816650c-9590-4145-850e-b428499fbb7c.png)'
- en: 'Note that this calculation is for a two-dimensional problem space. For an *n*-dimensional
    problem space, we can calculate the distance between two points  **A**  and  **B**
    as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此计算是针对二维问题空间的。对于*n*维问题空间，我们可以计算两点**A**和**B**之间的距离如下：
- en: '![](assets/ab5364d1-0bdf-420c-bbfe-443875a74304.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ab5364d1-0bdf-420c-bbfe-443875a74304.png)'
- en: Manhattan distance
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 曼哈顿距离
- en: 'In many situations, measuring the shortest distance between two points using
    the Euclidean distance measure will not truly represent the similarity or closeness
    between two points—for example, if two data points represent locations on a map,
    then the actual distance from point A to point B using ground transportation,
    such as a car or taxi, will be more than the distance calculated by the Euclidean
    distance. For situations such as these, we use Manhattan distance, which marks
    the longest route between two points and is a better reflection of the closeness
    of two points in the context of source and destination points that can be traveled
    to in a busy city. The comparison between the Manhattan and Euclidean distance
    measures is shown in the following plot:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，使用欧几里得距离度量来测量两点之间的最短距离将无法真正代表两点之间的相似性或接近程度——例如，如果两个数据点代表地图上的位置，则使用地面交通工具（如汽车或出租车）从点A到点B的实际距离将大于欧几里得距离计算出的距离。对于这类情况，我们使用曼哈顿距离，它标记了两点之间的最长路线，并更好地反映了在繁忙城市中可以前往的源点和目的地点之间的接近程度。曼哈顿和欧几里得距离度量之间的比较如下图所示：
- en: '![](assets/2c74e092-8cbb-4082-84c4-f3adbdfc47e3.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2c74e092-8cbb-4082-84c4-f3adbdfc47e3.png)'
- en: Note that the Manhattan distance will always be equal or larger than the corresponding
    Euclidean distance calculated.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离始终大于或等于相应的欧几里得距离。
- en: Cosine distance
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 余弦距离
- en: 'Euclidean and Manhattan distance measures do not perform well in high-dimensional
    space. In a high-dimensional problem space, cosine distance more accurately reflects
    the closeness between two data points in a multidimensional problem space. The
    cosine distance measure is calculated by measuring the cosine angle created by
    two points connected to a reference point. If the data points are close, then
    the angle will be narrow, irrespective of the dimensions they have. On the other
    hand, if they are far away, then the angle will be large:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得和曼哈顿距离度量在高维空间中表现不佳。在高维问题空间中，余弦距离更准确地反映了多维问题空间中两个数据点之间的接近程度。余弦距离度量是通过测量由两个连接到参考点的点所创建的余弦角来计算的。如果数据点接近，则角度将很窄，而不管它们具有的维度如何。另一方面，如果它们相距很远，那么角度将很大：
- en: '![](assets/d5d492a0-e445-4361-80c4-7a92031aebd7.png)Textual data can almost
    be considered a highly dimensional space. As the cosine distance measure works
    very well with h-dimensional spaces, it is a good choice when dealing with textual
    data.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d5d492a0-e445-4361-80c4-7a92031aebd7.png)文本数据几乎可以被视为高维空间。由于余弦距离度量在高维空间中表现非常好，因此在处理文本数据时是一个不错的选择。'
- en: Note that in the preceding figure, the cosine of the angle between  **A(2,5)**
    and **B(4.4)** is the cosine distance. The reference between these points is the
    origin—that is, **X(0,0)**. But in reality, any point in the problem space can
    act as the reference data point, and it does not have to be the origin.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的图中，**A(2,5)**和**B(4.4)**之间的角的余弦是余弦距离。这些点之间的参考点是原点——即**X(0,0)**。但实际上，问题空间中的任何点都可以充当参考数据点，并且不一定是原点。
- en: K-means clustering algorithm
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K均值聚类算法
- en: The name of the k-means clustering algorithm comes from the fact that it tries
    to create a number of clusters, *k*,calculating the means to find the closeness
    between the data points. It uses a relatively simple clustering approach, but
    is still popular because of its scalability and speed. Algorithmically, k-means
    clustering uses an iterative logic that moves the centers of the clusters until
    they reflect the most representative data point of the grouping they belong to.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类算法的名称来自于它试图创建*k*个聚类，通过计算均值来找到数据点之间的接近程度。它使用了一个相对简单的聚类方法，但由于其可扩展性和速度而仍然受欢迎。从算法上讲，k-means聚类使用了一个迭代逻辑，将聚类的中心移动到它们所属的分组的最具代表性的数据点。
- en: It is important to note that k-means algorithms lack one of the very basic functionalities
    needed for clustering. That missing functionality is that for a given dataset,
    the k-means algorithm cannot determine the most appropriate number of clusters.
    The most appropriate number of clusters, *k*, is dependent on the number of natural
    groupings in a particular dataset. The philosophy behind this omission is to keep
    the algorithm as simple as possible, maximizing its performance. This lean-and-mean
    design makes k-means suitable for larger datasets. The assumption is that an external
    mechanism will be used to calculate *k*. The best way to determine *k* will depend
    on the problem we are trying to solve. In some cases, *k* is directly specified
    by the clustering problem's context—for example, if we want to divide a class
    of data-science students into two clusters, one consisting of the students with
    the data science skill and the other with programming skills, then *k* will be
    two. In some other problems, the value of *k* may not be obvious. In such cases,
    an iterative trial-and-error procedure or a heuristic-based algorithm will have
    to be used to estimate the most appropriate number of clusters for a given dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，k-means算法缺乏聚类所需的非常基本的功能之一。这个缺失的功能是，对于给定的数据集，k-means算法无法确定最合适的聚类数。最合适的聚类数*k*取决于特定数据集中自然分组的数量。这种省略背后的哲学是尽可能简化算法，最大限度地提高其性能。这种精益简洁的设计使k-means适用于更大的数据集。假设将使用外部机制来计算*k*。确定*k*的最佳方法将取决于我们试图解决的问题。在某些情况下，*k*直接由聚类问题的上下文指定，例如，如果我们想将一类数据科学学生分成两个聚类，一个由具有数据科学技能的学生组成，另一个由具有编程技能的学生组成，那么*k*将为2。在其他一些问题中，*k*的值可能不明显。在这种情况下，将不得不使用迭代的试错程序或基于启发式的算法来估计给定数据集的最合适的聚类数。
- en: The logic of k-means clustering
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means聚类的逻辑
- en: This section describes the logic of the k-means clustering algorithm. Let's
    look at them one by one.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了k-means聚类算法的逻辑。让我们逐一看一下。
- en: Initialization
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化
- en: In order to group them, the k-means algorithm uses a distance measure to find
    the similarity or closeness between data points. Before using the k-means algorithm,
    the most appropriate distance measure needs to be selected. By default, the Euclidean
    distance measure will be used. Also, if the dataset has outliers, then a mechanism
    needs to be devised to determine the criteria that are to be identified and remove
    the outliers of the dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '为了对它们进行分组，k-means算法使用距离度量来找到数据点之间的相似性或接近程度。在使用k-means算法之前，需要选择最合适的距离度量。默认情况下，将使用欧氏距离度量。此外，如果数据集中有异常值，则需要制定机制来确定要识别和删除数据集的异常值的标准。 '
- en: The steps of the k-means algorithm
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means算法的步骤
- en: 'The steps involved in the k-means clustering algorithm are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类算法涉及的步骤如下：
- en: '| Step 1 | We choose the number of clusters, *k*. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 步骤1 | 我们选择聚类的数量*k*。|'
- en: '| Step 2 | Among the data points, we randomly choose *k* points as cluster
    centers. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 步骤2 | 在数据点中，我们随机选择*k*个点作为聚类中心。|'
- en: '| Step 3 | Based on the selected distance measure, we iteratively compute the
    distance from each point in the problem space to each of the *k* cluster centers.
    Based on the size of the dataset, this may be a time-consuming step—for example,
    if there are 10,000 points in the cluster and *k* = 3, this means that 30,000
    distances need to be calculated. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 步骤3 | 基于所选的距离度量，我们迭代地计算问题空间中每个点到*k*个聚类中心的距离。根据数据集的大小，这可能是一个耗时的步骤，例如，如果聚类中有10,000个点，*k*=3，这意味着需要计算30,000个距离。|'
- en: '| Step 4 | We assign each data point in the problem space to the nearest cluster
    center. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 步骤4 | 我们将问题空间中的每个数据点分配给最近的聚类中心。|'
- en: '| Step 5 | Now each data point in our problem space has an assigned cluster
    center. But we are not done, as the selection of the initial cluster centers was
    based on random selection. We need to verify that the current randomly selected
    cluster centers are actually the center of gravity of each cluster. We recalculate
    the cluster centers by computing the mean of the constituent data points of each
    of the *k* clusters. This step explains why this algorithm is called k-means.
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 步骤5 | 现在我们问题空间中的每个数据点都有一个分配的聚类中心。但我们还没有完成，因为初始聚类中心的选择是基于随机选择的。我们需要验证当前随机选择的聚类中心实际上是每个聚类的重心。我们通过计算每个*k*聚类的组成数据点的平均值来重新计算聚类中心。这一步解释了为什么这个算法被称为k-means。|'
- en: '| Step 6 | If the cluster centers have shifted in step 5, this means that we
    need to recompute the cluster assignment for each data point. For this, we will
    go back to step  3 to repeat that compute-intensive step. If the cluster centers
    have not shifted or if our predetermined stop condition (for example, the number
    of maximum iterations) has been satisfied, then we are done. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 步骤6 | 如果在步骤5中聚类中心发生了变化，这意味着我们需要重新计算每个数据点的聚类分配。为此，我们将回到步骤3重复这个计算密集的步骤。如果聚类中心没有发生变化，或者我们的预定停止条件（例如，最大迭代次数）已经满足，那么我们就完成了。|'
- en: 'The following figure shows the result of running the k-means algorithm in a
    two-dimensional problem space:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了在二维问题空间中运行k-means算法的结果：
- en: '![](assets/70ce3a57-73b1-4c76-97fe-fdbcc956e36d.png)(a) Data points before
    clustering; (b) Resultant clusters after running the k-means clustering algorithm'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/70ce3a57-73b1-4c76-97fe-fdbcc956e36d.png)（a）聚类前的数据点；（b）运行k均值聚类算法后的结果集群'
- en: Note that the two resulting clusters created after running k-means are well
    differentiated in this case.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在运行k均值后创建的两个结果集群在这种情况下有很好的区分度。
- en: Stop condition
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停止条件
- en: 'For the k-means algorithm, the default stop condition is when there is no more
    shifting of cluster centers in step 5\. But as with many other algorithms, k-means
    algorithms may take lot of time to converge, especially while processing large
    datasets in a high-dimensional problem space. Instead of waiting for the algorithm
    to converge, we can also explicitly define the stop condition as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于k均值算法，默认的停止条件是在第5步中不再移动集群中心。但是与许多其他算法一样，k均值算法可能需要很长时间才能收敛，特别是在处理高维问题空间中的大型数据集时。我们可以明确定义停止条件，而不是等待算法收敛，如下所示：
- en: 'By specifying the maximum execution time:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过指定最大执行时间：
- en: '**Stop condition**:  *t>t[max]*, where  *t*  is the current execution time
    and  *t[max]*  is the  maximum execution time we have set for the algorithm.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停止条件**：*如果t>t[max]*，其中*t*是当前执行时间，*t[max]*是我们为算法设置的最大执行时间。'
- en: 'By specifying the maximum iterations:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过指定最大迭代次数：
- en: '**Stop condition**:  *if m>m[max]*, where *m* is the current iteration and  *m*[*max*]
    is the maximum number of iterations we have set for the algorithm.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停止条件**：*如果m>m[max]*，其中*m*是当前迭代次数，*m[max]*是我们为算法设置的最大迭代次数。'
- en: Coding the k-means algorithm
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写k均值算法
- en: 'Let''s look at how we can code the k-means algorithm in Python:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中编写k均值算法：
- en: 'First, let''s import the packages that we will need to code for the k-means
    algorithm. Note that we are importing the `sklearn`  package for k-means clustering:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入编写k均值算法所需的软件包。请注意，我们正在导入`sklearn`软件包进行k均值聚类：
- en: '[PRE0]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To use k-means clustering, let''s create 20 data points in a two-dimensional
    problem space that we will be using for k-means clustering:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用k均值聚类，让我们在二维问题空间中创建20个数据点，这些数据点将用于k均值聚类：
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s have two clusters (*k* = 2) and then create the cluster by calling the
    `fit` functions:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们有两个集群（*k*=2），然后通过调用`fit`函数创建集群：
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s create a variable named `centroid` that is an array that holds the location
    of the center of the clusters formed. In our case, as *k* = 2, the array will
    have a size of 2\. Let''s also create another variable named `label` that represents
    the assignment of each data point to one of the two clusters. As there are 20
    data points, this array will have a size of 20:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个名为`centroid`的变量，它是一个包含形成的集群中心位置的数组。在我们的情况下，*k*=2，数组的大小将为2。让我们还创建另一个名为`label`的变量，表示每个数据点分配给两个集群中的一个。由于有20个数据点，这个数组的大小将为20：
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let''s print these two arrays, `centroids` and `labels`:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们打印这两个数组，`centroids`和`labels`：
- en: '![](assets/1bd564b9-9201-40d0-8cb3-4c7109677fdd.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1bd564b9-9201-40d0-8cb3-4c7109677fdd.png)'
- en: Note that the first array shows the assignment of the cluster with each data
    point and the second one shows the two cluster centers.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一个数组显示了每个数据点与集群的分配，第二个数组显示了两个集群中心。
- en: 'Let''s plot and look at the clusters using `matplotlib`:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`matplotlib`绘制并查看这些集群：
- en: '![](assets/6cc48dd3-517d-4311-a93e-5f3e206d7f1f.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6cc48dd3-517d-4311-a93e-5f3e206d7f1f.png)'
- en: Note that the bigger dots in the plot are the centroids as determined by the
    k-means algorithm.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，图中的较大点是由k均值算法确定的中心点。
- en: Limitation of k-means clustering
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k均值聚类的局限性
- en: 'The k-means algorithm is designed to be a simple and fast algorithm. Because
    of the intentional simplicity in its design, it comes with the following limitations:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: k均值算法旨在成为一种简单快速的算法。由于其设计上的故意简单性，它具有以下限制：
- en: The biggest limitation of k-means clustering is that the initial number of clusters
    has to be predetermined.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k均值聚类的最大限制是初始集群数量必须预先确定。
- en: The initial assignment of cluster centers is random. This means that each time
    the algorithm is run, it may give slightly different clusters.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中心的初始分配是随机的。这意味着每次运行算法时，可能会得到略有不同的集群。
- en: Each data point is assigned to only one cluster.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据点只分配给一个集群。
- en: k-means clustering is sensitive to outliers.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k均值聚类对异常值敏感。
- en: Hierarchical clustering
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: k-means clustering uses a top-down approach because we start the algorithm from
    the most important data points, which are the cluster centers. There is an alternative
    approach of clustering where, instead of starting from the top, we start the algorithm
    from the bottom. The bottom in this context is each of the individual data points
    in the problem space. The solution is to keep on grouping similar data points
    together as it progresses up toward the cluster centers. This alternative bottom-up
    approach is used by hierarchical clustering algorithms, and is discussed in this
    section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: k均值聚类使用自上而下的方法，因为我们从最重要的数据点开始算法，即集群中心。还有一种聚类的替代方法，即不是从顶部开始，而是从底部开始算法。在这种情况下，底部是问题空间中的每个单独数据点。解决方案是在向上移向集群中心的过程中不断将相似的数据点分组在一起。这种替代的自下而上方法由层次聚类算法使用，并在本节中讨论。
- en: Steps of hierarchical clustering
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类的步骤
- en: 'The following steps are involved in hierarchical clustering:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类涉及以下步骤：
- en: We create a separate cluster for each data point in our problem space. If our
    problem space consists of 100 data points, then it will start with 100 clusters.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在问题空间中为每个数据点创建一个单独的集群。如果我们的问题空间包含100个数据点，那么它将从100个集群开始。
- en: We group only those points that are closest to each other.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只将彼此最接近的点分组。
- en: We check for the stop condition; if the stop condition is not yet satisfied,
    then we repeat step 2.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查停止条件；如果停止条件尚未满足，则重复步骤2。
- en: The resulting clustered structure is called a **dendrogram**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的集群结构称为**树状图**。
- en: 'In a dendrogram, the height of the vertical lines determines how close the
    items are, as shown in the following diagram:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在树状图中，垂直线的高度决定了物品的接近程度，如下图所示：
- en: '![](assets/5557743e-1874-453f-b274-47d2f3ca06c7.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/5557743e-1874-453f-b274-47d2f3ca06c7.png)'
- en: Note that the stopping condition is shown as a dotted line in the preceding
    figure.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，停止条件显示为上图中的虚线。
- en: Coding a hierarchical clustering algorithm
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写一个分层聚类算法
- en: 'Let''s learn how we can code a hierarchical algorithm in Python:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何在Python中编写一个分层算法：
- en: 'We will first import `AgglomerativeClustering` from the `sklearn.cluster` library,
    along with the `pandas` and `numpy` packages:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先从`sklearn.cluster`库中导入`AgglomerativeClustering`，以及`pandas`和`numpy`包：
- en: '[PRE4]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then we will create 20 data points in a two-dimensional problem space:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将在二维问题空间中创建20个数据点：
- en: '[PRE5]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then we create the hierarchical cluster by specifying the hyperparameters.
    We use the `fit_predict` function to actually process the algorithm:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过指定超参数来创建分层集群。我们使用`fit_predict`函数来实际处理算法：
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let''s look at the association of each data point to the two clusters that
    were created:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们看一下每个数据点与创建的两个簇的关联：
- en: '![](assets/055dc165-fddc-4091-b73f-259376a09a82.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/055dc165-fddc-4091-b73f-259376a09a82.png)'
- en: You can see that the cluster assignment for both hierarchical and k-means algorithms
    are very similar.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到分层和k均值算法的集群分配非常相似。
- en: Evaluating the clusters
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估聚类
- en: 'The objective of good quality clustering is that the data points that belong
    to the separate clusters  should be differentiable. This implies the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 良好质量的聚类的目标是属于不同簇的数据点应该是可区分的。这意味着以下内容：
- en: The data points that belong to the same cluster should be as similar as possible.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属于同一簇的数据点应尽可能相似。
- en: Data points that belong to separate clusters should be as different as possible.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属于不同簇的数据点应尽可能不同。
- en: 'Human intuition can be used to evaluate the clustering results by visualizing
    the clusters, but there are mathematical methods that can quantify the quality
    of the clusters.  Silhouette analysis is one such technique that compares the
    tightness and separation in the clusters created by the k-means algorithm. The
    silhouette draws a plot that displays the closeness each point in a particular
    cluster has with respect to the other points in the neighboring clusters. It associates
    a number in the range of [-0, 1] with each cluster. The following table shows
    what the figures in this range signify:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 人类直觉可以用来通过可视化集群结果来评估集群结果，但也有数学方法可以量化集群的质量。轮廓分析是一种比较k均值算法创建的集群中的紧密度和分离度的技术。轮廓绘制了一个图，显示了特定集群中每个点与相邻集群中其他点的接近程度。它将与每个集群关联的数字范围为[-0,
    1]。以下表显示了此范围中的数字表示什么：
- en: '| **Range** | Meaning | Description |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **范围** | 意义 | 描述 |'
- en: '| 0.71–1.0 | Excellent | This means that the k-means clustering resulted in
    groups that are quite differentiable from each other. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 0.71–1.0 | 优秀 | 这意味着k均值聚类导致的组在相当程度上是可区分的。|'
- en: '| 0.51–0.70 | Reasonable | This means that the k-means clustering resulted
    in groups that are somewhat differentiable from each other. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 0.51–0.70 | 合理 | 这意味着k均值聚类导致的组在某种程度上是可区分的。|'
- en: '| 0.26–0.50 | Weak | This means that the k-means clustering resulted in grouping,
    but the quality of the grouping should not be relied upon. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 0.26–0.50 | 弱 | 这意味着k均值聚类导致了分组，但不应依赖分组的质量。|'
- en: '| <0.25 | No clustering has been found | Using the parameters selected and
    the data used, it was not possible to create grouping using k-means clustering.
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| <0.25 | 未找到任何聚类 | 使用选择的参数和使用的数据，无法使用k均值聚类创建分组。|'
- en: Note that each cluster in the problem space will get a separate score.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，问题空间中的每个簇将获得一个单独的分数。
- en: Application of clustering
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类的应用
- en: Clustering is used wherever we needed to discover the underlying patterns in
    datasets.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类用于我们需要在数据集中发现潜在模式的地方。
- en: 'In government use cases, clustering can be used for the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在政府使用案例中，聚类可用于以下目的：
- en: Crime-hotspot analysis
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 犯罪热点分析
- en: Demographic social analysis
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人口社会分析
- en: 'In market research, clustering can be used for the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在市场研究中，聚类可用于以下目的：
- en: Market segmentation
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场细分
- en: Targeted advertisements
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定向广告
- en: Customer categorization
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户分类
- en: '**Principal component analysis** (**PCA**) is also used for generally exploring
    the data and removing noise from real-time data, such as stock-market trading.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）也用于通常探索数据并从实时数据中去除噪音，例如股票市场交易。
- en: Dimensionality reduction
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: 'Each feature in our data corresponds to a dimension in our problem space. Minimizing
    the number of features to make our problem space simpler is called **dimensionality
    reduction**. It can be done in one of the following two ways:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据中的每个特征对应于问题空间中的一个维度。将特征的数量最小化以使问题空间更简单称为降维。可以通过以下两种方式之一来完成：
- en: '**Feature selection**: Selecting a set of features that are important in the
    context of the problem we are trying to solve'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择：选择在我们试图解决的问题的上下文中重要的一组特征
- en: '**Feature aggregation**: Combining two or more features to reduce dimensions
    using one of the following algorithms:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征聚合：使用以下算法之一组合两个或多个特征以减少维度：
- en: '**PCA**: A linear unsupervised ML algorithm'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA：线性无监督ML算法
- en: '**Linear discriminant analysis**  (**LDA**): A linear supervised ML algorithm'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）：线性监督ML算法
- en: '**Kernel principal component analysis**: A nonlinear algorithm'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核主成分分析：一种非线性算法
- en: Let's look deeper at one of the popular dimensionality reduction algorithms,
    namely PCA, in more detail.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解一种流行的降维算法，即PCA。
- en: Principal component analysis
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: 'PCA is an unsupervised machine learning technique that can be used to  reduce
    dimensions using linear transformation. In the following figure, we can see two
    principle components,  **PC1**  and  **PC2**, which show the shape of the spread
    of the data points.  PC1 and  PC2 can be used to summarize the data points with
    appropriate coefficients:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种无监督的机器学习技术，可以使用线性变换来降低维度。在下图中，我们可以看到两个主成分**PC1**和**PC2**，它们显示了数据点的分布形状。PC1和PC2可以用适当的系数来总结数据点：
- en: '![](assets/2a037f1f-58b8-4a7b-9a9b-8ad91808dfcb.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2a037f1f-58b8-4a7b-9a9b-8ad91808dfcb.png)'
- en: 'Let''s consider the following code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下代码：
- en: '[PRE7]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now let''s print the coefficients of our PCA model:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印我们的PCA模型的系数：
- en: '![](assets/07d52f65-eb9d-4f86-b9c0-01ab5f62c25b.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/07d52f65-eb9d-4f86-b9c0-01ab5f62c25b.png)'
- en: Note that the original DataFrame has four features, `Sepal.Length`, `Sepal.Width`,
    `Petal.Length`, and `Petal.Width`. The preceding DataFrame specifies the coefficients
    of the four principal components, PC1, PC2, PC3, and PC4—for example, the first
    row specifies the coefficients of PC1 that can be used to replace the original
    four variables.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，原始的DataFrame有四个特征，`Sepal.Length`、`Sepal.Width`、`Petal.Length`和`Petal.Width`。前面的DataFrame指定了四个主成分PC1、PC2、PC3和PC4的系数，例如，第一行指定了可以用来替换原始四个变量的PC1的系数。
- en: 'Based on these coefficients, we can calculate the PCA components for our input
    DataFrame X:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些系数，我们可以计算我们输入DataFrame X的PCA组件：
- en: '[PRE8]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let''s print X after the calculation of the PCA components:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在计算PCA组件后打印X：
- en: '![](assets/3d1712cf-5e6f-407c-81c9-b2c66a593b73.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3d1712cf-5e6f-407c-81c9-b2c66a593b73.png)'
- en: 'Now let''s print the variance ratio and try to understand the implications
    of using PCA:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印方差比率，并尝试理解使用PCA的影响：
- en: '![](assets/808e5134-ae27-41c7-82fb-59d557f5eccd.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/808e5134-ae27-41c7-82fb-59d557f5eccd.png)'
- en: 'The variance ratio indicates the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 方差比率表示如下：
- en: If we choose to replace the original four features with PC1, then we will be
    able to capture about 92.3% of the variance of the original variables. We will
    introduce some approximations by not capturing 100% of the variance of the original
    four features.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们选择用PC1替换原始的四个特征，那么我们将能够捕获大约92.3%的原始变量的方差。我们通过不捕获原始四个特征100%的方差来引入一些近似。
- en: If we choose to replace the original four features with PC1 and PC2, then we
    will capture an additional 5.3 % of the variance of the original variables.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们选择用PC1和PC2替换原始的四个特征，那么我们将捕获额外的5.3%的原始变量的方差。
- en: If we choose to replace the original four features with PC1, PC2, and PC3, then
    we will now capture a further 0.017 % of the variance of the original variables.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们选择用PC1、PC2和PC3替换原始的四个特征，那么我们现在将捕获原始变量进一步的0.017%的方差。
- en: If we choose to replace the original four features with four principal components,
    then we will capture 100% of the variance of the original variables (92.4 + 0.053
    + 0.017 + 0.005), but replacing four original features with four principal components
    is meaningless as we did not reduce the dimensions at all and achieved nothing.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们选择用四个主成分替换原始的四个特征，那么我们将捕获原始变量的100%的方差（92.4 + 0.053 + 0.017 + 0.005），但用四个主成分替换四个原始特征是没有意义的，因为我们没有减少维度，也没有取得任何成果。
- en: Limitations of PCA
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA的局限性
- en: 'The following are the limitations of PCA:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的局限性如下：
- en: PCA can only be used for continuous variables and is not relevant for category
    variables.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA只能用于连续变量，对于类别变量无关。
- en: While aggregating, PCA approximates the component variables; it simplifies the
    problem of dimensionality at the expense of accuracy. This trade-off should be
    carefully studied before using PCA.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在聚合时，PCA近似了组件变量；它以准确性为代价简化了维度的问题。在使用PCA之前，应该仔细研究这种权衡。
- en: Association rules mining
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关联规则挖掘
- en: 'Patterns in a particular dataset are the treasure that needs to be discovered,
    understood, and mined for the information they contain. There is an important
    set of algorithms that try to focus on the pattern analysis in a given dataset.
    One of the more popular algorithms in this class of algorithm is called the **association
    rules mining** algorithm, which provides us with the following capabilities:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 特定数据集中的模式是需要被发现、理解和挖掘的宝藏。有一组重要的算法试图专注于给定数据集中的模式分析。在这类算法中，较受欢迎的算法之一称为**关联规则挖掘**算法，它为我们提供了以下功能：
- en: The ability to measure the frequency of a pattern
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量模式频率的能力
- en: The ability to establish *cause*-and-*effect* relationship among the patterns.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立模式之间*因果*关系的能力。
- en: The ability to quantify the usefulness of patterns by comparing their accuracy
    to random guessing
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将它们的准确性与随机猜测进行比较，量化模式的有用性
- en: Examples of use
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用示例
- en: 'Association rules mining  is used when we are trying to investigate the cause-and-effect
    relationships between different variables of a dataset. The following are example
    questions that it can help to answer:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们试图调查数据集中不同变量之间的因果关系时，使用关联规则挖掘。以下是它可以帮助回答的示例问题：
- en: Which values of humidity, cloud cover, and temperature can lead to rain tomorrow?
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些湿度、云层覆盖和温度值可能导致明天下雨？
- en: What type of insurance claim can indicate fraud?
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么类型的保险索赔可能表明欺诈？
- en: What combinations of medicine may lead to complications for patients?
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些药物的组合可能会导致患者并发症？
- en: Market basket analysis
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 市场篮分析
- en: In this book, recommendation engines are discussed in  [Chapter 8](e1eec5e4-0365-4aeb-94d7-2e3ae02fc18c.xhtml),  *Neural
    Network Algorithms*. Basket analysis is a simpler way of learning recommendations.
    In basket analysis, our data contains only the information regarding what items
    were bought together. It does not have any information about the user or whether
    the user enjoyed individual items. Note that it is much easier to get this data
    than it is to get ratings data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，推荐引擎在[第8章](e1eec5e4-0365-4aeb-94d7-2e3ae02fc18c.xhtml)“神经网络算法”中进行了讨论。篮子分析是学习推荐的一种简单方法。在篮子分析中，我们的数据只包含有关哪些物品一起购买的信息。它没有任何关于用户或用户是否喜欢个别物品的信息。请注意，获取这些数据要比获取评级数据容易得多。
- en: 'For example, this kind of data is generated when we shop at Walmart, and no
    special technique is required to get the data. This data, when collected over
    a period of time, is called  **transnational**  **data**. When association rules
    analysis is applied to transnational data sets of the shopping carts being used
    in convenience stores, supermarkets, and fast-food chains, it is called  **market
    basket analysis**. It measures the conditional probability of buying a set of
    items together, which helps to answer the following questions:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们在沃尔玛购物时，就会产生这种数据，而不需要任何特殊技术来获取数据。这些数据在一段时间内收集起来，被称为**交易数据**。当将关联规则分析应用于便利店、超市和快餐连锁店中使用的购物车的交易数据集时，就称为**市场篮子分析**。它衡量了一组物品一起购买的条件概率，有助于回答以下问题：
- en: What is the optimal placement of items on the shelf?
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 货架上物品的最佳摆放位置是什么？
- en: How should the items appear in the marketing catalog?
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物品在营销目录中应该如何出现？
- en: What should be recommended, based on a user's buying patterns?
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于用户的购买模式，应该推荐什么？
- en: As market basket analysis can estimate how items are related to each other,
    it is often used for mass-market retail, such as supermarkets, convenience stores,
    drug stores, and fast-food chains. The advantage of market basket analysis is
    that the results are almost self-explanatory, which means that they are easily
    understood by the business users.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于市场篮子分析可以估计物品之间的关系，因此它经常用于大众市场零售，如超市、便利店、药店和快餐连锁店。市场篮子分析的优势在于其结果几乎是不言自明的，这意味着它们很容易被业务用户理解。
- en: Let's look at a typical superstore. All the unique items that are available
    in the store can be represented by a set, ![](assets/ce31fde4-ba41-4fa9-bb71-4c63b428aec9.png)  =
    {item  [1]  , item  [2]  , . . . , item  [m]  }. So, if that superstore is selling
    500 distinct items, then ![](assets/cd637cd5-f4fc-4481-8df6-6dc079d692cb.png)
    will be a set of size 500.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个典型的超市。商店中所有可用的唯一物品可以用一个集合![](assets/ce31fde4-ba41-4fa9-bb71-4c63b428aec9.png)={item[1]，item[2]，...，item[m]}来表示。因此，如果那家超市销售500种不同的物品，那么![](assets/cd637cd5-f4fc-4481-8df6-6dc079d692cb.png)将是一个大小为500的集合。
- en: People will buy items from this store. Each time someone buys an item and pays
    at the counter, it is added to a set of the items in a particular transaction,
    called an  **itemset**. In a given period of time, the transactions are grouped
    together in a set represented by ![](assets/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png),
    where ![](assets/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png)  = {t  [1]  ,t  [2]  ,
    . . . ,t  [n] }.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 人们会从这家商店购买物品。每当有人购买物品并在柜台付款时，它就会被添加到一个特定交易中的物品集合中，称为**项目集**。在一段时间内，交易被分组在一个由![](assets/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png)表示的集合中，其中![](assets/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png)={t[1],t[2],...,t[n]}。
- en: 'Let''s look at the following simple transaction data consisting of only four
    transactions. These transactions are summarized in the following table:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下只包含四个交易的简单交易数据。这些交易总结在下表中：
- en: '| t1 | Wickets, pads |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| t1 | 球门，护腕 |'
- en: '| t2 | Bat, wickets, pads, helmet |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| t2 | 球棒，球门，护腕，头盔 |'
- en: '| t3 | Helmet, ball |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| t3 | 头盔，球 |'
- en: '| t4 | Bat, pads, helmet |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| t4 | 球棒、护腕、头盔 |'
- en: 'Let''s look at this example in more detail:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下这个例子：
- en: '![](assets/ce31fde4-ba41-4fa9-bb71-4c63b428aec9.png)  = {bat  , wickets,  pads,
    helmet, ball  }, which represents all the unique items available at the store.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/ce31fde4-ba41-4fa9-bb71-4c63b428aec9.png)={球棒，球门，护腕，头盔，球}，它代表了商店中所有可用的唯一物品。'
- en: Let's consider one of the transactions, t3, from ![](assets/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png).
    Note that items bought in t3 can be represented in the itemset[t3]= {helmet,ball},
    which indicates that a customer purchased two items. As there are two items in
    this itemset, the size of itemset[t5] is said to be two.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们考虑来自![](assets/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png)的一个交易t3。请注意，t3中购买的物品可以用itemset[t3]={头盔，球}表示，这表明顾客购买了两件物品。由于这个itemset中有两件物品，因此itemset[t5]的大小被称为两。 '
- en: Association rules
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关联规则
- en: An association rule mathematically describes the relationship items involved
    in various transactions. It does this by investigating the relationship between
    two itemsets in the form *X* ⇒ *Y*, where *X* ⊂![](assets/4a0f1bae-4e77-4c84-86a7-6691729c3e57.png),
    *Y* ⊂![](assets/1735cf94-543b-47a9-b5b8-24cec3a6055c.png). In addition, *X* and
    *Y* are nonoverlapping itemsets; which means that ![](assets/25efda0c-53d6-46dc-b473-443ee7bc0dfa.png)  .
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则通过数学方式描述了各种交易中涉及的物品之间的关系。它通过研究形式为*X*⇒*Y*的两个项目集之间的关系来实现这一点，其中*X*⊂![](assets/4a0f1bae-4e77-4c84-86a7-6691729c3e57.png)，*Y*⊂![](assets/1735cf94-543b-47a9-b5b8-24cec3a6055c.png)。此外，*X*和*Y*是不重叠的项目集；这意味着![](assets/25efda0c-53d6-46dc-b473-443ee7bc0dfa.png)。
- en: 'An association rule could be described in the following form:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则可以用以下形式描述：
- en: '{helmet,balls}⇒  {bike}'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '{头盔，球}⇒{自行车}'
- en: Here, {helmet,ball} is *X* and {ball} is *Y*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，{头盔，球}是*X*，{球}是*Y*。
- en: Types of rule
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规则类型
- en: 'Running associative analysis algorithms will typically result in the generation
    of a large number of rules from a transaction dataset. Most of them are useless.
    To pick rules that can result in useful information, we can classify them as one
    of the following three types:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 运行关联分析算法通常会从交易数据集中生成大量规则。其中大部分是无用的。为了挑选出可以提供有用信息的规则，我们可以将它们分类为以下三种类型之一：
- en: Trivial
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 琐碎
- en: Inexplicable
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 莫名其妙
- en: Actionable
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可操作
- en: Let's look at each of these types in more detail.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看每种类型。
- en: Trivial rules
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 琐碎的规则
- en: Among the large numbers of rules generated, many that are derived will be  useless
    as they summarize common knowledge about the business. They are called trivial
    rules. Even if the confidence in the trivial rules is high, they remain useless
    and cannot be used for any data-driven decision making. We can safely ignore all
    trivial rules.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的大量规则中，许多派生的规则将是无用的，因为它们总结了关于业务的常识。它们被称为琐碎规则。即使琐碎规则的置信度很高，它们仍然是无用的，不能用于任何数据驱动的决策。我们可以安全地忽略所有琐碎规则。
- en: 'The following are examples of trivial rules:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是琐碎规则的例子：
- en: Anyone who jumps from a high-rise building is likely to die.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何从高楼跳下的人都有可能死亡。
- en: Working harder leads to better scores in exams.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更努力工作会导致考试成绩更好。
- en: The sales of heaters increase as the temperature drops
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着温度下降，取暖器的销量会增加
- en: Driving a car over the speed limit on a highway leads to a higher chance of
    an accident.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高速公路上超速驾驶会增加事故的可能性。
- en: Inexplicable rules
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不可解释规则
- en: Among the rules that are generated after running the association rules algorithm,
    the ones that have no obvious explanation are the trickiest to use. Note that
    a rule can only be useful if it can help us discover and understand a new pattern
    that is expected to eventually lead toward a certain course of action. If that
    is not the case, and we cannot explain why event *X* led to event *Y*, then it
    is an inexplicable rule, because it's just a mathematical formula that ends up
    exploring the pointless relationship between two events that are unrelated and
    independent.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行关联规则算法后生成的规则中，那些没有明显解释的规则是最难使用的。请注意，规则只有在能帮助我们发现和理解预期最终会导致某种行动的新模式时才有用。如果不是这种情况，我们无法解释事件*X*导致事件*Y*的原因，那么它就是一个不可解释的规则，因为它只是一个最终探索两个无关和独立事件之间毫无意义关系的数学公式。
- en: 'The following are examples of inexplicable rules:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是不可解释规则的例子：
- en: People who wear red shirts tend to score better in exams.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 穿红衬衫的人在考试中得分更高。
- en: Green bicycles are more likely to be stolen.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绿色自行车更容易被盗。
- en: People who buy pickles end up buying diapers as well.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购买泡菜的人最终也会购买尿布。
- en: Actionable rules
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可操作规则
- en: Actionable rules are the golden rules we are looking for. They are understood
    by the business and lead to insights. They can help us to discover the possible
    causes of an event when presented to an audience familiar with the business domain—for
    example, actionable rules may suggest the best placement in a store for a particular
    product based on current buying patterns. They may also suggest which items to
    place together to maximize their chances of selling as users tend to buy them
    together.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 可操作规则是我们正在寻找的黄金规则。它们被业务理解并引发见解。当呈现给熟悉业务领域的观众时，它们可以帮助我们发现事件可能的原因，例如，可操作规则可能根据当前的购买模式建议产品在商店中的最佳摆放位置。它们还可能建议将哪些商品放在一起，以最大化它们一起销售的机会。
- en: 'The following are examples of actionable rules and their corresponding actions:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可操作规则及其相应的行动的例子：
- en: '**Rule 1:**  Displaying ads to users'' social media accounts results in a higher
    likelihood of sales.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规则1：**向用户的社交媒体账户展示广告会增加销售的可能性。'
- en: '**Actionable item:** Suggests alternative ways of advertising a product'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**可操作项目：**建议产品的替代广告方式'
- en: '**Rule 2:**  Creating more price points increases the likelihood of sales.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规则2：**创建更多的价格点会增加销售的可能性。'
- en: '**Actionable item:** One item may be advertised in a sale, while the price
    of another item is raised.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**可操作项目：**一个商品可能在促销中进行广告，而另一个商品的价格可能会上涨。'
- en: Ranking rules
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排名规则
- en: 'Association rules are measured in three ways:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则有三种衡量方式：
- en: Support (frequency) of items
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物品的支持（频率）
- en: Confidence
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 置信度
- en: Lift
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升
- en: Let's look at them in more detail.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看它们。
- en: Support
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持
- en: The support measure is a number that quantifies how frequent the pattern we
    are looking for is in our dataset. It is calculated by first counting the number
    of occurrences of our pattern of interest and then dividing it by the total number
    of all the transactions.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 支持度量是一个数字，用来量化我们在数据集中寻找的模式有多频繁。首先计算我们感兴趣的模式出现的次数，然后将其除以所有交易的总数来计算。
- en: 'Let''s look at the following formula for a particular *itemset[a]*:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看特定*itemset[a]*的以下公式：
- en: '*numItemset[a] = Number of transactions that contain itemset[a]*'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*numItemset[a] =包含itemset[a]的交易数*'
- en: '*num[total]  = Total number of transactions*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*num[total] =交易总数*'
- en: '![](assets/b03e04f2-449b-4843-ba17-698bb6ffe74c.png)By just looking at the
    support, we can get an idea of how rare the occurrence of a pattern is. A low
    support means that we are looking for a rare event.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/b03e04f2-449b-4843-ba17-698bb6ffe74c.png)仅通过支持，我们就可以了解到模式发生的罕见程度。低支持意味着我们在寻找一种罕见事件。'
- en: For example, if *itemset[a] = {helmet, ball}* appears in two transactions out
    of six, then support (itemset[a]  ) = 2/6 = 0.33.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果*itemset[a] = {头盔，球}*在六次交易中出现了两次，那么支持（itemset[a]）= 2/6 = 0.33。
- en: Confidence
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 置信度
- en: The confidence is a number that quantifies how strongly we can associate the
    left side (*X*) with the right side (*Y*) by calculating the conditional probability.
    It calculates the probability that event *X* will lead toward the event *Y*, given
    that event *X* occurred.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 置信度是一个数字，通过计算条件概率来量化我们可以将左侧（*X*）与右侧（*Y*）关联的强度。它计算了事件*X*发生的情况下，事件*Y*会发生的概率。
- en: Mathematically, consider the rule *X* ⇒ *Y*.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，考虑规则*X* ⇒ *Y*。
- en: 'The confidence of this rule is represented as confidence(*X* ⇒ *Y* ) and is
    measured as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这条规则的置信度表示为confidence(*X* ⇒ *Y*)，并按以下方式测量：
- en: '![](assets/74b2952b-14dc-4f9e-b791-77fff35337da.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/74b2952b-14dc-4f9e-b791-77fff35337da.png)'
- en: 'Let''s look at an example. Consider the following rule:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子。考虑以下规则：
- en: '{helmet, ball} ⇒ {wickets}'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '{头盔，球} ⇒ {球门}'
- en: 'The confidence of this rule is calculated by the following formula:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这条规则的置信度由以下公式计算：
- en: '![](assets/1d0a3ece-4655-4222-b625-51cc092e2c38.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1d0a3ece-4655-4222-b625-51cc092e2c38.png)'
- en: This means that if someone has {helmet, balls} in the basket, then there is
    0.5 or 50 percent probability that they will also have wickets to go with it.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果有人的篮子里有{头盔，球}，那么他们还有球门的概率是0.5或50%。
- en: Lift
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升
- en: 'Another way to estimate the quality of a rule is by calculating the lift. The
    lift returns a number that quantifies how much improvement has been achieved by
    a rule at predicting the result compared to just assuming the result at the right-hand
    side of the equation. If the *X* and *Y* itemsets were independent, then the lift
    is calculated as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 估计规则质量的另一种方法是通过计算提升。提升返回一个数字，量化了规则在预测结果方面相对于仅假设等式右侧的结果的改进程度。如果*X*和*Y*项集是独立的，那么提升的计算如下：
- en: '![](assets/01fae583-e035-4d40-b73d-32212d50c0e2.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/01fae583-e035-4d40-b73d-32212d50c0e2.png)'
- en: Algorithms for association analysis
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关联分析算法
- en: 'In this section, we will explore the following two algorithms that can be used
    for association analysis:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨以下两种可用于关联分析的算法：
- en: '**Apriori algorithm**: Proposed by Agrawal, R. and Srikant in 1994.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apriori算法**：由Agrawal, R.和Srikant于1994年提出。'
- en: '**FP-growth algorithm**: An improvement suggested by Han et al. in 2001.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP-growth算法**：由Han等人于2001年提出的改进建议。'
- en: Let's look at each of these algorithms.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些算法各自的情况。
- en: Apriori Algorithm
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apriori算法
- en: The apriori algorithm is an iterative and multiphase algorithm used to generate
    association rules. It is based on a generation-and-test approach.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法是一种迭代和多阶段的算法，用于生成关联规则。它基于生成和测试的方法。
- en: 'Before executing the apriori algorithm, we need to define two variables: support[threshold]
    and Confidence[threshold.]'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行apriori算法之前，我们需要定义两个变量：support[threshold]和Confidence[threshold]。
- en: 'The algorithm consists of the following two phases:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法包括以下两个阶段：
- en: '**Candidate-generation phase**: It generates the candidate itemsets, which
    contain sets of all itemsets above support[threshold].'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**候选生成阶段**：它生成包含所有高于support[threshold]的项集的候选项集。'
- en: '**Filter phase**: It filters out all rules below the expected confidence[threshold].'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤阶段**：它过滤掉所有低于预期confidence[threshold]的规则。'
- en: After filtering, the resulting rules are the answer.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤后，得到的规则就是答案。
- en: Limitations of the apriori algorithm
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apriori算法的局限性
- en: The major bottleneck in the apriori algorithm is the generation of candidate
    rules in Phase 1—for example, ![](assets/1486d41a-07b6-45b2-bab4-71873083bfd2.png)
    = {item  [1]  , item  [2]  , . . . , item  [m]  } can produce 2^m  possible itemsets.
    Because of its multiphase design, it first generates these itemsets and then works
    toward finding the frequent itemsets. This limitation is a huge performance bottleneck
    and makes the apriori algorithm unsuitable for larger items.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法中的主要瓶颈是第1阶段候选规则的生成，例如，![](assets/1486d41a-07b6-45b2-bab4-71873083bfd2.png)
    = {item  [1]  , item  [2]  , . . . , item  [m]  } 可以产生2^m个可能的项集。由于其多阶段设计，它首先生成这些项集，然后努力找到频繁项集。这个限制是一个巨大的性能瓶颈，使得apriori算法不适用于更大的项。
- en: FP-growth algorithm
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FP-growth算法
- en: 'The **frequent pattern growth** (**FP-growth**) algorithm is an improvement
    on the apriori algorithm. It starts by showing the frequent transaction FP-tree,
    which is an ordered tree. It consists of two steps:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**频繁模式增长**（**FP-growth**）算法是对apriori算法的改进。它首先展示频繁交易FP树，这是一个有序树。它包括两个步骤：'
- en: Populating the FP-tree
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充FP树
- en: Mining frequent patterns
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挖掘频繁模式
- en: Let's look at these steps one by one.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地看这些步骤。
- en: Populating the FP-tree
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充FP树
- en: 'Let''s consider the transaction data shown in the following table. Let''s first
    represent it as a sparse matrix:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑下表中显示的交易数据。让我们首先将其表示为稀疏矩阵：
- en: '| **ID** | **Bat** | **Wickets** | **Pads** | **Helmet** | **Ball** |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | **球棒** | **球门** | **防护板** | **头盔** | **球** |'
- en: '| 1 | 0 | 1 | 1 | 0 | 0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | 1 | 0 | 0 |'
- en: '| 2 | 1 | 1 | 1 | 1 | 0 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 1 | 1 | 1 | 0 |'
- en: '| 3 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 0 | 0 | 1 | 1 |'
- en: '| 4 | 1 | 0 | 1 | 1 | 0 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 | 0 | 1 | 1 | 0 |'
- en: 'Let''s calculate the frequency of each item and sort them in descending order
    by frequency:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算每个项的频率，并按频率降序排序：
- en: '| **Item** | **Frequency** |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| **项** | **频率** |'
- en: '| pads | 3 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 防护板 | 3 |'
- en: '| helmet | 3 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 头盔 | 3 |'
- en: '| bat | 2 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 球棒 | 2 |'
- en: '| wicket | 2 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 球门 | 2 |'
- en: '| ball | 1 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 球 | 1 |'
- en: 'Now let''s rearrange the transaction-based data based on the frequency:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们根据频率重新排列基于交易的数据：
- en: '| **ID** | **Original Items** | **Reordered Items** |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | **原始项** | **重新排序的项** |'
- en: '| t1 | Wickets, pads | Pads, wickets |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| t1 | 防护板，球门 | 防护板，球门 |'
- en: '| t2 | Bat, wickets, pads, helmet | Helmet, pads, wickets, bat |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| t2 | 球棒，球门，防护板，头盔 | 头盔，防护板，球门，球棒 |'
- en: '| t3 | Helmet, ball | Helmet, ball |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| t3 | 头盔，球 | 头盔，球 |'
- en: '| t4 | Bat, pads, helmet | Helmet, pads, bat |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| t4 | 球棒，防护板，头盔 | 头盔，防护板，球棒 |'
- en: 'To build the FP-tree, let''s start with the first branch of the FP-tree. The
    FP-tree starts with a **Null** as the root. To build the tree, we can represent
    each item with a node, as shown in the following diagram (the tree representation
    of t[1]  is shown here). Note that the label of each node is the name of the item
    and its frequency is appended after the colon. Also, note that the **pads**  item  has
    a frequency of 1:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建FP树，让我们从FP树的第一个分支开始。FP树以**Null**作为根开始。为了构建树，我们可以用一个节点表示每个项，如下图所示（这里显示了t[1]的树表示）。请注意，每个节点的标签都是项的名称，冒号后面附加了其频率。还要注意**pads**项的频率为1：
- en: '![](assets/a1831fc4-21ae-4229-8d09-c125d2451a45.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/a1831fc4-21ae-4229-8d09-c125d2451a45.png)'
- en: 'Using the same pattern, let''s draw all four transactions, resulting in the
    full FP-tree. The FP-tree has four leaf nodes, each representing the itemset associated
    with the four transactions. Note that we need to count the frequencies of each
    item and need to increase it when used multiple times—for example, when adding
    t[2] to the FP-tree, the frequency of **helmet** was increased to two. Similarly,
    while adding t[4], it was increased again to three. The resulting tree is shown
    in the following diagram:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的模式，让我们绘制所有四个交易，得到完整的FP树。FP树有四个叶节点，每个节点代表与四个交易相关的项集。请注意，我们需要计算每个项的频率，并在多次使用时增加它-例如，将t[2]添加到FP树时，**头盔**
    的频率增加到了两次。类似地，当添加t[4]时，它再次增加到了三次。结果树如下图所示：
- en: '![](assets/81575675-cbb3-418b-b2ba-e58240c27b40.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/81575675-cbb3-418b-b2ba-e58240c27b40.png)'
- en: Note that the FP-tree generated in the preceding diagram is an ordered tree.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面图中生成的FP树是有序树。
- en: Mining Frequent Patterns
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挖掘频繁模式
- en: The second phase of the FP-growth tree involves mining the frequent patterns
    from the FP-tree. By creating an ordered tree, the intention is to create an efficient
    data structure that can be easily navigated to search for frequent patterns.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: FP-growth树的第二阶段涉及从FP树中挖掘频繁模式。通过创建一个有序树，意图是创建一个高效的数据结构，可以轻松导航以搜索频繁模式。
- en: 'We start from a leaf node (that is, the end node) and move upward—for example,
    let''s start from one of the leaf node items, **bat**. Then we need to calculate
    the conditional pattern base for **bat**. The conditional pattern base is calculated
    by specifying all the paths from the leaf item node to the top. The conditional
    pattern base for **bat** will be as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从叶节点（即末端节点）开始向上移动-例如，让我们从叶节点项之一 **球棒** 开始。然后我们需要计算 **球棒** 的条件模式基。通过指定从叶节点项到顶部的所有路径来计算条件模式基。**球棒**
    的条件模式基如下：
- en: '| Wicket: 1 | Pads: 1 | Helmet: 1 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 球门: 1 | 护腕: 1 | 头盔: 1 |'
- en: '| Pad: 1 | Helmet: 1 |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 护腕: 1 | 头盔: 1 |  |'
- en: 'The  **frequent pattern**  for **bat** will be as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '**球棒** 的 **频繁模式** 如下：'
- en: '*{wicket, pads, helmet} : bat*'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '*{球门, 护腕, 头盔} : 球棒*'
- en: '*{pad,helmet} : bat*'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '*{护腕,头盔} : 球棒*'
- en: Code for using FP-growth
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FP-growth的代码
- en: 'Let''s see how we can generate association rules using the FP-growth algorithm
    in Python. For this, we will be using the `pyfpgrowth`  package. First, if we
    have never used  `pyfpgrowth`  before, let''s install it first:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Python中的FP-growth算法生成关联规则。为此，我们将使用 `pyfpgrowth` 软件包。首先，如果我们以前从未使用过
    `pyfpgrowth`，让我们首先安装它：
- en: '[PRE9]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, let''s import the packages that we need to use to implement this algorithm:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们导入实现此算法所需的软件包：
- en: '[PRE10]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we will create the input data in the form of  `transactionSet`:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建以 `transactionSet` 形式的输入数据：
- en: '[PRE11]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once the input data is generated, we will generate patterns that will be based
    on the parameters that we passed in the  `find_frequent_patterns()`. Note that
    the second parameter passed to this function is the minimum support, which is
    1 in this case:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成了输入数据，我们将生成基于我们传递给 `find_frequent_patterns()` 的参数的模式。请注意，传递给此函数的第二个参数是最小支持度，在本例中为1：
- en: '[PRE12]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The patterns have been generated. Now let''s print the patterns. The patterns
    list the combinations of items with their supports:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 模式已生成。现在让我们打印模式。模式列出了项的组合及其支持：
- en: '![](assets/1c898244-8bbf-4f3f-acd7-0d71e554569c.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1c898244-8bbf-4f3f-acd7-0d71e554569c.png)'
- en: 'Now let''s generate the rules:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们生成规则：
- en: '![](assets/a253eee2-98ff-43df-b71b-315b8921f0d6.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/a253eee2-98ff-43df-b71b-315b8921f0d6.png)'
- en: Each rule has a left-hand side and a right-hand side, separated by a colon (:).
    It also gives us the support of each of the rules in our input dataset.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 每个规则都有左侧和右侧，由冒号（:）分隔。它还为我们提供了输入数据集中每个规则的支持。
- en: Practical application– clustering similar tweets together
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际应用-将相似的推文进行聚类
- en: 'Unsupervised machine learning algorithms can also be applied in real time to
    cluster similar tweets together. They will do the following:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习算法也可以实时应用于将相似的推文进行聚类。它们将执行以下操作：
- en: 'STEP 1- **Topic Modeling**: Discover various topics from a given set of tweets'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤1- **主题建模：** 从给定的一组推文中发现各种主题
- en: STEP 2- **Clustering:** Associate each of the tweets with one of the discovered
    topics
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤2- **聚类：** 将每个推文与发现的主题之一关联起来
- en: 'This use of unsupervised learning is shown in the following diagram:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这种无监督学习的应用如下图所示：
- en: '![](assets/8a9a0326-d16f-4c14-9707-a123d24850bb.png)Note that this example
    requires real-time processing of input data.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/8a9a0326-d16f-4c14-9707-a123d24850bb.png)请注意，此示例需要实时处理输入数据。'
- en: Let's look into these steps one by one.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一看看这些步骤。
- en: Topic modeling
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: 'Topic Modeling is the process of discovering the concepts in a set of documents
    that can be used to differentiate them. In the context of tweets, it is about
    finding which are the most appropriate topics in which a set of tweets can be
    divided. Latent Dirichlet Allocation is a popular algorithm that is used for topic
    modeling. Because each of the tweet are short 144 character document usually about
    a very particular topic, we can write a simpler algorithm for topic modeling purposes.
    The algorithm is described as following:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是发现一组文档中的概念的过程，这些概念可以用来区分它们。在推文的背景下，这是关于找出一组推文可以被分成哪些最合适的主题。潜在狄利克雷分配是一种用于主题建模的流行算法。因为每条推文都是一个短的144个字符的文档，通常涉及一个非常特定的主题，我们可以为主题建模目的编写一个更简单的算法。该算法描述如下：
- en: Tokenize tweets.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对推文进行标记化处理。
- en: Preprocess the data. Remove stopwords, numbers, symbols and perform stemming
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据。删除停用词、数字、符号并进行词干处理
- en: Create a Term-Document-Matrix (TDM) for the tweets. Choose the top 200 words
    that appear most frequently in unique tweets.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为推文创建一个术语-文档矩阵（TDM）。选择在唯一推文中出现最频繁的前200个词。
- en: Choose top 10 word that directly or indirectly represent a concept or a topic.
    For example Fashion, New York, Programming, Accident. These 10 words are now the
    topics that we have successfully discovered and will become the cluster centers
    for the tweets.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择直接或间接代表概念或主题的前 10 个单词。例如时尚、纽约、编程、事故。这 10 个单词现在是我们成功发现的主题，并将成为 tweets 的聚类中心。
- en: Let's move to the next step that is clustering
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一步，即聚类
- en: Clustering
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Once we have discovered the topics we will choose them as the center of the
    cluster. Then we can run k-means clustering algorithm that will assign each of
    the tweets to one of the cluster center.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们发现了主题，我们将选择它们作为聚类的中心。然后我们可以运行 k-means 聚类算法，将每个 tweet 分配到其中一个聚类中心。
- en: So, this the practical example that how a set of tweets can be clustered into
    topics discovered.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是一个实际的例子，说明一组 tweets 如何被聚类成发现的主题。
- en: Anomaly-detection algorithms
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常检测算法
- en: The dictionary definition of an *anomaly* is something that is different,  abnormal,  peculiar,
    or not  easily  classified. It is a deviation  from  the  common  rule. In the
    context of data science, an anomaly is a data point that deviates a lot from the
    expected pattern. Techniques to find such data points are called anomaly-detection
    techniques.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '*异常* 的词典定义是与众不同、异常、奇特或不容易分类的东西。它是偏离常规规则的。在数据科学的背景下，异常是偏离预期模式很多的数据点。寻找这样的数据点的技术被称为异常检测技术。'
- en: 'Now let''s see some applications of anomaly-detection algorithms:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看异常检测算法的一些应用：
- en: Credit card fraud
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用卡欺诈
- en: Finding a malignant tumor in a  **magnetic resonance imaging (MRI**) scan
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 **磁共振成像（MRI）** 扫描中发现恶性肿瘤
- en: Fault prevention in clusters
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中的故障预防
- en: Impersonation in exams
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考试中的冒名顶替
- en: Accidents on a highway
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高速公路上的事故
- en: In the upcoming sections, we will see various anomaly-detection techniques.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到各种异常检测技术。
- en: Using clustering
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用聚类
- en: Clustering algorithms such as k-means can be used to group similar data points
    together. A threshold can be defined and any point beyond that threshold can be
    classified as an anomaly. The problem with this approach is that the grouping
    created by k-means clustering may itself be biased because of the presence of
    anomalous data points and may affect the usefulness and accuracy of the approach.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 k-means 的聚类算法可以用来将相似的数据点分组在一起。可以定义一个阈值，任何超出该阈值的点都可以被分类为异常。这种方法的问题在于，由于异常数据点的存在，k-means
    聚类创建的分组本身可能会存在偏差，并可能影响方法的实用性和准确性。
- en: Using density-based anomaly detection
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于密度的异常检测
- en: A density-based approach tries to find dense neighborhoods. The **k-nearest
    neighbors** (**KNN**) algorithm can be used for this purpose. Abnormalities that
    are far away from the discovered dense neighborhoods are marked as anomalies.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的方法试图找到密集的邻域。**k-最近邻**（**KNN**）算法可以用于此目的。远离发现的密集邻域的异常被标记为异常。
- en: Using support vector machines
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机
- en: The **Support Vector Machine** (**SVM**) algorithm can be used to learn the
    boundaries of the data points. Any points beyond those discovered boundaries are
    identified as anomalies.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）算法可以用来学习数据点的边界。任何超出这些发现的边界的点都被识别为异常。'
- en: Summary
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at the various unsupervised machine learning techniques.
    We looked at the circumstances in which it is a good idea to try to reduce the
    dimensionality of the problem we are trying to solve and the different methods
    of doing this. We also studied the  practical examples  where unsupervised machine
    learning techniques can be very helpful, including market basket analysis and
    anomaly detection.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看了各种无监督的机器学习技术。我们看了尝试减少我们试图解决的问题的维度的情况，以及不同的方法。我们还研究了无监督机器学习技术在哪些情况下非常有帮助，包括市场篮分析和异常检测。
- en: In the next chapter, we will look at the various supervised learning techniques.
    We will start with linear regression and then we will look at more sophisticated
    supervised machine learning techniques, such as decision-tree-based algorithms,
    SVM, and XGBoast. We will also study the naive Bayes algorithm, which is best
    suited for unstructured textual data.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看看各种监督学习技术。我们将从线性回归开始，然后我们将看看更复杂的监督机器学习技术，如基于决策树的算法、SVM 和 XGBoast。我们还将研究朴素贝叶斯算法，它最适合于非结构化的文本数据。
