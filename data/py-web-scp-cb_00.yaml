- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: The internet contains a wealth of data. This data is both provided through structured
    APIs as well as by content delivered directly through websites. While the data
    in APIs is highly structured, information found in web pages is often unstructured
    and requires collection, extraction, and processing to be of value. And collecting
    data is just the start of the journey, as that data must also be stored, mined,
    and then exposed to others in a value-added form.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网包含大量数据。这些数据既通过结构化API提供，也通过网站直接提供。虽然API中的数据高度结构化，但在网页中找到的信息通常是非结构化的，需要收集、提取和处理才能有价值。收集数据只是旅程的开始，因为这些数据还必须存储、挖掘，然后以增值形式向他人展示。
- en: With this book, you will learn many of the core tasks needed in collecting various
    forms of information from websites. We will cover how to collect it, how to perform
    several common data operations (including storage in local and remote databases),
    how to perform common media-based tasks such as converting images an videos to
    thumbnails, how to clean unstructured data with NTLK, how to examine several data
    mining and visualization tools, and finally core skills in building a microservices-based
    scraper and API that can, and will, be run on the cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这本书，您将学习从网站收集各种信息所需的核心任务。我们将介绍如何收集数据，如何执行几种常见的数据操作（包括存储在本地和远程数据库中），如何执行常见的基于媒体的任务，如将图像和视频转换为缩略图，如何使用NTLK清理非结构化数据，如何检查几种数据挖掘和可视化工具，以及构建基于微服务的爬虫和API的核心技能，这些技能可以并且将在云上运行。
- en: Through a recipe-based approach, we will learn independent techniques to solve
    specific tasks involved in not only scraping but also data manipulation and management,
    data mining, visualization, microservices, containers, and cloud operations. These
    recipes will build skills in a progressive and holistic manner, not only teaching how
    to perform the fundamentals of scraping but also taking you from the results of
    scraping to a service offered to others through the cloud. We will be building
    an actual web-scraper-as-a-service using common tools in the Python, container,
    and cloud ecosystems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基于配方的方法，我们将学习独立的技术，以解决不仅仅是爬取，还包括数据操作和管理、数据挖掘、可视化、微服务、容器和云操作中涉及的特定任务。这些配方将以渐进和整体的方式建立技能，不仅教授如何执行爬取的基础知识，还将带您从爬取的结果到通过云向他人提供的服务。我们将使用Python、容器和云生态系统中的常用工具构建一个实际的网络爬虫服务。
- en: Who this book is for
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书适合谁
- en: This book is for those who want to learn to extract data from websites using
    the process of scraping and also how to work with various data management tools
    and cloud services. The coding will require basic skills in the Python programming
    language.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书适合那些想要学习使用爬取过程从网站提取数据以及如何使用各种数据管理工具和云服务的人。编码将需要基本的Python编程语言技能。
- en: The book is also for those who wish to learn about a larger ecosystem of tools
    for retrieving, storing, and searching data, as well as using modern tools and
    Pythonic libraries to create data APIs and cloud services. You may also be using
    Docker and Amazon Web Services to package and deploy a scraper on the cloud.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书还适合那些希望了解更大的工具生态系统，用于检索、存储和搜索数据，以及使用现代工具和Python库创建数据API和云服务的人。您可能还会使用Docker和Amazon
    Web Services在云上打包和部署爬虫。
- en: What this book covers
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书涵盖内容
- en: '[Chapter 1](106f76e7-8e79-4ed5-81e2-9dfba963aaa7.xhtml), *Getting Started with
    Scraping*, introduces several concepts and tools for web scraping. We will examine
    how to install and do basic tasks with tools such as requests, urllib, BeautifulSoup,
    Scrapy, PhantomJS and Selenium.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第1章，“开始爬取”，介绍了网页爬取的几个概念和工具。我们将研究如何安装并使用工具，如requests、urllib、BeautifulSoup、Scrapy、PhantomJS和Selenium进行基本任务。
- en: '[Chapter 2](fd05e7a0-2ea8-4e8f-9efb-ac3add48bc0a.xhtml), *Data Acquisition
    and Extraction*, is based on an understanding of the structure of HTML and how
    to find and extract embedded data. We will cover many of the concepts in the DOM
    and how to find and extract data using BeautifulSoup, XPath, LXML, and CSS selectors.
    We also briefly examine working with Unicode / UTF8.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章，“数据获取和提取”，基于对HTML结构的理解以及如何查找和提取嵌入式数据。我们将涵盖DOM中的许多概念以及如何使用BeautifulSoup、XPath、LXML和CSS选择器查找和提取数据。我们还简要介绍了Unicode
    / UTF8的工作。
- en: '[Chapter 3](acb4595a-ad11-49ca-91bd-71ee144229d9.xhtml), *Processing Data*,
    teaches you to load and manipulate data in many formats, and then how to store
    that data in various data stores (S3, MySQL, PostgreSQL, and ElasticSearch). Data
    in web pages is represented in various formats, the most common being HTML, JSON,
    CSV, and XML We will also examine the use of message queue systems, primarily
    AWS SQS, to help build robust data processing pipelines.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章，“处理数据”，教你如何以多种格式加载和操作数据，然后如何将数据存储在各种数据存储中（S3、MySQL、PostgreSQL和ElasticSearch）。网页中的数据以各种格式表示，最常见的是HTML、JSON、CSV和XML。我们还将研究使用消息队列系统，主要是AWS
    SQS，来帮助构建强大的数据处理管道。
- en: '[Chapter 4](ad2a0e60-42d2-4254-b1f5-10e65b5e2f4a.xhtml), *Working with Images,
    Audio and other Assets*, examines the means of retrieving multimedia items, storing
    them locally, and also performing several tasks such as OCR, generating thumbnails,
    making web page screenshots, audio extraction from videos, and finding all video
    URLs in a YouTube playlist.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章，“处理图像、音频和其他资产”，研究了检索多媒体项目的方法，将它们存储在本地，并执行诸如OCR、生成缩略图、制作网页截图、从视频中提取音频以及在YouTube播放列表中找到所有视频URL等多项任务。
- en: '[Chapter 5](713f8046-658b-4346-9ece-59e2e92a2c53.xhtml), *Scraping – Code of
    Conduct*, covers several concepts involved in the legality of scraping, and practices
    for performing polite scraping. We will examine tools for processing robots.txt
    and sitemaps to respect the web host''s desire for acceptable behavior. We will
    also examine the control of several facets of crawling, such as using delays,
    containing the depth and length of crawls, using user agents, and implementing
    caching to prevent repeated requests.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](713f8046-658b-4346-9ece-59e2e92a2c53.xhtml)，*爬取-行为准则*，涵盖了与爬取的合法性有关的几个概念，以及进行礼貌爬取的实践。我们将研究处理robots.txt和站点地图的工具，以尊重网络主机对可接受行为的要求。我们还将研究爬行的几个方面的控制，比如使用延迟、包含爬行的深度和长度、使用用户代理以及实施缓存以防止重复请求。'
- en: '[Chapter 6](390110fd-80c7-42e7-bc1a-b93fe5b87f31.xhtml), *Scraping Challenges
    and Solutions*, covers many of the challenges that writing a robust scraper is
    rife with, and how to handle many scenarios. These scenarios are pagination, redirects,
    login forms, keeping the crawler within the same domain, retrying requests upon
    failure, and handling captchas.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](390110fd-80c7-42e7-bc1a-b93fe5b87f31.xhtml)，*爬取挑战与解决方案*，涵盖了编写健壮爬虫时面临的许多挑战，以及如何处理许多情况。这些情况包括分页、重定向、登录表单、保持爬虫在同一域内、请求失败时重试以及处理验证码。'
- en: '[Chapter 7](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml), *Text Wrangling and
    Analysis*, examines various tools such as using NLTK for natural language processing
    and how to remove common noise words and punctuation. We often need to process
    the textual content of a web page to find information on the page that is part
    of the text and neither structured/embedded data nor multimedia. This requires
    knowledge of using various concepts and tools to clean and understand text.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml)，*文本整理和分析*，探讨了各种工具，比如使用NLTK进行自然语言处理，以及如何去除常见的噪音词和标点符号。我们经常需要处理网页的文本内容，以找到页面上作为文本一部分的信息，既不是结构化/嵌入式数据，也不是多媒体。这需要使用各种概念和工具来清理和理解文本。'
- en: '[Chapter 8](8a120fd5-e37d-4d54-9c12-93cf320cee47.xhtml), *Searching, Mining,
    and Visualizing Data*, covers several means of searching for data on the Web,
    storing and organizing data, and deriving results from the identified relationships.
    We will see how to understand the geographic locations of contributors to Wikipedia,
    finding relationships between actors on IMDB, and finding jobs on Stack Overflow
    that match specific technologies.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](8a120fd5-e37d-4d54-9c12-93cf320cee47.xhtml)，*搜索、挖掘和可视化数据*，涵盖了在网上搜索数据、存储和组织数据，以及从已识别的关系中得出结果的几种方法。我们将看到如何理解维基百科贡献者的地理位置，找到IMDB上演员之间的关系，以及在Stack
    Overflow上找到与特定技术匹配的工作。'
- en: '[Chapter 9](c91fe735-b615-42c9-9513-29121a7f022a.xhtml), *Creating a Simple
    Data API*, teaches us how to create a scraper as a service. We will create a REST
    API for a scraper using Flask. We will run the scraper as a service behind this
    API and be able to submit requests to scrape specific pages, in order to dynamically
    query data from a scrape as well as a local ElasticSearch instance.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](c91fe735-b615-42c9-9513-29121a7f022a.xhtml)，*创建一个简单的数据API*，教会我们如何创建一个爬虫作为服务。我们将使用Flask为爬虫创建一个REST
    API。我们将在这个API后面运行爬虫作为服务，并能够提交请求来爬取特定页面，以便从爬取和本地ElasticSearch实例中动态查询数据。'
- en: '[Chapter 10](ae07f922-9f75-4b94-9902-e5a5ebeec167.xhtml), *Creating Scraper
    Microservices with Docker*, continues the growth of our scraper as a service by
    packaging the service and API in a Docker swarm and distributing requests across
    scrapers via a message queuing system (AWS SQS). We will also cover scaling of
    scraper instances up and down using Docker swarm tools.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[第10章](ae07f922-9f75-4b94-9902-e5a5ebeec167.xhtml)，*使用Docker创建爬虫微服务*，通过将服务和API打包到Docker集群中，并通过消息队列系统（AWS
    SQS）分发请求，继续扩展我们的爬虫服务。我们还将介绍使用Docker集群工具来扩展和缩减爬虫实例。'
- en: '[Chapter 11](05a687e5-e5d7-4ffb-aa28-6078fbab5fea.xhtml), *Making the Scraper
    as a Service Real*, concludes by fleshing out the services crated in the previous
    chapter to add a scraper that pulls together various concepts covered earlier.
    This scraper can assist in analyzing job posts on StackOverflow to find and compare
    employers using specified technologies. The service will collect posts and allow
    a query to find and compare those companies.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[第11章](05a687e5-e5d7-4ffb-aa28-6078fbab5fea.xhtml)，*使爬虫成为真正的服务*，通过充实上一章中创建的服务来结束，添加一个爬虫，汇集了之前介绍的各种概念。这个爬虫可以帮助分析StackOverflow上的职位发布，以找到并比较使用指定技术的雇主。该服务将收集帖子，并允许查询以找到并比较这些公司。'
- en: To get the most out of this book
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为了充分利用本书
- en: The primary tool required for the recipes in this book is a Python 3 interpreter.
    The recipes have been written using the free version of the Anaconda Python distribution,
    specifically version 3.6.1\. Other Python version 3 distributions should work
    well but have not been tested.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中所需的主要工具是Python 3解释器。这些配方是使用Anaconda Python发行版的免费版本编写的，具体版本为3.6.1。其他Python
    3发行版应该也能很好地工作，但尚未经过测试。
- en: The code in the recipes will often require the use of various Python libraries.
    These are all available for installation using `pip` and accessible using `pip
    install`. Wherever required, these installations will be elaborated in the recipes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 配方中的代码通常需要使用各种Python库。这些都可以使用`pip`进行安装，并且可以使用`pip install`进行访问。在需要的地方，这些安装将在配方中详细说明。
- en: Several recipes require an Amazon AWS account. AWS accounts are available for
    the first year for free-tier access. The recipes will not require anything more
    than free-tier services. A new account can be created at [https://portal.aws.amazon.com/billing/signup](https://portal.aws.amazon.com/billing/signup).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个配方需要亚马逊AWS账户。AWS账户在第一年可以免费使用免费层服务。配方不需要比免费层服务更多的东西。可以在[https://portal.aws.amazon.com/billing/signup](https://portal.aws.amazon.com/billing/signup)上创建一个新账户。
- en: Several recipes will utilize Elasticsearch. There is a free, open source version
    available on GitHub at [https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch),
    with installation instructions on that page. Elastic.co also offers a fully capable
    version (also with Kibana and Logstash) hosted on the cloud with a 14-day free
    trial available at [http://info.elastic.co](http://info.elastic.co) (which we
    will utilize). There is a version for docker-compose with all x-pack features
    available at [https://github.com/elastic/stack-docker](https://github.com/elastic/stack-docker),
    all of which can be started with a simple `docker-compose up` command.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 几个食谱将利用Elasticsearch。GitHub上有一个免费的开源版本，网址是[https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch)，该页面上有安装说明。Elastic.co还提供了一个完全功能的版本（还带有Kibana和Logstash），托管在云上，并提供为期14天的免费试用，网址是[http://info.elastic.co](http://info.elastic.co)（我们将使用）。还有一个docker-compose版本，具有所有x-pack功能，网址是[https://github.com/elastic/stack-docker](https://github.com/elastic/stack-docker)，所有这些都可以通过简单的`docker-compose
    up`命令启动。
- en: Finally, several of the recipes use MySQL and PostgreSQL as database examples
    and several common clients for those databases. For those recipes, these will
    need to be installed locally. MySQL Community Server is available at [https://dev.mysql.com/downloads/mysql/](https://dev.mysql.com/downloads/mysql/),
    and PostgreSQL can be found at [https://www.postgresql.org/](https://www.postgresql.org/).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一些食谱使用MySQL和PostgreSQL作为数据库示例，以及这些数据库的几个常见客户端。对于这些食谱，这些都需要在本地安装。 MySQL Community
    Server可在[https://dev.mysql.com/downloads/mysql/](https://dev.mysql.com/downloads/mysql/)上找到，而PostgreSQL可在[https://www.postgresql.org/](https://www.postgresql.org/)上找到。
- en: We will also look at creating and using docker containers for several of the
    recipes. Docker CE is free and is available at [https://www.docker.com/community-edition](https://www.docker.com/community-edition).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将研究创建和使用多个食谱的docker容器。 Docker CE是免费的，可在[https://www.docker.com/community-edition](https://www.docker.com/community-edition)上获得。
- en: Download the example code files
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from your account at [www.packtpub.com](http://www.packtpub.com).
    If you purchased this book elsewhere, you can visit [www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[www.packtpub.com](http://www.packtpub.com)的帐户中下载本书的示例代码文件。如果您在其他地方购买了本书，可以访问[www.packtpub.com/support](http://www.packtpub.com/support)并注册，文件将直接发送到您的邮箱。
- en: 'You can download the code files by following these steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下步骤下载代码文件：
- en: Log in or register at [www.packtpub.com](http://www.packtpub.com/support).
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[www.packtpub.com](http://www.packtpub.com/support)上登录或注册。
- en: Select the SUPPORT tab.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择“支持”选项卡。
- en: Click on Code Downloads & Errata.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“代码下载和勘误”。
- en: Enter the name of the book in the Search box and follow the onscreen instructions.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索框中输入书名，然后按照屏幕上的说明操作。
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文件后，请确保使用最新版本的解压缩或提取文件夹：
- en: WinRAR/7-Zip for Windows
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WinRAR/7-Zip for Windows
- en: Zipeg/iZip/UnRarX for Mac
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zipeg/iZip/UnRarX for Mac
- en: 7-Zip/PeaZip for Linux
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 7-Zip/PeaZip for Linux
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Python-Web-Scraping-Cookbook](https://github.com/PacktPublishing/Python-Web-Scraping-Cookbook).
    We also have other code bundles from our rich catalog of books and videos available
    at **[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)**.
    Check them out!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该书的代码包也托管在GitHub上，网址是[https://github.com/PacktPublishing/Python-Web-Scraping-Cookbook](https://github.com/PacktPublishing/Python-Web-Scraping-Cookbook)。我们还有其他代码包，来自我们丰富的书籍和视频目录，可在**[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)**上找到。去看看吧！
- en: Conventions used
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了许多文本约定。
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    Here is an example: "This will loop through up to 20 characters and drop them
    into the `sw` index with a document type of `people`"'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`CodeInText`：表示文本中的代码单词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟URL、用户输入和Twitter句柄。这是一个例子：“这将循环遍历多达20个字符，并将它们放入`sw`索引中，文档类型为`people`”'
- en: 'A block of code is set as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块设置如下：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都按如下方式编写：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For example, words in menus or dialog boxes appear in the text like this. Here
    is an example: "Select System info from the Administration panel."'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示一个新术语、一个重要单词或屏幕上看到的单词。例如，菜单或对话框中的单词会出现在文本中。这是一个例子：“从管理面板中选择系统信息。”'
- en: Warnings or important notes appear like this.Tips and tricks appear like this.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要说明会出现在这样的地方。提示和技巧会出现在这样的地方。
