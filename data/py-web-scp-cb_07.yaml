- en: Text Wrangling and Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行词形还原
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如何做
- en: Installing NLTK
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些过程，比如我们将使用的过程，需要额外下载它们用于执行各种分析的各种数据集。可以通过执行以下操作来下载它们：安装NLTK
- en: Performing sentence splitting
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装NLTK
- en: Performing tokenization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](assets/75aae6dd-a071-42ea-a6be-8ccd5f961b95.png)NTLK GUI'
- en: Performing stemming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行词干提取
- en: Performing lemmatization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先我们从NLTK导入句子分词器：
- en: Identifying and removing stop words
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和删除短词
- en: Calculating the frequency distribution of words
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子分割的第一个例子在`07/01_sentence_splitting1.py`文件中。这使用NLTK中内置的句子分割器，该分割器使用内部边界检测算法：
- en: Identifying and removing rare words
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和删除罕见单词
- en: Identifying and removing short words
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后使用`sent_tokenize`分割句子，并报告句子：
- en: Removing punctuation marks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们按照以下步骤进行：
- en: Piecing together n-grams
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍
- en: Scraping a job listing from StackOverflow
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用语言参数选择所需的语言。例如，以下内容将基于德语进行分割：
- en: Reading and cleaning the description in the job listCreating a word cloud from
    a StackOverflow job listing
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读和清理工作列表中的描述
- en: Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挖掘数据通常是工作中最有趣的部分，文本是最常见的数据来源之一。我们将使用NLTK工具包介绍常见的自然语言处理概念和统计模型。我们不仅希望找到定量数据，比如我们已经抓取的数据中的数字，还希望能够分析文本信息的各种特征。这种文本信息的分析通常被归类为自然语言处理（NLP）的一部分。Python有一个名为NLTK的库，提供了丰富的功能。我们将调查它的几种功能。
- en: Mining the data is often the most interesting part of the job, and text is one
    of the most common data sources. We will be using the NLTK toolkit to introduce
    common natural language processing concepts and statistical models. Not only do
    we want to find quantitative data, such as numbers within data that we have scraped,
    we also want to be able to analyze various characteristics of textual information.
    This analysis of textual information is often lumped into a category known as
    natural language processing (NLP).  There exists a library for Python, NLTK, that
    provides rich capabilities.  We will investigate several of it's capabilities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在Mac上，这实际上会弹出以下窗口：
- en: Installing NLTK
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: In this recipe we learn to install NTLK, the natural language toolkit for Python.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK的核心可以使用pip安装：
- en: How to do it
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从StackOverflow抓取工作列表
- en: 'We proceed with the recipe as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 文本整理和分析
- en: 'The core of NLTK can be installed using pip:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择安装所有并按下下载按钮。工具将开始下载许多数据集。这可能需要一段时间，所以喝杯咖啡或啤酒，然后不时地检查一下。完成后，您就可以继续进行下一个步骤了。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Some processes, such as those we will use, require an additional download of
    various data sets that they use to perform various analyses. They can be downloaded
    by executing the following:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行句子分割
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'On a Mac, this actually pops up the following window:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除标点符号
- en: '![](assets/75aae6dd-a071-42ea-a6be-8ccd5f961b95.png)The NTLK GUI'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Select install all and press the Download button.  The tools will begin to download
    a number of data sets.  This can take a while, so grab a coffee or beer and check
    back every now and then.  When completed, you are ready to progress to the next
    recipe.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 执行标记化
- en: Performing sentence splitting
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在这个配方中，我们学习安装Python的自然语言工具包NTLK。
- en: Many NLP processes require splitting a large amount of text into sentences. 
    This may seem to be a simple task, but for computers it can be problematic.  A
    simple sentence splitter can look just for periods (.), or use other algorithms
    such as predictive classifiers.  We will examine two means of sentence splitting
    with NLTK.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 许多NLP过程需要将大量文本分割成句子。这可能看起来是一个简单的任务，但对于计算机来说可能会有问题。一个简单的句子分割器可以只查找句号（。），或者使用其他算法，比如预测分类器。我们将使用NLTK来检查两种句子分割的方法。
- en: How to do it
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: 'We will use a sentence stored in thee `07/sentence1.txt` file.  It has the
    following content, which was pulled from a random job listing on StackOverflow:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用存储在`07/sentence1.txt`文件中的句子。它包含以下内容，这些内容是从StackOverflow上的随机工作列表中提取的：
- en: 'We are seeking developers with demonstrable experience in: ASP.NET, C#, SQL
    Server, and AngularJS. We are a fast-paced, highly iterative team that has to
    adapt quickly as our factory grows. We need people who are comfortable tackling
    new problems, innovating solutions, and interacting with every facet of the company
    on a daily basis. Creative, motivated, able to take responsibility and support
    the applications you create. Help us get rockets out the door faster!'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在寻找具有以下经验的开发人员：ASP.NET，C＃，SQL Server和AngularJS。我们是一个快节奏，高度迭代的团队，必须随着我们的工厂的增长迅速适应。我们需要那些习惯于解决新问题，创新解决方案，并且每天与公司的各个方面进行互动的人。有创意，有动力，能够承担责任并支持您创建的应用程序。帮助我们更快地将火箭送出去！
- en: 'The first example of sentence splitting is in the `07/01_sentence_splitting1.py`
    file.  This uses the built-in sentence splitter in NLTK, which uses an internal
    boundary detection algorithm:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 识别和删除停用词
- en: 'First we import the sentence tokenizer from NLTK:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从StackOverflow工作列表创建词云
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then load the file:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后加载文件：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then the sentence is split using `sent_tokenize`, and the sentences are reported:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拼接n-gram
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想创建自己的分词器并自己训练它，那么可以使用`PunktSentenceTokenizer`类。`sent_tokenize`实际上是这个类的派生类，默认情况下实现了英语的句子分割。但是您可以从17种不同的语言模型中选择：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you want to create your own tokenizer and train it yourself, then you can
    use the `PunktSentenceTokenizer` class.  `sent_tokenize` is actually a derived
    class of this class that implements sentence splitting in English by default. 
    But there are 17 different language models you can pick from:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行句子分割
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can select the desired language by using the language parameter.  As an
    example, the following would split based on using German:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算单词的频率分布
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There's more...
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: To learn more about this algorithm, you can read the source paper available
    at [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5017&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5017&rep=rep1&type=pdf).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于这个算法的信息，可以阅读[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5017&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5017&rep=rep1&type=pdf)上提供的源论文。
- en: Performing tokenization
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行标记化
- en: Tokenization is the process of converting text into tokens.  These tokens can
    be paragraphs, sentences, and common individual words, and are commonly based
    at the word level.  NLTK comes with a number of tokenizers that will be demonstrated
    in this recipe.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化是将文本转换为标记的过程。这些标记可以是段落、句子和常见的单词，通常是基于单词级别的。NLTK提供了许多标记器，将在本教程中进行演示。
- en: How to do it
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'The code for this example is in the `07/02_tokenize.py` file.  This extends
    the sentence splitter to demonstrate five different tokenization techniques. 
    The first sentence in the file will be the only one tokenized so that we keep
    the amount of output to a reasonable amount:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的代码在`07/02_tokenize.py`文件中。它扩展了句子分割器，演示了五种不同的标记化技术。文件中的第一句将是唯一被标记化的句子，以便我们保持输出的数量在合理范围内：
- en: 'The first step is to simply use the built-in Python string `.split()` method. 
    This results in the following:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是简单地使用内置的Python字符串`.split()`方法。结果如下：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The sentence is split on space boundaries.  Note that punctuation such as ":"
    and "," are included in the resulting tokens.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 句子是在空格边界上分割的。注意，诸如“:”和“,”之类的标点符号包括在生成的标记中。
- en: 'The following demonstrates using the tokenizers built into NLTK.  First, we
    need to import them:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下演示了如何使用NLTK中内置的标记器。首先，我们需要导入它们：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following demonstrates using the `word_tokenizer`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下演示了如何使用`word_tokenizer`：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The result now has also split the punctuation into their own tokens.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 结果现在还将标点符号分割为它们自己的标记。
- en: 'The following uses the regex tokenizer, which allows you to apply any regex
    expression as a tokenizer.  It uses a  `''\w+''`  regex and has the following
    result:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下使用了正则表达式标记器，它允许您将任何正则表达式表达为标记器。它使用了一个`'\w+'`正则表达式，结果如下：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `wordpunct_tokenizer` has the following results:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`wordpunct_tokenizer`的结果如下：'
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And `blankline_tokenize` produces the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`blankline_tokenize`产生了以下结果：'
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As can be seen, this is not quite a simple problem as might be thought.  Depending
    upon the type of text being tokenized, you can come out with quite different results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，这并不是一个简单的问题。根据被标记化的文本类型的不同，你可能会得到完全不同的结果。
- en: Performing stemming
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行词干提取
- en: Stemming is the process of cutting down a token to its *stem*. Technically,
    it is the process or reducing inflected (and sometimes derived) words to their
    word stem - the base root form of the word.  As an example, the words *fishing*,
    *fished*, and *fisher* stem from the root word *fish*.  This helps to reduce the
    set of words being processed into a smaller base set that is more easily processed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是将标记减少到其*词干*的过程。从技术上讲，它是将屈折（有时是派生）的单词减少到它们的词干形式的过程-单词的基本根形式。例如，单词*fishing*、*fished*和*fisher*都来自根词*fish*。这有助于将被处理的单词集合减少到更容易处理的较小基本集合。
- en: The most common algorithm for stemming was created by Martin Porter, and NLTK
    provides an implementation of this algorithm in the PorterStemmer.  NLTK also
    provides an implementation of a Snowball stemmer, which was also created by Porter,
    and designed to handle languages other than English.  There is one more implementation
    provided by NLTK referred to as a Lancaster stemmer. The Lancaster stemmer is
    considered the most aggressive stemmer of the three.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的词干提取算法是由Martin Porter创建的，NLTK提供了PorterStemmer中这个算法的实现。NLTK还提供了Snowball词干提取器的实现，这也是由Porter创建的，旨在处理英语以外的其他语言。NLTK还提供了一个名为Lancaster词干提取器的实现。Lancaster词干提取器被认为是这三种中最激进的词干提取器。
- en: How to do it
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'NLTK provides an implementation of the Porter stemming algorithm in its PorterStemmer
    class. An instance of this can easily be created by the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK在其PorterStemmer类中提供了Porter词干提取算法的实现。可以通过以下代码轻松创建一个实例：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The script in the `07/03_stemming.py` file applies the Porter and Lancaster
    stemmers to the first sentence of our input file.  The primary section of the
    code performing the stemming is the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`07/03_stemming.py`文件中的脚本将Porter和Lancaster词干提取器应用于我们输入文件的第一句。执行词干提取的主要部分是以下内容：'
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And this results in the following output:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Looking at the results, it can be seen that the Lancaster stemmer is indeed
    more aggressive than the Porter stemmer, as several of the words have been cut
    down further with the latter stemmer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，Lancaster词干提取器确实比Porter词干提取器更激进，因为后者将几个单词进一步缩短了。
- en: Performing lemmatization
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行词形还原
- en: Lemmatization is a more methodical process of converting words to their base. 
    Where stemming generally just chops off the ends of words, lemmatization takes
    into account the morphological analysis of words, evaluating the context and part
    of speech to determine the inflected form, and makes a decision between different
    rules to determine the root.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原是一个更系统的过程，将单词转换为它们的基本形式。词干提取通常只是截断单词的末尾，而词形还原考虑了单词的形态分析，评估上下文和词性以确定屈折形式，并在不同规则之间做出决策以确定词根。
- en: How to do it
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'Lemmatization can be utilized in NTLK using the `WordNetLemmatizer`. This class
    uses the WordNet service, an online semantic database to make its decisions.  The
    code in the `07/04_lemmatization.py` file extends the previous stemming example
    to also calculate the lemmatization of each word.  The code of importance is the
    following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在NTLK中可以使用`WordNetLemmatizer`进行词形还原。这个类使用WordNet服务，一个在线语义数据库来做出决策。`07/04_lemmatization.py`文件中的代码扩展了之前的词干提取示例，还计算了每个单词的词形还原。重要的代码如下：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And it results in the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: There is a small amount of variance in the results using the lemmatization process. 
    The point of this is that, depending upon your data, one of these may be more
    suitable for your needs than the other, so give all of them a try if needed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词形还原过程的结果有一些差异。这表明，根据您的数据，其中一个可能比另一个更适合您的需求，因此如果需要，可以尝试所有这些方法。
- en: Determining and removing stop words
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定和去除停用词
- en: Stop words are common words that, in a natural language processing situation,
    do not provide much contextual meaning.  These words are often the most common
    words in a language.  These tend to, at least in English, be articles and pronouns,
    such as *I*, *me*, *the*, *is*, *which*, *who*, *at*, among others.  Processing
    of meaning in documents can often be facilitated by removal of these words before
    processing, and hence many tools support this ability.  NLTK is one of these,
    and comes with support for stop word removal for roughly 22 languages.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是在自然语言处理情境中不提供太多上下文含义的常见词。这些词通常是语言中最常见的词。这些词在英语中至少包括冠词和代词，如*I*，*me*，*the*，*is*，*which*，*who*，*at*等。在处理文档中的含义时，通常可以通过在处理之前去除这些词来方便处理，因此许多工具都支持这种能力。NLTK就是其中之一，并且支持大约22种语言的停用词去除。
- en: How to do it
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'Proceed with the recipe as follows (code is available in `07/06_freq_dist.py`):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行（代码在`07/06_freq_dist.py`中可用）：
- en: 'The following demonstrates stop word removal using NLTK.  First, start with
    importing stop words:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下演示了使用NLTK去除停用词。首先，从导入停用词开始：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then select the stop words for your desired language. The following selects
    English:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后选择所需语言的停用词。以下选择英语：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The English stop list has 153 words:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 英语停用词列表有153个单词：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'That''s not too many that we can''t show them all here:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这不是太多，我们可以在这里展示它们所有：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The removal of stop words from a list of words can be performed easily with
    a simple python statement.  This is demonstrated in the `07/05_stopwords.py` file. 
    The script starts with the required imports and readies the sentence we want to
    process:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从单词列表中去除停用词可以通过简单的Python语句轻松完成。这在`07/05_stopwords.py`文件中有演示。脚本从所需的导入开始，并准备好我们要处理的句子：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This yields the following output, which we are familiar with:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这产生了我们熟悉的以下输出：
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next we tokenize that sentence:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们对该句子进行标记化：
- en: '[PRE25]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'With the following output:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下输出：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then we can remove tokens that are in the stop list with the following statements:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以使用以下语句去除停用词列表中的标记：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Using the following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下输出：
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: There's more...
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Stop word removal has its purposes.  It is helpful, as we will see in a later
    recipe where we create a word cloud (stop words don't give much information in
    a word cloud), but can also be detrimental.  Many other NLP processes that deduce
    meaning based upon sentence structure can be greatly hampered by their removal.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 去除停用词有其目的。这是有帮助的，正如我们将在后面的一篇文章中看到的，我们将在那里创建一个词云（停用词在词云中不提供太多信息），但也可能是有害的。许多其他基于句子结构推断含义的自然语言处理过程可能会因为去除停用词而受到严重阻碍。
- en: Calculating the frequency distributions of words
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算单词的频率分布
- en: A frequency distribution counts the number of occurrences of distinct data values. 
    These are of value as we can use them to determine which words or phrases within
    a document are most common, and from that infer those that have greater or lesser
    value.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 频率分布计算不同数据值的出现次数。这些对我们很有价值，因为我们可以用它们来确定文档中最常见的单词或短语，从而推断出哪些具有更大或更小的价值。
- en: Frequency distributions can be calculated using several different techniques. 
    We will examine them using the facilities built into NLTK.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用几种不同的技术来计算频率分布。我们将使用内置在NLTK中的工具来进行检查。
- en: How to do it
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'NLTK provides a class, `ntlk.probabilities.FreqDist`, that allow us to very
    easily calculate the frequency distribution of values in a list.  Let''s examine
    using this class (code is in `07/freq_dist.py`):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK提供了一个类，`ntlk.probabilities.FreqDist`，可以让我们非常容易地计算列表中值的频率分布。让我们使用这个类来进行检查（代码在`07/freq_dist.py`中）：
- en: 'To create a frequency distribution using NLTK, start by importing the feature
    from NTLK (and also tokenizers and stop words):'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用NLTK创建频率分布，首先从NTLK中导入该功能（还有标记器和停用词）：
- en: '[PRE29]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then we can use the `FreqDist` function to create a frequency distribution
    given a list of words.  We will examine this by reading in the contents of `wotw.txt`
    (The War of the Worlds - courtesy of Gutenberg), tokenizing, and removing stop
    words:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以使用`FreqDist`函数根据单词列表创建频率分布。我们将通过读取`wotw.txt`（《世界大战》- 古腾堡出版社提供）的内容，对其进行标记化并去除停用词来进行检查：
- en: '[PRE30]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can then calculate the frequency distribution of the remaining words:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以计算剩余单词的频率分布：
- en: '[PRE31]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`freq_dist` is a dictionary of words to the counts of those words.  The following
    prints all of them (only a few lines of output shown as there are thousands of
    unique words):'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`freq_dist`是一个单词到单词计数的字典。以下打印了所有这些单词（只显示了几行输出，因为有成千上万个唯一单词）：'
- en: '[PRE32]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can use the frequency distribution to identify the most common words. The
    following reports the 10 most common words:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用频率分布来识别最常见的单词。以下报告了最常见的10个单词：
- en: '[PRE33]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: I was hoping that martians was in the top 5\. It's number 4.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望火星人在前5名中。它是第4名。
- en: There's more...
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can also use this to identify the least common words, by slicing the result
    of `.most_common()` with a negative value.  As an example, the following finds
    the 10 least common words:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用这个来识别最不常见的单词，通过使用`.most_common()`的负值进行切片。例如，以下内容找到了最不常见的10个单词：
- en: '[PRE34]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'There are quite a few words with only one occurrence, so this only gets a subset
    of those values.  The number of words with only one occurrence can be determined
    by the following (truncated due to there being 3,224 words):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有相当多的单词只出现一次，因此这只是这些值的一个子集。只出现一次的单词数量可以通过以下方式确定（由于有3,224个单词，已截断）：
- en: '[PRE35]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Identifying and removing rare words
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别和去除罕见单词
- en: We can remove words with low occurences by leveraging the ability to find words
    with low frequency counts, that fall outside of a certain deviation of the norm,
    or just from a list of words considered to be rare within the given domain.  But
    the technique we will use works the same for either.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过利用查找低频词的能力来删除低频词，这些词在某个领域中属于正常范围之外，或者只是从给定领域中被认为是罕见的单词列表中删除。但我们将使用的技术对两者都适用。
- en: How to do it
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'Rare words can be removed by building a list of those rare words and then removing
    them from the set of tokens being processed.  The list of rare words can be determined
    by using the frequency distribution provided by NTLK. Then you decide what threshold
    should be used as a rare word threshold:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 罕见单词可以通过构建一个罕见单词列表然后从正在处理的标记集中删除它们来移除。罕见单词列表可以通过使用 NTLK 提供的频率分布来确定。然后您决定应该使用什么阈值作为罕见单词的阈值：
- en: 'The script in the `07/07_rare_words.py` file extends that of the frequency
    distribution recipe to identify words with two or fewer occurrences and then removes
    those words from the tokens:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`07/07_rare_words.py` 文件中的脚本扩展了频率分布配方，以识别出现两次或更少的单词，然后从标记中删除这些单词：'
- en: '[PRE36]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output results in:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为：
- en: '[PRE37]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Through these two steps, removing stop words and then words with 2 or fewer
    occurrences, we have moved the total number of words from 6,613 to 2,252, which 
    is roughly one third.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这两个步骤，删除停用词，然后删除出现 2 次或更少的单词，我们将单词的总数从 6,613 个减少到 2,252 个，大约是原来的三分之一。
- en: Identifying and removing rare words
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别和删除罕见单词
- en: Removal of short words can also be useful in removing noise words from the content. 
    The following examines removing words of a certain length or shorter.  It also
    demonstrates the opposite by selecting the words not considered short (having
    a length of more than the specified short word length).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 删除短单词也可以用于去除内容中的噪声单词。以下内容检查了删除特定长度或更短单词。它还演示了通过选择不被视为短的单词（长度超过指定的短单词长度）来进行相反操作。
- en: How to do it
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'We can leverage the frequency distribution from NLTK to efficiently calculate
    the short words.  We could just scan all of the words in the source, but it is
    simply more efficient to scan the lengths of all of the keys in the resulting
    distribution as it will be a significantly smaller set of data:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用 NLTK 的频率分布有效地计算短单词。我们可以扫描源中的所有单词，但扫描结果分布中所有键的长度会更有效，因为它将是一个显著较小的数据集：
- en: 'The script in the `07/08_short_words.py` file exemplifies this process.  It
    starts by loading the content of `wotw.txt` and then calculating the word frequency
    distribution (after short word removal).  Then it identifies the words of thee
    characters or less:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`07/08_short_words.py` 文件中的脚本举例说明了这个过程。它首先加载了 `wotw.txt` 的内容，然后计算了单词频率分布（删除短单词后）。然后它识别了三个字符或更少的单词：'
- en: '[PRE38]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This results in:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致：
- en: '[PRE39]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The words not considered short can be found by changing the logic operator
    in the list comprehension:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过更改列表推导中的逻辑运算符可以找到不被视为短的单词：
- en: '[PRE40]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'And results in:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 结果为：
- en: '[PRE41]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Removing punctuation marks
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除标点符号
- en: Depending upon the tokenizer used, and the input to those tokenizers, it may
    be desired to remove punctuation from the resulting list of tokens.  The `regexp_tokenize`
    function with `'\w+'` as the expression removes punctuation well, but `word_tokenize`
    does not do it very well and will return many punctuation marks as their own tokens.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用的分词器和这些分词器的输入，可能希望从生成的标记列表中删除标点符号。`regexp_tokenize` 函数使用 `'\w+'` 作为表达式可以很好地去除标点符号，但
    `word_tokenize` 做得不太好，会将许多标点符号作为它们自己的标记返回。
- en: How to do it
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做
- en: 'Removing punctuation marks from our tokens is done similarly to the removal
    of other words within our tokens by using a list comprehension and only selecting
    those items that are not punctuation marks. The script  `07/09_remove_punctuation.py`
    file demonstrates this.  Let''s walk through the process:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通过列表推导和仅选择不是标点符号的项目，类似于从标记中删除其他单词的标点符号的删除。`07/09_remove_punctuation.py` 文件演示了这一点。让我们一起走过这个过程：
- en: 'We''ll start with the following, which will `word_tokenize` a string from a
    job listing:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从以下开始，它将从工作列表中`word_tokenize`一个字符串：
- en: '[PRE42]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now we can remove the punctuation with the following:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以用以下方法去除标点符号：
- en: '[PRE43]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This process can be encapsulated in a function.  The following is in the `07/punctuation.py`
    file, and will remove punctuation:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程可以封装在一个函数中。以下是在 `07/punctuation.py` 文件中，将删除标点符号：
- en: '[PRE44]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: There's more...
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Removal of punctuation and symbols can be a difficult problem.  While they
    don''t add value to many searches, punctuation can also be required to be kept
    as part of a token.  Take the case of searching a job site and trying to find
    C# programming positions, such as in the example in this recipe. The tokenization
    of C# gets split into two tokens:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 删除标点符号和符号可能是一个困难的问题。虽然它们对许多搜索没有价值，但标点符号也可能需要保留作为标记的一部分。以搜索工作网站并尝试找到 C# 编程职位为例，就像在这个配方中的示例一样。C#
    的标记化被分成了两个标记：
- en: '[PRE45]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We actually have two problems here.  By having C and # separated, we lost knowledge
    of C# being in the source content.  And then if we removed the # from the tokens,
    then we lose that information as we also cannot reconstruct C# from adjacent tokens.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '实际上我们有两个问题。将 C 和 # 分开后，我们失去了 C# 在源内容中的信息。然后，如果我们从标记中删除 #，那么我们也会失去这些信息，因为我们也无法从相邻的标记中重建
    C#。'
- en: Piecing together n-grams
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拼接 n-gram
- en: Much has been written about NLTK being used to identify n-grams within text. 
    An n-gram is a set of words, *n* words in length, that are common within a document/corpus
    (occurring 2 or more times).  A 2-gram is any two words commonly repeated, a 3-gram
    is a three word phrase, and so on.  We will not look into determining the n-grams
    in a document.  We will focus on reconstructing known n-grams from our token streams,
    as we will consider those n-grams to be more important to a search result than
    the 2 or 3 independent words found in any order.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 关于NLTK被用于识别文本中的n-gram已经写了很多。n-gram是文档/语料库中常见的一组单词，长度为*n*个单词（出现2次或更多）。2-gram是任何常见的两个单词，3-gram是一个三个单词的短语，依此类推。我们不会研究如何确定文档中的n-gram。我们将专注于从我们的标记流中重建已知的n-gram，因为我们认为这些n-gram对于搜索结果比任何顺序中找到的2个或3个独立单词更重要。
- en: In the domain of parsing job listings, important 2-grams can be things such
    as  **Computer Science**, **SQL Server**, **Data Science**, and **Big Data**. 
    Additionally, we could consider C# a 2-gram of `'C'` and `'#'`, and hence why
    we might not want to use the regex parser or `'#'` as punctuation when processing
    a job listing.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析工作列表的领域中，重要的2-gram可能是诸如**计算机科学**、**SQL Server**、**数据科学**和**大数据**之类的东西。此外，我们可以将C#视为`'C'`和`'#'`的2-gram，因此在处理工作列表时，我们可能不希望使用正则表达式解析器或`'#'`作为标点符号。
- en: We need to have a strategy to recognize these known combinations from out token
    stream.  Let's look at how to do this.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要有一个策略来识别我们的标记流中的这些已知组合。让我们看看如何做到这一点。
- en: How to do it
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'First, this example does not intend to make an exhaustive examination or one
    that is optimally performant.  Just one that is simple to understand and can be
    easily applied and extended to our example of parsing job listings:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这个例子并不打算进行详尽的检查或者最佳性能的检查。只是一个简单易懂的例子，可以轻松应用和扩展到我们解析工作列表的例子中：
- en: 'We will examine this process using the following sentences from a `StackOverflow`
    job listing for SpaceX:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用来自`StackOverflow` SpaceX的工作列表的以下句子来检查这个过程：
- en: '*We are seeking developers with demonstrable experience in: ASP.NET, C#, SQL
    Server, and AngularJS. We are a fast-paced, highly iterative team that has to
    adapt quickly as our factory grows.*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们正在寻找具有以下方面经验的开发人员：ASP.NET、C#、SQL Server和AngularJS。我们是一个快节奏、高度迭代的团队，随着我们的工厂的增长，我们必须快速适应。*'
- en: 'There are a number of high value 2-grams in these two sentences (and I think
    job listings are a great place to look for 2-grams).  Just looking at it, I can
    pick out the following as being important:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这两个句子中有许多高价值的2-gram（我认为工作列表是寻找2-gram的好地方）。仅仅看一下，我就可以挑出以下内容是重要的：
- en: ASP.NET
  id: totrans-177
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: ASP.NET
- en: C#
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C#
- en: SQL Server
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL Server
- en: fast-paced
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快节奏
- en: highly iterative
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度迭代
- en: adapt quickly
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速适应
- en: demonstrable experience
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可证明的经验
- en: 'Now, while these may not be 2-grams in the technical definition, when we parse
    them, they will all be separated into independent tokens.  This can be shown in
    the `07/10-ngrams.py` file, and in the following example:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，虽然这些在技术上的定义可能不是2-gram，但当我们解析它们时，它们都将被分开成独立的标记。这可以在`07/10-ngrams.py`文件中显示，并在以下示例中显示：
- en: '[PRE46]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This produces the following output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '[PRE47]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We want to remove punctuation from this set, but we would like to do it after
    constructing some 2-grams, specifically so that we can piece "C#" back into a
    single token.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望从这个集合中去掉标点，但我们希望在构建一些2-gram之后再去做，特别是这样我们可以将"C#"拼接成一个单个标记。
- en: 'The script in the `07/10-reconstruct-2grams.py` file demonstrates a function
    to facilitate this.  First, we need to describe the 2-grams that we want to reconstruct. 
    In this file, they are defined as the following:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`07/10-reconstruct-2grams.py`文件中的脚本演示了一个函数来实现这一点。首先，我们需要描述我们想要重建的2-gram。在这个文件中，它们被定义为以下内容：'
- en: '[PRE48]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`grams` is a dictionary, where the keys specify the `"Left"` side of the 2-gram. 
    Each key has a list of dictionaries, where each dictionary key can be the right
    side of the 2-gram, and the value is a string that will be placed between the
    left and right.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`grams`是一个字典，其中键指定了2-gram的“左”侧。每个键都有一个字典列表，其中每个字典键可以是2-gram的右侧，值是将放在左侧和右侧之间的字符串。'
- en: With this definition, we are able to see `"C"` and  `"#"` in our tokens be reconstructed
    to "C#".  `"SQL"` and `"Server"` will be `"SQL Server"`.  `"fast"` and  `"paced"`
    will result in `"faced-paced"`.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这个定义，我们能够看到我们的标记中的`"C"`和`"#"`被重构为"C#"。`"SQL"`和`"Server"`将成为`"SQL Server"`。`"fast"`和`"paced"`将导致`"faced-paced"`。
- en: 'So we just need a function to make this all work.  This function is defined
    in the `07/buildgrams.py` file:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们只需要一个函数来使这一切工作。这个函数在`07/buildgrams.py`文件中定义：
- en: '[PRE49]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This function, given a set of tokens and a dictionary in the format described
    earlier, will return a revised set of tokens with any matching 2-grams put into
    a single token.  The following demonstrates some simple cases of its use:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个函数，给定一组标记和一个以前描述的格式的字典，将返回一组修订后的标记，其中任何匹配的2-gram都被放入一个单个标记中。以下演示了它的一些简单用法：
- en: '[PRE50]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This results in the following output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE51]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now let''s apply it to our input.  The complete script for this is in the `07/10-reconstruct-2grams.py`
    file (and adds a few 2-grams):'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们将其应用到我们的输入中。这个完整的脚本在`07/10-reconstruct-2grams.py`文件中（并添加了一些2-gram）：
- en: '[PRE52]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The results are the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE53]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Perfect!
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！
- en: There's more...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We are providing a dictionary to the `build_2grams()` function that defines
    rules for identifying 2-grams.  In this example, we predefined these 2-grams. 
    It is possible to use NLTK to find 2-grams (and n-grams in general), but with
    this small sample of one job positing, it's likely that none will be found.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向`build_2grams()`函数提供一个字典，该字典定义了识别2-gram的规则。在这个例子中，我们预定义了这些2-gram。可以使用NLTK来查找2-gram（以及一般的n-gram），但是在这个小样本的一个工作职位中，可能找不到任何2-gram。
- en: Scraping a job listing from StackOverflow
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从StackOverflow抓取工作列表
- en: Now let's pull a bit of this together to scrape information from a StackOverflow
    job listing.  We are going to look at just one listing at this time so that we
    can learn the structure of these pages and pull information from them.  In later
    chapters, we will look at aggregating results from multiple listings.  Let's now
    just learn how to do this.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将一些内容整合起来，从StackOverflow的工作列表中获取信息。这次我们只看一个列表，这样我们就可以了解这些页面的结构并从中获取信息。在后面的章节中，我们将研究如何从多个列表中聚合结果。现在让我们学习如何做到这一点。
- en: Getting ready
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: StackOverflow actually makes it quite easy to scrape data from their pages. 
    We are going to use content from a posting at[ https://stackoverflow.com/jobs/122517/spacex-enterprise-software-engineer-full-stack-spacex?so=p&sec=True&pg=1&offset=22&cl=Amazon%3b+](https://stackoverflow.com/jobs/122517/spacex-enterprise-software-engineer-full-stack-spacex?so=p&sec=True&pg=1&offset=22&cl=Amazon%3b+). 
    This likely will not be available at the time you read it, so I've included the
    HTML of this page in the `07/spacex-job-listing.html` file, which we will use
    for the examples in this chapter.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，StackOverflow使得从他们的页面中抓取数据变得非常容易。我们将使用来自[https://stackoverflow.com/jobs/122517/spacex-enterprise-software-engineer-full-stack-spacex?so=p&sec=True&pg=1&offset=22&cl=Amazon%3b+](https://stackoverflow.com/jobs/122517/spacex-enterprise-software-engineer-full-stack-spacex?so=p&sec=True&pg=1&offset=22&cl=Amazon%3b+)的内容。在您阅读时，这可能不再可用，因此我已经在`07/spacex-job-listing.html`文件中包含了此页面的HTML，我们将在本章的示例中使用。
- en: 'StackOverflow job listings pages are very structured.  It''s probably because
    they''re created by programmers and for programmers.  The page (at the time of
    writing) looks like the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: StackOverflow的工作列表页面非常有结构。这可能是因为它们是由程序员创建的，也是为程序员创建的。页面（在撰写本文时）看起来像下面这样：
- en: '![](assets/dfb46fdc-eb94-4b80-93a8-885f9ce8a756.png)A StackOverflow job listing'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/dfb46fdc-eb94-4b80-93a8-885f9ce8a756.png)StackOverflow工作列表'
- en: 'All of this information is codified within the HTML of the page.  You can see
    for yourself by analyzing the page content.  But what StackOverflow does that
    is so great is that it puts much of its page data in an embedded JSON object. 
    This is placed within a `<script type="application/ld+json">` HTML tag, so it''s
    really easy to find.  The following shows a truncated section of this tag (the
    description is truncated, but all the tags are shown):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些信息都被编码在页面的HTML中。您可以通过分析页面内容自行查看。但StackOverflow之所以如此出色的原因在于它将其大部分页面数据放在一个嵌入的JSON对象中。这是放置在`<script
    type="application/ld+json>`HTML标签中的，所以很容易找到。下面显示了此标签的截断部分（描述被截断，但所有标记都显示出来）：
- en: '![](assets/5fecad0b-3acf-4cb6-90d9-4da452fd469b.png)The JSON embedded in a
    job listing'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/5fecad0b-3acf-4cb6-90d9-4da452fd469b.png)工作列表中嵌入的JSON'
- en: This makes it very easy to get the content, as we can simply retrieve the page,
    find this tag, and then convert this JSON into a Python object with the `json`
    library.  In addition to the actual job description, is also included  much of
    the "metadata" of the job posting, such as skills, industries, benefits, and location
    information.  We don't need to search the HTML for the information - just find
    this tag and load the JSON.  Note that if we want to find items, such as Job Responsibilities**,**
    we still need to parse the description.  Also note that the description contains
    full HTML, so when parsing that, we would need to still deal with HTML tags.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得获取内容非常容易，因为我们可以简单地检索页面，找到这个标签，然后使用`json`库将此JSON转换为Python对象。除了实际的工作描述，还包括了工作发布的大部分“元数据”，如技能、行业、福利和位置信息。我们不需要在HTML中搜索信息-只需找到这个标签并加载JSON。请注意，如果我们想要查找项目，比如工作职责**，我们仍然需要解析描述。还要注意，描述包含完整的HTML，因此在解析时，我们仍需要处理HTML标记。
- en: How to do it
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: Let's go and get the job description from this page.  We will simply retrieve
    the contents in this recipe.  We will clean it up in the next recipe.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们去获取这个页面的工作描述。我们将在下一个示例中对其进行清理。
- en: 'The full code for this example is in the `07/12_scrape_job_stackoverflow.py`
    file.  Let''s walk through it:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的完整代码在`07/12_scrape_job_stackoverflow.py`文件中。让我们来看一下：
- en: 'First we read the file:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先我们读取文件：
- en: '[PRE54]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then we load the content into a `BeautifulSoup` object, and retrieve the `<script
    type="application/ld+json">` tag:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将内容加载到`BeautifulSoup`对象中，并检索`<script type="application/ld+json">`标签：
- en: '[PRE55]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now that we have that tag, we can load its contents into a Python dictionary
    using the `json` library:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了这个标签，我们可以使用`json`库将其内容加载到Python字典中：
- en: '[PRE56]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output of this looks like the following (this is truncated for brevity):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出看起来像下面这样（为了简洁起见，这是截断的）：
- en: '[PRE57]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This is great because we can now do some simple tasks with this without involving
    HTML parsing.  As an example, we can retrieve the skills required for the job
    with just the following code:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这很棒，因为现在我们可以做一些简单的任务，而不涉及HTML解析。例如，我们可以仅使用以下代码检索工作所需的技能：
- en: '[PRE58]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'It produces the following output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下输出：
- en: '[PRE59]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: There's more...
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The description is still stored in HTML within the description property of this
    JSON object.  We will examine the parsing of that data in the next recipe.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 描述仍然存储在此JSON对象的描述属性中的HTML中。我们将在下一个示例中检查该数据的解析。
- en: Reading and cleaning the description in the job listing
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阅读和清理工作列表中的描述
- en: The description of the job listing is still in HTML.  We will want to extract
    the valuable content out of this data, so we will need to parse this HTML and
    perform tokenization, stop word removal, common word removal, do some tech 2-gram
    processing, and in general all of those different processes.  Let's look at doing
    these.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 工作列表的描述仍然是HTML。我们将要从这些数据中提取有价值的内容，因此我们需要解析这个HTML并执行标记化、停用词去除、常用词去除、进行一些技术2-gram处理，以及一般的所有这些不同的过程。让我们来做这些。
- en: Getting ready
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: I have collapsed the code for determining tech-based 2-grams into the `07/tech2grams.py`
    file.  We will use the `tech_2grams` function within the file.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经将确定基于技术的2-gram的代码折叠到`07/tech2grams.py`文件中。我们将在文件中使用`tech_2grams`函数。
- en: How to do it...
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The code for this example is in the `07/13_clean_jd.py` file.  It continues
    on where the `07/12_scrape_job_stackoverflow.py` file ends:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的代码在`07/13_clean_jd.py`文件中。它延续了`07/12_scrape_job_stackoverflow.py`文件的内容：
- en: 'We start by creating a `BeautifulSoup` object from the description key of the
    description we loaded.  We will also print this to see what it looks like:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从我们加载的描述的描述键创建一个`BeautifulSoup`对象。我们也会打印出来看看它是什么样子的：
- en: '[PRE60]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We want to go through this and remove all of the HTML and only be left with
    the text of the description.  That will be what we then tokenize.  Fortunately,
    throwing out all the HTML tags is easy with `BeautifulSoup`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要浏览一遍，去掉所有的HTML，只留下描述的文本。然后我们将对其进行标记。幸运的是，使用`BeautifulSoup`很容易就能去掉所有的HTML标签：
- en: '[PRE61]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Just super!  We now have this, and it is already broken down into what can be
    considered sentences!
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们现在已经有了这个，它已经被分解成可以被视为句子的部分！
- en: 'Let''s join these all together, word tokenize them, get rid of stop words,
    and also apply common tech job 2-grams:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们把它们全部连接在一起，对它们进行词标记，去掉停用词，并应用常见的技术工作2-gram：
- en: '[PRE62]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'And this has the following output:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就会得到以下输出：
- en: '[PRE63]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: I think that's a very nice and refined set of keywords pulled out of that job
    listing.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是从工作清单中提取出来的一组非常好的和精细的关键词。
